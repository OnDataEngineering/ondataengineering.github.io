<?xml version="1.0" encoding="utf-8"?> <feed xmlns="http://www.w3.org/2005/Atom"> <id>http://ondataengineering.net/</id><title>OnDataEngineering</title><updated>2017-02-07T07:53:42+00:00</updated> <subtitle>A collaborative site for independent, critical and technical thinking on the use cases, architectural patterns and technologies relating to the transformation and preparation of data for exploitation.</subtitle> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author><link href="http://ondataengineering.net/atom.xml" rel="self" type="application/rss+xml" /><link href="http://ondataengineering.net/" rel="alternate" type="text/html" /> <generator uri="http://jekyllrb.com" version="3.2.1">Jekyll</generator> <entry> <id>http://ondataengineering.net/technologies/apache-crunch/</id><title>Apache Crunch</title><link href="http://ondataengineering.net/technologies/apache-crunch/" rel="alternate" type="text/html" title="Apache Crunch" /> <updated>2017-02-07T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-crunch/"> &lt;p&gt;An abstraction layer over MapReduce (and now Spark) that provides a high level Java API for creating data transformation pipelines, originally designed to make working with MapReduce easier based on the Google FlumeJava paper. Also includes connectors for HBase, Hive and Kafka, Java 8 lambda support, an experimental Scala wrapper for the API (Scrunch), and support for in memory pipelines and helper classes to support testing. Open sourced by Cloudera in October 2011, donated to the Apache Foundation in May 2012, before graduating in February 2013. Support for Spark was added as part of v0.10 in June 2014. Still being maintained, and appears to have had been adopted at a number of large companies, but with limited new development.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Crunch&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;February 2017 - 0.14&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://crunch.apache.org/&quot;&gt;https://crunch.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://crunch.apache.org/user-guide.html&quot;&gt;https://crunch.apache.org/user-guide.html&lt;/a&gt; - user guide&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2011/10/introducing-crunch/&quot;&gt;http://blog.cloudera.com/blog/2011/10/introducing-crunch/&lt;/a&gt; - initial intro blog post&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://crunch.apache.org/user-guide.html#motivation&quot;&gt;https://crunch.apache.org/user-guide.html#motivation&lt;/a&gt; - the motivation behind Crunch&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://crunch.apache.org/scrunch.html&quot;&gt;http://crunch.apache.org/scrunch.html&lt;/a&gt; - Scrunch page&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://crunch.apache.org/download.html&quot;&gt;https://crunch.apache.org/download.html&lt;/a&gt; - new releases only appear to be announced on download page&lt;/li&gt; &lt;/ul&gt; </content><published>2017-02-07T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-apex/</id><title>Apache Apex</title><link href="http://ondataengineering.net/technologies/apache-apex/" rel="alternate" type="text/html" title="Apache Apex" /> <updated>2017-02-06T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-apex/"> &lt;p&gt;Data transformation engine based on Directed Acyclic Graph (DAG) flows configured through a Java API or via JSON, with a stated focus on performance, code re-use, testability and ease of operations. Runs over YARN and HDFS with native support for both micro-batch streaming and batch uses cases, and includes a range of standard operators and connectors (called Apex Malhar). An Apache project, graduating in April 2016, having been originally donated in August 2015 by DataTorrent from their DataTorrent RTS product which launched in June 2014. Java based, with development lead by DataTorrent who distribute it as DataTorrent RTS in two editions - a Community Edition (which also includes a basic management GUI and a tool for configuring Apex for data ingestion), and an Enterprise Edition (which further includes a graphical transformation editor, a self service dashboard, security integration and commercial support, and is also available as a cloud offering).&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Apex, DataTorrent RTS&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;February 2017 - v3.5 (Apex Core), v3.6 (Apex Malhar), v3.7 (DataTorrent RTS)&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://apex.apache.org/&quot;&gt;https://apex.apache.org/&lt;/a&gt; - Apex homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://apex.apache.org/docs.html&quot;&gt;https://apex.apache.org/docs.html&lt;/a&gt; -Apex documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.datatorrent.com/products-services/datatorrent-rts/&quot;&gt;https://www.datatorrent.com/products-services/datatorrent-rts/&lt;/a&gt; - DataTorrent RTS home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://docs.datatorrent.com/&quot;&gt;http://docs.datatorrent.com/&lt;/a&gt; - DataTorrent documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.datatorrent.com/blog/introducing-apache-apex-incubating/&quot;&gt;https://www.datatorrent.com/blog/introducing-apache-apex-incubating/&lt;/a&gt; - introductory blog post&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.datatorrent.com/products-services/edition-comparison/&quot;&gt;https://www.datatorrent.com/products-services/edition-comparison/&lt;/a&gt; - DataTorrent editions comparison&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://apex.apache.org/&quot;&gt;https://apex.apache.org/&lt;/a&gt; - release announcements&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.datatorrent.com/blog/&quot;&gt;https://www.datatorrent.com/blog/&lt;/a&gt; - DataTorrent blog&lt;/li&gt; &lt;/ul&gt; </content><published>2017-02-06T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/blog/2017/02/03/the-week-that-was/</id><title>The Week That Was - 03/02/2017</title><link href="http://ondataengineering.net/blog/2017/02/03/the-week-that-was/" rel="alternate" type="text/html" title="The Week That Was - 03/02/2017" /> <updated>2017-02-03T16:50:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/blog/2017/02/03/the-week-that-was/"> &lt;p&gt;And another week goes by - let’s have a look back over the technologies we’ve looked at this week.&lt;/p&gt; &lt;p&gt;We kicked off the week by looking at &lt;a href=&quot;/technologies/kite/&quot;&gt;Kite&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-datafu/&quot;&gt;Apache DataFu&lt;/a&gt;, a couple of utilities designed to make working in the Hadoop ecosystem a little easier, before moving on to &lt;a href=&quot;/technologies/apache-phoenix&quot;&gt;Apache Phoenix&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-tajo&quot;&gt;Apache Tajo&lt;/a&gt;. a couple of query engines (also over the Hadoop ecosystem), and then finishing with &lt;a href=&quot;/technologies/apache-mahout&quot;&gt;Apache Mahout&lt;/a&gt;. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/technologies/kite/&quot;&gt;Kite&lt;/a&gt; is a solid concept and feels like it fills a bit of a void - how can I easily create Hive tables and load data in from a variety of sources without writing code - however it doesn’t ever seem to have gained much traction, and it looks like even Cloudera aren’t developing and maintaining it any more. It’s also the first non Apache technology we’re looked at on this site! I’m definitely planning to revisit Kite and some of it’s concepts when we talk about Data Lakes in the future however.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/technologies/apache-datafu/&quot;&gt;Apache DataFu&lt;/a&gt; is actually two things - a set of user defined functions for Pig, and a MapReduce framework for calculating aggregations over regular ingestions of data into Hadoop based on only processing the new data called Hourglass. The first of these sounds well worth a look if you any sort of significant work in Pig. The second I’m less sure about - you’ll have to be using MapReduce, and you’d have to want to follow their pattern, however as a concept or an exemplar it could well be worth a look.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/technologies/apache-phoenix&quot;&gt;Apache Phoenix&lt;/a&gt; surprised me - it appears to be an extremely active project, with excellent documentation and a great range of companies that are using it in production, and in providing a SQL query later over HBase, fills an interesting niche within the Hadoop ecosystem. Hive and Impala are great if you have batch updates (and ideally just appends), but they don’t support low latency random updates and queries (along with the full table scans) that HBase (and therefore Phoenix) does. It’s going to be interesting to see how this stacks up against Kudu as this matures and gains adoption, and how the major Hadoop distributions look to support this use case.&lt;/p&gt; &lt;p&gt;I’m not quite sure what to make of &lt;a href=&quot;/technologies/apache-tajo&quot;&gt;Apache Tajo&lt;/a&gt;. It seems like a great technology, with significant commercial backing from Gruter, however I’m not sure it’s getting much traction, and I’m not sure what niche it’s trying to target - it feels uncomfortably close to Hive and Impala. Maybe prior to Hive on Tez/Spark Tajo had some differentiation in terms of low latency queries.&lt;/p&gt; &lt;p&gt;And last (but not least) &lt;a href=&quot;/technologies/apache-mahout&quot;&gt;Apache Mahout&lt;/a&gt;. Mahout has been a staple of most Hadoop distributions for a while (probably as a result of it being one of the first machine learning technologies in the Hadoop space), but what’s interesting is that it completely reinvented itself in April 2015 to become a general purpose distributed linear algebra engine that can run over Spark (with H2O and Flink support coming), and (if running on Spark) is fully compatible with other Spark libraries such as MLlib.&lt;/p&gt; &lt;p&gt;That’s it for this week - have a great weekend.&lt;/p&gt; </content> <category term="Site" /><category term="Technologies" /> <category term="Kite" /><category term="Kite Data" /><category term="Kite Maven Plugin" /><category term="DataFu" /><category term="DataFu Pig" /><category term="DataFu Hourglass" /><category term="Phoenix" /><category term="Tajo" /><category term="Mahout" /><published>2017-02-03T16:50:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-mahout/</id><title>Apache Mahout</title><link href="http://ondataengineering.net/technologies/apache-mahout/" rel="alternate" type="text/html" title="Apache Mahout" /> <updated>2017-02-03T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-mahout/"> &lt;p&gt;Machine learning technology comprising of a Scala based linear algebra engine (codenamed Samsara) with an R-like DSL/API that runs over Spark (with experimental support for H2O and Flink), an optimiser, a wide variety of pre-made algorithms, and a Scala REPL (based on Spark Shell) for interactive execution. Can be embedded and integrated within larger applications, for example MLlib when running over Spark. Also includes some original, now deprecated, algorithms implemented over MapReduce. Created in January 2008 as a Lucene sub-project, becoming a top level Apache project in April 2010. The original MapReduce algorithms were deprecated and Samsara introduced as part of v0.10 in April 2015. Supported by most major Hadoop distributions, and still under active development.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Mahout&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;February 2017 - 0.12&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://mahout.apache.org/&quot;&gt;https://mahout.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.weatheringthroughtechdays.com/2015/04/mahout-010x-first-mahout-release-as.html&quot;&gt;http://www.weatheringthroughtechdays.com/2015/04/mahout-010x-first-mahout-release-as.html&lt;/a&gt; - introduction to new architecture introduced in v0.10&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://mahout.apache.org/general/release-notes.html&quot;&gt;https://mahout.apache.org/general/release-notes.html&lt;/a&gt; - new releases&lt;/li&gt; &lt;/ul&gt; </content><published>2017-02-03T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-tajo/</id><title>Apache Tajo</title><link href="http://ondataengineering.net/technologies/apache-tajo/" rel="alternate" type="text/html" title="Apache Tajo" /> <updated>2017-02-02T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-tajo/"> &lt;p&gt;Distributed analytical database engine. Supports HDFS, Amazon S3, Google Cloud Storage, OpenStack Swift and local storage, and querying over Postgres, HBase and Hive tables. Provides a standard SQL interface, JDBC driver, and supports partitioning, compression and indexing (currently experimental). An Apache project, donated by Gruter in March 2013, and graduated in April 2014. Java based, with development lead by Gruter who also supply commercial support, a Tajo managed service, a data analytics hub (Qrytica) built on Tajo, and a Tajo Data Warehouse appliance.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Tajo&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;February 2017 - v0.11&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://tajo.apache.org/&quot;&gt;http://tajo.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://tajo.apache.org/docs/current/&quot;&gt;http://tajo.apache.org/docs/current/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://gruter.com/technology/tajo/&quot;&gt;http://gruter.com/technology/tajo/&lt;/a&gt; - Gruter product page&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://tajo.apache.org/&quot;&gt;http://tajo.apache.org/&lt;/a&gt; - news and releases&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.gruter.com/blog/&quot;&gt;http://www.gruter.com/blog/&lt;/a&gt; - Gruter blog&lt;/li&gt; &lt;/ul&gt; </content><published>2017-02-02T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-phoenix/</id><title>Apache Phoenix</title><link href="http://ondataengineering.net/technologies/apache-phoenix/" rel="alternate" type="text/html" title="Apache Phoenix" /> <updated>2017-02-01T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-phoenix/"> &lt;p&gt;A SQL query engine over Apache HBase tables that supports a subset of SQL 92 (including joins), and comes with a JDBC driver. Supports a range of features including ACID transactions (via Apache Tephra), user defined functions, secondary indexes, atomic upserts, views, multi tenancy tables (where each user or tenant can only see their data) and dynamic columns (which are only specified at query time). Supports a range of SQL DDL commands, creating and modifying underlying HBase tables as required, or can run over existing HBase tables in a read only mode. Comes with connectors to allow Spark, Hive, Pig, Flume and MapReduce to read and write Phoenix tables, and a number of utilities, including a bulk loader and a command line SQL tool. Open sourced by SalesForce in January 2013 at v1.0, donated to the Apache foundation in December 2013, before graduating in May 2014. Commercial support available through Hortonworks as part of HDP, with Cloudera making it available via Cloudera Labs without support. Active project with a range of contributors, including many from SalesForce and Hortonworks.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Phoenix&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;February 2017 - v4.9&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://phoenix.apache.org/&quot;&gt;http://phoenix.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/phoenix/entry/announcing_phoenix_4_9_released&quot;&gt;https://blogs.apache.org/phoenix/entry/announcing_phoenix_4_9_released&lt;/a&gt; - 4.9 announcement (homepage doesn’t appear to have been updated)&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/phoenix/entry/apache_phoenix_released_next_major&quot;&gt;https://blogs.apache.org/phoenix/entry/apache_phoenix_released_next_major&lt;/a&gt; - 3.0/4.0 announcement&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blog.cloudera.com/blog/2015/05/apache-phoenix-joins-cloudera-labs&quot;&gt;https://blog.cloudera.com/blog/2015/05/apache-phoenix-joins-cloudera-labs&lt;/a&gt; - Cloudera labs announcement&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://phoenix-hbase.blogspot.co.uk/&quot;&gt;http://phoenix-hbase.blogspot.co.uk/&lt;/a&gt; - original blog, now superseded by the Apache blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/phoenix/&quot;&gt;https://blogs.apache.org/phoenix/&lt;/a&gt; - project blog including release announcements&lt;/li&gt; &lt;/ul&gt; </content><published>2017-02-01T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-datafu/datafu-pig/</id><title>DataFu Pig</title><link href="http://ondataengineering.net/technologies/apache-datafu/datafu-pig/" rel="alternate" type="text/html" title="DataFu Pig" /> <updated>2017-01-31T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-datafu/datafu-pig/"> &lt;p&gt;A set of user defined functions for Apache Pig, including support for statistical calculations, bag and set operations, sessionisation of streams of data, cardinality estimation, sampling, hashing, PageRank and others.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-datafu/&quot;&gt;Apache DataFu&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Uses&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-pig/&quot;&gt;Apache Pig&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://datafu.incubator.apache.org/docs/datafu/getting-started.html&quot;&gt;https://datafu.incubator.apache.org/docs/datafu/getting-started.html&lt;/a&gt; - homepage (see linked blog posts for further information and examples)&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://datafu.incubator.apache.org/docs/datafu/guide.html&quot;&gt;https://datafu.incubator.apache.org/docs/datafu/guide.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;/ul&gt; </content><published>2017-01-31T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-datafu/datafu-hourglass/</id><title>DataFu Hourglass</title><link href="http://ondataengineering.net/technologies/apache-datafu/datafu-hourglass/" rel="alternate" type="text/html" title="DataFu Hourglass" /> <updated>2017-01-31T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-datafu/datafu-hourglass/"> &lt;p&gt;A framework over MapReduce that supports the efficient generation of statistics of dated data by incrementally updating the previous days output. Supports both fixed length and fixed start point windows, and the generation of statistics by input partition or as a total over all input data.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-datafu/&quot;&gt;Apache DataFu&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://datafu.incubator.apache.org/docs/hourglass/getting-started.html&quot;&gt;https://datafu.incubator.apache.org/docs/hourglass/getting-started.html&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://datafu.incubator.apache.org/blog/2013/10/03/datafus-hourglass-incremental-data-processing-in-hadoop.html&quot;&gt;https://datafu.incubator.apache.org/blog/2013/10/03/datafus-hourglass-incremental-data-processing-in-hadoop.html&lt;/a&gt; - introduction (contains examples and information not available in the documentation)&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://datafu.incubator.apache.org/docs/hourglass/1.3.1/&quot;&gt;http://datafu.incubator.apache.org/docs/hourglass/1.3.1/&lt;/a&gt; - Javadoc (note link from &lt;a href=&quot;https://datafu.incubator.apache.org/docs/hourglass/javadoc.html&quot;&gt;here&lt;/a&gt; is wrong)&lt;/li&gt; &lt;/ul&gt; </content><published>2017-01-31T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-datafu/</id><title>Apache DataFu</title><link href="http://ondataengineering.net/technologies/apache-datafu/" rel="alternate" type="text/html" title="Apache DataFu" /> <updated>2017-01-31T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-datafu/"> &lt;p&gt;A set of libraries for working with data in Hadoop. Consists of two sub-projects - DataFu Pig (a set of Pig User Defined Functions) and DataFu Hourglass (a framework for incremental processing using MapReduce). Originally created at LinkedIn, with the Pig UDFs being open sourced in January 2012 as DataFu, with a v1.0 release in September 2013. Split into sub-projects in October 2013 when LinkedIn open sourced DataFu Hourglass and added it to the project. Donated to the Apache Foundation in January 2014, however is still incubating and has not yet graduated. Last release was v1.3 in November 2015 (albeit with a very minor v1.3.1 release in August 2016), with little development activity since this time.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;DataFu&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v1.3&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Sub-projects&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; Apache DataFu&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-datafu/datafu-hourglass/&quot;&gt;DataFu Hourglass&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A framework over MapReduce that supports the efficient generation of statistics of dated data by incrementally updating the previous days output. Supports both fixed length and fixed start point windows, and the generation of statistics by input partition or as a total over all input data.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache DataFu&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-datafu/datafu-pig/&quot;&gt;DataFu Pig&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A set of user defined functions for Apache Pig, including support for statistical calculations, bag and set operations, sessionisation of streams of data, cardinality estimation, sampling, hashing, PageRank and others.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://datafu.incubator.apache.org&quot;&gt;https://datafu.incubator.apache.org&lt;/a&gt; - homepage&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://datafu.incubator.apache.org/blog/&quot;&gt;https://datafu.incubator.apache.org/blog/&lt;/a&gt; - blog&lt;/li&gt; &lt;/ul&gt; </content><published>2017-01-31T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/kite/morphlines/</id><title>Morphlines</title><link href="http://ondataengineering.net/technologies/kite/morphlines/" rel="alternate" type="text/html" title="Morphlines" /> <updated>2017-01-30T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/kite/morphlines/"> &lt;p&gt;A configuration driven in-memory transformation pipeline that can be embedded into any Java code base, with specific support for Flume, MapReduce, HBase, Spark and Solr. Supports multiple different file types including CSV, Avro, JSON, Parquet, RCFile, SequenceFile, ProtoBuf and XML plus gzip, bzip2, tar zip and jar files. Also supports a number of transformation steps out of the box, including integration with Apache Tika for reading common file formats.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/kite/&quot;&gt;Kite&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://kitesdk.org/docs/current/morphlines/&quot;&gt;http://kitesdk.org/docs/current/morphlines/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://kitesdk.org/docs/current/morphlines/morphlines-reference-guide.html&quot;&gt;http://kitesdk.org/docs/current/morphlines/morphlines-reference-guide.html&lt;/a&gt; - reference guide&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2013/07/morphlines-the-easy-way-to-build-and-integrate-etl-apps-for-apache-hadoop/&quot;&gt;http://blog.cloudera.com/blog/2013/07/morphlines-the-easy-way-to-build-and-integrate-etl-apps-for-apache-hadoop/&lt;/a&gt; - intro&lt;/li&gt; &lt;/ul&gt; </content><published>2017-01-30T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/kite/kite-maven-plugin/</id><title>Kite Maven Plugin</title><link href="http://ondataengineering.net/technologies/kite/kite-maven-plugin/" rel="alternate" type="text/html" title="Kite Maven Plugin" /> <updated>2017-01-30T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/kite/kite-maven-plugin/"> &lt;p&gt;A Maven plugin that supports the packaging, deployment and execution of applications onto Hadoop.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/kite/&quot;&gt;Kite&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://kitesdk.org/docs/current/maven/plugin-info.html&quot;&gt;http://kitesdk.org/docs/current/maven/plugin-info.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;/ul&gt; </content><published>2017-01-30T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/kite/kite-data/</id><title>Kite Data</title><link href="http://ondataengineering.net/technologies/kite/kite-data/" rel="alternate" type="text/html" title="Kite Data" /> <updated>2017-01-30T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/kite/kite-data/"> &lt;p&gt;Library that provides a logical dataset and record abstraction over HDFS, S3, local filesystems and HBase, including support for partitioning and views (which allow datasets to be filtered and supports automatic partition pruning). Provides a command line interface and Maven plugin for managing and viewing datasets. Supports Crunch, Flume, Spark and MapReduce, and can integrate with a Hive Metastore to make datasets available through Hive and Impala. Stores data using Avro (utilising Avro schema evolution / resolution) or Parquet.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/kite/&quot;&gt;Kite&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://kitesdk.org/docs/current/Kite-SDK-Guide.html&quot;&gt;http://kitesdk.org/docs/current/Kite-SDK-Guide.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;/ul&gt; </content><published>2017-01-30T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/kite/</id><title>Kite</title><link href="http://ondataengineering.net/technologies/kite/" rel="alternate" type="text/html" title="Kite" /> <updated>2017-01-30T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/kite/"> &lt;p&gt;A set of libraries, tools, examples, and documentation focused on making it easier to build systems on top of the Hadoop ecosystem. Consists of three sub-projects - Kite Data (a logical dataset abstraction over Hadoop), Morphlines (embeddable configuration driven transformation pipelines) and Kite Maven Plugin (a Maven plugin for deploying Hadoop applications). Java based, Open Source under the Apache 2.0 licence and hosted on GitHub. First released in May 2013 by Cloudera as the Cloudera Development Kit (CDK), renamed to Kite in December 2013, and reached a v1.0 release in February 2015 with a number of external contributors. Last release was v1.1 in June 2015, with very little development activity since this time.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Cloudera Development Kit, CDK&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v1.1&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Sub-projects&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; Kite&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/kite/kite-data/&quot;&gt;Kite Data&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Library that provides a logical dataset and record abstraction over HDFS, S3, local filesystems and HBase, including support for partitioning and views (which allow datasets to be filtered and supports automatic partition pruning). Provides a command line interface and Maven plugin for managing and viewing datasets. Supports Crunch, Flume, Spark and MapReduce, and can integrate with a Hive Metastore to make datasets available through Hive and Impala. Stores data using Avro (utilising Avro schema evolution / resolution) or Parquet.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Kite&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/kite/kite-maven-plugin/&quot;&gt;Kite Maven Plugin&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A Maven plugin that supports the packaging, deployment and execution of applications onto Hadoop.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Kite&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/kite/morphlines/&quot;&gt;Morphlines&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A configuration driven in-memory transformation pipeline that can be embedded into any Java code base, with specific support for Flume, MapReduce, HBase, Spark and Solr. Supports multiple different file types including CSV, Avro, JSON, Parquet, RCFile, SequenceFile, ProtoBuf and XML plus gzip, bzip2, tar zip and jar files. Also supports a number of transformation steps out of the box, including integration with Apache Tika for reading common file formats.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://kitesdk.org/docs/current/&quot;&gt;http://kitesdk.org/docs/current/&lt;/a&gt; - homepage and documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/kite-sdk/kite&quot;&gt;https://github.com/kite-sdk/kite&lt;/a&gt; - source code&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Any updates to Kite are likely to be published on the Cloudera blog&lt;/li&gt; &lt;/ul&gt; </content><published>2017-01-30T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/alluxio/</id><title>Alluxio</title><link href="http://ondataengineering.net/technologies/alluxio/" rel="alternate" type="text/html" title="Alluxio" /> <updated>2017-01-27T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/alluxio/"> &lt;p&gt;A distributed virtual storage layer, supporting key-value and filesystem interfaces (including HDFS compatibility and a FUSE driver) with support for a range of computation and storage frameworks (including Spark, MapReduce, HBase and Hive) over multiple storage layers (including in-memory, local, network, cloud and cluster file systems) with the ability to create unified and tiered storage, for example to create an in memory filesystem backed by disk to accelerate analytics jobs. Supports a POSIX like access control model, and a CLI and web interface for browsing the storage layer. Java based, Open Source under the Apache 2.0 licence, hosted on GitHub, with development led by Alluxio (with significant external contributions), although they don't appear to yet provide commercial support (but do provide training). Started in December 2012, open sourced in April 2013, with a v1.0 release in February 2016. Formally known as Tachyon.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Alluxio, Tachyon&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v1.4&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://alluxio.org/&quot;&gt;http://alluxio.org/&lt;/a&gt; - product home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/Alluxio/alluxio&quot;&gt;https://github.com/Alluxio/alluxio&lt;/a&gt; - source code&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.alluxio.org/docs/master/en/&quot;&gt;http://www.alluxio.org/docs/master/en/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.alluxio.com/2016/02/alluxio-formerly-tachyon-is-entering-a-new-era-with-1-0-release/&quot;&gt;http://www.alluxio.com/2016/02/alluxio-formerly-tachyon-is-entering-a-new-era-with-1-0-release/&lt;/a&gt; - history&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.alluxio.com/blog&quot;&gt;https://www.alluxio.com/blog&lt;/a&gt; - blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.alluxio.org/download&quot;&gt;http://www.alluxio.org/download&lt;/a&gt; - details of releases&lt;/li&gt; &lt;/ul&gt; </content><published>2017-01-27T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/blog/2017/01/27/the-week-that-was/</id><title>The Week That Was - 27/01/2017</title><link href="http://ondataengineering.net/blog/2017/01/27/the-week-that-was/" rel="alternate" type="text/html" title="The Week That Was - 27/01/2017" /> <updated>2017-01-27T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/blog/2017/01/27/the-week-that-was/"> &lt;p&gt;So rather than waiting until we’ve finished looking at all the technologies included in Apache Bigtop before talking about them, let’s try wrapping up each week with a blog post summarising what we’ve looked at, and maybe at some point summarising some of the news of the week. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;So what have we looked at this week?&lt;/p&gt; &lt;p&gt;Firstly, a couple of graph computation frameworks - &lt;a href=&quot;/technologies/apache-hama/&quot;&gt;Apache Hama&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-giraph&quot;&gt;Apache Giraph&lt;/a&gt;. Along with &lt;a href=&quot;/technologies/apache-spark/graphx/&quot;&gt;Spark GraphX&lt;/a&gt;, these are probably the big three graph computation frameworks on Hadoop. What’s interesting is that both GraphX and Hama seem to have seen very little development recently - either meaning they’re done and meet most people’s use cases, or there just isn’t the demand for them. Giraph still seems to be going strong, however this is mainly being used at extreme scale by Facebook and LinkedIn. My guess is that graph technologies (both computation frameworks and graph databases) are being pushed as hot technologies at the moment, however most organisations aren’t quite sure what to do with them.&lt;/p&gt; &lt;p&gt;We’re also looked at a couple of Hadoop in-memory storage accelerators - &lt;a href=&quot;/technologies/apache-ignite&quot;&gt;Apache Ignite&lt;/a&gt; and &lt;a href=&quot;/technologies/alluxio&quot;&gt;Alluxio&lt;/a&gt; (formally known as Tachyon). These are both interesting, promising performance boosts for Hadoop computation jobs by providing an in memory HDFS compatible filesystem, as well a bunch of other features - both support an in memory key-value store, Alluxio supports tiered storage over multiple storage layers (in-memory, local and remote disk), and Ignite provides a more general purpose compute layer that supports streaming computation and arbitrary compute. If you’ve used either of these and can talk to their benefits I’d be very interested to chat in the forums.&lt;/p&gt; &lt;p&gt;And we started the week by talking about &lt;a href=&quot;/technologies/apache-bigtop&quot;&gt;Apache Bigtop&lt;/a&gt;, which I’ve already &lt;a href=&quot;/blog/2017/01/23/hadoop-distros-apache-bigtop/&quot;&gt;talked about&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Finally - you’ll now see that we’re proudly displaying our options for &lt;a href=&quot;/site/subscribe/&quot;&gt;subscribing&lt;/a&gt; to our content on the front page. Pick your poison - we support e-mail (daily and weekly), Twitter updates plus RSS and Atom feeds.&lt;/p&gt; </content> <category term="Site" /><category term="Technologies" /><category term="Tech Categories" /> <category term="Giraph" /><category term="Hama" /><category term="Ignite" /><category term="Alluxio" /><category term="Bigtop" /><category term="Hadoop Distributions" /><published>2017-01-27T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-hama/</id><title>Apache Hama</title><link href="http://ondataengineering.net/technologies/apache-hama/" rel="alternate" type="text/html" title="Apache Hama" /> <updated>2017-01-26T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-hama/"> &lt;p&gt;A general purpose BSP (Bulk Synchronous Parallel) processing engine inspired by Pregel and DistBelief that runs over Mesos or YARN. Supports BSP, graph computing and machine learning programming models, as well as Apache MRQL. An Apache project, donated in 2008, and graduated in 2012. Java based, with no commercial support available, limited case studies for it's use and limited active developers, with the last release being in June 2015.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Hama&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Open Source - Quiet&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v0.7&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://hama.apache.org/&quot;&gt;http://hama.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://wiki.apache.org/hama/&quot;&gt;https://wiki.apache.org/hama/&lt;/a&gt; - Wiki / documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://wiki.apache.org/incubator/HamaProposal&quot;&gt;https://wiki.apache.org/incubator/HamaProposal&lt;/a&gt; - details of Apache incubation proposal&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/hama/&quot;&gt;https://blogs.apache.org/hama/&lt;/a&gt; - blog&lt;/li&gt; &lt;/ul&gt; </content><published>2017-01-26T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-giraph/</id><title>Apache Giraph</title><link href="http://ondataengineering.net/technologies/apache-giraph/" rel="alternate" type="text/html" title="Apache Giraph" /> <updated>2017-01-25T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-giraph/"> &lt;p&gt;An iterative, highly scalable graph processing system built on top of MapReduce and based on Pregel, with a number of features added including a framework for creating re-usable code (called blocks). An Apache project, graduating in May 2012, having been originally donated by Yahoo in August 2011. Java based, no commercial support available, but is mature and has been adopted by a number of companies (including LinkedIn and most famously Facebook who scaled it to process a trillion edges), and has a number of active developers.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Giraph&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Open Source - Active&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v1.2&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://giraph.apache.org/&quot;&gt;http://giraph.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://giraph.apache.org/intro.html&quot;&gt;http://giraph.apache.org/intro.html&lt;/a&gt; - introduction&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://engineering.linkedin.com/open-source/apache-giraph-framework-large-scale-graph-processing-hadoop-reaches-01-milestone&quot;&gt;https://engineering.linkedin.com/open-source/apache-giraph-framework-large-scale-graph-processing-hadoop-reaches-01-milestone&lt;/a&gt; - v0.1 release announcement from LinkedIn&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.facebook.com/notes/facebook-engineering/scaling-apache-giraph-to-a-trillion-edges/10151617006153920/&quot;&gt;https://www.facebook.com/notes/facebook-engineering/scaling-apache-giraph-to-a-trillion-edges/10151617006153920/&lt;/a&gt; - Facebook’s story on scaling Giraph to a trillion edges&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://giraph.apache.org/&quot;&gt;http://giraph.apache.org/&lt;/a&gt; - news via homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/giraph&quot;&gt;https://blogs.apache.org/giraph&lt;/a&gt; - blog, with first entry being v1.2 announcement&lt;/li&gt; &lt;/ul&gt; </content><published>2017-01-25T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-ignite/</id><title>Apache Ignite</title><link href="http://ondataengineering.net/technologies/apache-ignite/" rel="alternate" type="text/html" title="Apache Ignite" /> <updated>2017-01-24T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-ignite/"> &lt;p&gt;A distributed in-memory data fabric/grid, supporting a number of use cases including a key value store (with SQL support), real time stream/event processing engine, arbitrary compute, long running service management, an in-memory HDFS compatible file system for acceleration of Hadoop jobs, and in-memory shared Spark RDDs. An Apache project, graduating in September 2015, having been originally donated by GridGain from their In-Memory Data Fabric product launched in 2007. Java based, with development lead by GridGain who also supply commercial support (as GridGain Professional with ongoing Q&amp;A and bug fixes before they're included in Ignite) along with GridGain Enterprise (which includes extra features such as a management GUI, enterprise security and rolling upgrades).&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Ignite, GridGain&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v1.8.0 (Ignite), v7.7 (GridGain)&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://ignite.apache.org/&quot;&gt;https://ignite.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.gridgain.com/&quot;&gt;http://www.gridgain.com/&lt;/a&gt; - Grid Gain home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.gridgain.com/products/pricing/&quot;&gt;http://www.gridgain.com/products/pricing/&lt;/a&gt; - details of GridGain editions&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://apacheignite.readme.io/docs&quot;&gt;https://apacheignite.readme.io/docs&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.infoq.com/presentations/apache-ignite&quot;&gt;https://www.infoq.com/presentations/apache-ignite&lt;/a&gt; - intro presentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://ignite.apache.org/blogs.html&quot;&gt;https://ignite.apache.org/blogs.html&lt;/a&gt; - Ingite blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.gridgain.com/resources/blog&quot;&gt;https://www.gridgain.com/resources/blog&lt;/a&gt; - GridGain blog&lt;/li&gt; &lt;/ul&gt; </content><published>2017-01-24T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/blog/2017/01/23/hadoop-distros-apache-bigtop/</id><title>Hadoop Distros: Apache Bigtop</title><link href="http://ondataengineering.net/blog/2017/01/23/hadoop-distros-apache-bigtop/" rel="alternate" type="text/html" title="Hadoop Distros: Apache Bigtop" /> <updated>2017-01-23T18:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/blog/2017/01/23/hadoop-distros-apache-bigtop/"> &lt;p&gt;And so to our first (hopefully of many) daily technology summary.&lt;/p&gt; &lt;p&gt;I want to continue our wander through the Apache Hadoop ecosystem by looking at the common Hadoop Distributions, starting with &lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, and with this introducing our first technology category - the &lt;a href=&quot;/tech-categories/hadoop_distributions/&quot;&gt;Hadoop Distribution&lt;/a&gt;. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Apache Bigtop is interesting for a couple of reasons - firstly because it’s the only true open source Hadoop distribution (meaning that it includes many components that the commercial distributions don’t, and the components it includes are often more up to date, assuming you’re happy to use the latest snapshot builds), and secondly because of it’s history as Cloudera’s attempt to create a common base Hadoop distribution with all the associated integration testing and packaging (there are links in the &lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Bigtop page&lt;/a&gt; that provide further reading on this).&lt;/p&gt; &lt;p&gt;You’ll see that the &lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Bigtop page&lt;/a&gt; links to the Hadoop technologies we’re already covered that it packages (and vice-versa), and the aim is that over the coming days we’ll complete this list as we add technology summaries for the rest of the technologies it packages.&lt;/p&gt; &lt;p&gt;And I shouldn’t let our first technology category go uncommented - as we’re going to be looking at the common Hadoop Distributions we’ll start collecting these together under an &lt;a href=&quot;/tech-categories/hadoop_distributions/&quot;&gt;Hadoop Distribution&lt;/a&gt; page and try and round these out over the coming weeks.&lt;/p&gt; &lt;p&gt;That’s it for now - we’ll speak again when we’ve worked our way through the remaining technologies bundled with Apache Bigtop.&lt;/p&gt; </content> <category term="Tech Categories" /><category term="Technologies" /> <category term="Bigtop" /><category term="Hadoop Distributions" /><published>2017-01-23T18:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-bigtop/</id><title>Apache Bigtop</title><link href="http://ondataengineering.net/technologies/apache-bigtop/" rel="alternate" type="text/html" title="Apache Bigtop" /> <updated>2017-01-23T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-bigtop/"> &lt;p&gt;An Apache open source distribution of Hadoop. Packages up a number of Apache Hadoop components, certifies their interoperability using an automated integration test suite, and packages them up as RPMs/DEBs packages for most flavours of Linux. Also includes virtual machine images and vagrant, docker and puppet recipes for deploying and working with Hadoop. Does not patch projects for distribution, but requires any fixes to be made upstream. An Apache Open Source project, started by Cloudera, donated to the Apache foundation in June 2011, graduating in September 2012, with a 1.0 release in August 2015 based on Hadoop 2.6. Since donating the project, Cloudera have backed away from it, with the project lead moving to Pivotal in December 2013. Now has a broad range of contributors, however usage by the major distributors is not clear.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Bigtop&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Categories&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-categories/hadoop_distributions/&quot;&gt;Hadoop Distributions&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Open Source - Active&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v1.1&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Packages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/alluxio/&quot;&gt;Alluxio&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-crunch/&quot;&gt;Apache Crunch&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-flume/&quot;&gt;Apache Flume&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-giraph/&quot;&gt;Apache Giraph&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Apache Hadoop&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hama/&quot;&gt;Apache Hama&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hbase/&quot;&gt;Apache HBase&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Apache Hive&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-ignite/&quot;&gt;Apache Ignite&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-oozie/&quot;&gt;Apache Oozie&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-phoenix/&quot;&gt;Apache Phoenix&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-pig/&quot;&gt;Apache Pig&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-solr/&quot;&gt;Apache Solr&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-sqoop/&quot;&gt;Apache Sqoop&lt;/a&gt;, &lt;a href=&quot;/technologies/kite/&quot;&gt;Kite&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;1.1.0&lt;/td&gt; &lt;td&gt;17th Feb 2016&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://blogs.apache.org/bigtop/entry/release_1_1_0_is&quot;&gt;https://blogs.apache.org/bigtop/entry/release_1_1_0_is&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.0.0&lt;/td&gt; &lt;td&gt;17th Aug 2015&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://blogs.apache.org/bigtop/entry/not_just_yet_another_release&quot;&gt;https://blogs.apache.org/bigtop/entry/not_just_yet_another_release&lt;/a&gt;&lt;/td&gt; &lt;td&gt;based on Hadoop 2.6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.8.0&lt;/td&gt; &lt;td&gt;6th Oct 2014&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://blogs.apache.org/bigtop/entry/release_of_apache_bigtop_01&quot;&gt;https://blogs.apache.org/bigtop/entry/release_of_apache_bigtop_01&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.7.0&lt;/td&gt; &lt;td&gt;6th Nov 2013&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://blogs.apache.org/bigtop/entry/release_of_apache_bigtop_0&quot;&gt;https://blogs.apache.org/bigtop/entry/release_of_apache_bigtop_0&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.6.0&lt;/td&gt; &lt;td&gt;22nd June 2013&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://blogs.apache.org/bigtop/entry/apache_bigtop_0_6_0&quot;&gt;https://blogs.apache.org/bigtop/entry/apache_bigtop_0_6_0&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.5.0&lt;/td&gt; &lt;td&gt;27th Dec 2012&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://blogs.apache.org/bigtop/entry/apache_bigtop_0_5_0&quot;&gt;https://blogs.apache.org/bigtop/entry/apache_bigtop_0_5_0&lt;/a&gt;&lt;/td&gt; &lt;td&gt;first release as TLP&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://bigtop.apache.org&quot;&gt;http://bigtop.apache.org&lt;/a&gt; - Homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/BIGTOP/Index&quot;&gt;https://cwiki.apache.org/confluence/display/BIGTOP/Index&lt;/a&gt; - The Apache Bigtop Wiki&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/apache/bigtop/blob/master/bigtop.bom&quot;&gt;https://github.com/apache/bigtop/blob/master/bigtop.bom&lt;/a&gt; - details of all included packages and their versions (as of current development snapshot)&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2012/07/update-on-apache-bigtop-incubating/&quot;&gt;http://blog.cloudera.com/blog/2012/07/update-on-apache-bigtop-incubating/&lt;/a&gt; - Cloudera introduction to Bigtop&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/bigtop/entry/bigtop_and_why_should_you&quot;&gt;https://blogs.apache.org/bigtop/entry/bigtop_and_why_should_you&lt;/a&gt; - Early introduction to Bigtop&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blog.pivotal.io/pivotal/pivotal-people/pivotal-people-roman-shaposhnik-founder-of-apache-bigtop-joins-pivotal&quot;&gt;https://blog.pivotal.io/pivotal/pivotal-people/pivotal-people-roman-shaposhnik-founder-of-apache-bigtop-joins-pivotal&lt;/a&gt; - Interview with Roman Shoposhnik on the aims of Bigtop&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/BIGTOP/?selectedTab=com.atlassian.jira.jira-projects-plugin:versions-panel&quot;&gt;https://issues.apache.org/jira/browse/BIGTOP/?selectedTab=com.atlassian.jira.jira-projects-plugin:versions-panel&lt;/a&gt; - Release list and link to release notes&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/bigtop/&quot;&gt;https://blogs.apache.org/bigtop/&lt;/a&gt; - The Apache Bigtop Blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://bigtop.apache.org/&quot;&gt;http://bigtop.apache.org/&lt;/a&gt; - details of latest release&lt;/li&gt; &lt;/ul&gt; </content> <category term="Hadoop Distributions" /><published>2017-01-23T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/tech-categories/hadoop_distributions/</id><title>Hadoop Distributions</title><link href="http://ondataengineering.net/tech-categories/hadoop_distributions/" rel="alternate" type="text/html" title="Hadoop Distributions" /> <updated>2017-01-23T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/tech-categories/hadoop_distributions/"> &lt;p&gt;Projects that bundle together specific versions of multiple components around the Hadoop ecosystem, certify that these work together, and deliver packages and other installation mechanisms for installing and managing these.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt; &lt;/td&gt; &lt;td&gt;An Apache open source distribution of Hadoop. Packages up a number of Apache Hadoop components, certifies their interoperability using an automated integration test suite, and packages them up as RPMs/DEBs packages for most flavours of Linux. Also includes virtual machine images and vagrant, docker and puppet recipes for deploying and working with Hadoop. Does not patch projects for distribution, but requires any fixes to be made upstream. An Apache Open Source project, started by Cloudera, donated to the Apache foundation in June 2011, graduating in September 2012, with a 1.0 release in August 2015 based on Hadoop 2.6. Since donating the project, Cloudera have backed away from it, with the project lead moving to Pivotal in December 2013. Now has a broad range of contributors, however usage by the major distributors is not clear.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;!-- Tech Vendor metadata --&gt; </content><published>2017-01-23T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/blog/2017/01/23/the-daily-technology-summary/</id><title>The Daily Technology Summary</title><link href="http://ondataengineering.net/blog/2017/01/23/the-daily-technology-summary/" rel="alternate" type="text/html" title="The Daily Technology Summary" /> <updated>2017-01-23T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/blog/2017/01/23/the-daily-technology-summary/"> &lt;p&gt;Up until today, I’ve been publishing new technology summaries in bulk every Friday. However I want to change up the pace a little, and so going forward I’m going to try to publish a new technology summary every week day.&lt;/p&gt; &lt;!--more--&gt; &lt;p&gt;Blog posts will be less frequent, and will usually be used to introduce and conclude a specific set of technologies we’re looking it. Because of this, you’ll notice that the site home page now supports a snazzy new summary of recent changes to content on the right hand side next to the summary of recent blog posts. In addition, the RSS and Atom feeds now include content updates as well as blog posts.&lt;/p&gt; &lt;p&gt;Also, in the coming days there will be new options to get the daily technology summary (and blog posts) delivered to your inbox via e-mail, and we’ll also start publishing these on Twitter (@OnDataEng) as well - just as soon as it’s all setup.&lt;/p&gt; </content> <category term="Site" /><published>2017-01-23T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-pig/</id><title>Apache Pig</title><link href="http://ondataengineering.net/technologies/apache-pig/" rel="alternate" type="text/html" title="Apache Pig" /> <updated>2017-01-20T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-pig/"> &lt;p&gt;Technology for running analytical and data processing jobs against data in Hadoop. Jobs are written in Pig Latin (a custom procedural language that can be extended using user defined functions in a range of languages), which is then translated into Map Reduce or Tez (with Spark in development) for execution. Supports both a batch mode for running pre-defined scripts and an interactive mode, and connectors for reading and writing to HBase and Accumulo as well as HDFS. Originally developed at Yahoo in 2006 before being donated to the Apache Foundation in October 2007. Graduated as an Hadoop sub-project in October 2008, before becoming a top level project in September 2010. Although has not had a v1.0 release, has been production quality for many years. Commercial support available as part of most Hadoop distributions&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Pig&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v0.16&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Is used by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-datafu/datafu-pig/&quot;&gt;DataFu Pig&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://pig.apache.org/&quot;&gt;https://pig.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://pig.apache.org/releases.html&quot;&gt;https://pig.apache.org/releases.html&lt;/a&gt; - release history&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://hortonworks.com/blog/announcing-apache-pig-0-14-0/&quot;&gt;http://hortonworks.com/blog/announcing-apache-pig-0-14-0/&lt;/a&gt; - Pig on Tez release announcement&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2014/09/pig-is-flying-apache-pig-on-apache-spark/&quot;&gt;http://blog.cloudera.com/blog/2014/09/pig-is-flying-apache-pig-on-apache-spark/&lt;/a&gt; - first details of Pig on Spark&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://pig.apache.org/&quot;&gt;https://pig.apache.org/&lt;/a&gt; - news and updates&lt;/li&gt; &lt;/ul&gt; </content><published>2017-01-20T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-parquet/</id><title>Apache Parquet</title><link href="http://ondataengineering.net/technologies/apache-parquet/" rel="alternate" type="text/html" title="Apache Parquet" /> <updated>2017-01-20T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-parquet/"> &lt;p&gt;Data serialisation framework that supports a columnar storage format to enable efficient querying of data. Built using Apache Thrift, and supports complex nested data structures, using techniques from the Google Dremel paper. Consists of three sub-projects, the specification and Thrift definitions (Parquet Format), the Java and Hadoop libraries (Parquet MR) and the C++ implementation (Parquet CPP). Created as a joint effort between Twitter and Cloudera based on work started as part of Avro Trevni, with an initial v1.0 release in July 2013. Donated to the Apache Foundation in May 2014, graduating in April 2015. Has seen significant adoption in the Hadoop ecosystem.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Avro&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v1.9&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://parquet.apache.org/&quot;&gt;http://parquet.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://parquet.apache.org/documentation/latest/&quot;&gt;http://parquet.apache.org/documentation/latest/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2013/03/introducing-parquet-columnar-storage-for-apache-hadoop/&quot;&gt;http://blog.cloudera.com/blog/2013/03/introducing-parquet-columnar-storage-for-apache-hadoop/&lt;/a&gt; - initial announcement&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blog.twitter.com/2013/dremel-made-simple-with-parquet&quot;&gt;https://blog.twitter.com/2013/dremel-made-simple-with-parquet&lt;/a&gt; - good introduction to Parquet&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blog.twitter.com/2013/announcing-parquet-10-columnar-storage-for-hadoop&quot;&gt;https://blog.twitter.com/2013/announcing-parquet-10-columnar-storage-for-hadoop&lt;/a&gt; - 1.0 release announcement&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces75&quot;&gt;https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces75&lt;/a&gt; - top level project announcement, including summary of technology that support it&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/apache/parquet-mr/blob/master/CHANGES.md&quot;&gt;https://github.com/apache/parquet-mr/blob/master/CHANGES.md&lt;/a&gt; - release and change summary&lt;/li&gt; &lt;/ul&gt; </content><published>2017-01-20T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-oozie/</id><title>Apache Oozie</title><link href="http://ondataengineering.net/technologies/apache-oozie/" rel="alternate" type="text/html" title="Apache Oozie" /> <updated>2017-01-20T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-oozie/"> &lt;p&gt;Technology for managing workflows of jobs on Hadoop clusters. Primary concepts include workflows (a sequence of jobs modelled as a directed acyclic graph), coordinators (schedule the execution of workflows based on the time or the presence of data) and bundles (collections of coordinators), with all configuration specified in XML. Supports a range of technologies, including MapReduce, Pig, Hive, Sqoop, Spark, Java executables and shell scripts. Includes a server component, a metadata database for holding definitions and state (with support for a range of database technologies), a command line interface and a read only web interface for viewing the status of jobs. Also supports the parameterisation of workflows, the modelling of datasets (and the use of these to manage dependencies between workflows within coordinators), automatic retry and failure handling, and the ability to send job status notifications via HTTP or JMS. Open sourced by Yahoo in June 2010. Donated to the Apache Foundation in July 2011, graduating in August 2012. Commercial support available as part of most Hadoop distributions&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Oozie&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v4.3&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://oozie.apache.org/&quot;&gt;http://oozie.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://oozie.apache.org/docs/4.3.0/index.html&quot;&gt;http://oozie.apache.org/docs/4.3.0/index.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://jaxenter.com/yahoos-hadoop-based-project-proposed-for-apache-incubator-103651.html&quot;&gt;https://jaxenter.com/yahoos-hadoop-based-project-proposed-for-apache-incubator-103651.html&lt;/a&gt; - intro interview&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Oozie release announcements only appear to be available via the Apache announcements mailing list&lt;/li&gt; &lt;/ul&gt; </content><published>2017-01-20T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-kafka/kafka-streams/</id><title>Kafka Streams</title><link href="http://ondataengineering.net/technologies/apache-kafka/kafka-streams/" rel="alternate" type="text/html" title="Kafka Streams" /> <updated>2017-01-20T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-kafka/kafka-streams/"> &lt;p&gt;Technology that allows stream processing to be added to a Kafka cluster, consuming and publishing events from and to Kafka topics (and potentially writing output to external systems). Based on an event-at-a-time model (i.e. not micro batch), with support for stateful processing, windowing, joining and re-processing data. Supports a low level DSL API, as well as a high level API that provides both stream and table abstractions (where tables present the latest record for each key). Introduced in Kafka 0.10.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://kafka.apache.org/documentation/streams&quot;&gt;http://kafka.apache.org/documentation/streams&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/product/kafka-streams/&quot;&gt;https://www.confluent.io/product/kafka-streams/&lt;/a&gt; - Confluent information page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/&quot;&gt;https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/&lt;/a&gt; - introduction blog post&lt;/li&gt; &lt;/ul&gt; </content><published>2017-01-20T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-kafka/kafka-connect/</id><title>Kafka Connect</title><link href="http://ondataengineering.net/technologies/apache-kafka/kafka-connect/" rel="alternate" type="text/html" title="Kafka Connect" /> <updated>2017-01-20T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-kafka/kafka-connect/"> &lt;p&gt;Framework for building scalable and reliable integrations between Kafka and other technologies, either for importing or exporting data. Part of the core Apache Kafka open source technology, connectors are available for a wide range of systems, including Hadoop, relational, NoSQL and analytical databases, search technologies and message queues amongst others. Runs separately to Kafka, in either a stand-alone or distributed cluster mode, with a REST API for managing connectors.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://kafka.apache.org/documentation/#connect&quot;&gt;http://kafka.apache.org/documentation/#connect&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/product/connectors/&quot;&gt;https://www.confluent.io/product/connectors/&lt;/a&gt; - Confluent information, including list of available connectors&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/blog/how-to-build-a-scalable-etl-pipeline-with-kafka-connect/&quot;&gt;https://www.confluent.io/blog/how-to-build-a-scalable-etl-pipeline-with-kafka-connect/&lt;/a&gt; - introduction blog post&lt;/li&gt; &lt;/ul&gt; </content><published>2017-01-20T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-kafka/</id><title>Apache Kafka</title><link href="http://ondataengineering.net/technologies/apache-kafka/" rel="alternate" type="text/html" title="Apache Kafka" /> <updated>2017-01-20T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-kafka/"> &lt;p&gt;Technology for buffering and storing real-time streams of data between publishers to subscribers, with a focus on high throughput at low latency. Based on a distributed, horizontally scalable architecture, with messages organised into topics which are partitioned and replicated across nodes to provide resilience and written to disk to provide persistence. Topics may have multiple publishers and subscribers, with ability to do fault tolerant reads and to load balance across subscribers. Records consist of a key, value and timestamp, with the ability to compact topics to remove updates and deletes by key. Supports a full security model, and the ability to set quotas. Comes with a Java client, but clients for a wide range of languages are also available. Has two sub-projects (Kafka Connect and Kafka Streams) that are bundled with the main product. Originally developed at LinkedIn, being open sourced in January 2011, before being donated to the Apache Foundation in July 2011. Graduated in October 2012, and although it has not had a v1.0 release is considered production quality and stable. Development is primarily led by Confluent (which was founded by the team that built Kafka at LinkedIn), who distribute a Confluent Open Source product (which includes further clients and connectors) and a subscription based Confluent Enterprise product (which includes management, replication and data balancing features and commercial support under a subscription licence). Commercial support is also available from most Hadoop vendors.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Kafka, Confluent&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v0.10&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Sub-projects&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; Apache Kafka&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-kafka/kafka-connect/&quot;&gt;Kafka Connect&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Framework for building scalable and reliable integrations between Kafka and other technologies, either for importing or exporting data. Part of the core Apache Kafka open source technology, connectors are available for a wide range of systems, including Hadoop, relational, NoSQL and analytical databases, search technologies and message queues amongst others. Runs separately to Kafka, in either a stand-alone or distributed cluster mode, with a REST API for managing connectors.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Kafka&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-kafka/kafka-streams/&quot;&gt;Kafka Streams&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Technology that allows stream processing to be added to a Kafka cluster, consuming and publishing events from and to Kafka topics (and potentially writing output to external systems). Based on an event-at-a-time model (i.e. not micro batch), with support for stateful processing, windowing, joining and re-processing data. Supports a low level DSL API, as well as a high level API that provides both stream and table abstractions (where tables present the latest record for each key). Introduced in Kafka 0.10.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://kafka.apache.org&quot;&gt;http://kafka.apache.org&lt;/a&gt; - project home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://kafka.apache.org/intro&quot;&gt;http://kafka.apache.org/intro&lt;/a&gt; - great introduction to Kafka&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blog.linkedin.com/2011/01/11/open-source-linkedin-kafka&quot;&gt;https://blog.linkedin.com/2011/01/11/open-source-linkedin-kafka&lt;/a&gt; - open source announcement&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/Clients&quot;&gt;https://cwiki.apache.org/confluence/display/KAFKA/Clients&lt;/a&gt; - list of clients&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem&quot;&gt;https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem&lt;/a&gt; - associated technologies&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/product/&quot;&gt;https://www.confluent.io/product/&lt;/a&gt; - Confluent product homepage&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://kafka.apache.org/downloads&quot;&gt;http://kafka.apache.org/downloads&lt;/a&gt; - release history&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/blog/&quot;&gt;https://www.confluent.io/blog/&lt;/a&gt; - Confluent blog&lt;/li&gt; &lt;/ul&gt; </content><published>2017-01-20T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-avro/</id><title>Apache Avro</title><link href="http://ondataengineering.net/technologies/apache-avro/" rel="alternate" type="text/html" title="Apache Avro" /> <updated>2017-01-20T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-avro/"> &lt;p&gt;Data serialisation framework that supports both messaging and data storage. Primarily uses a compact binary format but also supports a JSON format. Supports a range of data structures (including records, enumerations, arrays and maps) with APIs for a wide range of both static and dynamically typed languages. Schema based, with schemas primarily specified in JSON, and support for both code generation from schema definitions as well as dynamic runtime usage. Schemas are serialised alongside data, with support for automatic schema resolution if the schema used to read the data differs from that used to write it. Started as an Hadoop sub-project by Cloudera in April 2009, with an initial v1.0 release in July 2009, before becoming a top level Apache project in May 2010. Has seen significant adoption in the Hadoop ecosystem.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Avro&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v1.8&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://avro.apache.org/&quot;&gt;http://avro.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://avro.apache.org/docs/current/&quot;&gt;https://avro.apache.org/docs/current/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2009/11/avro-a-new-format-for-data-interchange/&quot;&gt;http://blog.cloudera.com/blog/2009/11/avro-a-new-format-for-data-interchange/&lt;/a&gt; - original introduction to Avro&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces4&quot;&gt;https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces4&lt;/a&gt; - Avro top level project announcement&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://avro.apache.org/releases.html&quot;&gt;http://avro.apache.org/releases.html&lt;/a&gt; - project releases&lt;/li&gt; &lt;/ul&gt; </content><published>2017-01-20T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/blog/2017/01/20/core-hadoop-technologies-pt3/</id><title>Core Hadoop Technologies (pt3)</title><link href="http://ondataengineering.net/blog/2017/01/20/core-hadoop-technologies-pt3/" rel="alternate" type="text/html" title="Core Hadoop Technologies (pt3)" /> <updated>2017-01-20T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/blog/2017/01/20/core-hadoop-technologies-pt3/"> &lt;p&gt;Up today, our final look at the core technologies within the Hadoop ecosystem. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;First up are &lt;a href=&quot;/technologies/apache-avro&quot;&gt;Avro&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-parquet&quot;&gt;Parquet&lt;/a&gt;, both of which are key data formats used within the Hadoop ecosystem, but with different and contrasting focuses.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/technologies/apache-kafka&quot;&gt;Kafka&lt;/a&gt;’s a hot technology at the moment - deliverying high bandwidth low latency storage and processing of data streams, with reference cases handling millions of events per second. If you’re looking at doing anything with streaming data it’s probably well worth a look. Note that I’ve broken out &lt;a href=&quot;/technologies/apache-kafka/kafka-connect&quot;&gt;Kafka Connect&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-kafka/kafka-streams&quot;&gt;Kafka Streams&lt;/a&gt; as sub-projects.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/technologies/apache-pig&quot;&gt;Pig&lt;/a&gt; was one of the first technologies to provide a ore user friendly abstraction over MapReduce for developing Hadoop jobs. It’s starting to show it’s age however, and although Hortonworks and Yahoo (who are heavy Pig users) seem to be investing heavily in Pig on Tez, and Cloudera seems to be supporting Pig on Spark (mirroring their Hive strategies), it’s difficult to see newcomers to Hadoop who don’t have an existing investment in Pig using it over Spark and other newer tools.&lt;/p&gt; &lt;p&gt;And finally &lt;a href=&quot;/technologies/apache-oozie&quot;&gt;Oozie&lt;/a&gt; - a job scheduling an orchestration engine. It’s been a staple of most Hadoop distributions for a while now, however it’s difficult to find many big references cases for it’s use, and it’s not the most user friendly tool. Orchestration and management of data transformation pipelines feels like a huge technology gap at the moment - if anyone knows of any great technologies in this space please shout.&lt;/p&gt; &lt;p&gt;As before - click on the links to see the technology information added to the site.&lt;/p&gt; &lt;p&gt;That’s it for this week, and for the core Hadoop technologies - it’s been fun. We’ll be back on Monday to start looking at Apache Bigtop, along with a change of pace…&lt;/p&gt; </content> <category term="Technologies" /> <category term="Apache" /><category term="Avro" /><category term="Parquet" /><category term="Kafka" /><category term="Pig" /><category term="Oozie" /><category term="Kafka Connect" /><category term="Kafka Streams" /><published>2017-01-20T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-sqoop/</id><title>Apache Sqoop</title><link href="http://ondataengineering.net/technologies/apache-sqoop/" rel="alternate" type="text/html" title="Apache Sqoop" /> <updated>2017-01-13T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-sqoop/"> &lt;p&gt;Specialist technology for moving bulk data between Hadoop and structured (relational) databases. Command line based, with the ability to import and export data between a range of databases (including mainframe partitioned datasets) and HDFS, Hive, HBase and Accumulo. Supports parallel partitioned unloads, writing to Avro, Sequence File, Parquet and text files, incremental imports and saved jobs that can be shared via a simple metadata store. An Apache project, started in May 2009 as an Hadoop contrib module, migrating to a Cloudera GitHub project in April 2010 (with a v1.0 release shortly after), before being donated to the Apache foundation in June 2011, graduating in March 2012. The last major release (v1.4) was in November 2011, with only minor releases since then. However in January 2012 a significant re-write was announced as part of a proposed v2.0 release to address a number of usability, security and architectural issues. This will introduce a new Sqoop Server and Metadata Repository, supporting both a CLI and web UI, centralising job definitions, database connections and credentials, as well as enabling support for a wider range of connectors including NoSQL databases, Kafka and (S)FTP folders. Java based, with commercial support available as part of most Hadoop distributions.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Sqoop&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v1.4&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://sqoop.apache.org&quot;&gt;http://sqoop.apache.org&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://sqoop.apache.org/docs/&quot;&gt;http://sqoop.apache.org/docs/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/sqoop/entry/apache_sqoop_highlights_of_sqoop&quot;&gt;https://blogs.apache.org/sqoop/entry/apache_sqoop_highlights_of_sqoop&lt;/a&gt; - introduction to Sqoop 2&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/sqoop/entry/apache_sqoop_graduates_from_incubator&quot;&gt;https://blogs.apache.org/sqoop/entry/apache_sqoop_graduates_from_incubator&lt;/a&gt; - early history of Sqoop&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://sqoop.apache.org/&quot;&gt;http://sqoop.apache.org/&lt;/a&gt; - details latest release, and hosts release notes for v1.4.0 onwards&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/sqoop/&quot;&gt;https://blogs.apache.org/sqoop/&lt;/a&gt; - project blog&lt;/li&gt; &lt;/ul&gt; </content><published>2017-01-13T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-spark/spark-streaming/</id><title>Spark Streaming</title><link href="http://ondataengineering.net/technologies/apache-spark/spark-streaming/" rel="alternate" type="text/html" title="Spark Streaming" /> <updated>2017-01-13T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-spark/spark-streaming/"> &lt;p&gt;Spark library for continuous stream processing, that allows stream and batch processing (including Spark SQL and MLlib operations) to be combined. Uses a micro-batch execution model, leveraging core Spark to process each micro-batch, and provides fault tolerance through exactly-once processing semantics. Supports a number of data sources (including HDFS, sockets, Flume, Kafka, Kinesis and messaging buses), as well as functions to maintain state and to execute windowed operations. First introduced in Spark 0.7, with a production release as part of Spark 0.9.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/sql/&quot;&gt;http://spark.apache.org/sql/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/sql-programming-guide.html&quot;&gt;http://spark.apache.org/docs/latest/sql-programming-guide.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;/ul&gt; </content><published>2017-01-13T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-spark/spark-sql/</id><title>Spark SQL</title><link href="http://ondataengineering.net/technologies/apache-spark/spark-sql/" rel="alternate" type="text/html" title="Spark SQL" /> <updated>2017-01-13T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-spark/spark-sql/"> &lt;p&gt;Spark library for processing structured data, using either SQL statements or a DataFrame API. Supports querying and writing to local datasets (including JSON, Parquet, Avro, Orc and CSV) as well as external data sources (including Hive and JDBC), including the ability to query across data sources. Includes Catalyst, a cost based optimiser that turns high level operations into low level Spark DAGs for execution. Also includes a Hive compatible Thrift JDBC/ODBC server that's compatible with Beeline and the Hive JDBC and ODBC drivers, and a REPL CLI for interactive queries. First introduced in Spark 1.0, with a production release as part of Spark 1.3.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/sql/&quot;&gt;http://spark.apache.org/sql/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/sql-programming-guide.html&quot;&gt;http://spark.apache.org/docs/latest/sql-programming-guide.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;/ul&gt; </content><published>2017-01-13T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-spark/mllib/</id><title>MLlib</title><link href="http://ondataengineering.net/technologies/apache-spark/mllib/" rel="alternate" type="text/html" title="MLlib" /> <updated>2017-01-13T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-spark/mllib/"> &lt;p&gt;Spark library for running Machine Learning algorithms. Supports a range of algorithms (including classifications, regressions, decision trees, recommendations, clustering and topic modelling), including iterative algorithms. First introduced in Spark 0.8 after being collaboratively developed with the UC Berkeley MLbase project.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/mllib//&quot;&gt;http://spark.apache.org/mllib//&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/ml-guide.html&quot;&gt;http://spark.apache.org/docs/latest/ml-guide.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.mlbase.org/&quot;&gt;http://www.mlbase.org/&lt;/a&gt; - the UC Berkeley MLbase project&lt;/li&gt; &lt;/ul&gt; </content><published>2017-01-13T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-spark/graphx/</id><title>GraphX</title><link href="http://ondataengineering.net/technologies/apache-spark/graphx/" rel="alternate" type="text/html" title="GraphX" /> <updated>2017-01-13T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-spark/graphx/"> &lt;p&gt;Spark library for processing graphs and running graph algorithms, based on graph model that supports directional edges with properties on both vertices and edges. Graphs are constructed from a pair of collections representing the edges and vertex, either directly from data on disk using builders, or prepared using other Spark functionality, with the ability to also view the graph as a set of triples. Supports a range of graph operations, as well as an optimised variant of the Pregel API, and a set of out of the box algorithms (including PageRank, connected components and triangle count). First introduced in Spark 0.9, with a production release as part of Spark 1.2, however has seen almost no new functionality since then.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/graphx/&quot;&gt;http://spark.apache.org/graphx/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/graphx-programming-guide.html&quot;&gt;http://spark.apache.org/docs/latest/graphx-programming-guide.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;/ul&gt; </content><published>2017-01-13T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-spark/</id><title>Apache Spark</title><link href="http://ondataengineering.net/technologies/apache-spark/" rel="alternate" type="text/html" title="Apache Spark" /> <updated>2017-01-13T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-spark/"> &lt;p&gt;A high performance general purpose distributed data processing engine based on directed acyclic graphs that primarily runs in memory, but can spill to disk if required, and which supports processing applications written in Java, Scala, Python and R. Includes a number of sub-projects that support more specialised analytics including Spark SQL, Spark Streaming, MLlib (machine learning) and GraphX (graph analytics). Requires a cluster manager (YARN, EC2 and Mesos are supported as well as standalone clusters) and can access data in a wide range of technologies (including HDFS, other Hadoop data sources, relational databases and NoSQL databases). An Apache project, originally started at UC Berkley in 2009, open sourced in 2010, and donated to the Apache foundation in June 2013, graduating in February 2014. v1.0 was released in May 2014, with a v2.0 release in July 2016. Java based, with development led by Databricks (who sell a Spark hosted service), and with commercial support available as part of most Hadoop distributions.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Spark&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v2.1&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Sub-projects&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; Apache Spark&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-spark/graphx/&quot;&gt;GraphX&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Spark library for processing graphs and running graph algorithms, based on graph model that supports directional edges with properties on both vertices and edges. Graphs are constructed from a pair of collections representing the edges and vertex, either directly from data on disk using builders, or prepared using other Spark functionality, with the ability to also view the graph as a set of triples. Supports a range of graph operations, as well as an optimised variant of the Pregel API, and a set of out of the box algorithms (including PageRank, connected components and triangle count). First introduced in Spark 0.9, with a production release as part of Spark 1.2, however has seen almost no new functionality since then.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Spark&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-spark/mllib/&quot;&gt;MLlib&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Spark library for running Machine Learning algorithms. Supports a range of algorithms (including classifications, regressions, decision trees, recommendations, clustering and topic modelling), including iterative algorithms. First introduced in Spark 0.8 after being collaboratively developed with the UC Berkeley MLbase project.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Spark&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-spark/spark-sql/&quot;&gt;Spark SQL&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Spark library for processing structured data, using either SQL statements or a DataFrame API. Supports querying and writing to local datasets (including JSON, Parquet, Avro, Orc and CSV) as well as external data sources (including Hive and JDBC), including the ability to query across data sources. Includes Catalyst, a cost based optimiser that turns high level operations into low level Spark DAGs for execution. Also includes a Hive compatible Thrift JDBC/ODBC server that's compatible with Beeline and the Hive JDBC and ODBC drivers, and a REPL CLI for interactive queries. First introduced in Spark 1.0, with a production release as part of Spark 1.3.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Spark&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-spark/spark-streaming/&quot;&gt;Spark Streaming&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Spark library for continuous stream processing, that allows stream and batch processing (including Spark SQL and MLlib operations) to be combined. Uses a micro-batch execution model, leveraging core Spark to process each micro-batch, and provides fault tolerance through exactly-once processing semantics. Supports a number of data sources (including HDFS, sockets, Flume, Kafka, Kinesis and messaging buses), as well as functions to maintain state and to execute windowed operations. First introduced in Spark 0.7, with a production release as part of Spark 0.9.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/&quot;&gt;http://spark.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/&quot;&gt;http://spark.apache.org/docs/latest/&lt;/a&gt; - documentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/news/index.html&quot;&gt;http://spark.apache.org/news/index.html&lt;/a&gt; - project news&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://databricks.com/blog/category/engineering&quot;&gt;https://databricks.com/blog/category/engineering&lt;/a&gt; - Databricks engineering blog&lt;/li&gt; &lt;/ul&gt; </content><published>2017-01-13T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-solr/</id><title>Apache Solr</title><link href="http://ondataengineering.net/technologies/apache-solr/" rel="alternate" type="text/html" title="Apache Solr" /> <updated>2017-01-13T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-solr/"> &lt;p&gt;A search server built on Apache Lucene with a REST-like API for loading and searching data. Supports a distributed deployment (SolrCloud) that can run over HDFS on an Hadoop cluster. Includes an administration web interface, an extensible plugin architecture, support for schemaless indexing, faceted, grouped and clustered results, hit highlighting, geo-spacial and graph searches, near real time indexing and searching, (experimental) streaming expressions for parallel compute (including support for MapReduce and SQL) and broad authentication and security capabilities. A sub-project of the Apache Lucene project, originally donated to the Apache foundation by CNET Networks in January 2006, graduating as a top level project in January 2007, before merging with the Lucene project in March 2010. Java based, with commercial support available as part of most Hadoop distributions (although this is bundled as Cloudera Search with CDH and HDP Search with HDP), as well as from Lucidworks.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Solr, Cloudera Search, HDP Search&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v6.3&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://lucene.apache.org/solr&quot;&gt;http://lucene.apache.org/solr&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://lucene.apache.org/solr/features.html&quot;&gt;http://lucene.apache.org/solr/features.html&lt;/a&gt; - good summary of Solr features&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.apache.org/dyn/closer.lua/lucene/solr/ref-guide/&quot;&gt;https://www.apache.org/dyn/closer.lua/lucene/solr/ref-guide/&lt;/a&gt; - PDF download of documentation for latest release&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/solr/&quot;&gt;https://cwiki.apache.org/confluence/display/solr/&lt;/a&gt; - online working version of documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Apache_Solr&quot;&gt;https://en.wikipedia.org/wiki/Apache_Solr&lt;/a&gt; - history of Solr&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://lucene.apache.org/solr/news.html&quot;&gt;http://lucene.apache.org/solr/news.html&lt;/a&gt; - project news&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://yonik.com/&quot;&gt;http://yonik.com/&lt;/a&gt; - details of new features from the creator of Solr&lt;/li&gt; &lt;/ul&gt; </content><published>2017-01-13T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/blog/2017/01/13/core-hadoop-technologies-pt2/</id><title>Core Hadoop Technologies (pt2)</title><link href="http://ondataengineering.net/blog/2017/01/13/core-hadoop-technologies-pt2/" rel="alternate" type="text/html" title="Core Hadoop Technologies (pt2)" /> <updated>2017-01-13T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/blog/2017/01/13/core-hadoop-technologies-pt2/"> &lt;p&gt;And onwards with our look at the core Hadoop technologies. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Today, I’ve added &lt;a href=&quot;/technologies/apache-solr/&quot;&gt;Solr&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-sqoop/&quot;&gt;Sqoop&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Spark&lt;/a&gt;, along with the Spark sub-projects &lt;a href=&quot;/technologies/apache-spark/spark-sql/&quot;&gt;Spark SQL&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-spark/spark-streaming/&quot;&gt;Spark Streaming&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-spark/mllib/&quot;&gt;MLlib&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-spark/graphx/&quot;&gt;GraphX&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Solr is one of the big two search technologies along with Elasticsearch, and although there’s debate around which is best (Elastic is probably slightly more developer friendly and supports slightly better analytics capabilities, whereas Solr has the more open development model being an Apache project), they’re both great technologies.&lt;/p&gt; &lt;p&gt;Sqoop is interesting because of where it’s going - whereas the current version of Sqoop focuses on integration with structured databases, the pending version 2 (which admittedly has been in development for a long time now) evolves it slightly to support the batch ingest of any data into Hadoop. It’s going to have stiff competition from Apache NiFi however if and when it’s finally released.&lt;/p&gt; &lt;p&gt;And so to Spark - which claims to be the most active Open Source project in Big Data (as well as many other things). What is clear however is it’s the one next gen data processing and transformation language that managed to catch significant momentum and adoption. We can argue the toss on whether it’s the best technology, but it’s now bundled with all the Hadoop distributions, has a rapidly growing base of trained and experienced developers, and a rich ecosystem, which means it’s becoming the default answer to a whole bunch of use cases.&lt;/p&gt; </content> <category term="Technologies" /> <category term="Apache" /><category term="Solr" /><category term="Sqoop" /><category term="Spark" /><category term="GraphX" /><category term="MLlib" /><category term="Spark SQL" /><category term="Spark Streaming" /><published>2017-01-13T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-hive/hive-server/</id><title>Hive Server</title><link href="http://ondataengineering.net/technologies/apache-hive/hive-server/" rel="alternate" type="text/html" title="Hive Server" /> <updated>2017-01-06T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-hive/hive-server/"> &lt;p&gt;Supports the execution of SQL queries over data in HDFS based on tables defined in the Hive Metastore, as well as DDL to query and update the Hive Metastore. Focus is on analytical (OLAP) use cases, with some support for batch updates to data. Originally executed queries as MapReduce jobs, but significant investment from has seen support for executing queries as Spark and as Tez jobs, with work underway to support sub second query times using Tez. Recent changes have also seen it achieve significant SQL compliance, with support for SQL:2011 analytical functions on-going. Accepts queries over an API with JDBC and ODBC drivers available, and includes Beeline, a command line JDBC client. Technically referred to as Hive Server 2, and was introduced in Hive 0.11 as a replacement for the original Hive Server to address a number of concurrency and security issues.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Apache Hive&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2013/07/how-hiveserver2-brings-security-and-concurrency-to-apache-hive/&quot;&gt;http://blog.cloudera.com/blog/2013/07/how-hiveserver2-brings-security-and-concurrency-to-apache-hive/&lt;/a&gt; - introduction to Hive Server 2&lt;/li&gt; &lt;/ul&gt; </content><published>2017-01-06T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-hive/hive-metastore/</id><title>Hive Metastore</title><link href="http://ondataengineering.net/technologies/apache-hive/hive-metastore/" rel="alternate" type="text/html" title="Hive Metastore" /> <updated>2017-01-06T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-hive/hive-metastore/"> &lt;p&gt;A metadata service that allows structured tables to be defined over files in HDFS (and also HBase or Accumulo), providing an API that allows the metadata to be queried and updated by other tools including Impala, Spark SQL or RecordService. Supports partitioned and clustered tables, as well as complex field types such as arrays, maps and structs. Backed by a relational database (either MySQL, Postgres and Oracle). Part of the original Hive code base.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Apache Hive&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin&quot;&gt;https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin&lt;/a&gt; - documentation home&lt;/li&gt; &lt;/ul&gt; </content><published>2017-01-06T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-hive/hcatalog/</id><title>HCatalog</title><link href="http://ondataengineering.net/technologies/apache-hive/hcatalog/" rel="alternate" type="text/html" title="HCatalog" /> <updated>2017-01-06T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-hive/hcatalog/"> &lt;p&gt;Libraries for MapReduce and Pig to read and write data to and from Hive tables, albeit with some limitations. Also supports a CLI for querying and updating the Hive Metastore, however this doesn't support the full range of Hive DDL commands. Includes WebHCat, a REST API over the HCatalog CLI that also supports the execution of MapReduce, Pig, Hive and Sqoop jobs. Donated to the Apache foundation by Yahoo in March 2011, had WebHCat folded in in July 2012, graduating as a top level project in February 2013, but then almost immediately was folded into Hive in March 2013 as part of the Hive 0.11 release. Has seem limited development since this time.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;WebHCat&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Apache Hive&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/HCatalog&quot;&gt;https://cwiki.apache.org/confluence/display/Hive/HCatalog&lt;/a&gt; - HCatalog documentation home&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/WebHCat&quot;&gt;https://cwiki.apache.org/confluence/display/Hive/WebHCat&lt;/a&gt; - WebHCat documentation home&lt;/li&gt; &lt;/ul&gt; </content><published>2017-01-06T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-hive/</id><title>Apache Hive</title><link href="http://ondataengineering.net/technologies/apache-hive/" rel="alternate" type="text/html" title="Apache Hive" /> <updated>2017-01-06T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-hive/"> &lt;p&gt;Technology that supports the exposure of data in Hadoop as structured tables and the execution of analytical SQL queries over these. Consists of a number of distinct components (that we treat as sub-projects) including Hive Metastore (stores the definitions of the structured tables), Hive Server (supports the execution of analytical SQL queries as MapReduce, Spark or Tez jobs) and HCatalog (allows MapReduce and Pig jobs to read and write Hive tables). First released by Facebook as an Hadoop contrib module in September 2008, becoming an Hadoop sub-project in November 2008, and a top level Apache project in September 2010, following a first official stable release (0.3) in April 2009. Java based, under active development from a number of large commercial sponsors, with commercial support available as part of most Hadoop distributions.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Hive&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v2.1&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Sub-projects&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; Apache Hive&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-hive/hcatalog/&quot;&gt;HCatalog&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Libraries for MapReduce and Pig to read and write data to and from Hive tables, albeit with some limitations. Also supports a CLI for querying and updating the Hive Metastore, however this doesn't support the full range of Hive DDL commands. Includes WebHCat, a REST API over the HCatalog CLI that also supports the execution of MapReduce, Pig, Hive and Sqoop jobs. Donated to the Apache foundation by Yahoo in March 2011, had WebHCat folded in in July 2012, graduating as a top level project in February 2013, but then almost immediately was folded into Hive in March 2013 as part of the Hive 0.11 release. Has seem limited development since this time.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Hive&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-hive/hive-metastore/&quot;&gt;Hive Metastore&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A metadata service that allows structured tables to be defined over files in HDFS (and also HBase or Accumulo), providing an API that allows the metadata to be queried and updated by other tools including Impala, Spark SQL or RecordService. Supports partitioned and clustered tables, as well as complex field types such as arrays, maps and structs. Backed by a relational database (either MySQL, Postgres and Oracle). Part of the original Hive code base.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Hive&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-hive/hive-server/&quot;&gt;Hive Server&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Supports the execution of SQL queries over data in HDFS based on tables defined in the Hive Metastore, as well as DDL to query and update the Hive Metastore. Focus is on analytical (OLAP) use cases, with some support for batch updates to data. Originally executed queries as MapReduce jobs, but significant investment from has seen support for executing queries as Spark and as Tez jobs, with work underway to support sub second query times using Tez. Recent changes have also seen it achieve significant SQL compliance, with support for SQL:2011 analytical functions on-going. Accepts queries over an API with JDBC and ODBC drivers available, and includes Beeline, a command line JDBC client. Technically referred to as Hive Server 2, and was introduced in Hive 0.11 as a replacement for the original Hive Server to address a number of concurrency and security issues.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://hive.apache.org/&quot;&gt;http://hive.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/Home&quot;&gt;https://cwiki.apache.org/confluence/display/Hive/Home&lt;/a&gt; - documentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://hive.apache.org/downloads.html&quot;&gt;http://hive.apache.org/downloads.html&lt;/a&gt; - details of new releases&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/category/hive/&quot;&gt;http://blog.cloudera.com/blog/category/hive/&lt;/a&gt; - Cloudera Hive News&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://hortonworks.com/blog/category/hive/&quot;&gt;http://hortonworks.com/blog/category/hive/&lt;/a&gt; - Hortonworks Hive News&lt;/li&gt; &lt;/ul&gt; </content><published>2017-01-06T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-hbase/</id><title>Apache HBase</title><link href="http://ondataengineering.net/technologies/apache-hbase/" rel="alternate" type="text/html" title="Apache HBase" /> <updated>2017-01-06T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-hbase/"> &lt;p&gt;NoSQL wide-column datastore based on Google BigTable. Focuses on random real-time access to data, and supports horizontal scalability, consistent reads and writes, versioning and fine grained security controls. Runs on Hadoop and HDFS, and is heavily integrated with the Hadoop ecosystem. An Apache project, first released as part of Hadoop 0.15 in October 2007 before graduating as a top level project in May 2010. Java based, with commercial support available as part of most Hadoop distributions.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;HBase&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v1.2&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://hbase.apache.org/&quot;&gt;http://hbase.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://hbase.apache.org/book.html&quot;&gt;http://hbase.apache.org/book.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://jimbojw.com/wiki/index.php?title=Understanding_Hbase_and_BigTable&quot;&gt;http://jimbojw.com/wiki/index.php?title=Understanding_Hbase_and_BigTable&lt;/a&gt; - HBase primer&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/hbase/entry/start_of_a_new_era&quot;&gt;https://blogs.apache.org/hbase/entry/start_of_a_new_era&lt;/a&gt; - introduction to HBase versioning scheme&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/hbase/&quot;&gt;https://blogs.apache.org/hbase/&lt;/a&gt; - Apache HBase Blog&lt;/li&gt; &lt;li&gt;HBase release announcements only appear to be available via the Apache announcements mailing list&lt;/li&gt; &lt;/ul&gt; </content><published>2017-01-06T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-flume/</id><title>Apache Flume</title><link href="http://ondataengineering.net/technologies/apache-flume/" rel="alternate" type="text/html" title="Apache Flume" /> <updated>2017-01-06T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-flume/"> &lt;p&gt;Specialist technology for the continuous movement of data using a set of independent agents connected together into pipelines. Supports a wide range of sources, targets and buffers (channels), along with the ability to chain agents together and to modify and drop events in-flight. Designed to be highly reliable, and to support reconfiguration without the need for a restart. Heavily integrated with the Hadoop ecosystem. An Apache project, donated by Cloudera in June 2011, graduating in June 2012, with a v1.2 release (the first considered ready for production use) in July 2012. Java based, with commercial support available as part of most Hadoop distributions.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Flume&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v1.7&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://flume.apache.org/&quot;&gt;http://flume.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://flume.apache.org/FlumeUserGuide.html&quot;&gt;http://flume.apache.org/FlumeUserGuide.html&lt;/a&gt; - user guide&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://flume.apache.org/&quot;&gt;http://flume.apache.org/&lt;/a&gt; - project updates and releases&lt;/li&gt; &lt;/ul&gt; </content><published>2017-01-06T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/blog/2017/01/06/core-hadoop-technologies/</id><title>Core Hadoop Technologies (pt1)</title><link href="http://ondataengineering.net/blog/2017/01/06/core-hadoop-technologies/" rel="alternate" type="text/html" title="Core Hadoop Technologies (pt1)" /> <updated>2017-01-06T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/blog/2017/01/06/core-hadoop-technologies/"> &lt;p&gt;And we’re back - Happy New Year!&lt;/p&gt; &lt;p&gt;Having started with the core Apache Hadoop project, we’re now going to look at the “core” technologies within the Hadoop space, based on those included in multiple distributions (many thanks to Merv Adrian from Gartner for his useful &lt;a href=&quot;http://blogs.gartner.com/merv-adrian/2016/07/30/hadoop-project-commercial-support-tracker-july-2016/&quot;&gt;tracker&lt;/a&gt;) &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;There’s three Apache technologies added to the catalogue this week - &lt;a href=&quot;/technologies/apache-flume/&quot;&gt;Flume&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hbase/&quot;&gt;HBase&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Hive&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;There’s no much to say have Flume or HBase right now, but we’ll take a more detailed look at both of these in the future. Hive however, is more interesting.&lt;/p&gt; &lt;p&gt;Firstly, it’s a hugely popular and important project that’s a corner stone of the Hadoop ecosystem, which in its short life has seen enormous change - a classic example of an Open Source technology that has mutated, evolved, consumed other projects and been pulled in multiple directions over time. I plan to dig into the history of Hive in the not too distant future as I think it’s a great example of how an Open Source project can evolve.&lt;/p&gt; &lt;p&gt;Secondly, it’s not one thing, but a collection of different components with very distinct roles all bundled together, which is why I’ve taken the decision to break it out into a number of sub-projects (&lt;a href=&quot;/technologies/apache-hive/hive-metastore/&quot;&gt;Hive Metastore&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hive/hive-server/&quot;&gt;Hive Server&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-hive/hcatalog/&quot;&gt;HCatalog&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;As before, click on the links above to see the information added to the site.&lt;/p&gt; &lt;p&gt;That’s it for this week - next up is Solr, Sqoop and Spark.&lt;/p&gt; </content> <category term="Technologies" /> <category term="Flume" /><category term="HBase" /><category term="Hive" /><category term="Apache" /><category term="HCatalog" /><category term="Hive Metastore" /><category term="Hive Server" /><published>2017-01-06T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-hadoop/yarn/</id><title>YARN</title><link href="http://ondataengineering.net/technologies/apache-hadoop/yarn/" rel="alternate" type="text/html" title="YARN" /> <updated>2016-12-16T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-hadoop/yarn/"> &lt;p&gt;Resource management and job scheduling &amp; monitoring for the Hadoop ecosystem. Includes support for capacity guarantees amongst other scheduling options. Added as an Apache Hadoop sub-project as part of Hadoop 2.x (with a GA release as part of 2.2 in October 2013) having been started in January 2008.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Yet Another Resource Negotiator&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Apache Hadoop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;December 2016&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html&quot;&gt;http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html&lt;/a&gt; - YARN architecture overview and documentation&lt;/li&gt; &lt;/ul&gt; </content><published>2016-12-16T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-hadoop/map-reduce/</id><title>MapReduce</title><link href="http://ondataengineering.net/technologies/apache-hadoop/map-reduce/" rel="alternate" type="text/html" title="MapReduce" /> <updated>2016-12-16T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-hadoop/map-reduce/"> &lt;p&gt;A data transformation and aggregation technology proven at extreme scale that works on key value pairs and consists of three transformation stages - map (a general transformation of the input key value pairs), shuffle (brings all pairs with the same key together) and reduce (an aggregation of all pairs with the same key). Part of the original Hadoop code base, becoming an Apache Hadoop sub-project in July 2009.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Apache Hadoop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;December 2016&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html&quot;&gt;http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html&lt;/a&gt; - MapReduce tutorial and documentation&lt;/li&gt; &lt;/ul&gt; </content><published>2016-12-16T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-hadoop/hdfs/</id><title>HDFS</title><link href="http://ondataengineering.net/technologies/apache-hadoop/hdfs/" rel="alternate" type="text/html" title="HDFS" /> <updated>2016-12-16T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-hadoop/hdfs/"> &lt;p&gt;A highly resilient distributed cluster file system proven at extreme scale that supports user authentication, extended ACLs, snapshots, quotas, central caching, a REST API, an NFS gateway, rolling upgrades, transparent encryption and heterogeneous storage. Part of the original Hadoop code base, becoming an Apache Hadoop sub-project in July 2009.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Hadoop Distributed File System&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Apache Hadoop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;December 2016&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html&quot;&gt;http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html&lt;/a&gt; - HDFS documentation home&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://bradhedlund.com/2011/09/10/understanding-hadoop-clusters-and-the-network/&quot;&gt;http://bradhedlund.com/2011/09/10/understanding-hadoop-clusters-and-the-network/&lt;/a&gt; - good intro the the architecture of HDFS&lt;/li&gt; &lt;/ul&gt; </content><published>2016-12-16T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/technologies/apache-hadoop/</id><title>Apache Hadoop</title><link href="http://ondataengineering.net/technologies/apache-hadoop/" rel="alternate" type="text/html" title="Apache Hadoop" /> <updated>2016-12-16T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/technologies/apache-hadoop/"> &lt;p&gt;A distributed storage and compute platform consisting of a distributed filesystem (HDFS) and a cluster workload and resource management layer (YARN), along with MapReduce, a solution built on HDFS and YARN for massive scale parallel processing of data. Has an extensive ecosystem of compatible technologies. An Apache Open Source project, started in January 2006 as a Lucene sub-project, becoming a top level project in January 2008, with a 1.0 release in December 2011 (containing HDFS and MapReduce), and a 2.2 release (the first 2.x GA release) in October 2013 (adding YARN). Very active, with a deep and broad range of contributors, and backing from multiple commercial vendors.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Hadoop&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;December 2016 - v2.7&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Sub-projects&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; Apache Hadoop&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-hadoop/hdfs/&quot;&gt;HDFS&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A highly resilient distributed cluster file system proven at extreme scale that supports user authentication, extended ACLs, snapshots, quotas, central caching, a REST API, an NFS gateway, rolling upgrades, transparent encryption and heterogeneous storage. Part of the original Hadoop code base, becoming an Apache Hadoop sub-project in July 2009.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Hadoop&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-hadoop/map-reduce/&quot;&gt;MapReduce&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A data transformation and aggregation technology proven at extreme scale that works on key value pairs and consists of three transformation stages - map (a general transformation of the input key value pairs), shuffle (brings all pairs with the same key together) and reduce (an aggregation of all pairs with the same key). Part of the original Hadoop code base, becoming an Apache Hadoop sub-project in July 2009.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Hadoop&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-hadoop/yarn/&quot;&gt;YARN&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Resource management and job scheduling &amp; monitoring for the Hadoop ecosystem. Includes support for capacity guarantees amongst other scheduling options. Added as an Apache Hadoop sub-project as part of Hadoop 2.x (with a GA release as part of 2.2 in October 2013) having been started in January 2008.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://hadoop.apache.org/&quot;&gt;http://hadoop.apache.org/&lt;/a&gt; - Project Homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://hadoop.apache.org/releases.html&quot;&gt;http://hadoop.apache.org/releases.html&lt;/a&gt; - full release history&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://medium.com/@markobonaci/the-history-of-hadoop-68984a11704&quot;&gt;https://medium.com/@markobonaci/the-history-of-hadoop-68984a11704&lt;/a&gt; - history of Hadoop&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blogs.gartner.com/merv-adrian/2016/07/30/hadoop-project-commercial-support-tracker-july-2016/&quot;&gt;http://blogs.gartner.com/merv-adrian/2016/07/30/hadoop-project-commercial-support-tracker-july-2016/&lt;/a&gt; - summary of the technologies in the common Hadoop distributions&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://hadoop.apache.org/&quot;&gt;http://hadoop.apache.org/&lt;/a&gt; - project updates and releases&lt;/li&gt; &lt;/ul&gt; </content><published>2016-12-16T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/tech-vendors/apache/</id><title>The Apache Software Foundation</title><link href="http://ondataengineering.net/tech-vendors/apache/" rel="alternate" type="text/html" title="The Apache Software Foundation" /> <updated>2016-12-16T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/tech-vendors/apache/"> &lt;p&gt;The Apache Software Foundation is a non-profit organisation that supports a wide range of open source projects, including providing and mandating a standard governance model (including the use of the Apache license), holding all trademarks for project names and logos, and providing legal protection to developers. It was founded in 1999 and now oversees nearly 200 projects.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Vendor Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Apache&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Vendor Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-apex/&quot;&gt;Apache Apex&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Data transformation engine based on Directed Acyclic Graph (DAG) flows configured through a Java API or via JSON, with a stated focus on performance, code re-use, testability and ease of operations. Runs over YARN and HDFS with native support for both micro-batch streaming and batch uses cases, and includes a range of standard operators and connectors (called Apex Malhar). An Apache project, graduating in April 2016, having been originally donated in August 2015 by DataTorrent from their DataTorrent RTS product which launched in June 2014. Java based, with development lead by DataTorrent who distribute it as DataTorrent RTS in two editions - a Community Edition (which also includes a basic management GUI and a tool for configuring Apex for data ingestion), and an Enterprise Edition (which further includes a graphical transformation editor, a self service dashboard, security integration and commercial support, and is also available as a cloud offering).&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-avro/&quot;&gt;Apache Avro&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Data serialisation framework that supports both messaging and data storage. Primarily uses a compact binary format but also supports a JSON format. Supports a range of data structures (including records, enumerations, arrays and maps) with APIs for a wide range of both static and dynamically typed languages. Schema based, with schemas primarily specified in JSON, and support for both code generation from schema definitions as well as dynamic runtime usage. Schemas are serialised alongside data, with support for automatic schema resolution if the schema used to read the data differs from that used to write it. Started as an Hadoop sub-project by Cloudera in April 2009, with an initial v1.0 release in July 2009, before becoming a top level Apache project in May 2010. Has seen significant adoption in the Hadoop ecosystem.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt; &lt;/td&gt; &lt;td&gt;An Apache open source distribution of Hadoop. Packages up a number of Apache Hadoop components, certifies their interoperability using an automated integration test suite, and packages them up as RPMs/DEBs packages for most flavours of Linux. Also includes virtual machine images and vagrant, docker and puppet recipes for deploying and working with Hadoop. Does not patch projects for distribution, but requires any fixes to be made upstream. An Apache Open Source project, started by Cloudera, donated to the Apache foundation in June 2011, graduating in September 2012, with a 1.0 release in August 2015 based on Hadoop 2.6. Since donating the project, Cloudera have backed away from it, with the project lead moving to Pivotal in December 2013. Now has a broad range of contributors, however usage by the major distributors is not clear.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-crunch/&quot;&gt;Apache Crunch&lt;/a&gt; &lt;/td&gt; &lt;td&gt;An abstraction layer over MapReduce (and now Spark) that provides a high level Java API for creating data transformation pipelines, originally designed to make working with MapReduce easier based on the Google FlumeJava paper. Also includes connectors for HBase, Hive and Kafka, Java 8 lambda support, an experimental Scala wrapper for the API (Scrunch), and support for in memory pipelines and helper classes to support testing. Open sourced by Cloudera in October 2011, donated to the Apache Foundation in May 2012, before graduating in February 2013. Support for Spark was added as part of v0.10 in June 2014. Still being maintained, and appears to have had been adopted at a number of large companies, but with limited new development.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-datafu/&quot;&gt;Apache DataFu&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A set of libraries for working with data in Hadoop. Consists of two sub-projects - DataFu Pig (a set of Pig User Defined Functions) and DataFu Hourglass (a framework for incremental processing using MapReduce). Originally created at LinkedIn, with the Pig UDFs being open sourced in January 2012 as DataFu, with a v1.0 release in September 2013. Split into sub-projects in October 2013 when LinkedIn open sourced DataFu Hourglass and added it to the project. Donated to the Apache Foundation in January 2014, however is still incubating and has not yet graduated. Last release was v1.3 in November 2015 (albeit with a very minor v1.3.1 release in August 2016), with little development activity since this time.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-flume/&quot;&gt;Apache Flume&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Specialist technology for the continuous movement of data using a set of independent agents connected together into pipelines. Supports a wide range of sources, targets and buffers (channels), along with the ability to chain agents together and to modify and drop events in-flight. Designed to be highly reliable, and to support reconfiguration without the need for a restart. Heavily integrated with the Hadoop ecosystem. An Apache project, donated by Cloudera in June 2011, graduating in June 2012, with a v1.2 release (the first considered ready for production use) in July 2012. Java based, with commercial support available as part of most Hadoop distributions.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-giraph/&quot;&gt;Apache Giraph&lt;/a&gt; &lt;/td&gt; &lt;td&gt;An iterative, highly scalable graph processing system built on top of MapReduce and based on Pregel, with a number of features added including a framework for creating re-usable code (called blocks). An Apache project, graduating in May 2012, having been originally donated by Yahoo in August 2011. Java based, no commercial support available, but is mature and has been adopted by a number of companies (including LinkedIn and most famously Facebook who scaled it to process a trillion edges), and has a number of active developers.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-hadoop/&quot;&gt;Apache Hadoop&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A distributed storage and compute platform consisting of a distributed filesystem (HDFS) and a cluster workload and resource management layer (YARN), along with MapReduce, a solution built on HDFS and YARN for massive scale parallel processing of data. Has an extensive ecosystem of compatible technologies. An Apache Open Source project, started in January 2006 as a Lucene sub-project, becoming a top level project in January 2008, with a 1.0 release in December 2011 (containing HDFS and MapReduce), and a 2.2 release (the first 2.x GA release) in October 2013 (adding YARN). Very active, with a deep and broad range of contributors, and backing from multiple commercial vendors.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-hama/&quot;&gt;Apache Hama&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A general purpose BSP (Bulk Synchronous Parallel) processing engine inspired by Pregel and DistBelief that runs over Mesos or YARN. Supports BSP, graph computing and machine learning programming models, as well as Apache MRQL. An Apache project, donated in 2008, and graduated in 2012. Java based, with no commercial support available, limited case studies for it's use and limited active developers, with the last release being in June 2015.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-hbase/&quot;&gt;Apache HBase&lt;/a&gt; &lt;/td&gt; &lt;td&gt;NoSQL wide-column datastore based on Google BigTable. Focuses on random real-time access to data, and supports horizontal scalability, consistent reads and writes, versioning and fine grained security controls. Runs on Hadoop and HDFS, and is heavily integrated with the Hadoop ecosystem. An Apache project, first released as part of Hadoop 0.15 in October 2007 before graduating as a top level project in May 2010. Java based, with commercial support available as part of most Hadoop distributions.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-hive/&quot;&gt;Apache Hive&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Technology that supports the exposure of data in Hadoop as structured tables and the execution of analytical SQL queries over these. Consists of a number of distinct components (that we treat as sub-projects) including Hive Metastore (stores the definitions of the structured tables), Hive Server (supports the execution of analytical SQL queries as MapReduce, Spark or Tez jobs) and HCatalog (allows MapReduce and Pig jobs to read and write Hive tables). First released by Facebook as an Hadoop contrib module in September 2008, becoming an Hadoop sub-project in November 2008, and a top level Apache project in September 2010, following a first official stable release (0.3) in April 2009. Java based, under active development from a number of large commercial sponsors, with commercial support available as part of most Hadoop distributions.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-ignite/&quot;&gt;Apache Ignite&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A distributed in-memory data fabric/grid, supporting a number of use cases including a key value store (with SQL support), real time stream/event processing engine, arbitrary compute, long running service management, an in-memory HDFS compatible file system for acceleration of Hadoop jobs, and in-memory shared Spark RDDs. An Apache project, graduating in September 2015, having been originally donated by GridGain from their In-Memory Data Fabric product launched in 2007. Java based, with development lead by GridGain who also supply commercial support (as GridGain Professional with ongoing Q&amp;A and bug fixes before they're included in Ignite) along with GridGain Enterprise (which includes extra features such as a management GUI, enterprise security and rolling upgrades).&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Technology for buffering and storing real-time streams of data between publishers to subscribers, with a focus on high throughput at low latency. Based on a distributed, horizontally scalable architecture, with messages organised into topics which are partitioned and replicated across nodes to provide resilience and written to disk to provide persistence. Topics may have multiple publishers and subscribers, with ability to do fault tolerant reads and to load balance across subscribers. Records consist of a key, value and timestamp, with the ability to compact topics to remove updates and deletes by key. Supports a full security model, and the ability to set quotas. Comes with a Java client, but clients for a wide range of languages are also available. Has two sub-projects (Kafka Connect and Kafka Streams) that are bundled with the main product. Originally developed at LinkedIn, being open sourced in January 2011, before being donated to the Apache Foundation in July 2011. Graduated in October 2012, and although it has not had a v1.0 release is considered production quality and stable. Development is primarily led by Confluent (which was founded by the team that built Kafka at LinkedIn), who distribute a Confluent Open Source product (which includes further clients and connectors) and a subscription based Confluent Enterprise product (which includes management, replication and data balancing features and commercial support under a subscription licence). Commercial support is also available from most Hadoop vendors.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-mahout/&quot;&gt;Apache Mahout&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Machine learning technology comprising of a Scala based linear algebra engine (codenamed Samsara) with an R-like DSL/API that runs over Spark (with experimental support for H2O and Flink), an optimiser, a wide variety of pre-made algorithms, and a Scala REPL (based on Spark Shell) for interactive execution. Can be embedded and integrated within larger applications, for example MLlib when running over Spark. Also includes some original, now deprecated, algorithms implemented over MapReduce. Created in January 2008 as a Lucene sub-project, becoming a top level Apache project in April 2010. The original MapReduce algorithms were deprecated and Samsara introduced as part of v0.10 in April 2015. Supported by most major Hadoop distributions, and still under active development.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-oozie/&quot;&gt;Apache Oozie&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Technology for managing workflows of jobs on Hadoop clusters. Primary concepts include workflows (a sequence of jobs modelled as a directed acyclic graph), coordinators (schedule the execution of workflows based on the time or the presence of data) and bundles (collections of coordinators), with all configuration specified in XML. Supports a range of technologies, including MapReduce, Pig, Hive, Sqoop, Spark, Java executables and shell scripts. Includes a server component, a metadata database for holding definitions and state (with support for a range of database technologies), a command line interface and a read only web interface for viewing the status of jobs. Also supports the parameterisation of workflows, the modelling of datasets (and the use of these to manage dependencies between workflows within coordinators), automatic retry and failure handling, and the ability to send job status notifications via HTTP or JMS. Open sourced by Yahoo in June 2010. Donated to the Apache Foundation in July 2011, graduating in August 2012. Commercial support available as part of most Hadoop distributions&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-parquet/&quot;&gt;Apache Parquet&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Data serialisation framework that supports a columnar storage format to enable efficient querying of data. Built using Apache Thrift, and supports complex nested data structures, using techniques from the Google Dremel paper. Consists of three sub-projects, the specification and Thrift definitions (Parquet Format), the Java and Hadoop libraries (Parquet MR) and the C++ implementation (Parquet CPP). Created as a joint effort between Twitter and Cloudera based on work started as part of Avro Trevni, with an initial v1.0 release in July 2013. Donated to the Apache Foundation in May 2014, graduating in April 2015. Has seen significant adoption in the Hadoop ecosystem.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-phoenix/&quot;&gt;Apache Phoenix&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A SQL query engine over Apache HBase tables that supports a subset of SQL 92 (including joins), and comes with a JDBC driver. Supports a range of features including ACID transactions (via Apache Tephra), user defined functions, secondary indexes, atomic upserts, views, multi tenancy tables (where each user or tenant can only see their data) and dynamic columns (which are only specified at query time). Supports a range of SQL DDL commands, creating and modifying underlying HBase tables as required, or can run over existing HBase tables in a read only mode. Comes with connectors to allow Spark, Hive, Pig, Flume and MapReduce to read and write Phoenix tables, and a number of utilities, including a bulk loader and a command line SQL tool. Open sourced by SalesForce in January 2013 at v1.0, donated to the Apache foundation in December 2013, before graduating in May 2014. Commercial support available through Hortonworks as part of HDP, with Cloudera making it available via Cloudera Labs without support. Active project with a range of contributors, including many from SalesForce and Hortonworks.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-pig/&quot;&gt;Apache Pig&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Technology for running analytical and data processing jobs against data in Hadoop. Jobs are written in Pig Latin (a custom procedural language that can be extended using user defined functions in a range of languages), which is then translated into Map Reduce or Tez (with Spark in development) for execution. Supports both a batch mode for running pre-defined scripts and an interactive mode, and connectors for reading and writing to HBase and Accumulo as well as HDFS. Originally developed at Yahoo in 2006 before being donated to the Apache Foundation in October 2007. Graduated as an Hadoop sub-project in October 2008, before becoming a top level project in September 2010. Although has not had a v1.0 release, has been production quality for many years. Commercial support available as part of most Hadoop distributions&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-solr/&quot;&gt;Apache Solr&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A search server built on Apache Lucene with a REST-like API for loading and searching data. Supports a distributed deployment (SolrCloud) that can run over HDFS on an Hadoop cluster. Includes an administration web interface, an extensible plugin architecture, support for schemaless indexing, faceted, grouped and clustered results, hit highlighting, geo-spacial and graph searches, near real time indexing and searching, (experimental) streaming expressions for parallel compute (including support for MapReduce and SQL) and broad authentication and security capabilities. A sub-project of the Apache Lucene project, originally donated to the Apache foundation by CNET Networks in January 2006, graduating as a top level project in January 2007, before merging with the Lucene project in March 2010. Java based, with commercial support available as part of most Hadoop distributions (although this is bundled as Cloudera Search with CDH and HDP Search with HDP), as well as from Lucidworks.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A high performance general purpose distributed data processing engine based on directed acyclic graphs that primarily runs in memory, but can spill to disk if required, and which supports processing applications written in Java, Scala, Python and R. Includes a number of sub-projects that support more specialised analytics including Spark SQL, Spark Streaming, MLlib (machine learning) and GraphX (graph analytics). Requires a cluster manager (YARN, EC2 and Mesos are supported as well as standalone clusters) and can access data in a wide range of technologies (including HDFS, other Hadoop data sources, relational databases and NoSQL databases). An Apache project, originally started at UC Berkley in 2009, open sourced in 2010, and donated to the Apache foundation in June 2013, graduating in February 2014. v1.0 was released in May 2014, with a v2.0 release in July 2016. Java based, with development led by Databricks (who sell a Spark hosted service), and with commercial support available as part of most Hadoop distributions.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-sqoop/&quot;&gt;Apache Sqoop&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Specialist technology for moving bulk data between Hadoop and structured (relational) databases. Command line based, with the ability to import and export data between a range of databases (including mainframe partitioned datasets) and HDFS, Hive, HBase and Accumulo. Supports parallel partitioned unloads, writing to Avro, Sequence File, Parquet and text files, incremental imports and saved jobs that can be shared via a simple metadata store. An Apache project, started in May 2009 as an Hadoop contrib module, migrating to a Cloudera GitHub project in April 2010 (with a v1.0 release shortly after), before being donated to the Apache foundation in June 2011, graduating in March 2012. The last major release (v1.4) was in November 2011, with only minor releases since then. However in January 2012 a significant re-write was announced as part of a proposed v2.0 release to address a number of usability, security and architectural issues. This will introduce a new Sqoop Server and Metadata Repository, supporting both a CLI and web UI, centralising job definitions, database connections and credentials, as well as enabling support for a wider range of connectors including NoSQL databases, Kafka and (S)FTP folders. Java based, with commercial support available as part of most Hadoop distributions.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-tajo/&quot;&gt;Apache Tajo&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Distributed analytical database engine. Supports HDFS, Amazon S3, Google Cloud Storage, OpenStack Swift and local storage, and querying over Postgres, HBase and Hive tables. Provides a standard SQL interface, JDBC driver, and supports partitioning, compression and indexing (currently experimental). An Apache project, donated by Gruter in March 2013, and graduated in April 2014. Java based, with development lead by Gruter who also supply commercial support, a Tajo managed service, a data analytics hub (Qrytica) built on Tajo, and a Tajo Data Warehouse appliance.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;!-- Technology metadata --&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.apache.org/&quot;&gt;https://www.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.apache.org/foundation/how-it-works.html&quot;&gt;https://www.apache.org/foundation/how-it-works.html&lt;/a&gt; - information on the foundation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://apache.org/foundation/mailinglists.html#foundation-announce&quot;&gt;http://apache.org/foundation/mailinglists.html#foundation-announce&lt;/a&gt; - the Apache Foundation announcements mailing list&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/&quot;&gt;https://blogs.apache.org/&lt;/a&gt;; &lt;a href=&quot;https://blogs.apache.org/planet/feed/entries/rss&quot;&gt;https://blogs.apache.org/planet/feed/entries/rss&lt;/a&gt; - The set of Apache Foundation blogs&lt;/li&gt; &lt;/ul&gt; </content><published>2016-12-16T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/blog/2016/12/16/apache-hadoop/</id><title>Apache Hadoop</title><link href="http://ondataengineering.net/blog/2016/12/16/apache-hadoop/" rel="alternate" type="text/html" title="Apache Hadoop" /> <updated>2016-12-16T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/blog/2016/12/16/apache-hadoop/"> &lt;p&gt;And so we begin our journey through the jungle of Data Engineering technologies by looking at the technology du jour - Apache Hadoop. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;There’s an entire ecosystem here that we’ll start to explore by looking at the major Hadoop vendors, but the first entries in our technology catalogue are the &lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Apache Hadoop&lt;/a&gt; project itself, along with its sub-projects: &lt;a href=&quot;/technologies/apache-hadoop/yarn/&quot;&gt;YARN&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hadoop/hdfs/&quot;&gt;HDFS&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-hadoop/map-reduce/&quot;&gt;MapReduce&lt;/a&gt;. Click on the links to view the technology information I’ve added to the site.&lt;/p&gt; &lt;p&gt;This also brings our first technology vendor - the &lt;a href=&quot;/tech-vendors/apache/&quot;&gt;Apache Software Foundation&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;And with that it’s time for Christmas! We’ll be back in three weeks with the first of the core technologies within the Hadoop space.&lt;/p&gt; </content> <category term="Technologies" /> <category term="Hadoop" /><category term="HDFS" /><category term="YARN" /><category term="MapReduce" /><category term="Apache" /><published>2016-12-16T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/blog/2016/12/14/the-technology-catalogue/</id><title>The Technology Catalogue</title><link href="http://ondataengineering.net/blog/2016/12/14/the-technology-catalogue/" rel="alternate" type="text/html" title="The Technology Catalogue" /> <updated>2016-12-14T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/blog/2016/12/14/the-technology-catalogue/"> &lt;p&gt;The first step in this journey is going to be creation of a catalogue of the technologies that are going to be of interest to us as we explore the world of data engineering. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;The aim is to provide a valuable reference for anyone starting a technology evaluation to understand what technology options they might have for a given situation, or to understand if and how a technology might fit into an existing ecosystem.&lt;/p&gt; &lt;p&gt;For each technology, the plan is therefore to provide a short summary describing the technology, along with its background and current status. As mentioned in my previous post, for some technologies, I also want to do a deep dive to provide a longer summary with more detail that gives a solid introduction to the technology, and my hope is that we’ll get contributions to provide these for the vast majority of the technologies that I won’t get time to look at.&lt;/p&gt; &lt;p&gt;As part of this we’ll need to look at providing a categorisation of technologies, although making this useful is going to be challenging given that multiple categories of technologies could be used to meet a given use case. The technologies we’ll look at broadly fall into three groups however:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Data Transformation tools, be they one of the established commercial products or a new Open Source technology&lt;/li&gt; &lt;li&gt;Data Platforms, be they a traditional relational database, an Hadoop based data platform, a real time broker such as Kafka, or a NoSQL data platform&lt;/li&gt; &lt;li&gt;Technologies that address the other supporting capabilities around these, such as data catalogues or metadata management&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The plan is to start by looking at the key vendors in the Data Engineering space including:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The pure play Hadoop distributions - Apache Big Top, Hortonworks, Cloudera and MapR&lt;/li&gt; &lt;li&gt;The rest of the Apache Data Engineering ecosystem&lt;/li&gt; &lt;li&gt;The major Cloud vendors - Amazon, Google and Azure&lt;/li&gt; &lt;li&gt;The big multi play vendors - IBM, Oracle, Teradata, Microsoft, Pivotal, SAP and SAS&lt;/li&gt; &lt;li&gt;The big specialist commercial data integration vendors - Ab Initio and Informatica&lt;/li&gt; &lt;li&gt;Other commercial data integration and data platform vendors - perhaps based at least partially on looking at the latest Gartner and Forrester reports&lt;/li&gt; &lt;li&gt;The major open source cloud scale companies, if only to see what they do in this space - Facebook, Netflix, LinkedIn, Google, Yahoo, eBay and Twitter&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This will only be a start on the technologies I’d expect to see in our catalogue however, so once this is done the plan is to then go through by technology category.&lt;/p&gt; &lt;p&gt;This is obviously going to take some time given the vast range of technologies, so if you’re interested in contributing in whatever form, if you spot any issues or omissions, or if you have any comments you’d like to add, then please do share your thoughts.&lt;/p&gt; </content> <category term="Site" /><published>2016-12-14T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/blog/2016/12/12/the-plan/</id><title>The Plan</title><link href="http://ondataengineering.net/blog/2016/12/12/the-plan/" rel="alternate" type="text/html" title="The Plan" /> <updated>2016-12-12T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/blog/2016/12/12/the-plan/"> &lt;p&gt;One more post before we get started.&lt;/p&gt; &lt;p&gt;The following are my current thoughts for some of the topics I’d like to cover on this site, both as a reference for my future self to look back at my naive optimism, but also if anyone wants to start contributing to any of these now, or to start a discussion on any the later topics to start framing and exploring them. &lt;!--more--&gt;&lt;/p&gt; &lt;h2 id=&quot;theme-1---the-technology-catalogue&quot;&gt;Theme 1 - the technology catalogue&lt;/h2&gt; &lt;p&gt;The plan here is to start building up a technology catalogue by looking at the key vendors in the Data Engineering space. This will only be a start on the technologies I’d expect to see in our catalogue however, so once this is done the plan is to then go through by technology category to complete the catalogue.&lt;/p&gt; &lt;p&gt;I’d also like to look at providing a concise yet detailed introduction to some technologies that describes exactly what it is, how it works, and what the key features are. So much material that can be found on the internet is marketing material that glosses over the information I’m interested in knowing to understand whether a technology might meet my use cases and integrate into my environment, and my hope is that I can use this site to address that.&lt;/p&gt; &lt;h2 id=&quot;theme-2---data-engineering-use-cases&quot;&gt;Theme 2 - data engineering use cases&lt;/h2&gt; &lt;p&gt;One thing I don’t want to do on this site is define another data ecosystem architecture - there are too many already, most of them are designed to sell specific technologies, and none of them will fit the range of different requirements and constraints that different organisations will have.&lt;/p&gt; &lt;p&gt;However, what I do want to do is look at the range of different of different use cases that you might use data engineering technologies for, from a Data Lake (and we’ll look at what that overloaded term actually means) to a Data Warehouse (and why they’re still relevant), from the acquisition of data to the preparation of a Query Focused Dataset, and from the management of a data catalogue to the monitoring of data quality metrics.&lt;/p&gt; &lt;p&gt;I’d then like to look at how different technologies and architectural patterns can support these use cases - how do you implement a Data Lake using Hadoop, what technologies support data governance and data catalogues, and how do the various streaming frameworks compare.&lt;/p&gt; &lt;p&gt;As part of this I also want to look at the core principles behind Data Transformation, what state of the art in this space looks like, and how the established enterprise technologies compare to the new Open Source upstarts.&lt;/p&gt; &lt;h2 id=&quot;theme-3---delivery&quot;&gt;Theme 3 - delivery&lt;/h2&gt; &lt;p&gt;As if the above isn’t already massively ambitious enough, I’d also like to talk about the delivery of data solutions, including:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How we can use best practice delivery concepts (e.g. configuration management, continuous integration and testing, automated deployment, infrastructure and database management) and what these mean within a data solution&lt;/li&gt; &lt;li&gt;How we can bring some the new best practices from Lean and Agile into the data space, and what data transformation tools need to do in order to be able to support this&lt;/li&gt; &lt;li&gt;Why data projects can have a reputation for late delivery, cost overruns, poor quality data and a high cost of change, and what can be done about this&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I think that will more than do us. Getting through that lot will take some time, but with help and contributions I think this site could be hugely valuable.&lt;/p&gt; </content> <category term="Site" /><published>2016-12-12T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/blog/2016/12/09/big-data/</id><title>Big Data</title><link href="http://ondataengineering.net/blog/2016/12/09/big-data/" rel="alternate" type="text/html" title="Big Data" /> <updated>2016-12-09T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/blog/2016/12/09/big-data/"> &lt;p&gt;Before we get stuck in, a short digression to talk about Big Data. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;There are many different definitions of Big Data, but for arguments sake let’s say this refers to the exploitation of data that would have previously been uneconomical due to the volume of data, the structure or format of the data (e.g. unstructured, semi-structured, or complex file formats such as video and audio) and the types of analytics required (e.g. path, graph or time series analysis)&lt;/p&gt; &lt;p&gt;I’ve a couple of comments to make on this topic.&lt;/p&gt; &lt;p&gt;Firstly, in my (humble) option Big Data is a marketing term (supported by new technologies - primarily Hadoop) that’s been exploited to sell these technologies and to give the industry something to talk about. However, I think this has been a broadly positive thing, in that it’s brought data analytics to the mainstream, spurred uptake of new technologies and encouraged companies to invest in analytics and data processing that they may not have done previously. In any case it’s probably just about run its course now (as demonstrated as it’s fall down the far side of the hype curve), and has definitely resulted in the devaluation of other (perhaps more traditional) analytical capabilities which still have a role to play and in many cases deliver capabilities that Big Data technologies can’t yet match.&lt;/p&gt; &lt;p&gt;Secondly, I think there’s been a lot of misinformation about Big Data and Big Data technologies. It’s not a replacement for existing BI/MI and analytical capabilities, and in fact needs to coexist and integrate with these in order to deliver on its promises. It’s not always cheaper or more performant than existing technologies, and won’t always reduce the timescales and costs for analytics or data exploration. And it’s not a new or innovative technology - I know of companies that were analysing multi-petabyte data stores and doing real time analytics over ten years ago, parallel distributed file systems have been around for a lot longer than that, and there are many established technologies that have data processing capabilities that the new technologies are only just starting to catch up to.&lt;/p&gt; &lt;p&gt;In terms of this site the plan is to look at the wider picture and take an holistic view of data transformation and exploitation. I’ve therefore no plans to talk explicitly about Big Data, but in looking at the wider picture we will absolutely cover everything relating to it (the technologies and the new use cases these enable) alongside coverage of other new technologies, more established technologies and capabilities, and the interesting intersection between them all.&lt;/p&gt; </content> <category term="Site" /><published>2016-12-09T00:00:00+00:00</published> </entry> <entry> <id>http://ondataengineering.net/blog/2016/12/07/welcome/</id><title>Welcome</title><link href="http://ondataengineering.net/blog/2016/12/07/welcome/" rel="alternate" type="text/html" title="Welcome" /> <updated>2016-12-07T00:00:00+00:00</updated> <author> <name>Peter</name> <uri>http://ondataengineering.net/</uri> </author> <content type="html" xml:base="http://ondataengineering.net/blog/2016/12/07/welcome/"> &lt;p&gt;For me, one of the biggest challenges in exploiting data (be that through reporting, big data analytics, machine learning or any one of a dozen similar capabilities) is making sure you have the right data in the right place at the right time to allow you to do this efficiently. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;For example, &lt;a href=&quot;http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html&quot;&gt;this&lt;/a&gt; article from the New York Times and &lt;a href=&quot;http://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/&quot;&gt;this&lt;/a&gt; more recent one from Forbes talks about how the analysis of big data promises unique business insights, but for big-data scientists there is significant manual ‘janitor work’ (up to 80% of their time) required to prepare data, and although the research is sponsored by Data Wrangling tool vendors, the conclusions will resonate with many data scientists. Combine this with the historical cost, delivery speed and agility issues typically associated with delivery data warehouse or reporting solutions, and for me it’s never been clearer that we need to get smarter at how we prepare and manage data.&lt;/p&gt; &lt;p&gt;Part of the solution to this is better Data Engineering, ensuring the processes, tools, technologies, data platforms, regular data feeds and their data preparation jobs are in place to allow the data to be exploited in an efficient, reliable and repeatable way. The aim of this site is therefore to try to offer independent, critical and technical thinking on the technologies, architectural patterns and delivery capabilities that can help address this.&lt;/p&gt; &lt;p&gt;My hope is that this becomes a community owned and authored site of trusted reference material on these topics. To that end, all the content on this site is licensed under the Creative Commons Attribution 4.0 International License and hosted in a public GitHub repository, and there are a set of Discourse forums for discussions. Details of how to contribute and get involved can be found on every page.&lt;/p&gt; </content> <category term="Site" /><published>2016-12-07T00:00:00+00:00</published> </entry> </feed>
