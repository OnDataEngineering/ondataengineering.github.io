<?xml version="1.0" encoding="UTF-8"?> <rss version="2.0"> <channel><title>OnDataEngineering</title><description>A collaborative site for independent, critical and technical thinking on the use cases, architectural patterns and technologies relating to the transformation and preparation of data for exploitation.</description><link>https://ondataengineering.net/</link><item><title>Content License</title><link>https://ondataengineering.net/site/content-license/</link><description>&lt;p&gt;The content of the OnDataEngineering.net site is copyright the relevant author and is licensed under a &lt;a href=&quot;http://creativecommons.org/licenses/by/4.0/&quot;&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Unless otherwise stated or granted, any code snippets or samples included in the content of the OnDataEngineering.net site are copyright their respective authors and are licensed under the &lt;a href=&quot;http://mit-license.org/&quot;&gt;MIT Licence&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The content of the OnDataEngineering.net site is defined as the main body of the pages in the &lt;a href=&quot;https://ondataengineering.net/technologies/&quot;&gt;Technologies&lt;/a&gt;, &lt;a href=&quot;/tech-categories/&quot;&gt;Tech Categories&lt;/a&gt;, &lt;a href=&quot;/tech-vendors/&quot;&gt;Tech Vendors&lt;/a&gt; and &lt;a href=&quot;https://ondataengineering.net/blog/&quot;&gt;Blog&lt;/a&gt; sections of the site.&lt;/p&gt; &lt;p&gt;For clarity, the CC BY 4.0 licence means that this content can be used for any commercial or non commercial purpose, as long as OnDataEngineering.net is clearly credited as the source of the information. Please see the &lt;a href=&quot;http://creativecommons.org/licenses/by/4.0/&quot;&gt;CC BY 4.0 licence summary&lt;/a&gt; for further details.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Contributing</title><link>https://ondataengineering.net/site/contributing/</link><description>&lt;p&gt;&lt;a href=&quot;https://ondataengineering.net&quot;&gt;OnDataEngineering&lt;/a&gt; is an open collaborative site, and lives on your contributions and participation in the community around the site. So welcome, it’s good to see you - can we suggest that a good place to start is the &lt;a href=&quot;/site/&quot;&gt;site information&lt;/a&gt; for more details on who we are and what we’re trying to do.&lt;/p&gt; &lt;p&gt;And before you go any further, can we ask you to sign up to our &lt;a href=&quot;http://discourse.ondataengineering.net&quot;&gt;Discourse forums&lt;/a&gt;. You’ll get updates on changes to the site, notifications of popular discussions, and can use this to discuss anything relating to this site including how you can help and contribute. While you’re there, please take a read of the &lt;a href=&quot;http://discourse.ondataengineering.net/t/welcome-to-discourse&quot;&gt;welcome post&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Once you’ve done that, we’d welcome your help in building out the content on this site and making it a fantastic resource. All submission of content is welcomed, and we pride ourselves on being friendly and welcoming place if you want to get involved. The following are a number of ways you can contribute:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;Discuss the contents of any page or blog post by clicking on the discuss button next to the page title or the link in the sidebar. Your thoughts, comments and feedback on what’s written on this site will make it’s content better.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;If you see typos, errors or omissions, or have information to add, edit the contents of any page by clicking on the edit button next to the page title or the link in the sidebar. This is literally a 30 second process for submitting changes, and will help make the contents of this site better. These edit links will take you to a GitHub edit page (you’ll need to login or create a login to GitHub first) - make your changes, fill in a description of your change in the form at the bottom of the page and click “Propose the change”, and then on the next screen click “Create pull request” twice and you’re done.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;As well as editing pages, you can also create new pages by clicking on the create button next to the page title or the link in the sidebar on the main index pages (for example &lt;a href=&quot;/technologies&quot;&gt;here&lt;/a&gt;). The process is exactly the same as editing a page.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;If you have any industry news you think we should be aware of then please send it to us as &lt;a href=&quot;mailto:news@ondataengineering.net&quot;&gt;news@ondataengineering.net&lt;/a&gt;. We follow the blogs and web pages listed in the News sections on each technology page, but there’s always a chance we’ll miss stuff, and we may not hear about technologies not yet listed on the site.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;With blog posts, feel free to correct typos or mistakes. If you have a blog post you’d like us to host please feel free to send us one via a pull request, however it’s worth getting in touch first via e-mail or the discussion forums, and we reserve the right to only take blog posts we feel are a good fit (posts trying to push specific technologies or copies of posts from elsewhere are generally not). Blog posts should be thought pieces; if you have factual information that fits elsewhere in the site please contribute it there.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;If the GitHub create and edit pages are a bit limited for you, using git and your favourite text editor is surprisingly straightforward. The &lt;a href=&quot;https://guides.github.com/&quot;&gt;GitHub guides&lt;/a&gt; (especially the Hello World and Forking Projects ones) and the &lt;a href=&quot;http://help.github.com/&quot;&gt;GitHub help pages&lt;/a&gt; are great places to start, and we use Visual Studio Code (which is completely free and has great git integration) with the markdownlint and SpellChcker plugins.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;All the content of this site is written in &lt;a href=&quot;https://en.wikipedia.org/wiki/Markdown&quot;&gt;Markdown&lt;/a&gt;. If you’re new to it, &lt;a href=&quot;http://www.markdowntutorial.com/&quot;&gt;Mardown Tutorial&lt;/a&gt; is a great place to start. We use the kramdown flavour - see the &lt;a href=&quot;http://kramdown.gettalong.org/quickref.html&quot;&gt;quick reference&lt;/a&gt; and &lt;a href=&quot;http://kramdown.gettalong.org/syntax.html&quot;&gt;syntax&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Oh, and we reserve the right to use the (correct) British spellings of words here, although we won’t object to people using new fangled alternative spellings.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hadoop Distributions Comparison</title><link>https://ondataengineering.net/tech-categories/hadoop-distributions/distribution-comparison/</link><description>&lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Component&lt;/th&gt; &lt;th&gt;Cloudera&lt;/th&gt; &lt;th&gt;HW&lt;/th&gt; &lt;th&gt;MapR&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/tech-categories/compute-cluster-managers/&quot;&gt;Compute Cluster Manager&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/yarn&quot;&gt;YARN&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/yarn&quot;&gt;YARN&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-slider&quot;&gt;Slider&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/yarn&quot;&gt;YARN&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-myriad&quot;&gt;Myriad&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/tech-categories/hadoop-compatible-filesystems&quot;&gt;Hadoop Compatible Filesystem&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/hdfs&quot;&gt;HDFS&lt;/a&gt;; &lt;a href=&quot;/technologies/recordservice&quot;&gt;RecordService&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/hdfs&quot;&gt;HDFS&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/mapr-fs&quot;&gt;MapR-FS&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;NoSQL Datastore&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hbase&quot;&gt;HBase&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-accumulo&quot;&gt;Accumulo&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hbase&quot;&gt;HBase&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-accumulo&quot;&gt;Accumulo&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/mapr-db&quot;&gt;MapR-DB&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-hbase&quot;&gt;HBase&lt;/a&gt;;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;SQL Datastore&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-kudu&quot;&gt;Kudu&lt;/a&gt; + &lt;a href=&quot;/technologies/apache-impala&quot;&gt;Impala&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-phoenix&quot;&gt;Phoenix&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-hawq&quot;&gt;Hawq&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/tech-categories/streaming-data-stores/&quot;&gt;Streaming Data Store&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-kafka&quot;&gt;Kafka&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-kafka&quot;&gt;Kafka&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/mapr-es/&quot;&gt;MapR-ES&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Batch Analytics&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hive&quot;&gt;Hive&lt;/a&gt; (on Spark); &lt;a href=&quot;/technologies/apache-hadoop/map-reduce&quot;&gt;MapReduce&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-pig&quot;&gt;Pig&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-spark&quot;&gt;Spark&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hive&quot;&gt;Hive&lt;/a&gt; (on Tez); &lt;a href=&quot;/technologies/apache-hadoop/map-reduce&quot;&gt;MapReduce&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-pig&quot;&gt;Pig&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-spark&quot;&gt;Spark&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-hawq&quot;&gt;Hawq&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hive&quot;&gt;Hive&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-hadoop/map-reduce&quot;&gt;MapReduce&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-pig&quot;&gt;Pig&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-spark&quot;&gt;Spark&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Streaming Analytics&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-spark/spark-streaming&quot;&gt;Spark Streaming&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-spark/spark-streaming&quot;&gt;Spark Streaming&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-storm&quot;&gt;Storm&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-spark/spark-streaming&quot;&gt;Spark Streaming&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-storm&quot;&gt;Storm&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/tech-categories/graph-analytics/&quot;&gt;Graph Analytics&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-spark/graphx&quot;&gt;Spark GraphX&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-spark/graphx&quot;&gt;Spark GraphX&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-spark/graphx&quot;&gt;Spark GraphX&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/tech-categories/query-engines/&quot;&gt;Query Engine&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-impala&quot;&gt;Impala&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hive&quot;&gt;Hive&lt;/a&gt; (LLAP)&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-drill&quot;&gt;Drill&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-impala&quot;&gt;Impala&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Machine Learning&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-mahout&quot;&gt;Mahout&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-spark/mllib&quot;&gt;Spark MLlib&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-mahout&quot;&gt;Mahout&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-spark/mllib&quot;&gt;Spark MLlib&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-mahout&quot;&gt;Mahout&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-spark/mllib&quot;&gt;Spark MLlib&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/tech-categories/analytical-search/&quot;&gt;Analytical Search&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-solr&quot;&gt;Solr&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-solr&quot;&gt;Solr&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-solr&quot;&gt;Solr&lt;/a&gt; (available as an add on pack)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Data Ingestion&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-sqoop&quot;&gt;Sqoop&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-flume&quot;&gt;Flume&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-sqoop&quot;&gt;Sqoop&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-flume&quot;&gt;Flume&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-sqoop&quot;&gt;Sqoop&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-flume&quot;&gt;Flume&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Data Flow Management&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-falcon&quot;&gt;Falcon&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Workflow Management&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-oozie&quot;&gt;Oozie&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-oozie&quot;&gt;Oozie&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-oozie&quot;&gt;Oozie&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Security&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-sentry&quot;&gt;Sentry&lt;/a&gt;; &lt;a href=&quot;/technologies/recordservice&quot;&gt;RecordService&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-knox&quot;&gt;Knox&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-ranger&quot;&gt;Ranger&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-sentry&quot;&gt;Sentry&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Management&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/cloudera-manager&quot;&gt;Cloudera Manager&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-ambari&quot;&gt;Ambari&lt;/a&gt;&lt;/td&gt; &lt;td&gt;MapR Installer; MapR Control System; &lt;a href=&quot;/technologies/mapr-monitoring&quot;&gt;MapR Monitoring&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Metadata&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/cloudera-navigator&quot;&gt;Cloudera Navigator&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-atlas&quot;&gt;Atlas&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Cloud Management&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/cloudera-director&quot;&gt;Cloudera Director&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/cloudbreak&quot;&gt;Cloudbreak&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;User Interfaces&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/hue&quot;&gt;Hue&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-zeppelin&quot;&gt;Zeppelin&lt;/a&gt;; &lt;a href=&quot;/technologies/hue&quot;&gt;Hue&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/hue&quot;&gt;Hue&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hadoop Ecosystem</title><link>https://ondataengineering.net/tech-categories/hadoop-distributions/ecosystem/</link><description>&lt;h2 id=&quot;hdfs-ecosystem&quot;&gt;HDFS Ecosystem&lt;/h2&gt; &lt;p&gt;&lt;img src=&quot;/images/hdfs-ecosystem.png&quot; alt=&quot;HDFS Ecosystem&quot; /&gt;&lt;/p&gt; &lt;h2 id=&quot;yarn-ecosystem&quot;&gt;YARN Ecosystem&lt;/h2&gt; &lt;p&gt;&lt;img src=&quot;/images/yarn-ecosystem.png&quot; alt=&quot;YARN Ecosystem&quot; /&gt;&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Site Information</title><link>https://ondataengineering.net/site/</link><description>&lt;p&gt;&lt;a href=&quot;https://ondataengineering.net&quot;&gt;OnDataEngineering&lt;/a&gt; is a site focused on how to make the right data available in the right form in the right place at the right time to allow users to efficiently exploit it. This covers a range of use cases (including the acquisition and staging of data, “big data” preparation and data warehousing) to support any type of exploitation (be it reporting, big data analytics, machine learning or any one of a dozen similar capabilities).&lt;/p&gt; &lt;p&gt;This site currently hosts a catalog of technologies that fit within this space, organised &lt;a href=&quot;/technologies/&quot;&gt;alphabetically&lt;/a&gt;, &lt;a href=&quot;/tech-categories/&quot;&gt;by category&lt;/a&gt; and &lt;a href=&quot;/tech-vendors/&quot;&gt;by vendor&lt;/a&gt;. We also have a &lt;a href=&quot;/blog/&quot;&gt;blog&lt;/a&gt; that covers updates to the site as well as industry news, and a set of &lt;a href=&quot;http://discourse.ondataengineering.net&quot;&gt;forums&lt;/a&gt; .&lt;/p&gt; &lt;p&gt;In the future it is planned for this site to also look at key use cases for different technologies within the Data Engineering space along with a set of independent, technical and critical evaluations of various technologies and architectural patterns against these. Please see the &lt;a href=&quot;/blog/2016/12/12/the-plan/&quot;&gt;introduction&lt;/a&gt; blog post for further details.&lt;/p&gt; &lt;p&gt;This is a community owned and authored site. All the content on this site is licensed under a Creative Commons Attribution license (see &lt;a href=&quot;/site/content-license/&quot;&gt;here&lt;/a&gt; for details) and the content of the site is hosted in a public GitHub repository &lt;a href=&quot;https://github.com/OnDataEngineering/OnDataEngineeringContent&quot;&gt;here&lt;/a&gt;, with contributions welcomed (see &lt;a href=&quot;/site/contributing/&quot;&gt;here&lt;/a&gt; for details).&lt;/p&gt; &lt;p&gt;For details on how to get new information added to the site delivered straight to you via e-mail, RSS or Twitter, please see &lt;a href=&quot;/site/subscribe&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;To get in contact with the site administrators please e-mail &lt;a href=&quot;mailto:admin@ondataengineering.net&quot;&gt;admin@ondataengineering.net&lt;/a&gt;&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Search</title><link>https://ondataengineering.net/site/search/</link><description> &lt;script language=&quot;Javascript&quot; type=&quot;text/javascript&quot;&gt; function google_search() { var query = document.getElementById(&quot;google-search&quot;).value; window.open(&quot;https://www.google.com/search?q=&quot; + query + &quot;+site:&quot; + &quot;https%3A%2F%2Fondataengineering.net%2F&quot;); } &lt;/script&gt; &lt;form id=&quot;search&quot; onsubmit=&quot;google_search(); return false;&quot;&gt; &lt;input type=&quot;text&quot; id=&quot;google-search&quot; placeholder=&quot;Enter search term and hit enter&quot; /&gt; &lt;/form&gt; &lt;noscript&gt; Search &lt;a href=&quot;https://www.google.com/search?q=site:https%3A%2F%2Fondataengineering.net%2F&quot; target=&quot;_blank&quot;&gt;Google&lt;/a&gt; for: &lt;pre&gt;&lt;code&gt;search-term site:https://ondataengineering.net/&lt;/code&gt;&lt;/pre&gt; &lt;/noscript&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Subscribe</title><link>https://ondataengineering.net/site/subscribe/</link><description>&lt;p&gt;Want to get updates to the site - both blog posts and new / changed content - delivered direct to you…&lt;/p&gt; &lt;h3 id=&quot;by-twitter&quot;&gt;By Twitter&lt;/h3&gt; &lt;p&gt;Follow us - we’re &lt;a href=&quot;https://twitter.com/OnDataEng&quot;&gt;@OnDataEng&lt;/a&gt;.&lt;/p&gt; &lt;h3 id=&quot;by-e-mail&quot;&gt;By E-mail&lt;/h3&gt; &lt;p&gt;You can sign up to our updates e-mail &lt;a href=&quot;http://eepurl.com/cyQSqv&quot;&gt;here&lt;/a&gt; (and un-subscribe &lt;a href=&quot;http://ondataengineering.us15.list-manage1.com/unsubscribe?u=2641f8b7b450d6b8685c38076&amp;amp;id=29bd4f4db6&quot;&gt;here&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;There’s generally new content on the site every weekday, and you have the option of choosing to get the update e-mail daily or weekly.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Please note - we will only ever use your e-mail address to deliver you the e-mail update; we will never share or sell this.&lt;/em&gt;&lt;/p&gt; &lt;h3 id=&quot;by-rss--atom-feed&quot;&gt;By RSS / Atom Feed&lt;/h3&gt; &lt;p&gt;Point your favourite feed reader at our &lt;a href=&quot;https://ondataengineering.net/atom.xml&quot;&gt;atom feed&lt;/a&gt;. If you don’t like atom, we have an &lt;a href=&quot;https://ondataengineering.net/feed.xml&quot;&gt;RSS feed&lt;/a&gt; as well.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Technology Categories</title><link>https://ondataengineering.net/tech-categories/</link><description>&lt;h2 id=&quot;operational-databases&quot;&gt;Operational Databases&lt;/h2&gt; &lt;p&gt;Databases that primarily support transactional use creates (creating, finding, updating and deleting records, often referred to as OLTP or online transaction processing), which underpin business or operational systems and generally support limited analytical use cases.&lt;/p&gt; &lt;p&gt;NOTE: These are of interest to us as potential sources of data, and although we have lists of these technologies on this site, they are not of primary interest and therefore we’re unlikely to have technology pages for any of the technologies that fall under these categories.&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Relational Databases&lt;/td&gt; &lt;td&gt;Databases that focus on operational (OLTP) use cases&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Key-Value Databases&lt;/td&gt; &lt;td&gt;NoSQL databases for storing data values indexed by a single key&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Document Databases&lt;/td&gt; &lt;td&gt;NoSQL databases for storing data as structured documents (e.g. JSON / XML)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/tech-categories/graph-databases/&quot;&gt;Graph Databases&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Graph databases that focus on operational use cases&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/tech-categories/rdf-databases/&quot;&gt;RDF Databases&lt;/a&gt;&lt;/td&gt; &lt;td&gt;RDF (Resource Description Framework) databases&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Multi Model Databases&lt;/td&gt; &lt;td&gt;Databases that support multiple use cases (e.g. relational, document and graph)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;analytical-databases&quot;&gt;Analytical Databases&lt;/h2&gt; &lt;p&gt;Databases that primarily support analytical use cases (aggregations, machine learning and other algorithms that run over large volumes of data, often referred to as OLAP or online analytical processing), and which provide integrated storage and query capabilities in a single technology.&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/tech-categories/analytical-databases/&quot;&gt;Analytical Databases&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Databases that focus on analytical (OLAP) use cases, including relational, graph and machine learning capabilities&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/tech-categories/analytical-search/&quot;&gt;Analytical Search&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Search technologies that also support analytical capabilities such as aggregations, graph analytics and machine learning&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/tech-categories/nosql-wide-column-stores/&quot;&gt;NoSQL Wide Column Stores&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Sparse multi-dimensional key value stores that support scan/iterate operators&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/tech-categories/time-series-databases/&quot;&gt;Time Series Databases&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Databases optimised for storing very large numbers of metrics and allowing these to be aggregated and analysed&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;hadoop&quot;&gt;Hadoop&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/tech-categories/hadoop-distributions/&quot;&gt;Hadoop Distributions&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Options for deploying an Apache Hadoop ecosystem&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Hadoop Supporting Components&lt;/td&gt; &lt;td&gt;Technologies for managing and monitoring an Apache Hadoop installation&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;data-storage&quot;&gt;Data Storage&lt;/h2&gt; &lt;p&gt;Technologies that support the storage of data, but with no (or limited) capabilities to analyse or exploit the data being stored.&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/tech-categories/hadoop-compatible-filesystems/&quot;&gt;Hadoop Compatible Filesystems&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A parallel distributed filesystem that implements the Hadoop FileSystem API and conforms to the Hadoop Compatible Filesystem specification, allowing it to be used in place of HDFS&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/tech-categories/object-stores/&quot;&gt;Object Stores&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Storage solutions whereby data is stored without any concept of folders or organisational structure, instead being referenced by a unique identifier, allowing for massively parallel and scalable solutions.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/tech-categories/streaming-data-stores/&quot;&gt;Streaming Data Stores&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Technologies for the persistent storage of continuous streams of data, with data access based on a publish/subscribe model.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/tech-categories/data-storage-formats/&quot;&gt;Data Storage Formats&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Libraries that support the storage of data on disk for data storage, real-time or batch analytics&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;query--analytics-engines&quot;&gt;Query / Analytics Engines&lt;/h2&gt; &lt;p&gt;Technologies that support the execution of queries or analytics over data in one or more external database or data storage technologies. Originally targeted at exploiting unprepared data at enormous scale, they are now starting to support capabilities that allow them to compete with analytical databases. The separation of storage and query engine provides flexibility, for example to exploit data in a much rawer state, or to exploit prepared data using multiple tools.&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/tech-categories/query-engines/&quot;&gt;Query Engines&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Engines that allow queries expressed in a high level language (often SQL) to be run over one or more underlying data stores or databases, often including Hadoop (aka SQL on Hadoop)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/tech-categories/graph-analytics/&quot;&gt;Graph Analytics&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Engines that allow graph analytics to be run over data in an underlying data store (generally HDFS)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;data-integration-engines&quot;&gt;Data Integration Engines&lt;/h2&gt; &lt;p&gt;Technologies that support the integration of data from multiple sources without data movement or transformation.&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/tech-categories/data-virtualization/&quot;&gt;Data Virtualization&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Technologies that allow data in multiple source databases to be accessed as a single integrated virtual database&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Enterprise Semantic Graphs&lt;/td&gt; &lt;td&gt;Technologies that allow data in multiple sources to be accessed as a single integrated RDF graph model&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;data-processing&quot;&gt;Data Processing&lt;/h2&gt; &lt;p&gt;Technologies that support the acquisition, ingest and processing of data&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/tech-categories/data-ingestion/&quot;&gt;Data Ingestion&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Specialist tools designed to acquire and ingest data into an analytical platform ready for analysis or for further transformation to support analysis.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;infrastructure&quot;&gt;Infrastructure&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/tech-categories/compute-cluster-managers/&quot;&gt;Compute Cluster Managers&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Technologies for managing the execution of jobs across a general purpose compute cluster&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Technology Vendors</title><link>https://ondataengineering.net/tech-vendors/</link><description> &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;https://ondataengineering.net/tech-vendors/amazon-web-services/&quot;&gt;Amazon Web Services&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A subsidiary of Amazon.com that provides infrastructure and platform cloud services, including virtual machines and storage infrastructure services and plus database, analytics, real time data processing and data pipeline platform services, with services available in 16 geographical regions. Launched to support internal Amazon.com services in July 2002, with the first launch of a public service in November 2004, and now comfortably the largest cloud services provider.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;https://ondataengineering.net/tech-vendors/cloudera/&quot;&gt;Cloudera&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Cloudera is a commercial company focusing on offerings based around an Apache Hadoop distribution that's supplemented with a number of commercial components, distributed as a free express version (with cut down versions of some of the commercial components), and as an enterprise version with an annual subscription fee. They are extreemly active in the Apache open source space, with committers on all the technologies they distribute, and with a history of donating projects to the Apache Foundation that they have either initiated or acquired. Formed in 2008 by ex-employees from Google, Yahoo, Facebook and Oracle, with Doug Cutting, the original author of Hadoop, joining in 2009 as Chief Architect.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;https://ondataengineering.net/tech-vendors/google-cloud-platform/&quot;&gt;Google Cloud Platform&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A cloud computing service operated by Google, with support for infrastructure, storage, databases and analytics services. First services were available in preview in April 2008.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;https://ondataengineering.net/tech-vendors/hortonworks/&quot;&gt;Hortonworks&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Hortonworks is a commercial company focusing on products that support the exploitation of data both at rest and in motion. Their business model is to provide support and professional services for a range of Apache open source technologies which they package and distribute for free. They are therefore extreemly active in the Apache open source space, with committers on all the technologies they distribute, and with a history of donating projects to the Apache Foundation that they have either initiated or acquired. Hortonworks was formed in June 2011 by ex-Yahoo employees.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;https://ondataengineering.net/tech-vendors/mapr/&quot;&gt;MapR&lt;/a&gt;&lt;/td&gt; &lt;td&gt;MapR is a commercial company focusing on products built around it's Converged Data Platform, which provides Hadoop compatibility plus NoSQL and streaming data storage capabilities, and which is bundled with a number of Hadoop open source products. They have started and are active in a number of open source components, including Apache Drill and Apache Myriad, both of which they founded. MapR was founded in 2009.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;https://ondataengineering.net/tech-vendors/mesosphere/&quot;&gt;Mesosphere&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Mesosphere is a commercial company developing the Mesosphere Datacenter Operating System (DC/OS). DC/OS is built around Apache Mesos and is itself an open source project. They are therefore extremely active in the open source space. Their business model is to sell subscription licenses based around an Enterprise version of DC/OS, provide training and support for DC/OS and partner-supported technologies. Mesosphere was founded in May 2013 by ex-engineers from Twitter and Airbnb.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;https://ondataengineering.net/tech-vendors/microsoft-azure/&quot;&gt;Microsoft Azure&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A cloud computing service operated by Microsoft, with support for infrastructure, storage, databases and analytics services, available in 34 geographical regions. Announced in Otober 2008, with first services available in February 2010. Previously known as Windows Azure.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;https://ondataengineering.net/tech-vendors/odpi/&quot;&gt;ODPi&lt;/a&gt;&lt;/td&gt; &lt;td&gt;ODPi is a non profit organisation and member of the Linux Foundation that distributes reference specifications for key Hadoop components and APIs to help drive compatibility between Hadoop distributions, sponsoring Apache Bigtop as a reference implementation. Compliance against the spec for platform vendors (to ensure any certified app will run on their platform) and software vendors (to ensure their app will run on any certified platform) is achieved through self-certification against a test suite that's bundled with Apache Bigtop. Current technologies covered by the specifications are HDFS, YARN, MapReduce, HCFS and Hive. Current certified distributions include Altiscale, ArenaData, Hortonworks, IBM and Infosys but notably does not include either Cloudera or MapR who have both publicly stated their objections to the organisation. Currently certified applications are limited to DataTorrent, Apache Hawq, SAS, Syncsort, WANDisco and a range of IBM technologies. Originally founded in February 2005 as the Open Data Platform with language that suggested it was looking to build a standard Hadoop core (the ODP core) based on HDFS, Ambari, YARN and MapReduce. Moved under the Linux Foundation in September 2015.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;https://ondataengineering.net/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt; &lt;td&gt;The Apache Software Foundation is a non-profit organisation that supports a wide range of open source projects, including providing and mandating a standard governance model (including the use of the Apache license), holding all trademarks for project names and logos, and providing legal protection to developers. It was founded in 1999 and now oversees nearly 200 projects.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Technologies</title><link>https://ondataengineering.net/technologies/</link><description> &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/alluxio/&quot;&gt;Alluxio&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A distributed virtual storage layer, supporting key-value and filesystem interfaces (including HDFS compatibility and a FUSE driver) with support for a range of computation and storage frameworks (including Spark, MapReduce, HBase and Hive) over multiple storage layers (including in-memory, local, network, cloud and cluster file systems) with the ability to create unified and tiered storage, for example to create an in memory filesystem backed by disk to accelerate analytics jobs. Supports a POSIX like access control model, and a CLI and web interface for browsing the storage layer and an S3 compatible API. Java based, Open Source under the Apache 2.0 licence, hosted on GitHub, with development led by Alluxio (with significant external contributions), although they don't appear to yet provide commercial support (but do provide training). Started in December 2012, open sourced in April 2013, with a v1.0 release in February 2016. Formally known as Tachyon.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/amazon-emr/&quot;&gt;Amazon EMR&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Service for dynamically provisioning Hadoop clusters on Amazon EC2 infrastructure, with one of more Hadoop based services pre-installed and configured. Supports selection of EC2 instance types, EC2 spot and reserved instances, programmatic execution of service jobs (steps), persistent or transient (terminate after pre-defined steps have been executed) clusters, automatic or manual scaling of live clusters, cloning of clusters, HDFS on local (EBS) node storage, an HDFS compatible filesystem (EMR File System - EMRFS) for accessing Amazon S3 storage (that supports consistency using DynamoDB for metadata), automatic configuration of Hadoop clusters and firewalls, integration with AWS CloudWatch and AWS Identity and Access Management, Hadoop encryption and Kerberos authentication, persistent storage of Hive metadata in AWS Glue Data Catalog, and bootstrap actions for custom configuration or installation of other services (with a GitHub repo of open source bootstrap action extensions). Manageable via the AWS Management Console, the AWS CLI, a REST API and a range of SDKs. Priced at an hourly rate (charged per second) based on the EC2 instance types being used, which is in addition to any EC2 or EBS charges.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/amazon-s3/&quot;&gt;Amazon S3&lt;/a&gt; &lt;/td&gt; &lt;td&gt;An object store service with eventual consistency, focusing on massive durability and scalability, with support for multiple storage tiers (including Amazon Glacier) and deep integration to the AWS ecosystem. Objects are organised into buckets and indexed by string, with the option to list objects by prefix and to summarise results based on a delimiter allowing a filesystem to be approximated. Metadata against objects is managed via S3 Object Tags, key-value pairs applied to objects that can be added, modified or deleted at any time. Lifecycle management policies can be assigned to name prefixes or object tags to automatically delete objects or move them between storage tiers. Supports versioning of objects, access control (at the bucket or object level), retrieving subsets of objects via server side queries (S3/Glacier select), replication of objects and metadata to a bucket in a different AWS region (cross-region replication), encryption of objects and support for SSL connections, immutable blobs (via Glacier Vault Lock), full auditing of all object operations, analytics on object operations, multi-part uploads, multi-object deletions, a flat-file output of object names and metadata (S3 Inventory), downloads via the bittorrent protocol, static website hosting and time limited object download URLs. Quotes a 99.999999999% guarentee that data won't be lost, with data stored redundantly across multiple devices and facilities within the chosen region, and scalability past trillions of objects. Provides a web based management console, mobile management app, a REST API and SDKs for a wide range of languages. First launched in March 2006.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-accumulo/&quot;&gt;Apache Accumulo&lt;/a&gt; &lt;/td&gt; &lt;td&gt;NoSQL wide-column datastore based on BigTable. Supports horizontal scalability, cell based access control (based on arbitrary boolean expressions of user security labels), high availability, atomic read-modify-write operations, map reduce support (both as a source and sink), table constraints, LDAP and Kerberos integration, the use of HDFS for underlying storage, and replication between instances. Comes with a web based monitoring interface (Accumulo Monitor) and a CLI. Written in Java, with thrift based API allowing access from other languages including C++, Python, Ruby. Originally developed at the NSA, donated to the Apache Foundation in September 2011, before graduating in March 2012, and is still under active development.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-ambari/&quot;&gt;Apache Ambari&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Platform for installing, managing and monitoring Apache Hadoop clusters. Supports the installation of different versions of different distributions of Hadoop through Stack definitions (with support for HDP out of the box, and further stacks and add ons available through management packs), and the specification of Blueprints (cluster layouts and configuration for a given Stack) that can be used to programmatically create multiple clusters (e.g. dev, test and production). Also supports both rolling (no downtime) and express (faster but with downtime) upgrades; cluster administration (including adding and removing nodes/services, viewing the status of nodes/services, and configuring services with the versioning of configuration and the ability to rollback changes); the automated Kerberization of clusters; the collection, storage (in HBase) and visualisation (via Grafana or through dashboards in Ambari) of system and Hadoop component metrics via the Ambari Metrics System (AMS); alerting on statuses and metrics; the collection, storage (in Solr) and searching/viewing of log entries from across the Hadoop cluster (currently in technical preview); and a framework for UI components within Ambari (Ambari Views, treated here as a sub-project). Web based, with a REST API, and backed by a backend database (Oracle, MySQL or Postgres). Donated to the Apache Foundation by Hortonworks, IBM and Yahoo in August 2011 as the Hadoop Management System (HMS), graduating in December 2013 after changing it's name to Ambari. Still under active development with a large number of contributors.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Ambari&amp;nbsp;&amp;gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-ambari/ambari-views/&quot;&gt;Ambari Views&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Framework within Ambari that allows new applications or views to be added to Ambari, based on new client side code (HTML, JavaScript and CSS) supported by new backend code (Java) that exposes REST API end points for the UI to consume. Comes with support for a number of views out of the box, including YARN Queue Manager (supports the creation and configuration of YARN capacity schedule queues), Files (supports copying and moving, uploading and setting permissions on files in HDFS), Falcon (supports defining, scheduling and monitoring data management pipelines), Hive (supports browsing databases, executing queries and viewing explain plans, saving queries, viewing query history and uploading data to Hive tables), Pig (supports executing Pig scripts and viewing execution history), SmartSense (supports capture and download of bundles), Storm (supports viewing cluster status, monitoring topologies, perform topology management and access metrics and logs) and Tez (supports viewing and debugging Tez jobs), along with technical previews of Workflow Designer, Zeppelin and Hue migration views. Views can be deployed into a standalone Ambari instance to separate these from the primary Ambari management instance and to support scaling out.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-apex/&quot;&gt;Apache Apex&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Data transformation engine based on Directed Acyclic Graph (DAG) flows configured through a Java API or via JSON, with a stated focus on performance, code re-use, testability and ease of operations. Runs over YARN and HDFS with native support for both micro-batch streaming and batch uses cases, and includes a range of standard operators and connectors (called Apex Malhar). An Apache project, graduating in April 2016, having been originally donated in August 2015 by DataTorrent from their DataTorrent RTS product which launched in June 2014. Java based, with development lead by DataTorrent who distribute it as DataTorrent RTS in two editions - a Community Edition (which also includes a basic management GUI and a tool for configuring Apex for data ingestion), and an Enterprise Edition (which further includes a graphical transformation editor, a self service dashboard, security integration and commercial support, and is also available as a cloud offering).&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-arrow/&quot;&gt;Apache Arrow&lt;/a&gt; &lt;/td&gt; &lt;td&gt;In-memory data structure specification for building columnar based data systems. Provides a standard interchange format to allow sharing of data between processes on a node without the overhead of moving or transforming the data, permits O(1) random access and has the ability to represent both flat relational structures and complex hierarchical nested data. Data is organised using a columnar structure memory-layout making it cache efficient for analytical workloads (which typically group all data relevant to a column operation together) and allows execution engines to take advantage of modern CPU SIMD (Single Instruction Multiple Data) instructions which work on multiple data values simultaneously in a single CPU clock cycle. Supports Java, C, C++, JavaScript, Python, Go, Ruby and Rust. Seeded from the Apache Drill project and promoted directly to a top level Apache project in February 2016 followed by an initial 0.1 release in October 2016. Used in a range of other projects including Drill, Spark, Impala, Kudu, Pandas and others. Has not yet reached a v1.0 milestone, but is still under active development with a range of contributors from a number of other Apache and non-Apache data projects.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-atlas/&quot;&gt;Apache Atlas&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A metadata and data governance solution for Hadoop. Supports an extensible metadata model with out of the box support for Hive datasets and data lineage from Hive queries and Sqoop imports, with limited support for Falcon, Storm and Kafka. Allows datasets and data items to be tagged (and for these tags to be used for access control by Apache Ranger), and includes support for business taxonomies as a technical preview. Implemented as a graph based database using Titan (which by default uses HBase and Solr), with a web based user interface and a REST API for searching and visualising/retrieving metadata, and Kafka topics for the ingest of metadata (primarily from hooks in metadata sources such as Hive or Sqoop) and the publishing of metadata change events. Donated to the Apache Foundation in May 2015 by the Hortonworks Data Governance Initiative in partnership with Aetna, Merck, Target, Schlumberger and SAS, graduating in June 2017. Has not yet reached a v1.0 milestone, but is still under active development.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-aurora/&quot;&gt;Apache Aurora&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A service scheduler for defining and managing bundled tasks as jobs across a cluster of servers using Mesos, leveraging Mesos for resource allocation and isolation at the task level. Operates as a Mesos framework, a Python based domain specific language (DSL) for job template definition, an executor for carrying out the workload described in the DSL, an associated command line interface for schedule management and a web interface providing read-only status of jobs and associated diagnostic information. Defines a fine-grained task state model to support resource allocation, rolling upgrades, health checking, priority-based scheduling and application maintenance. Handles cross-cutting concerns like observability and log collection. Supports priority-based scheduling, using pre-emption so that when resources are low, lower priority jobs can be stopped to make room for the higher priority tasks. An Apache project, originally created at Twitter, donated to the Apache Foundation in October 2013, graduating in March 2015 (0.8.0 Released). Hasn't yet reached a v1.0 milestone, however still under development from a range of contributors.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-avro/&quot;&gt;Apache Avro&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Data serialisation framework that supports both messaging and data storage. Primarily uses a compact binary format but also supports a JSON format. Supports a range of data structures (including records, enumerations, arrays and maps) with APIs for a wide range of both static and dynamically typed languages. Schema based, with schemas primarily specified in JSON, and support for both code generation from schema definitions as well as dynamic runtime usage. Schemas are serialised alongside data, with support for automatic schema resolution if the schema used to read the data differs from that used to write it. Started as an Hadoop sub-project by Cloudera in April 2009, with an initial v1.0 release in July 2009, before becoming a top level Apache project in May 2010. Has seen significant adoption in the Hadoop ecosystem.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-beam/&quot;&gt;Apache Beam&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Unified batch and streaming programming model to define portable data processing pipelines and execute these using a range of different engines. Originating from the Google Dataflow model, focuses on unifying both styles of processing by treating static data sets as streams (which happen to have a beginning and an end), while achieving data correctness and the ability to handle late-arriving data through a set of abstractions and concepts that give users control over estimated quality of arrived data (completeness), duration to wait for results (latency) and how much speculative/redundant computation to do (cost). Allows business logic, data characteristics and trade-off strategies to be defined via different programming languages through pluggable language SDKs (with out of the box support for Java and Python). Supports a range of pluggable runtime platforms through pipeline runners, with support for a direct runner (for development and testing pipelines in a non-distributed environment), Apache Apex, Flink, Spark, and (under development) Gearpump runners, and a Google Cloud Dataflow runner. Also supports a growing set of connectors that allow pipelines to read and write data to various data storage systems (IOs). An Apache project, opened sourced by Google in January 2016, graduated in January 2017, with a first stable release (2.0) in May 2017. Written in Java and Python and under active development with a large number of contributors including Google, data Artisans, Talend and PayPal.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt; &lt;/td&gt; &lt;td&gt;An Apache open source distribution of Hadoop. Packages up a number of Apache Hadoop components, certifies their interoperability using an automated integration test suite, and packages them up as RPMs/DEBs packages for most flavours of Linux. Also includes virtual machine images and vagrant, docker and puppet recipes for deploying and working with Hadoop. Does not patch projects for distribution, but requires any fixes to be made upstream. An Apache Open Source project, started by Cloudera, donated to the Apache foundation in June 2011, graduating in September 2012, with a 1.0 release in August 2015 based on Hadoop 2.6. Since donating the project, Cloudera have backed away from it, with the project lead moving to Pivotal in December 2013. Now has a broad range of contributors, however usage by the major distributors is not clear.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-calcite/&quot;&gt;Apache Calcite&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A framework for building SQL based data access capabilities. Supports a SQL parser and validator, tools for the transformation and (cost based) optimisation of SQL expression trees, and an adapter framework for accessing metadata and executing queries (including out of the box adapters for a number of database technologies as well as CSV files and POJO objects), along with specific support for streaming SQL queries and optimising data cube queries to use materialised views. Also includes (as a sub-project named Avatica), a framework for building database drivers with support for a standard JDBC driver, server and wire protocols, plus a local embeddable JDBC driver. Used in a range of other projects including Drill, Flink, Hive, Kylin, Phoenix, Samza, Storm and Cascading. An Apache project, originally created by Julian Hyde in May 2012 as Optiq, donated to the Apache Foundation in May 2014, graduating in October 2015 following a v1.0 release in January 2015. Under active development with a range of contributors.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-carbondata/&quot;&gt;Apache CarbonData&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Unified storage solution for Hadoop based on an indexed columnar data format, focusing on providing efficient processing and querying capabilities for disparate data access patterns. Data is loaded in batch, encoded, indexed using multiple strategies, compressed and written to HDFS using a columnar file format. Provides a number of highly configurable indexes (multi-dimensional key, min/max index, and inverted index), global dictionary encoding and column grouping to support interactive style OLAP queries, high throughput scan queries, low latency point queries and individual record queries. Also supports batch updates and deletes using delta bitmap files and compaction. Written in Java using Apache Thrift, supports all common primitive data types and complex nested data types including array and structures. Consists of several modules, the format specification and core implementation (columnar storage, indexing, compression, encoding), Hadoop input/output format interface, deep integration with Spark, interfacing to Spark SQL and the DataFrame API and connectors for Hive and Presto. Started back in 2013 at Huawei's India R&amp;amp;D center, donated to the Apache Foundation in 2015, graduated in April 2017, with a stable (1.1.0) release in May 2017, and under active development.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-cassandra/&quot;&gt;Apache Cassandra&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Distributed wide-column datastore based on Amazon Dynamo and Google BigTable. Focuses on fault tolerance, linear scalability and operational simplicity with zero downtime based on a distributed masterless node and peer-to-peer design. Supports high availability using network topology aware data replication to avoid single points of failure, fast real-time and durable ingestion of data using an append-only log, strong query performance based on an in-memory index (log-structured merge-tree) that is persisted as a sorted string table (SST) for fast sequential retrieval, and tunable consistency (between strong and eventual) allowing availability (number of replicas on which a write must succeed), data accuracy (number of replicas must respond to a read request before returning data) and performance to be traded off on a global or per-operation basis. Does not support joins nor subqueries, rather, emphasises denormalisation through features like collections. Comes with a command line shell (cqlsh) for using Cassandra Query Language (resembling SQL), a wide number of drivers for many languages including Java, Python, Ruby, C++ and Go, and Nodetool, a CLI for cluster management. Metrics can be queried via JMX or pushed to external monitoring systems, SSL encryption provides secure communication, authentication and authorisation is provided based on internally controlled rolename/passwords and object permission management. An Apache project, graduating in February 2010, having been originally opened sourced in July 2008 by Facebook. Written in Java and under active development with major contributions from DataStax who distribute it as a part of their DataStax Enterprise offering. Other commercial vendors include Instaclustr and Winguzone who provide hosted and managed Apache Cassandra as a service on a number of major cloud providers.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-crunch/&quot;&gt;Apache Crunch&lt;/a&gt; &lt;/td&gt; &lt;td&gt;An abstraction layer over MapReduce (and now Spark) that provides a high level Java API for creating data transformation pipelines, originally designed to make working with MapReduce easier based on the Google FlumeJava paper. Also includes connectors for HBase, Hive and Kafka, Java 8 lambda support, an experimental Scala wrapper for the API (Scrunch), and support for in memory pipelines and helper classes to support testing. Open sourced by Cloudera in October 2011, donated to the Apache Foundation in May 2012, before graduating in February 2013. Support for Spark was added as part of v0.10 in June 2014. Still being maintained, and appears to have had been adopted at a number of large companies, but with limited new development.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-datafu/&quot;&gt;Apache DataFu&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A set of libraries for working with data in Hadoop. Consists of two sub-projects - DataFu Pig (a set of Pig User Defined Functions) and DataFu Hourglass (a framework for incremental processing using MapReduce). Originally created at LinkedIn, with the Pig UDFs being open sourced in January 2012 as DataFu, with a v1.0 release in September 2013. Split into sub-projects in October 2013 when LinkedIn open sourced DataFu Hourglass and added it to the project. Donated to the Apache Foundation in January 2014, graduating in February 2018. Last major release was v1.3 in November 2015, with a handful of bug fix releases but little development activity since then.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache DataFu&amp;nbsp;&amp;gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-datafu/datafu-hourglass/&quot;&gt;DataFu Hourglass&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A framework over MapReduce that supports the efficient generation of statistics of dated data by incrementally updating the previous days output. Supports both fixed length and fixed start point windows, and the generation of statistics by input partition or as a total over all input data.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache DataFu&amp;nbsp;&amp;gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-datafu/datafu-pig/&quot;&gt;DataFu Pig&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A set of user defined functions for Apache Pig, including support for statistical calculations, bag and set operations, sessionisation of streams of data, cardinality estimation, sampling, hashing, PageRank and others.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-drill/&quot;&gt;Apache Drill&lt;/a&gt; &lt;/td&gt; &lt;td&gt;An MPP query engine that supports queries over one or more underlying databases or datasets without first defining a schema and with the ability to join data from multiple datastores together. Supports a range of underlying technologies including HDFS, NAS, HBase, MongoDB, MapR-DB, MapR-FS, Kafka, OpenTSDB, Amazon S3, Azure Blob Storage, Google Cloud Storage, JDBC, Avro, JSON and Parquet. Pushes queries down to underlying datastores where possible, and supports an in-memory columnar datastore based on a schema free JSON document model for performing cross datastore query operations. Supports dynamic schema discovery, with support for complex and nested types, including a number of SQL extensions. Supports standard SQL, UDFs (including Hive UDFs) and comes with JDBC and ODBC drivers, a REST API, plus a shell, web console and C++ API. Designed to be horizontally scalable and to support high throughput and low latency use cases, and can run over YARN. Supports Kerberos and username/password authentication, plus a full authorisation model. Created by MapR Based on Google's Dremel paper, donated to the Apache Foundation in September 2012, graduating in November 2014, with a 1.0 release in May 2015, and is still under active development&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-druid/&quot;&gt;Apache Druid (Incubating)&lt;/a&gt; &lt;/td&gt; &lt;td&gt;An open source distributed database built to support sub-second OLAP / star schema style queries on both real-time and historical data, based on columnar storage and inverted indexes. All data must have a timestamp, one or more dimension fields, and then one or more measures, with data being aggregated by timestamp and dimension fields on ingest. Comes with a batch ingestor (with support for reading from HDFS, S3 and local files), a streaming ingestor (with support for local files and an HTTP endpoint), and a streaming data endpoint (Tranquility, with support for Kafka, Storm and Spark Streaming and an API for use with other systems), with real-time ingests not guaranteed under failure, but with supports hybrid architectures whereby real-time data ingests are replaced with batch refreshes when available. Architecture based on a number of different node types - historical nodes (which serve queries against a local cache of data that's been persisted in S3 or HDFS), real-time nodes (which support ingest and querying of streaming data, with data persisted and handed over to an historical node once aged), and broker nodes (which distribute queries to appropriate real-time and historical nodes and then collate the results). All data is segmented by date and time, with a metadata database (e.g. MySQL, PostgreSQL, or Derby) tracking segments and which nodes are serving them, and Apache ZooKeeper used for co-ordination and communication between nodes. Supports low latency lock free ingestion, a JSON REST endpoint for queries (with support for a range of query types including timeseries, TopN, groupBy and select), a range of SDKs, approximate and exact computations, multiple storage tiers (including data lifecycle rules on tiering and dropping data), metrics (for queries, ingestion, and coordination), rolling upgrades, HTTP authentication (including Kerberos, but no further security controls), and a number of experimental features including small dimension lookups (note that general joins are not supported), multi-value dimension fields and a SQL interface based on Apache Calcite. Started in 2011 by Metamarkets, open sourced under the GPL licence in October 2012, moving to an Apache licence in February 2015 and donated to the Apache Incubator in February 2018. Has a wide range of companies listed on the Druid website as users, and natively supported by Apache Superset and Grafana (via a plugin). Commercial support available from Imply (which distribute their own product based on Druid including a SQL interface and a data exploration tool called Pivot), and currently in tech preview as part of the Hortonworks Data Platform, where it's being integrated with Apache Hive.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-falcon/&quot;&gt;Apache Falcon&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Data feed management system for Hadoop. Supports the definition, scheduling and orchestration (including support for late data and retry policies) of data processing pipelines (referred to as processes, with support for Ozzie, Spark, Hive and Pig jobs), the management of the data produced and consumed by these pipelines (referred to as feeds, with support for data in HDFS and Hive) and the generation and visualisation of pipeline lineage information, all across multiple Hadoop clusters. Also includes the ability to mirror or replicate HDFS and Hive data between clusters, to failover processing between clusters and to import and export data using Sqoop. Supports both a web and command line interface and a REST API. An Apache project, graduating in December 2014, having been originally donated by inMobi in April 2013. Hasn't yet reached a v1.0 milestone, is seeing very little development activity, and as of HDP 3.0 will no longer be distributed by Hortonworks.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-flink/&quot;&gt;Apache Flink&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Specialised stream processing technology inspired by the Google Data Flow model. Based on a single record (not micro batch) model, with exactly once processing semantics (for supported sources and sinks) via light weight checkpointing, and focusing on high throughput, low latency use cases. Supports both a Java and Scala API, with a fluent DataStream API for working with continuous data flows (including a flexible windowing API that supports both event time and processing time windows and support for out of order or late data), and a DataSet API for working with batch data sets (that uses the same streaming execution engine). Also supports a number of connectors and extra libraries, including experimental support for SQL expressions, a CEP library (FlinkCEP) that can be used to detect complex event patterns, a beta package for running Storm apps on Flink, a graph processing library (Gelly) and a machine learning library (FlinkML). Clustered, with support for YARN and Mesos as well as standalone clusters. Open sourced by Data Artisans in April 2013, donated to the Apache Foundation in April 2014 before graduating in August 2014. Under active development with a large number of contributors and a range of user case studies. Sold as a hosted managed service (dA Platform) by Data Artisans who also supply training.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-flume/&quot;&gt;Apache Flume&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Specialist technology for the continuous movement of data using a set of independent agents connected together into pipelines. Supports a wide range of sources, targets and buffers (channels), along with the ability to chain agents together and to modify and drop events in-flight. Designed to be highly reliable, and to support reconfiguration without the need for a restart. Heavily integrated with the Hadoop ecosystem. An Apache project, donated by Cloudera in June 2011, graduating in June 2012, with a v1.2 release (the first considered ready for production use) in July 2012. Java based, with commercial support available as part of most Hadoop distributions.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-giraph/&quot;&gt;Apache Giraph&lt;/a&gt; &lt;/td&gt; &lt;td&gt;An iterative, highly scalable graph processing system built on top of MapReduce and based on Pregel, with a number of features added including a framework for creating re-usable code (called blocks). An Apache project, graduating in May 2012, having been originally donated by Yahoo in August 2011. Java based, no commercial support available, but is mature and has been adopted by a number of companies (including LinkedIn and most famously Facebook who scaled it to process a trillion edges), and has a number of active developers.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-gobblin/&quot;&gt;Apache Gobblin (Incubating)&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Java based framework for ingesting data into Hadoop. Ingestion jobs are defined through job configuration files, and are made up of a number of stages - a Source identifies work to be done and generates Work Units which are then processed by Tasks, with Tasks consisting of an Extractor (reads the records to be processed), one or more Converters (a 1:N transformation of records), a Quality Checker (covers both record and file checks), a Fork Operator (allows data to be written to multiple targets) and a Writer (writes out completed records), with the output of a completed task being committed by a Publisher. Gobblin ships with a number of standard components, including support for a range of sources and targets, as well as supporting custom implementations of any stage. Jobs can be run using a number of frameworks, including MapReduce (with all tasks running as mapper only jobs), YARN, and as Java threads within a single JVM, with some modes also supporting an internal scheduler and job management engine. Supports job locks (to ensure multiple instances of the same job don't run at the same time), job history metadata (via a job execution history store that supports a REST API that can be used to monitor jobs), exactly-once processing support (via Publisher commits), failure handling (retrying both within and across jobs), capture and forwarding of execution and data quality metrics, post processing of data (e.g. to remove duplicates or generate aggregations), partitioned writers, job configuration file templates, Hive table registration, high availability, data retention management (automatically deleting old data according to a number of retention rules), and data purging (Gobblin Compliance). Developed at LinkedIn from late 2013, first announced in November 2014 and open sourced shortly afterwards, before being donated to the Apache Foundation in February 2017, and with stated deployments at a number of large organisations.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-hadoop/&quot;&gt;Apache Hadoop&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A distributed storage and compute platform consisting of a distributed filesystem (HDFS) and a cluster workload and resource management layer (YARN), along with MapReduce, a solution built on HDFS and YARN for massive scale parallel processing of data. Has an extensive ecosystem of compatible technologies. An Apache Open Source project, started in January 2006 as a Lucene sub-project, becoming a top level project in January 2008, with a 1.0 release in December 2011 (containing HDFS and MapReduce), and a 2.2 release (the first 2.x GA release) in October 2013 (adding YARN). Work is currently underway to split out the data storage layer of HDFS (the HDDS sub-project) and to implement an object store on top of this that can co-exist with HDFS (the Ozone sub-project). Very active, with a deep and broad range of contributors, and backing from multiple commercial vendors.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Hadoop&amp;nbsp;&amp;gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-hadoop/hdds/&quot;&gt;HDDS&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A common distributed and resilient block storage layer that will eventually underpin HDFS and Ozone, delivering increased scalability. Implemented as a Storage Container Manager (SCM) service (that performs block management) and DataNode services (inherited from HDFS that run on storage nodes and manage block IO). Blocks are arranged into containers (with the replication strategy defined at the container level). Currently under active development as part of the development of Ozone. Previously known as HDSL (Hadoop Distributed Storage Layer)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Hadoop&amp;nbsp;&amp;gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-hadoop/hdfs/&quot;&gt;HDFS&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A highly resilient distributed cluster file system proven at extreme scale. Consists of a single NameNode service (that's responsible for all metadata management, including the filesystem namespace and block management) plus DataNode services that run on all storage nodes (that manage block IO). Supports NameNode high availability, metadata resilience (via a transaction log), data resilience (via block replication or erasure coding), user authentication, extended ACLs, snapshots, quotas, central caching, a REST API, an NFS gateway, rolling upgrades, rack awareness, transparent encryption, NameNode federation (support for multiple independant NameNodes on the same cluster serving different namespaces) and support for heterogeneous storage. Part of the original Hadoop code base, becoming an Apache Hadoop sub-project in July 2009. Currently being updated to run over the new HDDS (Hadoop Distributed Data Storage) layer, moving block management from the NameNode to a new Storage Container Manager to increase scalability.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Hadoop&amp;nbsp;&amp;gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-hadoop/map-reduce/&quot;&gt;MapReduce&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A data transformation and aggregation technology proven at extreme scale that works on key value pairs and consists of three transformation stages - map (a general transformation of the input key value pairs), shuffle (brings all pairs with the same key together) and reduce (an aggregation of all pairs with the same key). Part of the original Hadoop code base, becoming an Apache Hadoop sub-project in July 2009.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Hadoop&amp;nbsp;&amp;gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-hadoop/ozone/&quot;&gt;Ozone&lt;/a&gt; &lt;/td&gt; &lt;td&gt;An object store built on top of the new Hadoop HDDS block storage layer that can co-exist with HDFS. Implemented as an Ozone Manager (OM) service that manages the object store namespace, utilising the HDDS Storage Container Manager for block management. Objects are arranged into buckets, which themselves are arranged into volumes. Supports consistent writes, an RPC API, an Amazon S3 compatible REST API, a CLI, a load generation tool (Freon, previously Corona), and an Hadoop Compatible File System (OzoneFS), with a stated plan for mountable LUN storage (Quadra). Originally announced in October 2014, re-invigorated under the Hortonwworks Open Hybrid Architecture Initiative in September 2018, and currently under active development with a suggested release as part of HDP 3.2.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Hadoop&amp;nbsp;&amp;gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-hadoop/yarn/&quot;&gt;YARN&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Resource management and job scheduling &amp;amp; monitoring for the Hadoop ecosystem. Includes support for capacity guarantees amongst other scheduling options, long running services, GPU and FPGA scheduling and isolation and experimental support for launching applications within docker containers. Added as an Apache Hadoop sub-project as part of Hadoop 2.x (with a GA release as part of 2.2 in October 2013) having been started in January 2008.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-hama/&quot;&gt;Apache Hama&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A general purpose BSP (Bulk Synchronous Parallel) processing engine inspired by Pregel and DistBelief that runs over Mesos or YARN. Supports BSP, graph computing and machine learning programming models, as well as Apache MRQL. An Apache project, donated in 2008, and graduated in 2012. Java based, with no commercial support available, limited case studies for it's use and limited active developers, with the last release being in June 2015.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-hawq/&quot;&gt;Apache HAWQ&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A port of the Greenplum MPP database (which itself is based on PostgreSQL) to run over YARN and HDFS. Supports all the features of Greenplum (ACID transactions, broad SQL support and in database language and analytics support, including support for Apache MADLib), integration with Apache Ambari, an Input Format for MapReduce to read HAWQ tables, and both row and Parquet (column) based storage of data managed by HAWQ. Also supports queries over data not managed by HAWQ via external tables, with a Java based framework (PXF) for accessing external data, and out of the box support for accessing data in HDFS (text, Avro, JSON), Hive and HBase, with a number of open source connectors also available. Fault tolerant and horizontally scalable, with the ability to scale up or down on the fly. Originally created as Pivotal HAWQ based on a fork of Greenplum in 2011, with an initial 1.0 release as part of Pivotal HD in July 2013. Open sourced and donated to the Apache Foundation in September 2015, becoming Apache HAWQ, with the first open source release (2.0) in October 2016, and graduating in August 2018. Development led by Pivotal, who also distribute binaries as Pivotal HDB and provide training, consultancy and support. Pivotal HDB is also available as Hortonworks HDB, with support and consultancy provided by Hortonworks.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-hbase/&quot;&gt;Apache HBase&lt;/a&gt; &lt;/td&gt; &lt;td&gt;NoSQL wide-column datastore based on Google BigTable. Data for an HBase table is distributed across regions, with each region made up of a store per column family (with stores either hosted in memory or on disk), with regions served and managed by region servers, which in turn are monitored and managed by master servers (which are also responsible for metadata changes and can run in a multi-master configuration), with the architecture supporting horizontal scalability and high availability. Supports strongly consistent reads and writes (with all reads and writes going through a single region server), with the option to perform non consistent reads from data replicated between multiple region servers given more consistent performance during region server failure. Supports get, put (insert/update), scan (iterating over a set of rows) and delete operations, the option to bulk load via Map Reduce and Spark, and the option to execute custom code within the HBase cluster via co-processors (observer co-processors execute either before or after specific events, endpoint co-processors allow execution of batch analytics). Also supports medium sized binary objects (up to 10Mb), versioning and fine grained RBAC security controls, including visibility expressions at the cell level for authorising end user access. Runs on Hadoop and HDFS, and is heavily integrated with the Hadoop ecosystem. Supports a CLI plus Java, Thrift and REST API, along with MapReduce and Spark integration as both a source and sink. An Apache project, first released as part of Hadoop 0.15 in October 2007 before graduating as a top level project in May 2010. Java based, with commercial support available as part of most Hadoop distributions.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-hive/&quot;&gt;Apache Hive&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Technology that supports the exposure of data in Hadoop as structured tables and the execution of analytical SQL queries over these. Consists of a number of distinct components (that we treat as sub-projects) including Hive Metastore (stores the definitions of the structured tables), Hive Server (supports the execution of analytical SQL queries as MapReduce, Spark or Tez jobs) and HCatalog (allows MapReduce and Pig jobs to read and write Hive tables). First released by Facebook as an Hadoop contrib module in September 2008, becoming an Hadoop sub-project in November 2008, and a top level Apache project in September 2010, following a first official stable release (0.3) in April 2009. Java based, under active development from a number of large commercial sponsors, with commercial support available as part of most Hadoop distributions.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Hive&amp;nbsp;&amp;gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-hive/hcatalog/&quot;&gt;HCatalog&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Libraries for MapReduce and Pig to read and write data to and from Hive tables, albeit with some limitations. Also supports a CLI for querying and updating the Hive Metastore, however this doesn't support the full range of Hive DDL commands. Includes WebHCat, a REST API over the HCatalog CLI that also supports the execution of MapReduce, Pig, Hive and Sqoop jobs. Donated to the Apache foundation by Yahoo in March 2011, had WebHCat folded in in July 2012, graduating as a top level project in February 2013, but then almost immediately was folded into Hive in March 2013 as part of the Hive 0.11 release. Has seem limited development since this time.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Hive&amp;nbsp;&amp;gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-hive/hive-metastore/&quot;&gt;Hive Metastore&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A metadata service that allows structured tables to be defined over files in HDFS (and also HBase or Accumulo), providing an API that allows the metadata to be queried and updated by other tools including Impala, Spark SQL or RecordService. Supports partitioned and clustered tables, as well as complex field types such as arrays, maps and structs. Backed by a relational database (either MySQL, Postgres and Oracle). Part of the original Hive code base.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Hive&amp;nbsp;&amp;gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-hive/hive-server/&quot;&gt;Hive Server&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Supports the execution of SQL queries over data in HDFS based on tables defined in the Hive Metastore, as well as DDL to query and update the Hive Metastore. Focus is on analytical (OLAP) use cases, with some support for batch updates to data. Originally executed queries as MapReduce jobs, but significant investment from has seen support for executing queries as Spark and as Tez jobs, with work underway to support sub second query times using Tez (Hive LLAP). Recent changes have also seen it achieve significant SQL compliance, with support for SQL:2011 analytical functions on-going. Accepts queries over an API with JDBC and ODBC drivers available, and includes Beeline, a command line JDBC client. Technically referred to as Hive Server 2, and was introduced in Hive 0.11 as a replacement for the original Hive Server to address a number of concurrency and security issues.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-ignite/&quot;&gt;Apache Ignite&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A distributed in-memory data fabric/grid with the ability to persist data to disk, supporting a number of use cases including a key value store (with SQL support), real time stream/event processing engine, arbitrary compute, long running service management, an in-memory HDFS compatible file system for acceleration of Hadoop jobs, an in-memory machine learning grid and in-memory shared Spark RDDs and Data Frames. An Apache project, graduating in September 2015, having been originally donated by GridGain from their In-Memory Data Fabric product launched in 2007. Java based, with development lead by GridGain who also supply commercial support (as GridGain Professional with ongoing Q&amp;amp;A and bug fixes before they're included in Ignite) along with GridGain Enterprise and Ultimate (which includes extra features such as a management GUI, enterprise security, rolling upgrades, backup and recovery) and GridGain Cloud (beta).&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-impala/&quot;&gt;Apache Impala&lt;/a&gt; &lt;/td&gt; &lt;td&gt;An MPP query engine that supports the execution of SQL queries over in HDFS, HBase, Kudu and S3 based on tables defined in the Hive Metastore. Focus is on analytical (OLAP) use cases, and more specifically on low latency interactive queries (rather than long running batch queries), with some support for batch inserts of data. Supports DDL statements for updating the Hive Metastore, uses (broadly) the same SQL syntax as Hive (including UDFs and a range of aggregate and analytical functions), as well as the same JDBC / ODBC drivers, and is therefore compatible with any Hive query tool (such as Beeline). Supports querying over data in Parquet, Text, Avro, RCFile and SequenceFile formats, with the ability to write Parquet and Text data. Support Kerberos and LDAP authentication, and integration with Apache Sentry for authorisation. Includes a shell (Impala Shell) that supports some shell only commands for tuning performance and diagnosing problems. Created by Cloudera, started in May 2011 and first announced in October 2012, with a 1.0 GA release in May 2013. Donated to the Apache Foundation in December 2015, graduating in November 2017, and is still under active development.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Technology for buffering and storing real-time streams of data between producers and consumers, with a focus on high throughput at low latency. Based on a distributed, horizontally scalable architecture, with messages organised into topics which are partitioned and replicated across nodes (called brokers by Kafka) to provide resilience and written to disk to provide persistence. Topics may have multiple producers and consumers, with ability to do fault tolerant reads and to load balance across consumers (consumer groups). Records consist of a key, value and timestamp, with the ability to compact topics to remove updates and deletes by key. Supports rolling upgrades, a full security model (including secure and authenticated connections and ACLs for controlling access to topics), the ability to set quotas (for data produced or consumed), Yammer metrics for both servers and clients, and tools to mirror data to a second cluster (mirror maker) and re-distribute partitions across nodes (for example when adding new nodes). Comes with a Java client, but clients for a wide range of languages are also available. Has two sub-projects (Kafka Connect and Kafka Streams) that are bundled with the main product. Originally developed at LinkedIn, being open sourced in January 2011, before being donated to the Apache Foundation in July 2011. Graduated in October 2012, and although it has not had a v1.0 release is considered production quality and stable. Development is primarily led by Confluent (which was founded by the team that built Kafka at LinkedIn), who have a number of open source and commercial offerings based around Kafka. Commercial support is also available from most Hadoop vendors.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Kafka&amp;nbsp;&amp;gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-kafka/kafka-connect/&quot;&gt;Kafka Connect&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Framework for building scalable and reliable integrations between Kafka and other technologies, either for importing or exporting data. Part of the core Apache Kafka open source technology, connectors are available for a wide range of systems, including Hadoop, relational, NoSQL and analytical databases, search technologies and message queues amongst others, with an API for developing custom connectors. Supports lightweight transformations, and runs separately to Kafka, in either a stand-alone or distributed cluster mode, with a REST API for managing connectors. Introduced in Kafka 0.9, previously known as Copycat&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Kafka&amp;nbsp;&amp;gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-kafka/kafka-streams/&quot;&gt;Kafka Streams&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A stream processing technologies that's tightly integrated to Apache Kafka, consuming and publishing events from and to Kafka topics (and potentially writing output to external systems). Based on an event-at-a-time model (i.e. not micro batch), with support for stateful processing, windowing, aggregations, joining and re-processing data. Supports a low level DSL API, as well as a high level API that provides both stream and table abstractions (where tables present the latest record for each key). Executes as a stand-alone process, with support for parallel processing across threads within a single instance and across multiple instances, with the ability to dynamically scale the number of instances. Introduced in Kafka 0.10.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-knox/&quot;&gt;Apache Knox&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A stateless gateway for the Apache Hadoop ecosystem that provides perimeter security. Includes support for user authentication (via LDAP, Active Directory and a number of single sign on solutions), access authorisation on a per service basis, transitions to Kerberos authentication, reverse proxying and auditing, extension points for supporting new services, audit capabilities, and out of the box support for a number of Hadoop technology end points. An Apache project, started by Hortonworks in February 2013, donated to the Apache Foundation two months later in April, before graduating in February 2014. Hit v1.0 in February 2018, and still under active development.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-kudu/&quot;&gt;Apache Kudu&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Columnar storage technology for tables of structured data, supporting low latency reads, updates and deletes by primary key, as well as analytical column/table scans. Provides Java, C++ and Python APIs, is queryable via Impala and Spark SQL, and provides Spark, Flume and MapReduce connectors. Supports cluster deployments (including co-existence with Hadoop), with tables partitioned into tablets (configurable on a per table basis), with tablets then replicated and distributed across the cluster, using the Raft Consensus Algorithm for consistency. Also supports variable column encoding (including bit shuffle, run length, dictionary and prefix encoding) and compression. Includes a web UI for reporting operational information, and metrics available from the command line, via HTTP or via a log file. Started in November 2012, with a initial beta release in September 2015. Donated to the Apache Foundation in December 2015, graduating in July 2016, with a 1.0 release in September 2016. Implemented in C++.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-kylin/&quot;&gt;Apache Kylin&lt;/a&gt; &lt;/td&gt; &lt;td&gt;An open source distributed analytic engine built to support sub-second OLAP / star schema style queries using SQL on extremely large datasets on Hadoop. Data is read from a star schema data model in Hive to build a data cube of pre-calculated metrics by dimensions using MapReduce or Spark with the results stored in a key-value datastore (HBase). SQL queries can be submitted to the query engine, with results returned with sub-second latency if the required data exists in an HBase cube, otherwise the query is optionally routed back to its original source on Hadoop. Supports compression of large datasets by dictionary encoding cube data using a triple data structure, combination pruning and aggregation grouping of dimensions for efficient data storage, and uses approximation query capability (HyperLogLog) to estimate distinct items and TopN to answer top-k queries. Row keys are composed by dimension encoded values and HBase's fuzzy row filtering is performed directly on the storage nodes to implement low latency lookups. Simple additive and aggregation operations (sum, count or like) are also performed on the storage nodes using HBase coprocessors to provide efficient computational parallelism and minimise network latency. Uses Apache Calcite for SQL parsing and optimisation, comes with an ODBC driver, a JDBC driver and a REST API to integrate with third party business intelligence tools such as Tableau, Microsoft Excel and PowerBI. Includes a web interface and REST API for model building and cube design (with support for hierarchy, joint and derived dimensions), job management (full, incremental and streaming builds) and monitoring and permission management (providing security at a project or cube level). New beta features include building cubes from Kafka streaming data and cube building using Spark instead of MapReduce. Originally developed at Ebay, donated to the Apache Foundation in November 2014, graduating in November 2015, with a 1.0 release in September 2015, and still under active development. Commercial support available from Kyligence, who distribute their own product based on Kylin replacing HBase with a custom columnar storage engine (with cell level access control and integration with LDAP), along with a web based BI tool for self service analysis and a dashboard for Kylin cluster management.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-livy/&quot;&gt;Apache Livy&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A service that allows Spark jobs (pre-compiled JARs) or code snippets (Scala or Python) to be executed by remote systems over a REST API or via clients for Java, Scala and Python. Supports re-use of Spark Contexts (and caching and sharing of RDDs across jobs and clients), multiple concurrent clients, secure authenticated communications and batch job submissions. Started in November 2015 based on code from Hue, with a formal announcement and first release in June 2016 based on development led by Cloudera, Hortonworks and Microsoft, before being donated to the Apache Foundation in June 2017. Hasn't yet graduated, but under active development, and used by tools such as Hue and Zeppelin.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-mahout/&quot;&gt;Apache Mahout&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Machine learning technology comprising of a Scala based linear algebra engine (codenamed Samsara) with an R-like DSL/API that runs over Spark (with experimental support for H2O and Flink), an optimiser, a wide variety of pre-made algorithms, and a Scala REPL (based on Spark Shell) for interactive execution. Can be embedded and integrated within larger applications, for example MLlib when running over Spark. Also includes some original, now deprecated, algorithms implemented over MapReduce. Created in January 2008 as a Lucene sub-project, becoming a top level Apache project in April 2010. The original MapReduce algorithms were deprecated and Samsara introduced as part of v0.10 in April 2015.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-mesos/&quot;&gt;Apache Mesos&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Open source cluster manager for providing efficient resource utilization across a cluster of servers through resource sharing and isolation. Allows a cluster of servers to be shared across diverse cluster computing frameworks so that different distributed workloads such as container orchestration, machine learning, analytics and stateful big data technologies can be run without interfering with each other. Has the ability to dynamically allocate resources across the servers as needed and delegates control over scheduling to the frameworks through an abstraction layer called a resource offer to support a wide array of computing frameworks. Resource isolation is implemented using a universal containeriser, supporting numerous containers including native Mesos containers and Docker containers. Fault tolerance of the Mesos instance in control of the cluster is implemented using ZooKeeper. Started as a research project in the UC Berkeley RAD Lab, open sourced in 2011, with a v1.0 release in July 2016, which, included the 'unified containeriser' and GPU-based scheduling. Written in C++, uses Google Protocol Buffers for messaging and serialization to allow frameworks to be written in a variety of languages including C++, Java, Python, Go, Haskell, and Scala. Under active development, open sourced under the Apache 2.0 license, hosted on the Apache git repository and mirrored on GitHub. Software startup Mesosphere sells the Datacenter Operating System, a distributed operating system, based on Apache Mesos.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-myriad/&quot;&gt;Apache Myriad&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Tool that allows YARN applications to run over Apache Mesos, allowing them to co-exist and share cluster resources. Consists of Myriad Executor, a Mesos managed task that in turns manages a YARN Node Manager, and Myriad Scheduler, a plugin for the YARN Resource Manager that delegates resource negotiation to Mesos (and launches Myriad Executor processes on required nodes via Mesos). Supports fixed resource allocation to YARN Node Managers, as well as fine-grained scaling where resources are dynamically requested from Mesos. Includes a web based user interface and REST API that includes support for scaling YARN resources when using fixed resource allocation. Originally created by eBay, MapR and Mesosphere and dondated to the Apache Foundation in March 2015. Has not yet graduated or reached a 1.0 release, with development activity seeming very quiet since October 2016.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-nifi/&quot;&gt;Apache NiFi&lt;/a&gt; &lt;/td&gt; &lt;td&gt;General purpose technology for the movement of data between systems, including the ingestion of data into an analytical platform. Based on directed acyclic graph of Processors and Connections, with the unit of work being a FlowFile (a blob of data plus a set of key/value pair attributes). Supports guaranteed delivery of FlowFiles, with NiFi resiliently storing state (by default to a local write ahead log) and data blobs (by default a set of local partitions on disk), with all FlowFile transformations executed via a thread pool within the NiFi instance (with the option to deploy multiple NiFi instances as a cluster). All flows are configured in a graphical user interface, which is also used for management and operations (starting/stopping individual Processors and viewing real time statuses, statistics and other information). Also supports some record level operations (via RecordReaders and RecordSetWriters), data provenance (reporting on the processing events and lineage of individual FlowFiles), scheduling of Processor execution (based on periodic execution timers or cron specifications), multi-threaded Processor execution, configuration of Processor batch sizes (to enable low latency or high throughput), prioritised queues within Connections (allowing FlowFiles to be processed based on their age or a priority attribute as an alternative to FIFO), back pressure (based on counts or data volume against individual Connections) and pressure release (automatic discarding of FlowFiles based on their age), the ability to stream data to and from other NiFi instances and other streaming technologies, the ability to import and export flows as XML (flow templates), an expression language for setting Processor configuration and populating FlowFile attributes, Controller Services to provide shared services to processors (e.g. access to credentials, shared state), Reporting Tasks to output status and statistics information and a user security model. Extensible through the addition of custom Processors, Controller Services, Reporting Tasks and Prioritizers, and integrates with Apache Ranger and Apache Ambari. Originally developed at the NSA as Niagara Files, before being donated to the Apache Foundation in November 2014, graduating in July 2015. Java based, with development lead by Hortonworks after their aquisition of Onyara (which was set up by original NiFi developers to provide commercial support and services).&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache NiFi&amp;nbsp;&amp;gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-nifi/minifi/&quot;&gt;MiNiFi&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Lightweight headless version of NiFi used to collect and process data at it's source, before forwarding it on for centralised processing. Supports all key NiFi functionality including all NiFi processors, guaranteed delivery, data buffering (including back pressure and pressure release) and prioritised queuing, however flows are specified in configuration files, status information and statistics are only available via Reporting Tasks or via a CLI, and provenance can only be viewed by exporting events via Reporting Tasks to log files or a full NiFi instance. Supports warm re-deployments, automatically restarting to load a new configuration written to disk or pushed or pulled over HTTP. Available as a Java or Native C++ executable. Started in March 2016, with a 0.1 release in December 2016.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache NiFi&amp;nbsp;&amp;gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-nifi/registry/&quot;&gt;NiFi Registry&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A solution for the configuration management of NiFi flows. Integrates with NiFi to allow users to store, retrieve and upgrade flows, keeping a full history of all changes to a flow committed to the registry, with flows stored and organised by buckets. Supports local users and groups, or authentication via certificates, LDAP or Kerberos, with access control policies allowing read, write and delete permissions to be specified for buckets, users and groups. Has a Web based UI and a REST interface for managing buckets, local users and groups, viewing flow history and for managing access control. First released in January 2018.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-oozie/&quot;&gt;Apache Oozie&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Technology for managing workflows of jobs on Hadoop clusters. Primary concepts include workflows (a sequence of jobs modelled as a directed acyclic graph), coordinators (schedule the execution of workflows based on the time or the presence of data) and bundles (collections of coordinators), with all configuration specified in XML. Supports a range of technologies, including MapReduce, Pig, Hive, Sqoop, Spark, Java executables and shell scripts. Includes a server component, a metadata database for holding definitions and state (with support for a range of database technologies), a command line interface and a read only web interface for viewing the status of jobs. Also supports the parameterisation of workflows, the modelling of datasets (and the use of these to manage dependencies between workflows within coordinators), automatic retry and failure handling, and the ability to send job status notifications via HTTP or JMS. Open sourced by Yahoo in June 2010. Donated to the Apache Foundation in July 2011, graduating in August 2012. Commercial support available as part of most Hadoop distributions&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-orc/&quot;&gt;Apache ORC&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Self-describing, type-aware, columnar file format to enable efficient querying and storage of data on Hadoop. Provides built-in storage indexes, column statistics and bloom filters to allow execution engines to implement predicate and projection push-down, partition pruning and cost based optimisation for low latency reads. Uses multi-version concurrency control to support ACID transactions and allow Hive to implement bulk insert, update, delete and streaming ingest (micro batch) use cases. Implements type-aware encoding for efficient compression (run-length for integer and dictionary for string). Schema definition is stored along side the data and supports all primitive data types and complex nested data structures. Uses protocol buffers to store meta data. Comes with a Java library for reading and writing the file format and includes a MapReduce compatible API, a C++ library for reading the file format (donated by Vertica) and a set of Java and C++ tools for inspecting and benchmarking ORC files. Created by Hortonworks in January 2013 as part of the initiative to massively speed up Hive and improve the storage efficiency of data stored in Hadoop, split off from Apache Hive to become a separate top level Apache project in April 2015 with a 1.0 release in January 2016.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-parquet/&quot;&gt;Apache Parquet&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Data serialisation framework that supports a columnar storage format to enable efficient querying of data. Built using Apache Thrift, and supports complex nested data structures, using techniques from the Google Dremel paper. Consists of three sub-projects, the specification and Thrift definitions (Parquet Format), the Java and Hadoop libraries (Parquet MR) and the C++ implementation (Parquet CPP). Created as a joint effort between Twitter and Cloudera based on work started as part of Avro Trevni, with an initial v1.0 release in July 2013. Donated to the Apache Foundation in May 2014, graduating in April 2015. Has seen significant adoption in the Hadoop ecosystem.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-phoenix/&quot;&gt;Apache Phoenix&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A SQL query engine over Apache HBase tables that supports a subset of SQL 92 (including joins), and comes with a JDBC driver. Supports a range of features including ACID transactions (via Apache Tephra), user defined functions, secondary indexes, atomic upserts, views, multi tenancy tables (where each user or tenant can only see their data) and dynamic columns (which are only specified at query time). Supports a range of SQL DDL commands, creating and modifying underlying HBase tables as required, or can run over existing HBase tables in a read only mode. Comes with connectors to allow Spark, Hive, Pig, Flume and MapReduce to read and write Phoenix tables, and a number of utilities, including a bulk loader and a command line SQL tool. Open sourced by SalesForce in January 2013 at v1.0, donated to the Apache foundation in December 2013, before graduating in May 2014. Commercial support available through Hortonworks as part of HDP, with Cloudera making it available via Cloudera Labs without support. Active project with a range of contributors, including many from SalesForce and Hortonworks.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-pig/&quot;&gt;Apache Pig&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Technology for running analytical and data processing jobs against data in Hadoop. Jobs are written in Pig Latin (a custom procedural language that can be extended using user defined functions in a range of languages), which is then translated into Map Reduce or Tez (with Spark in preview) for execution. Supports both a batch mode for running pre-defined scripts and an interactive mode, and connectors for reading and writing to HBase and Accumulo as well as HDFS. Originally developed at Yahoo in 2006 before being donated to the Apache Foundation in October 2007. Graduated as an Hadoop sub-project in October 2008, before becoming a top level project in September 2010. Although has not had a v1.0 release, has been production quality for many years. Commercial support available as part of most Hadoop distributions&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-ranger/&quot;&gt;Apache Ranger&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A centralised security framework for managing access to data in Hadoop. Supports integration with LDAP and Active Directory for user authentication, a central policy server/store (with a web based administration interface and REST API), and plugins for Hadoop components (including HDFS, Hive, HBase, Storm, Knox, Solr, Kafka, YARN, Atlas and NiFi) to manage authorisation of user access to data. Supports data masking and row level access policies (currently only supported by Hive), the ability to define policies against tags as well as directly against resources (with tags assigned to resources externally, e.g. in Apache Atlas), and the ability to use more complex conditions (e.g. denying access after an expiration date or based on a users location). Extendable with the ability to add support for new services (Ranger Stacks) and to add custom decision rules (via content enrichers and condition evaluators). Also supports a full audit capability of access requests and decisions, and a key management service for HDFS encryption keys. An Apache project, donated in July 2014 as Argus by the Hortonworks following their acquisition of XA Secure, graaduating in February 2017. Reached v1.0 in March 2018, and is still under active development with a range of contributors.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-sentry/&quot;&gt;Apache Sentry&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A centralised security framework for managing access to data in Hadoop. Supports integration with LDAP and Active Directory for user authentication, a central policy server/store, and plugins for Hadoop components (including Hive, Solr, Impala and HDFS, with support for Kafka and Sqoop2 in preview) to manage authorisation of user access to data, although HDFS support is limited to Hive data only. Also supports row level filtering policies for Solr, and historical support for defining policies in files per service (Sentry Policy Files). Integrates with the Hue security app (to manage permissions) and with Cloudera Navigator (for authorisation audit events). Started in 2012 as Cloudera Access, with an initial 1.0 release in 2013 as Sentry. Donated to the Apache Foundation in August 2013, graduating in March 2016. &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-slider/&quot;&gt;Apache Slider (retired)&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Framework for hosting long running distributed applications on YARN, allowing YARN to manage the resources these applications use. Can handle any application that supports a base set of requirements (including being able to install and run from a tarball), with experimental support for docker packaged applications. Operates as a YARN application master (the Slider AM), an associated command line interface and lightweight agents to manage running components. Supports manual scaling, automatic recovery, rolling upgrades and component placement controls, and includes out of the box configuration for a number of applications including Accumulo, HBase, Kafka, Memcached, Solr, Storm and Tomcat. Originally donated to the Apache Foundation in April 2014 based on the Hortonworks Hoya (HBase on YARN) project, and subsequently consumed the DataTorrent Koya (Kafka on YARN) project. Retired before graduating in May 2018 following the plan to add support for long running services directly into YARN under YARN-4692.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-solr/&quot;&gt;Apache Solr&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A search server built on Apache Lucene with a REST-like API for loading and searching data. Supports a distributed deployment (SolrCloud) that can run over HDFS on an Hadoop cluster. Includes an administration web interface, an extensible plugin architecture, support for schemaless indexing, faceted, grouped and clustered results, hit highlighting, geo-spacial and graph searches, near real time indexing and searching, (experimental) streaming expressions for parallel compute (including support for MapReduce and SQL) and broad authentication and security capabilities. A sub-project of the Apache Lucene project, originally donated to the Apache foundation by CNET Networks in January 2006, graduating as a top level project in January 2007, before merging with the Lucene project in March 2010. Java based, with commercial support available as part of most Hadoop distributions (although this is bundled as Cloudera Search with CDH and HDP Search with HDP), as well as from Lucidworks.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A high performance general purpose distributed data processing engine based on directed acyclic graphs that primarily runs in memory, but can spill to disk if required, and which supports processing applications written in Java, Scala, Python and R (SparkR). Includes a number of sub-projects that support more specialised analytics including Spark SQL (batch and streaming analytics using declarative logic over structured data), Spark Streaming (micro-batch stream processing), MLlib (machine learning) and GraphX (graph analytics). Requires a cluster manager (YARN, EC2, Kubernetes and Mesos are supported as well as standalone clusters) and can access data in a wide range of technologies (including HDFS, other Hadoop data sources, relational databases and NoSQL databases). An Apache project, originally started at UC Berkley in 2009, open sourced in 2010, and donated to the Apache foundation in June 2013, graduating in February 2014. v1.0 was released in May 2014, with a v2.0 release in July 2016. Java based, with development led by Databricks (who sell a Spark hosted service), and with commercial support available as part of most Hadoop distributions.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Spark&amp;nbsp;&amp;gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-spark/graphx/&quot;&gt;GraphX&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Spark library for processing graphs and running graph algorithms, based on graph model that supports directional edges with properties on both vertices and edges. Graphs are constructed from a pair of collections representing the edges and vertex, either directly from data on disk using builders, or prepared using other Spark functionality, with the ability to also view the graph as a set of triples. Supports a range of graph operations, as well as an optimised variant of the Pregel API, and a set of out of the box algorithms (including PageRank, connected components and triangle count). First introduced in Spark 0.9, with a production release as part of Spark 1.2, however has seen almost no new functionality since then.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Spark&amp;nbsp;&amp;gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-spark/mllib/&quot;&gt;MLlib&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Spark library for running Machine Learning algorithms. Supports a range of algorithms (including classifications, regressions, decision trees, recommendations, clustering and topic modelling), including iterative algorithms. As of Spark 2.0 utilises a DataFrame (Spark SQL) based API, with the original RDD based API now in maintenance only. First introduced in Spark 0.8 after being collaboratively developed with the UC Berkeley MLbase project, and still under active development.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Spark&amp;nbsp;&amp;gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-spark/spark-sql/&quot;&gt;Spark SQL&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Spark library for processing structured data, using either SQL statements or a DataFrame API. Supports querying and writing to local datasets (including JSON, Parquet, Avro, Orc and CSV) as well as external data sources (including Hive and JDBC), including the ability to query across data sources. Includes Catalyst, a cost based optimiser that turns high level operations into low level Spark DAGs for execution. Also includes a Hive compatible Thrift JDBC/ODBC server that's compatible with Beeline and the Hive JDBC and ODBC drivers, and a REPL CLI for interactive queries. Introduced in Spark 1.0 with a production release in Spark 1.3, with substantially improved SQL functionalities in Spark 2.0.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Spark&amp;nbsp;&amp;gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-spark/spark-streaming/&quot;&gt;Spark Streaming&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Spark library for continuous stream processing, using a DStream (discretized stream) API. Uses a micro-batch execution model leveraging core Spark to execute the specified logic against each micro-batch (a DStream is a sequence of Spark RDDs), with the ability to also use other Spark batch operations (including Spark SQL and MLlib) against each micro-batch. This model also provides fault tolerance through exactly-once processing semantics. Supports a number of data sources (including HDFS, sockets, Flume, Kafka, Kinesis and messaging buses), as well as functions to maintain state and to execute windowed operations. First introduced in Spark 0.7, with a production release as part of Spark 0.9, however development appears to be largely stopped following the introduction of Structured Streaming in Spark 2.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Spark&amp;nbsp;&amp;gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-spark/structured-streaming/&quot;&gt;Structured Streaming&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Extension to the Spark SQL DataFrame API to allow Spark SQL queries to be executed over streams of data, with the engine continuously updating and maintaining the result as new data arrives. Uses the full Spark SQL engine (including the Catalyst optimiser), and supports end-to-end exactly-once semantics via checkpointing when sources have sequential offsets. Supports aggregations over sliding event-time windows, including support for late data and watermarking. Introduced in Spark 2.0 with a production release in Spark 2.2.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-sqoop/&quot;&gt;Apache Sqoop&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Specialist technology for moving bulk data between Hadoop and structured (relational) databases. Command line based, with the ability to import and export data between a range of databases (including mainframe partitioned datasets) and HDFS, Hive, HBase and Accumulo. Executes as MapReduce jobs, supports parallel partitioned unloads, writing to Avro, Sequence File, Parquet and text files, incremental imports and saved jobs that can be shared via a simple metadata store. An Apache project, started in May 2009 as an Hadoop contrib module, migrating to a Cloudera GitHub project in April 2010 (with a v1.0 release shortly after), before being donated to the Apache foundation in June 2011, graduating in March 2012. The last major release (v1.4) was in November 2011, with only minor releases since then. However in January 2012 a significant re-write was announced as part of a proposed v2.0 release to address a number of usability, security and architectural issues. This will introduce a new Sqoop Server and Metadata Repository, supporting both a CLI and web UI, centralising job definitions, database connections and credentials, as well as enabling support for a wider range of connectors including NoSQL databases, Kafka and (S)FTP folders. Java based, with commercial support available as part of most Hadoop distributions.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-storm/&quot;&gt;Apache Storm&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Specialised distributed stream processing technology based on a single record (not micro batch) model with at least once processing semantics. Processing flows are called topologies based on a directed acyclic graph of spouts (which produce unbounded streams of tuples) and bolts (which process streams and optionally produce output streams). Supports high throughput and low latency use cases, horizontal scalability, fault tolerance (failed workers are automatically restarted and failed over to new nodes if required), back pressure, windowing (with support for sliding and tumbling windows based on time or event counts), stateful bolts and a shared bolt storage cache (that's updatable from the command line). Also includes a higher level micro batch API (Trident) that supports exactly-once processing semantics, fault-tolerant state management and higher level operations including joins, aggregations and groupings, support for SQL (StormSQL) and frameworks and utilities to make defining and deploying topologies easier (Flux). Has both a graphical web based and command line interface, plus a REST API. Primarily written in Clojure, JVM based, but supports multiple languages through the use of Thrift for defining and submitting topologies, and the use of spouts that can interface to other languages using JSON over stdin/stdout. Originally created at BackType, before being open sourced in September 2011 after the acquisition of BackType by Twitter. Donated to the Apache Foundation in September 2013, graduating in September 2014, with a 1.0 release in April 2016. Has multiple reference cases for being deployed at scale, including Twitter, and is still under active development.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-superset/&quot;&gt;Apache Superset (incubating)&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Web based tool for interactive exploration for OLAP style data, supporting interactive drag and drop querying, composable dashboards and a SQL workspace (SQL Lab). Originally built to query Druid, but now supports a wide range of SQL (and NoSQL) databases, with a lightweight semantic layer allowing control of how data sources are displayed in the UI and which fields can be filtered and aggregated. Users can create Slices (a visualisation of the results of an OLAP style query, with support for a range of visualisations including charts, heat maps, maps, pivot tables, and word clouds amongst others, the ability to configure the query using UI controls, and the ability to configure and customise the visualisation), with multiple slides then composable into a Dashboard (that also support interative filters that connect to multiple slices). Also supports a full SQL IDE (SQL Lab) that supports multiple tabs, a full query history, the ability to apply any data visualisation to results and to browse database metadata, and support for long-running queries using a backend query handler and results store. Other features include query results caching, a plug-in and extensibility framework, the ability to brand and skin the web application, and a robust security model for controlling access to slices, dashboards and data, with support for a range of authentication methods including OpenID, LDAP and OAuth. Originally developed by AirBnB in 2015 as Panoramix, before being renamed to Caravel and then to Superset. Donated to the Apache Foundation in June 2017 and still incubating, with development now led by AirBnB and Hortonworks.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-tajo/&quot;&gt;Apache Tajo&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Distributed analytical database engine. Supports HDFS, Amazon S3, Google Cloud Storage, OpenStack Swift and local storage, and querying over Postgres, HBase and Hive tables. Provides a standard SQL interface, JDBC driver, and supports partitioning, compression and indexing (currently experimental). An Apache project, donated by Gruter in March 2013, and graduated in April 2014. Java based, with development lead by Gruter who also supply commercial support, a Tajo managed service, a data analytics hub (Qrytica) built on Tajo, and a Tajo Data Warehouse appliance.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-tez/&quot;&gt;Apache Tez&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Data processing framework based on Directed Acyclic Graphs (DAGs), that runs natively on YARN and was designed to be a replacement for the use of MapReduce within Hadoop analytical tools (primarily Hive and Pig), and therefore offer better performance with similar scalability. Targeted more at application developers rather than data engineers, includes a number of performance optimisations (including dynamic DAG re-configuration during execution and re-use of sessions and containers), and comes with a UI for viewing live and historic Tez job executions based on information in the YARN Application Timeline Server. Created by Hortonworks and donated to the Apache Foundation in February 2013 before graduating in July 2014. Still under active development, and now used by Cascading and Flink in addition to Hive and Pig.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-whirr/&quot;&gt;Apache Whirr&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A set of libraries (now moved to the Apache Attic and no longer maintained) for deploying and managing a supported set of services in a cloud environment. Written in Java, with explicit support for a set of standard services (including Hadoop, Cassandra, HBase, Elasticsearch and Solr) configured through property files. Uses jclouds to provision and manage cloud infrastructure, and provides both a CLI and Java API. Originally a set of python scripts maintained as an Hadoop contrib project. Donated to the Apache Foundation in May 2010, graduating in August 2011. Development ceased in September 2012, with the project being moved to the Apache Attic in March 2015.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-zeppelin/&quot;&gt;Apache Zeppelin&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A web based notebook for interactive data analytics. Supports a wide range of interpreters (including Spark, JDBC SQL, Pig, Elasticsearch, Beam, Flink, Shell, Python amongst many others), a range of output formats (plain text, HTML, mathematical expressions using MathJax and tabular data), a range of visualisations for tabular data (including the ability to add more via a JavaScript NPM based plugin system called Helium), forms for user entry of parameters, and an Angular API to enable dynamic and interactive functionality within notebooks. Has a plugable storage for notebooks (with out of the box support for git, S3, Azure and ZeppelinHub), support for multi-user environments and a security model. Open sourced by NFLabs (now called ZEPL) in 2013 before being donated to the Apache Foundation in December 2014, graduating in May 2016. Under active development with a wide range of contributors, led by ZEPL, who sell Zeppelin as a managed service (previously called ZeppelinHub, now just called Zepl).&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-zookeeper/&quot;&gt;Apache ZooKeeper&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Service for managing coordination (e.g. configuration information and synchronisation) of distributed and clustered systems. Based on a hierarchical key-value store, with support for things such as sequential nodes (whose names are automatically assigned a sequence number suffix), ephemeral nodes (which only exist whilst their owners session exists) and the ability to watch nodes. Guarantees that all writes are serial and ordered (i.e. all clients will see them in the same order), meaning it's more appropriate for low write high read scenarios. Can run in a high available cluster called an ensemble. Originally an Hadoop sub-project, but graduated to a top level Apache project in January 2011. Java based, still under active development, and used by a range of technologies including Hadoop, Mesos, HBase, Kafka and Solr.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/azure-hdinsight/&quot;&gt;Azure HDInsight&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Service for dynamically provisioning Hadoop clusters on Azure Virtual Machines based on a set of pre-defined cluster templates for Hadoop, Spark, HBase, Storm, Hive LLAP, Kafka or Machine Learning. Based on the Hortonworks HDP distribution of Hadoop, with support for Azure Blob Storage and Azure Data Lake Storage (both strongly consistent) but not local HDFS. Supports manual scaling of in-flight clusters, integration with Azure Log Analytics, encryption, use of external SQL database for Hive metadata and script actions (scripts that can be run during or after cluster creation). Comes with an Enterprise Security Package add-on that adds integration with Azure Active Directory, role based access control for Hive and Spark using Apache Ranger and security audit logs. Manageable via the Azure Portal, Powershell, a REST API and integrates with a number of development IDEs (e.g. for interactive development of Spark jobs). Priced at an hourly rate (billed per minute) based on the VM instance types being used, in addition to any Virtual Machine charges.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/chronos/&quot;&gt;Chronos&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A framework for Apache Mesos to schedule and orchestrate jobs to periodically run at fixed times, dates or intervals in a clustered environment. Leverages Mesos for resource allocation and isolation and provides a REST API and web interface for job definition and job management. Reoccurring jobs are defined using ISO8601 repeating interval notation and may also be triggered by the completion of other jobs to create dependency based jobs. Uses ZooKeeper for state management and typically deployed as a service under Marathon for high-availability. Supports writing and exporting of job metrics to various systems for further analysis and notifications to various endpoints such as email and chat messaging systems. Originally created at AirBnB and written in Scala, opened sourced in March 2013 under the Apache 2.0 license, hosted under the Apache Mesos Community Projects group-owned repositories on GitHub.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/cloudbreak/&quot;&gt;Cloudbreak&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Solution for deploying and managing Hadoop clusters on cloud infrastructure based on automatically provisioned infrastructure running base docker images with Hadoop provisioned on top via Apache Ambari using Blueprints. Includes out of the box support for Amazon Web Services, Microsoft Azure, Google Cloud Platform and OpenStack, plus a Service Provider Interface (SPI) for adding support for new providers. Supports automated scaling of clusters based on Ambari Metrics and Alerts (Periscope), custom scripts that can be run on hosts before or after deployment (Recipes), a number of out of the box Blueprints, the use of custom docker images, data locality specifiers, Kerberized clusters and support for external AD/LDAP servers. Manageable through a web UI, a REST API, a CLI and an interactive shell. Originally created by SequenceIQ, with an initial beta release in July 2014, with SequenceIQ then acquired by Hortonworks in April 2015, and a 1.0 release of Cloudbreak included in HDP 2.3 in July 2015. Open sourced under the Apache 2.0 licence, with a stated plan for the code to be donated to the Apache Foundation.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/cloudera-altus/&quot;&gt;Cloudera Altus&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Platform for accessing individual CDH capabilities as services. Currently supports the deployment and management of CDH clusters on cloud infrastructure (Director, previously Cloudera Director), the execution of Spark, MapReduce or Hive over Spark or MapReduce jobs (Altus Data Engineering), the dynamic provisioning of Impala clusters (Altus Data Warehouse), with a stated future plan for R- and Python-based machine learning workloads (Altus Data Science) and an HBase based operational database service. Runs on Amazon Web Services or Microsoft Azure over external data in Amazon S3 or Azure Data Lake Storage, with a stated plan to expand support to other cloud service providers (specifically the Google Cloud Platform) in the future. Includes Altus SDX, allowing metadata (e.g. Hive table definitions) to be automatically persisted across transient workloads, referenced via a namespace. Supports a web based UI, a (Python) CLI and a Java SDK, with full user authentication and role based access management, and integration with AWS and Azure security. Launched in May 2017, with a per node / per hour pricing model.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Cloudera Altus&amp;nbsp;&amp;gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/cloudera-altus/data-engineering/&quot;&gt;Cloudera Altus Data Engineering&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Managed service for the execution of Spark, MapReduce or Hive (over MapReduce or Spark) jobs using managed CDH clusters on AWS and Azure cloud infrastructure over data in Amazon S3 or Azure Data Lake Storage (ADLS). Jobs run on clusters within a defined AWS or Azure environment, which can be transient (created and terminated on demand) or persistent, with each cluster supporting one service type (Hive, Spark, MapReduce) with a fixed node count. Jobs can then be queued individually or in batch for execution against an existing cluster or against a dynamically created cluster, with jobs specified either by uploading a JAR to S3 (for Spark or MapReduce) or via a Hive script (either directly uploaded or uploaded to S3), and the ability to either halt or continue the queue on job failure. Supports access to clusters via SSH, read only access to Cloudera Manager, a SOCKS proxy to cluster web UIs (including the CM admin console, YARN history server and Spark history server), and access to server and workload logs (including the ability to write these to S3 for access after clusters have been terminated). All nodes managed by Altus are tagged with the cluster name and node role (master, worker or Cloudera Manager) and bootstrap scripts can be specified for execution on nodes after cluster startup.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Cloudera Altus&amp;nbsp;&amp;gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/cloudera-altus/data-warehouse/&quot;&gt;Cloudera Altus Data Warehouse&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Impala as a managed service, supporting the dynamic provisionng of Impala clusters on AWS and Azure cloud infrastructure over data in Amazon S3 or Azure Data Lake Storage (ADLS). Clusters consist of a coordinator node and multiple worker nodes, with read-only access to a Cloudera Manager instance, with the node count fixed on creation. Supports JDBC and ODBC access to data, along with access to clusters via SSH, read only access to Cloudera Manager and a SOCKS proxy for access to the Impala web UIs. Previously known as Cloudera Altus Analytical DB.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Cloudera Altus&amp;nbsp;&amp;gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/cloudera-altus/director/&quot;&gt;Cloudera Altus Director&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Solution for deploying and managing Cloudera CDH Hadoop clusters on cloud infrastructure based on automatically provisioned infrastructure with Hadoop provisioned on top via Cloudera Manager. Includes out of the box support for Amazon Web Services, Microsoft Azure and Google Cloud Platform, with support for vSphere available from VMWare, with a Service Provider Interface (SPI) for adding support for new providers. Server component must be manually deployed via an RPM. Supports the ability to scale clusters up and down, clone clusters, run post deployment scripts, and create Kerberized and highly available clusters. Manageable through a web UI, a REST API (with Python and Java APIs) and a CLI. Released as Cloudera Director at 1.0 in October 2014 as part of Cloudera Enterprise 5.2, being renamed to Cloudera Altus Director in September 2018 as part of CDH 6. Free to download and use, with commercial support available as part of a Cloudera Enterprise subscription.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/cloudera-cdh/&quot;&gt;Cloudera CDH&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A distribution of Hadoop based on the addition of a number of closed source products, including Cloudera Manager (for installing and managing clusters) and Cloudera Navigator (for managing metadata and the encryption of data). Bundled projects tend to lag the open source versions and pull forward more patches than other distributions. Also comes with a number of add-ons, including ODBC and JDBC drivers for Hive and Impala, a number of Apache projects that aren't (yet) part of the core CDH distribution, and Workload XM (a cloud based service for analysing job logs). Available via RPMs, or can be installed using Cloudera Manager (for local installs) or Cloudera Director (for installation on cloud platforms). Comes in a number of editions including Cloudera Enterprise (under an annual per node or elastic cloud licence model with commercial support) and Cloudera Express (a free version without some enterprise features), with Cloudera Enterprise coming in a range of licence options (listed on the Cloudera website under products) with each including support for different Apache products. First released in March 2009.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/cloudera-data-science-workbench/&quot;&gt;Cloudera Data Science Workbench&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A web based notebook for interactive data analytics that uses docker to provide custom execution environments for each notebook. Supports Python, R and Scala interpreters, plus remote execution of Spark with out of the box support for Hadoop security. Notebook code is run within a docker container in a managed Kubernetes instance, allowing different libraries to be installed and used by different notebooks, and other dependancies to be installed via terminal access to the container or via custom Docker images. Also includes support for version control (via git), tracking of model tests (Experiments), automatic deployment of models and all dependancies behind a REST endpoint (Models), collaboration via shared projects, sharing of notebooks via HTTP URLs, publishing of notebooks as HTML and scheduled execution of notebooks via workflows (including dependancies on other jobs). Originally created by Sense.io, which was acquired by Cloudera in March 2016. Initial GA release was 1.0 in April 2017.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/cloudera-manager/&quot;&gt;Cloudera Manager&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Platform for installing, managing and monitoring Cloudera CDH Hadoop clusters. Supports creation of clusters using step by step wizards, plus cluster templates for creating multiple clusters with the same configuration (e.g. dev, test and production), using either native OS packages or parcels (a Cloudera Manager distribution format that has a number of advantages over packages). Also supports the administration and configuration of clusters (including user and resource management, and the ability to manage multiple clusters); the automated Kerberization of clusters; monitoring of cluster, host and service statuses, health and metrics; generation of events and the use of custom triggers to take action on these; the visualisation of metrics; centralised log management; HDFS reports and automatic replication of data to a backup/DR cluster. Also integrates directly with Cloudera Support to enable proactive support. Web based, with a REST API and a full security model with auditing of all actions, and the ability to add support for custom services. Introduced in January 2012 as a replacement for the Cloudera Management Suite (CMS). Available for free without some enterprise features, or as part of a Cloudera CDH subscription.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/cloudera-navigator/&quot;&gt;Cloudera Navigator&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A suite of solutions including Navigator Data Management (technical metadata management, lineage, cluster activity and analytics, cluster audit and automated policy actions), Navigator Encryption (filesystem level encryption, key management and integration with HDFS transparent encryption), and Navigator Optimizer (a solution for identifying SQL workloads that are candidates for migration to Hadoop and then optimising these once on Hadoop)) built around the Cloudera CDH Hadoop distribution. All products are commercial closed source products, that are only available with an appropriate Cloudera Enterprise licence.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/cloudera-navigator/data-encryption/&quot;&gt;Cloudera Navigator Data Encryption&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A suite of products that complement HDFS transparent encryption to provide data at rest encryption across an Hadoop cluster. Includes Navigator Encrypt (a solution for encrypting Linux filesystems, with access granted to approved processes), Navigator Key Trustee Server (a software based solution for managing encryption keys), Navigator Key HSM (allows Navigator Key Trustee Server to use a Hardware Security Module as the root of trust for keys), Navigator Key Trustee KMS (an Hadoop Key Management Service that uses Navigator Key Trustee Server as the underlying key store) and Navigator HSM KMS (an hadoop Key Management Service backed by an HSM where encryption zone keys originate on and never leave the HSM). First released in 2014 following the acquisition of Gazzang.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/cloudera-navigator/data-management/&quot;&gt;Cloudera Navigator Data Management&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Solution for managing data on a CDH Hadoop cluster. Automatically extracts metadata relating to HDFS, Hive, Impala, MapReduce, Oozie, Pig, S3, Spark, Sqoop and YARN, including data structures (databases, tables and columns) and jobs (relating to data transformation) based on activity within a cluster (rather than statically analysing code), allowing it to be searched, filtered and viewed, including displaying lineage diagrams showing how data moves through the system, a Data Stewardship dashboard of key data management information (including statistics on the data held in the cluster and the activity relating to this data), analytics on the data held in HDFS, and a full audit capability of all activity on the cluster. Allows custom metadata to be added to objects, including descriptions, key-value pairs and tags, with the option to define metadata namespaces and data types / value constraints (managed metadata), plus the ability to pre-set custom attributes (via job properties for MapReduce jobs and JSON .navigator files for HDFS files), and the ability to define data lifecycle management policies (allowing actions to be specified based on metadata, e.g. to archive any files that haven't been accessed for six months). Web based, with a full user security model, and a REST API and Java SDK for integrating external applications with metadata held in Navigator. Initial 1.0 release was in February 2013.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/cloudera-navigator/optimizer/&quot;&gt;Cloudera Navigator Optimizer&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A web based hosted service for analysing SQL logs from a range of relational databases to provide guidance on offloading workloads to Hadoop, and from Hive and Impala to provide guidance on optimising workloads running on Hadoop. Can analyse query logs, query metadata, schemas and statistics, and includes a Java utility to mask literal values in SQl queries and logs, and to encrypting schema identifiers before files are uploaded. Provides analytics on the overall query workload (including by similarity and risk, as well as by uploaded metrics such as cpu usage, memory usage and file system reads/writes) and recommendations for improvements to queries (to reduce risk, and to make external queries Hadoop compatible), with risk representing the level of Hadoop compatibility. Formally Xplain.io which was founded in 2013, acquired by Cloudera in February 2015, with a GA release as Cloudera Navigator Optimizer in July 2016.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/cloudera-search/&quot;&gt;Cloudera Search&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A distribution of Apache Solr that also includes a number of tools for integrating with Solr using Morphlines. Includes two utilities for loading data from HDFS, the Crunch Indexer Tool (direct Solr inserts using Crunch over Spark or MapReduce), and the MapReduce Indexer Tool (creates Solr index files using Map Reduce, optionally putting these live), plus two utilities for loading data from HBase based on the Lily HBase Indexer, the Batch Indexer (for batch loads) and the NRT (Near Real Time) Indexer (for continuous replication of HBase events). First released in June 2013, with a GA release in September 2013 as part of CDH 4.3. Included tools are open sourced under an Apache 2.0 licence.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/confluent-enterprise/&quot;&gt;Confluent Enterprise&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A package of software built around Apache Kafka and the Confluent Open Source product, with the addition of a number of commercial closed source products including a JMS client, Control Centre (for managing Kafka clusters), Multi DC Replication (active-active replication between Kafka clusters) and Auto Data Balancing. The JMS client is an implementation of the standard JMS provider interface over a Kafka topic. Control Centre is a web based UI that supports system health monitoring (broker and topic metrics and statuses based on information from the Confluent Metrics Reporter, a plugin for Kafka clusters that reports metrics to a Kafka topic), real time stream monitoring (statistics on the production and consumption of messages including the level of consumption and latency based on statistics from Confluent Monitoring Interceptors, a plugin for Kafka producers and consumers that reports statistics to a Kafka topic), the GUI based creation of Kafka connect pipelines, viewing of cluster and topic information, and e-mail alerting based on custom triggers on on topic, consumer group or broker metrics. Multi DC Replication is an optional licenced connector for Kafka connect that enables replication between two remote Kafka clusters, including active-active synchronisation. Auto Data Balancing is a tool for re-balancing topic partitions across cluster nodes, recommending moves based on information form the Confluent Metrics Reporter and rack awareness to ensure load is distributed evenly across the cluster, and easily allowing for the additional or removal of nodes. Also includes the Confluent Support Metrics features which collects broker and cluster metadata and metrics and forwards these onto Confluent for proactive support. Confluent Enterprise is the commercial version of their Confluent Platform, with an open source version also available as Confluent Open Source. Includes full commercial support for all open and closed source products. First GA release was version 1.0 of the Confluent Platform in February 2015.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/confluent-open-source/&quot;&gt;Confluent Open Source&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A package of open source projects built around Apache Kafka with the addition of the Confluent Schema Registry, Kafka REST Proxy, a number of connectors for Kafka Connect and a number of Kafka clients (language SDKs). The Schema Registry allows Kafka message schemas to be defined and versioned centrally, with schemas stored in a Kafka topic, a REST interface for managing schemas, support for schema evolution (with support for backwards, forwards and full compatibility between versions), plugins for Kafka clients to serialise / deserialise messages using the schemas, and support for running as a distributed service. The REST Proxy provides a REST interface onto a Kafka cluster, with support for viewing cluster metadata (covering brokers, topics, partitions and configuration) and both submitting and consuming messages, with support for JSON, JSON-encoded Avro and base64 messages, and integration to the Schema Registry for Avro messages. Bundled connectors for Kafka Connect include HDFS, JDBC, Elasticsearch and S3. Bundled client libraries (all open source) include those for C/C++, Go, .NET and Python. Also includes a Version Collector that reports version information to Confluent. Used to include Camus, a tool for unloading Kafka topics to HDFS, but this has now been deprecated in favour of Kafka Connect. Development of the open source projects is led by Confluent, who then bundle and distribute them for free as the Confluent Open Source version of their Confluent Platform, with the Confluent Enterprise version adding a number of closed source features and commercial support for all open and closed source products. Available as a zip, tar, deb or rpm package from Confluent, with all source code hosted on GitHub. First GA release was version 1.0 of the Confluent Platform in February 2015.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/denodo-platform/&quot;&gt;Denodo Platform&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Data Virtualisation platform, enabling a logical schema to be defined over a range of relational, NoSQL, flat file and application / data APIs that can then be queried through a range of end points. Supported data sources include a wide range of databases (relational, in memory, MPP, Hadoop, cloud, OLAP cube and NoSQL), flat files (Hadoop, text, binary, Office, including support for (S)FTP, compression and encryption), application APIs (e.g. Salesforce, SAP, Oracle e-business, Twitter), RDF semantic repositories via SPARQL, mainframes, data APIs (SOAP and REST), JMS queues and the ability to scrape web pages, with data accessible via SQL (JDBC, ODBC), data APIs (SOAP and REST) and web widgets (Sharepoint, Java, AJAX), with the ability to transform, cleanse and combine data from multiple sources into a single semantic model using the relevant source system query language. Supports a dynamic query optimiser (which pushes query logic down to the underlying data source, with the ability to move data between sources and take advantage of data replicated in multiple sources to maximise logic pushdown), caching (either by query or by full materialisation, allowing tables in the semantic layer to be pre-generated, with support for scheduled and incremental updates and the use of external ETL tools), data writes back to source (with support for a distributed transaction manager and 2-phase commits), a full security model (role based access control at the row/column level, with authentication pass-through to data sources), resource and workload management, metadata visualisation (including search and lineage views), self service data discovery (execution of ad-hoc queries outside of the semantic layer), search (over data and metadata, including support for semantic mining and extraction of text sources), an SDK (for adding new source adapters, custom functions and stored procedures), and a graphical UI (supporting wizard-driven configuration, integration with external configuration management tools and release management). Can we deployed stand alone or as a cluster (supporting both active/active and active/passive configurations and shared caches, with support for geo-replication), and is also available for AWS and as a free Denodo Express version (with limits on the number of active queries and results). A commercial product from Denodo Technologies Inc, who were founded in 1999 with the first release of the Denodo Platform shortly afterwards.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/elastic-cloud/&quot;&gt;Elastic Cloud&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Solution for provisioning, managing and monitoring Elasticsearch clusters (including Kibana and X-Pack) either on AWS or Google Cloud Platform as a managed service under a subscription model (Elastic Cloud) or on your own physical or cloud infrastructure (Elastic Cloud Enterprise). Includes support for scripting and plugins, high availability across multiple zones, and supports automated security configuration, upgrades, scaling and backups through a management web UI (Elastic Cloud Console / Cloud UI). Elastic Cloud Enterprise supports the same capability on your own infrastructure, and includes an API in addition to the web UI for configuring and managing clusters, with Elasticsearch and Kibana provisioned using Docker containers. Elastic Cloud is available under a range of subscription licence tiers with differing levels of support and some feature differences; Elastic Cloud Enterprise is freely available but requires you to provide your own Elasticsearch licences. Elastic Cloud was launched in July 2015; Elastic Cloud Enterprise was first released as alpha in December 2016, with a 1.0 GA release in May 2017. Elastic Cloud is the only Elasticsearch service offering that includes the Elastic X-Pack features.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/elastic-x-pack/&quot;&gt;Elastic X-Pack&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A set of commercial extensions to the Elastic open source products (Elasticsearch, Kibana and Logstash) that were discontinued in June 2018 with the release of version 6.3 of the Elastic stack, with the individual components now open sourced under an Elastic licence and bundled with the relevent Elastic open source products, although the majority still require a commercial licence from Elastic to be enabled. Included Security (formally Shield), Alerting (formally Watcher), Monitoring (formally Marvel), Reporting, Graph, Machine Learning and Application Performance Monitoring (APM).&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/elasticsearch/&quot;&gt;Elasticsearch&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A distributed search server built on Apache Lucene that supports a number of advanced analytics over search results. Data is stored in indexes, with each index able to support multiple schemas (types), with the data itself sharded to support distributed parallel queries, with multiple replicas of each shard providing resilience and redundancy. Supports both pre-defined and schemaless types, all standard Lucene functionality (including faceting, grouping, clustering, hit highlighting, geo support, near real time indexing), the ability to update and delete documents (by id or query), upsert operations, batch operations, re-indexing (from one index into a second index), generated or calculated fields, document versioning and optimistic concurrency control, nested searches based on sub-documents or explicit parent-child document links, templated searches, a range of aggregations (include support for metrics, bucketing results, matrix calculations and custom aggregations using pipelines), custom analysers for indexing data, custom transformation pipelines prior to indexing (via an ingest node), the ability to query across clusters (cross cluster search), a plugin framework, registered queries that are executed against newly indexed data (percolation) and the ability to snapshot and restore indexes using HDFS, S3, Azure and Google Cloud. Also now includes a number of features that were previously bundled separately in the Elastic X-Pack, including Security (encryption of data and links, authentication via LDAP and Active Directory, authorisation at the cluster, index, document and field level, and full audit logging), Monitoring (export of cluster, nod and index metrics), Alerting (via Watcher, allowing registration of scheduled queries over monitoring data that can perform a number of extensible actions), Graph (APIs for working with relationships, with connections between indexed terms generated on the fly using Elasticsearch aggregations and relevance scoring), SQL access (via a REST API, CLI or JDBC interface), Machine Learning (support for automated anomaly detection jobs over time-series data run on the ElasticSearch cluster) and Rollup (aggregation of historical data), the majority of which require a commercial licence from Elastic in order to be enabled. Comes with a REST API, with clients available for a range of languages including Java, C#, Python, JavaScript, PHP, Perl and Ruby. First released in February 2010, with a 1.0 release in February 2014. Open source under the Apache licence, with the exception of the X-Pack components which are under an Elastic licence following the open sourcing of X-Pack in version 6.3. Development is led by Elastic, who were formed in 2012 by the creator of Elasticsearch and a lead Lucene contributor, and who provide commercial support, licences to enable the commercial X-Pack features, and an on-site or public cloud service offering (Elastic Cloud).&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/elasticsearch-hadoop/&quot;&gt;Elasticsearch-Hadoop&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A suite of open source components for querying and writing documents to Elasticsearch from a range of Hadoop technologies, including MapReduce, Hive, Pig, Spark, Cascading and Storm. Specific functionality includes InputFormat and OutputFormat libraries for MapReduce, a Hive storage handler allowing external tables to be defined over Elasticsearch indexes, read and write functions for Pig, Java and Scala RDD based libraries for Spark, Spark SQL support, Spark Streaming support, an Elasticsearch Tap for Cascading and a dedicated Spout and Bolt for Storm. Used to include functionality for writing snapshots of Elasticsearch indexes to HDFS which is now part of the Snapshot and Restore functionality in Elasticsearch. Certified with CDH, MapR and HDP.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/google-cloud-dataproc/&quot;&gt;Google Cloud DataProc&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Service for dynamically provisioning Hadoop clusters on Google Compute Engine based on a single standard set of Hadoop services. Supports selection of virtual machines (including custom machine types and machines with GPUs), usage of custom VM images, a claimed cluster startup time of less than 90 seconds, local storage and HDFS filesystem, programmatic execution of jobs, workflows (parameterisable operations that create clusters, run jobs and then delete the cluster), manual and automatic scaling, initialisation actions (to install extra services or run scripts, with a set of open source actions available), optional components (automatic addition of extra services), automatic deletion of clusters (based on time, usage or idleness), integration with Stackdriver Logging and Monitoring and encryption of data in HDFS and Cloud Storage. Manageable via the Google Cloud Console Web UI and SDK plus an RPC and REST API. Priced an an hourly rate (charged per second) based on the specification of the VMs being used, which is in addition to any Compute Engine or Persistent Disk charges.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/google-cloud-storage/&quot;&gt;Google Cloud Storage&lt;/a&gt; &lt;/td&gt; &lt;td&gt;An object store service with strong consistency, multiple storage tiers and deep integration to the Google Cloud ecosystem. Objects are organised into buckets and indexed by string, with the option to list objects by prefix and to summarise results based on a delimiter allowing a filesystem to be approximated. Storage tiers supported include multi-regional (data is distributed across regions in a geo area), regional, nearline and coldline (designed for data accessed less than once per month/year respectively). Supports object lifecycle management allowing for automatic deletion or moving of objects between storage tiers. Supports versioning of objects, access control (via Google Cloud IAM, bucket and object ACLs and time-limited access via signed URLs), encryption of objects and support for SSL connections, auditing of object operations via Google Cloud Audit, gzip uncompression on read, custom domains, multi-part uploads via merging of objects after upload (Composite Objects), acccess and storage logs as downloadable CSV files and batching of request. Quotes a 99.999999999% guarentee that data won't be lost, and availability of 99.9% for regional and 99.95% for multi-regional storage tiers. Provides a web based management console (Google Cloud Platform Console), CLI (gsutil), JSON and XML REST API and SDKs for a wide range of languages.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/greenplum/&quot;&gt;Greenplum&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A shared nothing, massively parallel processing (MPP) database optimised for analytical / OLAP workloads. Based on a fork PostgreSQL, it is essentially multiple PostgreSQL databases working together as a single logical database. Supports a cost-based query optimiser optimised for large analytical workloads, multiple storage models (including append only, columnar and heap), full ACID compliance and concurrent transactions, multiple index types, broad SQL support, a range of client connectors (including ODBC and JDBC), high capacity bulk load and unload tools, in database query language support (including Python, R, Perl, Java and C), and in database analytics support (including machine learning via Apache MADLib, search using Solr via GPText, geographic analytics via PostGIS and encryption via PGCrypto). Originally created by Greenplum (the company) which was founded in September 2003 before being brought by EMC in 2010, with Greenplum (the database) then spun out as part of Pivotal Software in 2013 before being open sourced in in October 2015 under the Apache 2.0 licence with the source code hosted on GitHub. Development is still led by Pivotal (with little evidence of outside contributions), who also distribute binaries as Pivotal Greenplum and provide training, consultancy and support.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/hortonworks-data-cloud-for-aws/&quot;&gt;Hortonworks Data Cloud for AWS&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Service that supports the creation and management of HDP clusters on Amazon Web Services (AWS). Management is done through a Cloud Controller AWS Product that provides a web interface and CLI for orchestrating the creation of AWS resources and the deployment of clusters using Ambari, and the subsequent scaling or cloning of the cluster. Supports a number of standard cluster types, including Data Science (Spark, Zeppelin), EDW-ETL (Hive, Spark) and EDW-Analytics (Hive, Zeppelin), with clusters also including Tez, Pig and Scoop, along with a number of standard node types, including worker nodes (that support HDFS and YARN) and computer nodes (that only support YARN). Clusters are designed to be ephemeral, however Amazon RDS can be used to provide persistent storage of Cloud Controller and Hive metadata, and Amazon S3 can be used to provide persistent cluster storage. Also supports Hortonworks SmartSense, cluster templates, the use of Spot Instances for compute nodes, and node recipes for executing custom scripts pre/post the Ambari cluster setup. Comes with free community support from Hortonworks. First launched in November 2016, but appears to be discontinued as of HDP 3.0 with Hortonworks move to a multi cloud strategy via Cloudbreak&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/hortonworks-data-flow/&quot;&gt;Hortonworks DataFlow&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A distribution of a set of Apache and Hortonworks open source technologies for processing and running analytics on data 'in motion', with all products integrated with Apache Ranger for security, Apache Ambari for management and Schema Registery for central schema management. All bundled Apache open source projects are based on official Apache project releases, with any patches for bug fixes or new features being official Apache project patches from later releases of the relevant project. Available as RPMs or through Apache Ambari (via a management pack), and as an on-site or in the cloud managed service (as Hortonworks Operational Services), but is not currently available via Cloudbreak or as a cloud service. The HDF softare is provided free of charge, with training, consultancy and support available from Hortonworks. First released in September 2015 as a distribution of just NiFi following the acquisition by Hortonworks of Onyara (a company founded by some of the original creators of NiFi).&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A distribution of Hadoop based on a commitment to the Apache open source ecosystem. All bundled projects are Apache open source projects based on official Apache project releases, with any patches for bug fixes or new features being official Apache project patches pulled from later releases of the relevant project. Available as RPMs, through Apache Ambari (for local installs) or Cloudbreak (for installation on cloud platforms), and as an on-site or in the cloud managed service (as Hortonworks Operational Services). Comes with a number of add-ons that aren't part of the core product, including HDP Search, Hortonworks HDB and ODBC and JDBC drivers for Hive, Spark SQL and Apache Phoenix. The HDP software is available free of charge, with training, consultancy and support available from Hortonworks, including a flex support subscription, a consumption based model for the use of HDP on-premise or in the cloud. Also available for IBM Power Systems. The Hortonworks Data Platform was first released in June 2012.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/hortonworks-data-platform-for-windows/&quot;&gt;Hortonworks Data Platform for Windows&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A version of the Hortonworks Data Platform natively compiled for Windows that was discontinued as of HDP 2.5 in August 2016. First announced in March 2013, with a GA release in May 2013. Didn't use Apache Ambari for installation and management (instead being installed via a standard Windows installer), didn't support SmartSense, and didn't include some technologies (such as Accumulo, Atlas, Kafka, Solr, Spark and Hue).&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/hortonworks-data-platform-search/&quot;&gt;Hortonworks Data Platform Search&lt;/a&gt; &lt;/td&gt; &lt;td&gt;An add on package to HDP that bundles up Solr, Banana, and a suite of libraries and tools for integrating with Solr from Hadoop (utilities for loading data from HDFS), Hive (a SerDe to allow Solr data to be read and written as a Hive table), Pig (store and load functions), HBase (replication of HBase events to Solr based on the Lily HBase indexer), Storm and Spark (both SDKs for integrating with Solr). Available as an add on Ambari management pack or as a set of RPMs. Built, maintained and supported by Lucidworks on behalf of Hortonworks, first announced in April 2014 as part of the introduction of Solr with HDP 2.1.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/hortonworks-dataplane/&quot;&gt;Hortonworks DataPlane&lt;/a&gt; &lt;/td&gt; &lt;td&gt;An extensible platform for managing data ecosystems, with capabilities delivered through plugable applications. Supports the registration and management of DataPlane applications and the registration of Ambari managed clusters that are then accessible to these applications. Supports role based access control, with LDAP integration for users and groups and support for app specific roles. Runs on docker, with state held in an external database, and integrates with Knox (for SSO and access to clusters). Future services referenced include Cloudbreak and IBM DSX. Stated plan is for this to be a cloud service, however this is not currently generally available, and the documentation currently details installation steps for a local machine. First released in November 2017.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Hortonworks DataPlane&amp;nbsp;&amp;gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/hortonworks-dataplane/data-analytics-studio/&quot;&gt;Data Analytics Studio&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A DataPlane application for running Hive queries, managing Hive tables, and diagnosing Hive query performance issues. Supports a query editor (with autocomplete, a visual explain plan, performance improvement recommendations, saved queries and results downloading), a query search tool (with pre-defined queries for expensive, long running, non-optimised and failed queries, a range of filters and saved searches), a database management tool (supporting searching, browsing, interrogation, creation and modification of databases, tables, partitions and columns as well as uploading of data from local storage or HDFS) and table impact reporting (showing reads, writes, projections, aggregations, filters and joins by table and column, with support for dynamic heatmaps overlaid on entity relationship diagrams). Requires a Ambari mangement pack (the DAS engine) to be installed on all clusters.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Hortonworks DataPlane&amp;nbsp;&amp;gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/hortonworks-dataplane/data-lifecycle-manager/&quot;&gt;Data Lifecycle Manager&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A DataPlane application for replicating HDFS and Hive data between two clusters along with any associated metadata and security policies. Clusters already registered with DataPlane can be paired, at which point replication policies can be defined, which result in replication jobs running at the selected interval. Supports replicating between HDFS and cloud object storage (with some caveats around replication of security policies), replication of encrypted HDFS data, TLS encryption of replication streams, reporting on and management of replication jobs and HDFS snapshottable directories, with jobs executed by DLM Engine processes on the appropriate cluster. Stated future plans include support for automatic tiering of data between clusters and point in time backup and restore.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Hortonworks DataPlane&amp;nbsp;&amp;gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/hortonworks-dataplane/data-steward-studio/&quot;&gt;Data Steward Studio&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A DataPlane application for viewing and understanding data assets, with supported data assets currently limited to Hive tables on clusters with Atlas and Ranger installed. Supports viewing metadata associated with data assets (including properties, lineage, security policies and audit logs), profiling of data (with profiling performed by a background Spark process, with support for data summarisation, identifying sensitive/personal data and profiling user access to data), grouping of data assets into asset collections, taging and rating of data assets and collections and dashboard views of metadata by cluster and collection.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Hortonworks DataPlane&amp;nbsp;&amp;gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/hortonworks-dataplane/streams-messaging-manager/&quot;&gt;Streams Messaging Manager&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A DataPlane application for monitoring Apache Kafka clusters. Provides an overview view of producers, topics (and their partitions), brokers and consumer groups, showing key statistics and the connections between them, with the ability to propagate filters based on these connections. Also provides detail views, profiles and historic graphs for each producer, topic, broker and consumer group, with the ability to link out to Atlas to see end to end lineage and Ambari Grafana for detailed metrics. Metrics and status information is also provided over a REST API, with a REST Server Agent running on each cluster being monitored.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/hortonworks-smartsense/&quot;&gt;Hortonworks SmartSense&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Supports the capture of diagnostic information from HDP and HDF clusters (including configuration, metrics and logs from both Hadoop and the Operating System) into a bundle for upload (either manually or automatically) to the Hortonworks support portal to assist in the resolution of support issues and the delivery of cluster optimisation and preventative action recommendations, with support for anonymisation (including IP addresses and host names, with support for further custom rules) and encryption of information in bundles and a SmartSense gateway to proxy uploads if direct internet access isn't available. Also includes functionality to help understand and analyse cluster activity include the Activity Analyser (aggregates data from YARN, Tez, MapReduce and HDFS into Ambari Metrics) and Activity Explorer (an embedded instance of Apache Zeppelin with pre-built notebooks for exploring and visualising cluster activity). Installable and manageable through Apache Ambari. Part of the Hortonworks support offering, introduced in June 2015 as part of HDP 2.3.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/hue/&quot;&gt;Hue&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Web application to allow users and administrators to work with a Hadoop cluster. Features include a SQL query tool (with auto-complete, a SQL expression builder, plotting results as a graph or on a map, and the ability to refine results) over any JDBC compatible database, a Pig query tool (with auto-complete and parameterised queries), a Solr search tool (drag a drop creation of Solr dashboards with grid, timeline, graph, map and filter widgets, a tool for indexing data into Solr and a Solr index browser), a query notebook (Spark, PySpark, Scala, Hive, Impala, Pig and R queries along with visualisation of results as graphs and maps), an Oozie management tool (graphical Oozie workflow, coordinator and bundle editors and an Oozie monitoring and management dashboard), an Apache Sentry configuration tool (for managing permissions to Hive tables and Solr collections), an HDFS and S3 file browser and manager (including the ability to upload and edit data), a YARN job browser (viewing logs and statistics), a Hive Metastore manager (browse, view sample data, create and manage databases and tables), an HBase table manager (browse, view, edit, create and manage tables), a Sqoop2 manager (create, manage and execute Sqoop2 jobs), a ZooKeeper manager (list, view and edit) and a user workspace for saving work done in Hue, organising this in folders and sharing it with other users. Originally released by Cloudera as Cloudera Desktop in October 2009, before being open sourced as Hue in June 2010. Python/Django based, under active development with a wide range of contributors, and available for all major Hadoop distributions.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/influxdb/&quot;&gt;InfluxDB&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A time series database implemented in Go available in both open source and enterprise editions. Each data point consists of a metric name (measurement), a UNIX nano timestamp, a set of tag key value pairs, and a set of value key value pairs, with the combination of measurement and tag keys refered to as a series. Data is stored in a custom time series index (TSI) engine which supports very large numbers of series allowing for huge cardinalities of tag and value keys. Queries are written in InfluxQL (a varient of SQL), which includes support for creating and managing databases and series, listing series metadata (including measurements, tag keys and values and field keys), managing queries, writing the results of queries back into InfluxDB into a new series, a range of analytical SQL functions including aggregations (e.g. sum, count, spread, stddev), selections (e.g. first, last, percentile, sample) and transformations (e.g. ceiling, derivative, moving_average), and support for registering continuous queries that are run automatically and periodically within a database to create aggregate tables. Also supports retention policies for the automatic deletion of historic data, basic authentication and authorisation (at the database level), HTTPS connections, service plugins that allow data to be written to InfluxDB in alternative protocols (with out of the box support for UDP, Graphite, CollectD, Prometheus and OpenTSDB protocols), snapshot backups, statistics and diagnostic information, and an HTTP API and CLI for writing and querying data. Available as an open source version (under an MIT licence but limited to a single node), and as two commercial products - InfluxEnterprise (with support for clustering, access control and incremental backups) and InfluxCloud (InfluxEnterprise as a cloud based service). Originally created in 2013, and is part of the open source TICK suite along with Telegraf (ingestion of data), Chronograf (admin UI and visualisation) and Kapacitor (streaming analytics and actions).&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/kite/&quot;&gt;Kite&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A set of libraries, tools, examples, and documentation focused on making it easier to build systems on top of the Hadoop ecosystem. Consists of three sub-projects - Kite Data (a logical dataset abstraction over Hadoop), Morphlines (embeddable configuration driven transformation pipelines) and Kite Maven Plugin (a Maven plugin for deploying Hadoop applications). Java based, Open Source under the Apache 2.0 licence and hosted on GitHub. First released in May 2013 by Cloudera as the Cloudera Development Kit (CDK), renamed to Kite in December 2013, and reached a v1.0 release in February 2015 with a number of external contributors. Last release was v1.1 in June 2015, with very little development activity since this time.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Kite&amp;nbsp;&amp;gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/kite/kite-data/&quot;&gt;Kite Data&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Library that provides a logical dataset and record abstraction over HDFS, S3, local filesystems and HBase, including support for partitioning and views (which allow datasets to be filtered and supports automatic partition pruning). Provides a command line interface and Maven plugin for managing and viewing datasets. Supports Crunch, Flume, Spark and MapReduce, and can integrate with a Hive Metastore to make datasets available through Hive and Impala. Stores data using Avro (utilising Avro schema evolution / resolution) or Parquet.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Kite&amp;nbsp;&amp;gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/kite/kite-maven-plugin/&quot;&gt;Kite Maven Plugin&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A Maven plugin that supports the packaging, deployment and execution of applications onto Hadoop.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Kite&amp;nbsp;&amp;gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/kite/morphlines/&quot;&gt;Morphlines&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A configuration driven in-memory transformation pipeline that can be embedded into any Java code base, with specific support for Flume, MapReduce, HBase, Spark and Solr. Supports multiple different file types including CSV, Avro, JSON, Parquet, RCFile, SequenceFile, ProtoBuf and XML plus gzip, bzip2, tar zip and jar files. Also supports a number of transformation steps out of the box, including integration with Apache Tika for reading common file formats.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/llama/&quot;&gt;Llama&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Framework for long running low-latency distributed applications to request resources from YARN, built to support Apache Impala. Operates as an un-managed YARN application master (that handles resource requests over a Thrift API and delivers resource notifications) and a node manager plugin (that delivers resource availability information to co-located services). Created by Cloudera in August 2013 and hosted on GitHub under an Apache 2.0 licence. Maintained by Cloudera to support new Impala and CDH releases, but now deprecated and will no longer be included in CDH from v6.0 onwards.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/mapr-converged-data-platform/&quot;&gt;MapR Converged Data Platform&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A data platform built that provides Hadoop compatibility (via YARN and the MapR-FS HDFS compatible API), NoSQL and streaming data storage via MapR-DB and MapR-ES respectively, and a bundle of open source Hadoop projects via the MapR Ecosystem Pack. Comes with an installer (MapR Installer), a web based user interface for management (MapR Control System), and a monitoring and alerting solution (MapR Monitoring). Available as a free community edition (which excludes some enterprise features such as snapshots, high availability, disaster recovery and replication), a full commercial edition, and as MapR Edge (a small footprint edition that can run on low power and embedded hardware close to data sources to perform initial data filtering and processing before forwarding data on to a central cluster via MapR replication), MapR-XD (an edition that focuses on MapR-FS plus the Orbit Cloud Suite to provide web scale file and container storage), MapR Converged Data Platform for Docker (a marketing name for using the Converged Data Platform as persistent storage for docker containers) and MapR Data Fabric for Kubernetes (ditto but for Kubernetes). Supports a number of add-ons, including the Persistent Application Client Container (PACC, a docker image containing the client libraries required to connect to a MapR Converged Data Platform), MapR Orbit Cloud Suite (which adds support for deployment of cloud infrastructure along with MapR, integration with cloud object stores, plus mirroring and replication, with support for multi-tenancy, object tiering and OpenStack integration announced) and MapR Data Science Refinery (a docker based analytics notebook powered by Apache Zeppelin that fully integrates with the MapR Converged Data Platform). Supports deployment in the cloud (AWS and Azure), and is available as a managed service. First released as MapR v1.0 in 2010&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/mapr-db/&quot;&gt;MapR-DB&lt;/a&gt; &lt;/td&gt; &lt;td&gt;NoSQL database built over MapR-FS, supporting wide column and JSON document tables and HBase and OJAI (Open JSON application interface) APIs. Tables are stored as first class objects in MapR-FS volumes, and are sharded into table regions / tablets. JSON document tables are schemaless, support read and write access to individual document fields, subsets of fields or whole documents, finding documents by id or native secondary indexes, a set of atomic operations for mutating documents, a change data capture API, and integration with Spark, Hive and MapReduce. Wide column (binary) tables are largely equivalent to HBase tables, and partially support the HBase API, but without support for custom HBase filters or co-processors. Supports replication at the table, column family or column level, either synchronously or asynchronously, and in either master-master or master-slave configurations, with support for replicating to Elasticsearch. Authentication is managed through access control expressions (ACEs) at the field level (for JSON document tables) or at the column level (for wide column tables). Introduced in MapR v4.0 in Sept 2014, with document supported added in MapR 5.1 in Feb 2016.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/mapr-es/&quot;&gt;MapR-ES&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Technology for buffering and storing real-time streams of data, built over MapR-FS, with support for a Kafka compatible API. Messages (key/value pairs where the key is optional) are organised into topics, which are partitioned and stored as first class objects within MapR-FS volumes, with topics then grouped into streams. Supports encryption of streams, automatic deletion of messages (via a time to live), consumer groups, authorisation using ACEs (access control expressions), plus replication of topics to one or more remote MapR-ES instances either synchronously or asynchronously, including support for Kafka's MirrorMaker. Comes with Java, C and Python libraries and includes a Kafka compatible API. Introduced in MapR 5.1 in Feb 2016. Previously called MapR Streams.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/mapr-expansion-pack/&quot;&gt;MapR Expansion Pack&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A package of open source Hadoop projects certified to work together against one or more versions of the MapR Converged Data Platform. Has new major releases roughtly once a quarter, with most components kept resonably up to date with the open source version, with any patching done publically in GitHub. Available as RPMs, and installable via the MapR Installer. These components were originally bundled as part of the MapR Converged Data Platform, but were broken out as the MapR Ecosystem Pack in September 2016 to allow them to be released independantly. Renamed to the MapR Expansion Pack as of version 4.0.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/mapr-fs/&quot;&gt;MapR-FS&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Resilient distributed cluster file system that supports HDFS and NFS/POSIX (v3/v4) access. An S3 compatible API is provided by the MapR Object Store gateway bundled as part of the MapR Expansion Pack. Supports POSIX compliance, arbitrary in place updates to files (unlike HDFS which is append only), distributed metadata (it has no equivalent of the HDFS Name Node), block level mirroring to a remote cluster for DR or load balancing, encryption at rest, automatic storage tiering (including to external object storage) and snapshots (which provide point in time read only views). Data is stored in containers (which manage data blocks and the replication of these over the cluster), and logically organised into volumes (which manage files, directories and block allocation across one or more containers), which also provide multi-tenancy support, with administrative control, data placement, job execution, snapshots and mirroring all configurable against a volume. Supports encrypted communications, full auditing capabilities, Kerberos and Linux PAM for authentication, authorisation via ACLs (against clusters, volumes and job queues), POSIX file permissions (against files and directories) and Access Control Expressions (ACEs, arbitrary boolean expressions against volumes, files and directories). First releases as part of MapR v1.0 in 2010.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/mapr-monitoring/&quot;&gt;MapR Monitoring&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A collection of open source components used to capture, store and visualise metrics and log messages across a MapR Converged Data Platform. Components used include collectd (to capture metrics), OpenTSDB (a time-series database that runs on top of MapR-DB to store metrics), Grafana (to visualise and graph metrics into dashboards), FluentD (to collect and parse log messages), Elasticsearch (to store and index log messages for search) and Kibana (to search and view log messages). Metrics captured include cpu, disk, memory and network metrics, plus metrics for Drill, YARN and the MapR components. Log messages are captured from system logs, plus YARN, ZooKeeper, Drill, Hbase, Hive, Oozie, Spark, the MapR component logs, and the logs for the MapR Monitoring components. Both Grafana and Kibana include starter sample dashboards. First released in June 2016 as part of the Spyglass initiative.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/mesosphere-dcos/&quot;&gt;Mesosphere DC/OS&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A distributed, hybrid-cloud operating system for elastic stateless micro services running in containers and stateful big data services, ensuring high datacenter utilization. At its core, Apache Mesos handles job scheduling, resource management and abstraction, high availability, infrastructure-level processes and pluggable containerizers for both Docker and native Mesos containers. Combined with Marathon, provides a container orchestration platform with support for launching, managing, scaling and networking containers. Focused on ease of use, provides an app-store-like service catalog (Universe) to install complex distributed systems including HDFS, Apache Spark, Apache Kafka, Apache Cassandra, CI/CD applications such as Jenkins, all of which have been optimised to run on Apache Mesos and a web interface for monitoring and management. Comes in two flavors; a free community edition for installation in the cloud and a commercial enterprise edition for on-premises, in the cloud, or across a hybrid environment and includes monitoring tools, support for enterprise security and compliance tools, advanced networking, and load balancing features. Offered via a subscription license, the enterprise edition also includes product support. Open sourced in April 2016 under the Apache 2.0 license, under active development led by Mesosphere with a range of contributors including Microsoft.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/mesosphere-marathon/&quot;&gt;Mesosphere Marathon&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A framework for Apache Mesos and Mesosphere's Datacenter Operating System (DC/OS) to launch long-running services in a clustered environment and ensure that they continue to run in the event of a hardware or software failure. Implemented as a Mesos framework, leverages Mesos for resource allocation and isolation and provides a REST API and web interface for service definition, discovery and management. Provides constraints control to support service placement for high-available and locality, an event bus and health checking to support rolling deployments and upgrades. Provides local and external persistent storage and resurrection on the same node in the event of a failure to support stateful services (in beta). Often used as an orchestrator for other applications and services, can be run in highly-available mode by running multiple copies of the framework and using ZooKeeper to perform leader election in the event on an failure. Written in Scala, open sourced under the Apache 2.0 license, hosted on GitHub, with development led by Mesosphere who also distribute it as part of their Mesosphere's Datacenter Operating System (DC/OS) commercial offering.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/microsoft-azure-blob-storage/&quot;&gt;Microsoft Azure Blob Storage&lt;/a&gt; &lt;/td&gt; &lt;td&gt;An object store service with strong consistency, with support for multiple blob types (block, page and append), multiple storage tiers (premium, hot, cold and archive) and deep integration to the Azure ecosystem. Block blobs are comprised of one or more blocks with operations done at the block level with changes made visible via a final commit; page blobs are collections of 512-byte pages optimised for random read and write operations against one or more pages; and append blobs only support modification via the addition of new data to the end of the blob. Objects are organised into containers and indexed by string, with the option to list objects by prefix and to summarise results based on a delimiter allowing a filesystem to be approximated. Supports name-value pair metadata against containers and objects, both optimistic and pessimistic (lock based) concurrency, snapshots (providing read only access to objects as they were when the snapshot was taken), soft deletes (allowing previous versions of objects to be recovered), immutable blobs, lifecycle management (in public preview), access control via access tokens (shared access signatures), public access to containers, configurable geo redundancy, encryption of objects (Azure Storage Service Encryption - SSE) and support for SSL connections, multi-part uploads, the use of custom domains, and logging and metrics (Azure Storage Analytics). Provides a REST API, web app (Azure Storage Explorer), a range of SDKs, a CLI and PowerShell integration.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/microsoft-azure-data-lake-store/&quot;&gt;Microsoft Azure Data Lake Store&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Massively scalable HDFS compatible filesystem as a service, based on Microsoft's Cosmos technology. Claims support for up to trillions of files and single files larger than one petabyte, with no limits on account sizes, file sizes or the amount of data that can be stored, and optimisation of parallel analytics workloads, with high throughput and IOPS performance. Supports user authentication via Azure Active Directory (AAD) (combined with OAuth and OpenID), role based access control for account management, POSIX ACLs for controlling access to data, encryption for both stored data and data in transit over the network,and built in auditing (of both data access and account management activities). Supports a standard WebHDFS API, an HDFS compatible interface (adl://) that's bundled with Apache Hadoop, a web UI (Data Explorer) and SDKs for a range of languages. Does not natively support geo-replication with filesystems limited to a region, but data can be manually replicated via a number of routes if required. First announced in April 2015, with a general availability release in November 2016.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/openlink-virtuoso-universal-server/&quot;&gt;OpenLink Virtuoso Universal Server&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Multi-model database (RDBMS, VDBMS) supporting tabular relational (SQL), graph relational (SPARQL), hybrid (SPARQL-in-SQL a/k/a SPASQL), XML (XPath, XQuery, XSLT), filesystem/objects, and other forms of data; First shipped in 1999, available as Open Source or Enterprise Edition; various add-ons available for Enterprise Edition; virtualizes local and/or remote tabular relational databases and/or other data sources as RDF semantic web data sources.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/openstack-swift/&quot;&gt;OpenStack Swift&lt;/a&gt; &lt;/td&gt; &lt;td&gt;An open source object store with eventual consistency, that's available from a number of vendors as both an on site solution and a cloud based service offering. Objects are organised into containers and indexed by string, with the option to list objects by prefix and to summarise results based on a delimiter allowing a filesystem to be approximated. Supports configurable storage policies (each using a different storage ring allowing for differing hardware and replication levels to be used), erasure coding as well as standard replication (with erase coding providing smaller storage overheads at the code of higher CPU and read and write data), and multi-region clusters (based on configuring affinity for local operations). Also supports container and object metadata, object versioning, container to container mirroring via background synchronisation, authorisation via tokens from OpenStack Keystone, access control via container ACLs, support for large objects via segmentation (multi-part uploads combined with a special manifest file), scheduled and bulk object deletion, time limited access URLs,and encryption of data at rest. Provides a REST API and client SDKs. Originally created by Rackspace in 2009, becoming one of the first OpenStack technologies, with contributors now including SwiftStack, RedHat, HP, Intel, IBM among others.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/opentsdb/&quot;&gt;OpenTSDB&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A time series database built on top of Apache HBase (with support for Google BigTable and Apache Cassandra recently added). Each data point consists of a metric name, a UNIX timestamp, a value (either integer or floating point) and a set of key value pair tags, where the tags define the potentially aggregations required. Data is stored with one row per metric, tag combination and hour, with all data points for that hour stored in that row under different column qualifiers based on the timestamp, allowing for more efficient in memory aggregations. Supports the recording (but not generation) of pre-aggregated data that will be used to accelerate queries, annotations (short text strings associated with timestamps and optionally time series that represent events), the organisation of metrics and tags into hierarchical trees, and the generation of statistics relating to performance, however currently does not support incrementing counters. Consists of a Time Series Daemon (TSD) (that exposes a Telnet RPC and HTTP JSON REST APIs and a simple web based UI for querying data) and a CLI (including the ability to batch import data), with each TSD opperating independantly of each other with no master or shared state allowing for horizontal scalability over a single underlying database. Supports a range of plugins, including the ability to support different deserialisation and authentication for HTTP REST calls, emmission of meta data (metrics, tags and annotations) to a search engine, real time publishing of data points to another destination and support for other RPC protocols. Open sourced on GitHub under both an LGPLv2.1+ and GPLv3+ licence, with development started in 2010, and has been adopted but a number of large organisations including MapR, Yahoo, Tumblr and ebay.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/pravega/&quot;&gt;Pravega&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Technology for the buffering and long term storage of streaming data, designed for low latency and high throughput, with support for exactly once semantics, durable writes, strict ordering, dynamic scaling, transactions and long term storage backed by HDFS. Data is stored in named streams (continuous streams of bytes with serialisation and de-serialisation done in clients), with streams partitioned by a Routing Key into stream segments. Data is stored in two tiers, the first using Apache BookKeeper for recent data, the second using HDFS for long term storage, with automatic ageing of data and seamless reads across tiers. Operates on a publish/subscribe model, with subscribers able to select any point in history to read from. Supports automatic scaling of streams (dynamically increasing or decreasing the number of stream segments based on the operations per second on the stream), exactly once semantics (ensuring records are read once and once only even after failure), durable writes (data is persisted before write operations are acknowledged), transactions (multiple events can be committed in a single operation), ordered streams (events will always be read in the same order they're written), ReaderGroups (allows multiple subscribers to co-ordinate reads from a single stream) and a state synchroniser API (allowing multiple clients to synchronise arbitrary state through Pravega). Supports a Java SDK and out of the box integration with Flink, along with support for deployment using docker swarm, dc/os and AWS (all currently in development). Open sourced under an Apache 2.0 licence, started in July 2016 within Dell EMC, and does not yet have a first formal release, but is under active development by a wider range of contributors. Stated plans for future functionality include automatic deletion of data based on a retention period, support for other tier 2 storage technologies, access control, runtime metrics and Spark support.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/presto/&quot;&gt;Presto&lt;/a&gt; &lt;/td&gt; &lt;td&gt;An MPP query engine that supports queries over one or more underlying databases with the ability to join data from multiple datastores together. Supports a range of underlying technologies including Accumulo, Cassandra, Hive (HDFS), Kafka, Kudu, Redshift and a number of relational databases, with schemas read from the underlying database and cached within Presto. Architecture consists of a co-coordinator node that parses and plan queries, and worker nodes that execute tasks and process data. Extensible for new database connectors, data types, functions, access control schemas and event listeners. Supports resource management, spilling to disks when processing large results sets, a cost based optimiser, Kerberos and LDAP authentication, a CLI, and web interface for monitoring and managing queries and JDBC and ODBC drivers. Created at Facebook, announced and open sourced in 2013. Commercial support was originally provided by Hadapt, which was acquired by Teradata in 2015, before being spun out as Starburst in late 2017, who now provide an enterprise distribution and commercial support and services.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/quantcast-file-system/&quot;&gt;Quantcast File System&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Open source HDFS compatible distributed file system, which focuses on improving performance and scalability over HDFS. Uses erase coding (specifically Reed-Solomon error correction) allowing each data block to be stored with a 50% overhead over 9 nodes with data able to be read from any 6 (half the space required by HDFS with 3 way replication). Also supports online addition of new data (chunk) nodes, automatic re-balancing and re-replication of data, Unix style permissions support and C++ and Java client libraries. Published benchmarks suggest a 50/75% read/write performance increase over HDFS, and significantly faster metadata operations. Now also runs over Amazon S3. Built and maintained by Quantcast, who open sourced it in August 2012. An evolution of the Kosmos File System (KFS), an open source project started by Kosmix in 2005, which Quantcast first adopted in 2007. Built in C++ and released under the Apache 2.0 licence.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/qubole-data-service/&quot;&gt;Qubole Data Service&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Hadoop as a managed service over AWS, Azure and Oracle Cloud. Supports Airflow, Hadoop, Presto and Spark cluster types, automatic management (starting, stopping and scaling) of clusters based on workload, automatic shared Hive metastores within accounts, role based access control (to accounts, clusters and UI/API functionality, with Hive authorisation to manage access to data), connectivity to external databases (Data Stores), labelling of clusters and routing of commands by label (allowing graceful cluster upgrades), custom node bootstrap commands, encryption, auditing, data caching (on AWS only via open source Rubix project), ODBC/JDBC drives. Has a rich web based user interface that supports exploration of data (in Hadoop, object stores and connected external databases), a command composer with auto completion (supporting Hive, Presto, Pig, Shell, Spark and Worklow commands) with auto completion and command history, parameterisable command templates, data management (import, export and upload), a visual query builder (Smart Query), Zeppelin based notebooks (including publication of public read only notebook views), command schedulers, cluster management and a range of usage and cluster metrics and graphs. Also supports a REST API. Priced per hour based on the cloud infrastructure being used, which is in addition to any cloud vendor costs. Launched in 2013.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/recordservice/&quot;&gt;RecordService&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Abstraction layer for accessing structured data in Hadoop that enforces fine grained access control (via Apache Sentry). Started in January 2015 and announced with an initial beta release in September 2015 and a stated plan to donate it to the Apache Foundation, however all code and documentation were taken down at the end of 2017, with the download page on Cloudera's website now simply stating that 'RecordService is in development'. Functionality that was available included support for reading data from HDFS and S3 in Parquet, Text, Sequence File, RC and Avro formats via a Hive table/view definition or a file path, with support for HBase and Kudu planned, direct access to data via C++ and Java APIs plus integration with MapReduce, Spark, Impala and Pig, with support for Hive planned, and support for the Apache Sentry security model, including table, view, file (via grants on uris to create external tables) and column level security, with row level filtering and data masking planned.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/rex-ray/&quot;&gt;REX-Ray&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Open source, storage management solution providing containers to access external storage systems outside of the container's host thus enabling stateful applications such as databases to be run inside containers. Allows applications to save data beyond the lifecycle of a container and provides high-availability features for container restarts across hosts. Operates as a command line interface and lightweight agent that can be integrated into container runtimes (e.g. Docker, Mesos) to provide storage functionality such as volume creation, attaching, and mounting processes as well as container orchestrators (e.g. Docker Swarm, Kubernetes, or Marathon for Mesos) to attach a volume to a new host and resume state in the event of a host failure. Built on top of the libStorage library (also from Dell EMC), provides a storage plugin framework that allows access to multiple storage providers and platforms (Amazon EBS, EFS, S3FS, Dell EMC ScaleIO, Isilon etc.) and a flexible architecture that allows it to be deployed in a standalone, decentralised fashion on each container host or as a centralised service for easier management at large scale. Written in Go, open sourced under the Apache 2.0 licence, hosted on GitHub, with development led by Dell EMC. Has not yet reached a v1.0 milestone, but is still under active development.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/scality-ring/&quot;&gt;Scality RING&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A massively scalable commercial object store available as software for deployment on premises on commodity hardware. Based around a native object store core, but with POSIX filesystem support, and support for a range of APIs including file based (NFS, SMB and Linux FUSE), object based (S3 compatible and native Scality REST APIs), and OpenStack compatible (Swift, Cinder, Glance and Manila). Supports both variable level data replication and erasure coding, object encryption, file and object versioning, multi-site support (via synchronous and asynchronous replication, including support for replicating to Amazon S3), data location control, support for arbitrarily large objects, rolling upgrades and full authentication and access controls (including support for LDAP, Active Directory, AWS IAM and Kerberos). Comes with a CLI and web based GUI, and an add on solution (Scality Cloud Monitor) is available for monitoring and management. Sold by Scality, who were founded in 2010 and who focus on selling and supporting Scality RING, but who have also open sourced their S3 API as Zenko Cloudserver (previously S3 Server).&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/schema-registry/&quot;&gt;Schema Registry&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A centralised registry for data schemas with support for NiFi, Kafka and Streaming Analytics Manager, allowing schemas to be defined and versioned centrally and removing the need to attach schema to every piece of data. Supports versioning of schemas (with a definable compatibility policy that validates that schemas are forward compatible, backward compatible, both or none), the ability to store and serve JAR files for serialising and de-serialising data, a REST API, Java SDK and web based user interface for managing schemas. NiFi integration supports record level operations (via RecordReaders and RecordSetWriters); Kafka integration supports Kafka Producers and Consumers. Requires a MySQL backend for schema storage, and either local of HDFS storage for serialiser/de-serialiser JAR files. Stated plan is to support a wider range of schema types (currently only Avro schemas are support), a range of other registry requirements (e.g. templates, machine learning models or business rules), and for integration with Apache Atlas and Ranger. Started by Hortonworks in October 2016, with an initial release as part of HDF 3.0 in June 2017.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/streaming-analytics-manager/&quot;&gt;Streaming Analytics Manager&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A suite of open source web based tools to develop and operate stream analytics solutions and analyse the results, with pluggable support for the underlying streaming engine. Consists of Stream Builder (a web based GUI for building streaming data flows), Stream Operations (a web based management and operations tools for streaming applications) and Stream Insight (a bundling of Druid and Apache Superset to serve and analyse the results of streaming applications). Stream Builder supports creation of streaming flows using a drag and drop GUI, with support for a range of sources (including Kafka and HDFS), processors (including joins, window/aggregate functions, normalisation/projection and PMML model execution), and sinks (including email, HDFS, HBase, Hive, JDBC, Druid, Cassandra, Kafka, OpenTSDB and Solr), as well as support for custom sources, processors, sinks and functions (including window functions), and the ability to automatically deploy and execute applications. Stream Operations supports the management of multiple execution environments, the deployment, execution and management of applications within an environment, the capture of stream metrics via pluggable metrics storage (with support for Ambari and OpenTSDB), and web based dashboards to monitor applications and visualise key metrics. Started by Hortonworks in May 2015, with an initial release as part of HDF 3.0 in June 2017.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/streamsets-data-collector/&quot;&gt;StreamSets Data Collector&lt;/a&gt; &lt;/td&gt; &lt;td&gt;General purpose technology for the movement of data between systems, including the ingestion of batch and streaming data into an analytical platform. Pipelines are configured in a graphical user interface, and consist of a single origin, one or more processor stages and then one or more destinations, with support for a wide range of source/destination technologies and processor transformations. Supports a wide range of data formats, executors (tasks that can be triggered based on events from pipelines, e.g. to send e-mails or run a shell script), handling of erroroneous records, support for CDC CRUD records, previewing of data within the editor UI, real-time reporting and alerting on a range of execution and data quality metrics, the ability to dynamically handle changes to schemas and the semantic meaning of data and a full Python SDK. Can run in standalone mode (as a single process, with the option to run single or multi-threaded), as a Spark Straming or MapReduce job on a cluster, or in an ultralight agent (StreamSets Data Collector Edge). Java based, Open Source under the Apache 2.0 licence, hosted on GitHub, with development led by StreamSets who also provide commercial support and a number of commercial add-ons, including Control Hub (cloud service for developing and managing pipelines), Dataflow Performance Manager (for managing data metrics) and Data Protector (for managing senstive data). Started in October 2014, with a v1.0 release in September 2015.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/zenko/&quot;&gt;Zenko&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A distributed and resilient Amazon S3 API compatible object storage gateway / proxy. Utilises Zenko CloudServer (previously S3 Server) to provide an S3 compatible API, to proxy requests to either Scality RING, Amazon S3, Azure Blob Storage or Google Cloud Storage, and to provide persistent local storage or transient in-memory storage. Current solution is a Docker Swarm stack of a cluster of Zenko CloudServer instances with nginx as a front end load balancer. Manageable via Zenko Orbit, a cloud based portal. Roadmap includes support for Azure Blob Storage, support for other container management systems such as Kubernetes, plus two new sub-projects - Backbeat (which will provide policy-based data workflows such as replication or migration) and Clueso (which will provide object metadata search and analytics using Apache Spark). First released in July 2017, and hosted on GitHub under an Apache 2.0 licence.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Zenko&amp;nbsp;&amp;gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/zenko/cloudserver/&quot;&gt;Zenko CloudServer&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Open source object storage server based on the S3 compatible API from Scality RING, with the ability to proxy requests to other S3 services (with support for Scality RING, Amazon S3, Azure Blob Storage and Google Cloud), or to store data in persistent local storage or transient in-memory storage, with support for concurrent use of multiple backends. Supports broad compatibility with the Amazon S3 API including bucket and object versioning, and has been tested against a range of Amazon S3 utilities, CLIs and SDKs. Written in Node.js, available as a Docker container, and can be deployed and used independantly of the rest of Zenko. Metadata and (locally persisted) data is managed by a data and metadata daemon (dmd), with the option to use a shared remote daemon (for example when running a cluster of CloudServers). First released in June 2016 as S3 Server before becoming being renamed to CloudServer and becoming part of Zenko in July 2017. Hosted on GitHub under an Apache 2.0 licence.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Welcome</title><link>https://ondataengineering.net/blog/2016/12/07/welcome/</link><description> &lt;p&gt;For me, one of the biggest challenges in exploiting data (be that through reporting, big data analytics, machine learning or any one of a dozen similar capabilities) is making sure you have the right data in the right place at the right time to allow you to do this efficiently. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;For example, &lt;a href=&quot;http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html&quot;&gt;this&lt;/a&gt; article from the New York Times and &lt;a href=&quot;http://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/&quot;&gt;this&lt;/a&gt; more recent one from Forbes talks about how the analysis of big data promises unique business insights, but for big-data scientists there is significant manual ‘janitor work’ (up to 80% of their time) required to prepare data, and although the research is sponsored by Data Wrangling tool vendors, the conclusions will resonate with many data scientists. Combine this with the historical cost, delivery speed and agility issues typically associated with delivery data warehouse or reporting solutions, and for me it’s never been clearer that we need to get smarter at how we prepare and manage data.&lt;/p&gt; &lt;p&gt;Part of the solution to this is better Data Engineering, ensuring the processes, tools, technologies, data platforms, regular data feeds and their data preparation jobs are in place to allow the data to be exploited in an efficient, reliable and repeatable way. The aim of this site is therefore to try to offer independent, critical and technical thinking on the technologies, architectural patterns and delivery capabilities that can help address this.&lt;/p&gt; &lt;p&gt;My hope is that this becomes a community owned and authored site of trusted reference material on these topics. To that end, all the content on this site is licensed under the Creative Commons Attribution 4.0 International License and hosted in a public GitHub repository, and there are a set of Discourse forums for discussions. Details of how to contribute and get involved can be found on every page.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Big Data</title><link>https://ondataengineering.net/blog/2016/12/09/big-data/</link><description> &lt;p&gt;Before we get stuck in, a short digression to talk about Big Data. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;There are many different definitions of Big Data, but for arguments sake let’s say this refers to the exploitation of data that would have previously been uneconomical due to the volume of data, the structure or format of the data (e.g. unstructured, semi-structured, or complex file formats such as video and audio) and the types of analytics required (e.g. path, graph or time series analysis)&lt;/p&gt; &lt;p&gt;I’ve a couple of comments to make on this topic.&lt;/p&gt; &lt;p&gt;Firstly, in my (humble) option Big Data is a marketing term (supported by new technologies - primarily Hadoop) that’s been exploited to sell these technologies and to give the industry something to talk about. However, I think this has been a broadly positive thing, in that it’s brought data analytics to the mainstream, spurred uptake of new technologies and encouraged companies to invest in analytics and data processing that they may not have done previously. In any case it’s probably just about run its course now (as demonstrated as it’s fall down the far side of the hype curve), and has definitely resulted in the devaluation of other (perhaps more traditional) analytical capabilities which still have a role to play and in many cases deliver capabilities that Big Data technologies can’t yet match.&lt;/p&gt; &lt;p&gt;Secondly, I think there’s been a lot of misinformation about Big Data and Big Data technologies. It’s not a replacement for existing BI/MI and analytical capabilities, and in fact needs to coexist and integrate with these in order to deliver on its promises. It’s not always cheaper or more performant than existing technologies, and won’t always reduce the timescales and costs for analytics or data exploration. And it’s not a new or innovative technology - I know of companies that were analysing multi-petabyte data stores and doing real time analytics over ten years ago, parallel distributed file systems have been around for a lot longer than that, and there are many established technologies that have data processing capabilities that the new technologies are only just starting to catch up to.&lt;/p&gt; &lt;p&gt;In terms of this site the plan is to look at the wider picture and take an holistic view of data transformation and exploitation. I’ve therefore no plans to talk explicitly about Big Data, but in looking at the wider picture we will absolutely cover everything relating to it (the technologies and the new use cases these enable) alongside coverage of other new technologies, more established technologies and capabilities, and the interesting intersection between them all.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Plan</title><link>https://ondataengineering.net/blog/2016/12/12/the-plan/</link><description> &lt;p&gt;One more post before we get started.&lt;/p&gt; &lt;p&gt;The following are my current thoughts for some of the topics I’d like to cover on this site, both as a reference for my future self to look back at my naive optimism, but also if anyone wants to start contributing to any of these now, or to start a discussion on any the later topics to start framing and exploring them. &lt;!--more--&gt;&lt;/p&gt; &lt;h2 id=&quot;theme-1---the-technology-catalogue&quot;&gt;Theme 1 - the technology catalogue&lt;/h2&gt; &lt;p&gt;The plan here is to start building up a technology catalogue by looking at the key vendors in the Data Engineering space. This will only be a start on the technologies I’d expect to see in our catalogue however, so once this is done the plan is to then go through by technology category to complete the catalogue.&lt;/p&gt; &lt;p&gt;I’d also like to look at providing a concise yet detailed introduction to some technologies that describes exactly what it is, how it works, and what the key features are. So much material that can be found on the internet is marketing material that glosses over the information I’m interested in knowing to understand whether a technology might meet my use cases and integrate into my environment, and my hope is that I can use this site to address that.&lt;/p&gt; &lt;h2 id=&quot;theme-2---data-engineering-use-cases&quot;&gt;Theme 2 - data engineering use cases&lt;/h2&gt; &lt;p&gt;One thing I don’t want to do on this site is define another data ecosystem architecture - there are too many already, most of them are designed to sell specific technologies, and none of them will fit the range of different requirements and constraints that different organisations will have.&lt;/p&gt; &lt;p&gt;However, what I do want to do is look at the range of different of different use cases that you might use data engineering technologies for, from a Data Lake (and we’ll look at what that overloaded term actually means) to a Data Warehouse (and why they’re still relevant), from the acquisition of data to the preparation of a Query Focused Dataset, and from the management of a data catalogue to the monitoring of data quality metrics.&lt;/p&gt; &lt;p&gt;I’d then like to look at how different technologies and architectural patterns can support these use cases - how do you implement a Data Lake using Hadoop, what technologies support data governance and data catalogues, and how do the various streaming frameworks compare.&lt;/p&gt; &lt;p&gt;As part of this I also want to look at the core principles behind Data Transformation, what state of the art in this space looks like, and how the established enterprise technologies compare to the new Open Source upstarts.&lt;/p&gt; &lt;h2 id=&quot;theme-3---delivery&quot;&gt;Theme 3 - delivery&lt;/h2&gt; &lt;p&gt;As if the above isn’t already massively ambitious enough, I’d also like to talk about the delivery of data solutions, including:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How we can use best practice delivery concepts (e.g. configuration management, continuous integration and testing, automated deployment, infrastructure and database management) and what these mean within a data solution&lt;/li&gt; &lt;li&gt;How we can bring some the new best practices from Lean and Agile into the data space, and what data transformation tools need to do in order to be able to support this&lt;/li&gt; &lt;li&gt;Why data projects can have a reputation for late delivery, cost overruns, poor quality data and a high cost of change, and what can be done about this&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I think that will more than do us. Getting through that lot will take some time, but with help and contributions I think this site could be hugely valuable.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Technology Catalogue</title><link>https://ondataengineering.net/blog/2016/12/14/the-technology-catalogue/</link><description> &lt;p&gt;The first step in this journey is going to be creation of a catalogue of the technologies that are going to be of interest to us as we explore the world of data engineering. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;The aim is to provide a valuable reference for anyone starting a technology evaluation to understand what technology options they might have for a given situation, or to understand if and how a technology might fit into an existing ecosystem.&lt;/p&gt; &lt;p&gt;For each technology, the plan is therefore to provide a short summary describing the technology, along with its background and current status. As mentioned in my previous post, for some technologies, I also want to do a deep dive to provide a longer summary with more detail that gives a solid introduction to the technology, and my hope is that we’ll get contributions to provide these for the vast majority of the technologies that I won’t get time to look at.&lt;/p&gt; &lt;p&gt;As part of this we’ll need to look at providing a categorisation of technologies, although making this useful is going to be challenging given that multiple categories of technologies could be used to meet a given use case. The technologies we’ll look at broadly fall into three groups however:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Data Transformation tools, be they one of the established commercial products or a new Open Source technology&lt;/li&gt; &lt;li&gt;Data Platforms, be they a traditional relational database, an Hadoop based data platform, a real time broker such as Kafka, or a NoSQL data platform&lt;/li&gt; &lt;li&gt;Technologies that address the other supporting capabilities around these, such as data catalogues or metadata management&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The plan is to start by looking at the key vendors in the Data Engineering space including:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The pure play Hadoop distributions - Apache Big Top, Hortonworks, Cloudera and MapR&lt;/li&gt; &lt;li&gt;The rest of the Apache Data Engineering ecosystem&lt;/li&gt; &lt;li&gt;The major Cloud vendors - Amazon, Google and Azure&lt;/li&gt; &lt;li&gt;The big multi play vendors - IBM, Oracle, Teradata, Microsoft, Pivotal, SAP and SAS&lt;/li&gt; &lt;li&gt;The big specialist commercial data integration vendors - Ab Initio and Informatica&lt;/li&gt; &lt;li&gt;Other commercial data integration and data platform vendors - perhaps based at least partially on looking at the latest Gartner and Forrester reports&lt;/li&gt; &lt;li&gt;The major open source cloud scale companies, if only to see what they do in this space - Facebook, Netflix, LinkedIn, Google, Yahoo, eBay and Twitter&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This will only be a start on the technologies I’d expect to see in our catalogue however, so once this is done the plan is to then go through by technology category.&lt;/p&gt; &lt;p&gt;This is obviously going to take some time given the vast range of technologies, so if you’re interested in contributing in whatever form, if you spot any issues or omissions, or if you have any comments you’d like to add, then please do share your thoughts.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Hadoop</title><link>https://ondataengineering.net/blog/2016/12/16/apache-hadoop/</link><description> &lt;p&gt;And so we begin our journey through the jungle of Data Engineering technologies by looking at the technology du jour - Apache Hadoop. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;There’s an entire ecosystem here that we’ll start to explore by looking at the major Hadoop vendors, but the first entries in our technology catalogue are the &lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Apache Hadoop&lt;/a&gt; project itself, along with its sub-projects: &lt;a href=&quot;/technologies/apache-hadoop/yarn/&quot;&gt;YARN&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hadoop/hdfs/&quot;&gt;HDFS&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-hadoop/map-reduce/&quot;&gt;MapReduce&lt;/a&gt;. Click on the links to view the technology information I’ve added to the site.&lt;/p&gt; &lt;p&gt;This also brings our first technology vendor - the &lt;a href=&quot;/tech-vendors/apache/&quot;&gt;Apache Software Foundation&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;And with that it’s time for Christmas! We’ll be back in three weeks with the first of the core technologies within the Hadoop space.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Core Hadoop Technologies (pt1)</title><link>https://ondataengineering.net/blog/2017/01/06/core-hadoop-technologies/</link><description> &lt;p&gt;And we’re back - Happy New Year!&lt;/p&gt; &lt;p&gt;Having started with the core Apache Hadoop project, we’re now going to look at the “core” technologies within the Hadoop space, based on those included in multiple distributions (many thanks to Merv Adrian from Gartner for his useful &lt;a href=&quot;http://blogs.gartner.com/merv-adrian/2016/07/30/hadoop-project-commercial-support-tracker-july-2016/&quot;&gt;tracker&lt;/a&gt;) &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;There’s three Apache technologies added to the catalogue this week - &lt;a href=&quot;/technologies/apache-flume/&quot;&gt;Flume&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hbase/&quot;&gt;HBase&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Hive&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;There’s no much to say have Flume or HBase right now, but we’ll take a more detailed look at both of these in the future. Hive however, is more interesting.&lt;/p&gt; &lt;p&gt;Firstly, it’s a hugely popular and important project that’s a corner stone of the Hadoop ecosystem, which in its short life has seen enormous change - a classic example of an Open Source technology that has mutated, evolved, consumed other projects and been pulled in multiple directions over time. I plan to dig into the history of Hive in the not too distant future as I think it’s a great example of how an Open Source project can evolve.&lt;/p&gt; &lt;p&gt;Secondly, it’s not one thing, but a collection of different components with very distinct roles all bundled together, which is why I’ve taken the decision to break it out into a number of sub-projects (&lt;a href=&quot;/technologies/apache-hive/hive-metastore/&quot;&gt;Hive Metastore&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hive/hive-server/&quot;&gt;Hive Server&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-hive/hcatalog/&quot;&gt;HCatalog&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;As before, click on the links above to see the information added to the site.&lt;/p&gt; &lt;p&gt;That’s it for this week - next up is Solr, Sqoop and Spark.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Core Hadoop Technologies (pt2)</title><link>https://ondataengineering.net/blog/2017/01/13/core-hadoop-technologies-pt2/</link><description> &lt;p&gt;And onwards with our look at the core Hadoop technologies. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Today, I’ve added &lt;a href=&quot;/technologies/apache-solr/&quot;&gt;Solr&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-sqoop/&quot;&gt;Sqoop&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Spark&lt;/a&gt;, along with the Spark sub-projects &lt;a href=&quot;/technologies/apache-spark/spark-sql/&quot;&gt;Spark SQL&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-spark/spark-streaming/&quot;&gt;Spark Streaming&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-spark/mllib/&quot;&gt;MLlib&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-spark/graphx/&quot;&gt;GraphX&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Solr is one of the big two search technologies along with Elasticsearch, and although there’s debate around which is best (Elastic is probably slightly more developer friendly and supports slightly better analytics capabilities, whereas Solr has the more open development model being an Apache project), they’re both great technologies.&lt;/p&gt; &lt;p&gt;Sqoop is interesting because of where it’s going - whereas the current version of Sqoop focuses on integration with structured databases, the pending version 2 (which admittedly has been in development for a long time now) evolves it slightly to support the batch ingest of any data into Hadoop. It’s going to have stiff competition from Apache NiFi however if and when it’s finally released.&lt;/p&gt; &lt;p&gt;And so to Spark - which claims to be the most active Open Source project in Big Data (as well as many other things). What is clear however is it’s the one next gen data processing and transformation language that managed to catch significant momentum and adoption. We can argue the toss on whether it’s the best technology, but it’s now bundled with all the Hadoop distributions, has a rapidly growing base of trained and experienced developers, and a rich ecosystem, which means it’s becoming the default answer to a whole bunch of use cases.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Core Hadoop Technologies (pt3)</title><link>https://ondataengineering.net/blog/2017/01/20/core-hadoop-technologies-pt3/</link><description> &lt;p&gt;Up today, our final look at the core technologies within the Hadoop ecosystem. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;First up are &lt;a href=&quot;/technologies/apache-avro&quot;&gt;Avro&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-parquet&quot;&gt;Parquet&lt;/a&gt;, both of which are key data formats used within the Hadoop ecosystem, but with different and contrasting focuses.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/technologies/apache-kafka&quot;&gt;Kafka&lt;/a&gt;’s a hot technology at the moment - deliverying high bandwidth low latency storage and processing of data streams, with reference cases handling millions of events per second. If you’re looking at doing anything with streaming data it’s probably well worth a look. Note that I’ve broken out &lt;a href=&quot;/technologies/apache-kafka/kafka-connect&quot;&gt;Kafka Connect&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-kafka/kafka-streams&quot;&gt;Kafka Streams&lt;/a&gt; as sub-projects.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/technologies/apache-pig&quot;&gt;Pig&lt;/a&gt; was one of the first technologies to provide a ore user friendly abstraction over MapReduce for developing Hadoop jobs. It’s starting to show it’s age however, and although Hortonworks and Yahoo (who are heavy Pig users) seem to be investing heavily in Pig on Tez, and Cloudera seems to be supporting Pig on Spark (mirroring their Hive strategies), it’s difficult to see newcomers to Hadoop who don’t have an existing investment in Pig using it over Spark and other newer tools.&lt;/p&gt; &lt;p&gt;And finally &lt;a href=&quot;/technologies/apache-oozie&quot;&gt;Oozie&lt;/a&gt; - a job scheduling an orchestration engine. It’s been a staple of most Hadoop distributions for a while now, however it’s difficult to find many big references cases for it’s use, and it’s not the most user friendly tool. Orchestration and management of data transformation pipelines feels like a huge technology gap at the moment - if anyone knows of any great technologies in this space please shout.&lt;/p&gt; &lt;p&gt;As before - click on the links to see the technology information added to the site.&lt;/p&gt; &lt;p&gt;That’s it for this week, and for the core Hadoop technologies - it’s been fun. We’ll be back on Monday to start looking at Apache Bigtop, along with a change of pace…&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Daily Technology Summary</title><link>https://ondataengineering.net/blog/2017/01/23/the-daily-technology-summary/</link><description> &lt;p&gt;Up until today, I’ve been publishing new technology summaries in bulk every Friday. However I want to change up the pace a little, and so going forward I’m going to try to publish a new technology summary every week day.&lt;/p&gt; &lt;!--more--&gt; &lt;p&gt;Blog posts will be less frequent, and will usually be used to introduce and conclude a specific set of technologies we’re looking it. Because of this, you’ll notice that the site home page now supports a snazzy new summary of recent changes to content on the right hand side next to the summary of recent blog posts. In addition, the RSS and Atom feeds now include content updates as well as blog posts.&lt;/p&gt; &lt;p&gt;Also, in the coming days there will be new options to get the daily technology summary (and blog posts) delivered to your inbox via e-mail, and we’ll also start publishing these on Twitter (@OnDataEng) as well - just as soon as it’s all setup.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hadoop Distros: Apache Bigtop</title><link>https://ondataengineering.net/blog/2017/01/23/hadoop-distros-apache-bigtop/</link><description> &lt;p&gt;And so to our first (hopefully of many) daily technology summary.&lt;/p&gt; &lt;p&gt;I want to continue our wander through the Apache Hadoop ecosystem by looking at the common Hadoop Distributions, starting with &lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, and with this introducing our first technology category - the &lt;a href=&quot;/tech-categories/hadoop_distributions/&quot;&gt;Hadoop Distribution&lt;/a&gt;. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Apache Bigtop is interesting for a couple of reasons - firstly because it’s the only true open source Hadoop distribution (meaning that it includes many components that the commercial distributions don’t, and the components it includes are often more up to date, assuming you’re happy to use the latest snapshot builds), and secondly because of it’s history as Cloudera’s attempt to create a common base Hadoop distribution with all the associated integration testing and packaging (there are links in the &lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Bigtop page&lt;/a&gt; that provide further reading on this).&lt;/p&gt; &lt;p&gt;You’ll see that the &lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Bigtop page&lt;/a&gt; links to the Hadoop technologies we’re already covered that it packages (and vice-versa), and the aim is that over the coming days we’ll complete this list as we add technology summaries for the rest of the technologies it packages.&lt;/p&gt; &lt;p&gt;And I shouldn’t let our first technology category go uncommented - as we’re going to be looking at the common Hadoop Distributions we’ll start collecting these together under an &lt;a href=&quot;/tech-categories/hadoop_distributions/&quot;&gt;Hadoop Distribution&lt;/a&gt; page and try and round these out over the coming weeks.&lt;/p&gt; &lt;p&gt;That’s it for now - we’ll speak again when we’ve worked our way through the remaining technologies bundled with Apache Bigtop.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Week That Was - 27/01/2017</title><link>https://ondataengineering.net/blog/2017/01/27/the-week-that-was/</link><description> &lt;p&gt;So rather than waiting until we’ve finished looking at all the technologies included in Apache Bigtop before talking about them, let’s try wrapping up each week with a blog post summarising what we’ve looked at, and maybe at some point summarising some of the news of the week. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;So what have we looked at this week?&lt;/p&gt; &lt;p&gt;Firstly, a couple of graph computation frameworks - &lt;a href=&quot;/technologies/apache-hama/&quot;&gt;Apache Hama&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-giraph&quot;&gt;Apache Giraph&lt;/a&gt;. Along with &lt;a href=&quot;/technologies/apache-spark/graphx/&quot;&gt;Spark GraphX&lt;/a&gt;, these are probably the big three graph computation frameworks on Hadoop. What’s interesting is that both GraphX and Hama seem to have seen very little development recently - either meaning they’re done and meet most people’s use cases, or there just isn’t the demand for them. Giraph still seems to be going strong, however this is mainly being used at extreme scale by Facebook and LinkedIn. My guess is that graph technologies (both computation frameworks and graph databases) are being pushed as hot technologies at the moment, however most organisations aren’t quite sure what to do with them.&lt;/p&gt; &lt;p&gt;We’re also looked at a couple of Hadoop in-memory storage accelerators - &lt;a href=&quot;/technologies/apache-ignite&quot;&gt;Apache Ignite&lt;/a&gt; and &lt;a href=&quot;/technologies/alluxio&quot;&gt;Alluxio&lt;/a&gt; (formally known as Tachyon). These are both interesting, promising performance boosts for Hadoop computation jobs by providing an in memory HDFS compatible filesystem, as well a bunch of other features - both support an in memory key-value store, Alluxio supports tiered storage over multiple storage layers (in-memory, local and remote disk), and Ignite provides a more general purpose compute layer that supports streaming computation and arbitrary compute. If you’ve used either of these and can talk to their benefits I’d be very interested to chat in the forums.&lt;/p&gt; &lt;p&gt;And we started the week by talking about &lt;a href=&quot;/technologies/apache-bigtop&quot;&gt;Apache Bigtop&lt;/a&gt;, which I’ve already &lt;a href=&quot;/blog/2017/01/23/hadoop-distros-apache-bigtop/&quot;&gt;talked about&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Finally - you’ll now see that we’re proudly displaying our options for &lt;a href=&quot;/site/subscribe/&quot;&gt;subscribing&lt;/a&gt; to our content on the front page. Pick your poison - we support e-mail (daily and weekly), Twitter updates plus RSS and Atom feeds.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Week That Was - 03/02/2017</title><link>https://ondataengineering.net/blog/2017/02/03/the-week-that-was/</link><description> &lt;p&gt;And another week goes by - let’s have a look back over the technologies we’ve looked at this week.&lt;/p&gt; &lt;p&gt;We kicked off the week by looking at &lt;a href=&quot;/technologies/kite/&quot;&gt;Kite&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-datafu/&quot;&gt;Apache DataFu&lt;/a&gt;, a couple of utilities designed to make working in the Hadoop ecosystem a little easier, before moving on to &lt;a href=&quot;/technologies/apache-phoenix&quot;&gt;Apache Phoenix&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-tajo&quot;&gt;Apache Tajo&lt;/a&gt;. a couple of query engines (also over the Hadoop ecosystem), and then finishing with &lt;a href=&quot;/technologies/apache-mahout&quot;&gt;Apache Mahout&lt;/a&gt;. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/technologies/kite/&quot;&gt;Kite&lt;/a&gt; is a solid concept and feels like it fills a bit of a void - how can I easily create Hive tables and load data in from a variety of sources without writing code - however it doesn’t ever seem to have gained much traction, and it looks like even Cloudera aren’t developing and maintaining it any more. It’s also the first non Apache technology we’re looked at on this site! I’m definitely planning to revisit Kite and some of it’s concepts when we talk about Data Lakes in the future however.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/technologies/apache-datafu/&quot;&gt;Apache DataFu&lt;/a&gt; is actually two things - a set of user defined functions for Pig, and a MapReduce framework for calculating aggregations over regular ingestions of data into Hadoop based on only processing the new data called Hourglass. The first of these sounds well worth a look if you any sort of significant work in Pig. The second I’m less sure about - you’ll have to be using MapReduce, and you’d have to want to follow their pattern, however as a concept or an exemplar it could well be worth a look.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/technologies/apache-phoenix&quot;&gt;Apache Phoenix&lt;/a&gt; surprised me - it appears to be an extremely active project, with excellent documentation and a great range of companies that are using it in production, and in providing a SQL query later over HBase, fills an interesting niche within the Hadoop ecosystem. Hive and Impala are great if you have batch updates (and ideally just appends), but they don’t support low latency random updates and queries (along with the full table scans) that HBase (and therefore Phoenix) does. It’s going to be interesting to see how this stacks up against Kudu as this matures and gains adoption, and how the major Hadoop distributions look to support this use case.&lt;/p&gt; &lt;p&gt;I’m not quite sure what to make of &lt;a href=&quot;/technologies/apache-tajo&quot;&gt;Apache Tajo&lt;/a&gt;. It seems like a great technology, with significant commercial backing from Gruter, however I’m not sure it’s getting much traction, and I’m not sure what niche it’s trying to target - it feels uncomfortably close to Hive and Impala. Maybe prior to Hive on Tez/Spark Tajo had some differentiation in terms of low latency queries.&lt;/p&gt; &lt;p&gt;And last (but not least) &lt;a href=&quot;/technologies/apache-mahout&quot;&gt;Apache Mahout&lt;/a&gt;. Mahout has been a staple of most Hadoop distributions for a while (probably as a result of it being one of the first machine learning technologies in the Hadoop space), but what’s interesting is that it completely reinvented itself in April 2015 to become a general purpose distributed linear algebra engine that can run over Spark (with H2O and Flink support coming), and (if running on Spark) is fully compatible with other Spark libraries such as MLlib.&lt;/p&gt; &lt;p&gt;That’s it for this week - have a great weekend.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Week That Was - 10/02/2017</title><link>https://ondataengineering.net/blog/2017/02/10/the-week-that-was/</link><description> &lt;p&gt;Wow, doesn’t time fly when you’re having fun.&lt;/p&gt; &lt;p&gt;We’re nearly at the end of our Apache Bigtop journey - this week we’ve had a look at a bunch of data processing tools (&lt;a href=&quot;/technologies/apache-apex&quot;&gt;Apache Apex&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-crunch&quot;&gt;Apache Crunch&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-flink&quot;&gt;Apache Flink&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-tez&quot;&gt;Apache Tez&lt;/a&gt;), and to round the week out &lt;a href=&quot;/technologies/apache-zookeeper&quot;&gt;Apache ZooKeeper&lt;/a&gt;. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;There are some interesting ideas in &lt;a href=&quot;/technologies/apache-apex&quot;&gt;Apache Apex&lt;/a&gt; - building low level flexible DAGs with custom operators is powerful, and their messages about minimising the amount of ceremony and focusing on business logic are great. However it feels very much like a commercial product in open source clothing (its absolutely not alone in this regard), and I’m not sure I see it making much headway. There could be potential in the graphical tools and extra stuff that comes as part of the DataTorrent commercial product however - something I’d like to take a deeper look at in the future.&lt;/p&gt; &lt;p&gt;I’m not fully informed on the history of &lt;a href=&quot;/technologies/apache-crunch/&quot;&gt;Apache Crunch&lt;/a&gt;, however I’m assuming that it was created to make working with MapReduce easier, and based on how many companies seem to have adopted it appears to have been pretty successful, with support for running over Spark presumably allowing these companies to migrate their Crunch works to Spark to gain the benefits. I’m also going to make a wild assumption that Crunch’s time has been and gone, and now that there are alternatives to MapReduce there’s little value in Crunch unless you’re already a heavy user.&lt;/p&gt; &lt;p&gt;To to &lt;a href=&quot;/technologies/apache-flink&quot;&gt;Apache Flink&lt;/a&gt;. There’s a lot to like about Flink, and it feels like it could be a serious contender as a leading stream processing platform if it can continue its growth, contributions and adoption. I’d love to hear from anyone that’s had experience with using Flink.&lt;/p&gt; &lt;p&gt;I have to say I’m conflicted about &lt;a href=&quot;/technologies/apache-tez&quot;&gt;Apache Tez&lt;/a&gt;. There’s part of me that sees it as an indulgence on Hortonworks part - the creation of new technology for Hive and Pig to use for executing queries when (maybe) other technologies existed already (not looking anywhere in particular Spark) that’s destined to become a footnote to history. However there’s another (perhaps larger) part of me that thinks there’s something here that I (and many people) don’t quite appreciate yet. I absolutely don’t see it as a competitor to Spark in the iterative analytics space, however I have a feeling that it might be a better and more scalable general purpose data processing engine that supports large Hive and Pig queries that Spark might struggle with. It’s great that Spark has set terabyte scale sorting benchmarks, but I’m not sure I’d want to use it to join terabyte sized datasets together. What’s going to be interesting is not the Spark vs Tez question, but more about how it holds up against new combined batch and streaming engines such as Flink.&lt;/p&gt; &lt;p&gt;And last but not least &lt;a href=&quot;/technologies/apache-zookeeper&quot;&gt;Apache ZooKeeper&lt;/a&gt;. You may not be aware you’re using it, but if you’re running almost any clustered Apache technology you probably are!&lt;/p&gt; &lt;p&gt;And with that we’re up to 28 technologies - only a couple of thousand to go!&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Bigtop to HDP</title><link>https://ondataengineering.net/blog/2017/02/17/apache-bigtop-to-hdp/</link><description> &lt;p&gt;And so we’ve come to the end of the technologies included in Apache Bigtop - it’s been a bit of a meandering trip, taking in some well known sights, some up and coming stuff, and some slightly odd and obscure pieces.&lt;/p&gt; &lt;p&gt;But time and tide wait for no man, so on we march, continuing our trip into the world of Hadoop Distributions. Up next, it’s the Hortonworks Data Platform (HDP) - we’ll start by looking at HDP itself, and then move on to the technologies it includes that we haven’t looked at yet.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Week That Was - 17/02/2017</title><link>https://ondataengineering.net/blog/2017/02/17/the-week-that-was/</link><description> &lt;p&gt;So let’s review the technologies we’ve looked at this week as we come to the end of our journey through the technologies bundled with Apache Bigtop.&lt;/p&gt; &lt;p&gt;The last Bigtop technologies we looked at where a couple of web based end user tools (&lt;a href=&quot;/technologies/hue&quot;&gt;Hue&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-zeppelin&quot;&gt;Apache Zeppelin&lt;/a&gt;), an HDFS compatible filesystem (&lt;a href=&quot;/technologies/quantcast-file-system&quot;&gt;Quantcast File System&lt;/a&gt;) and an MPP database (&lt;a href=&quot;/technologies/greenplum&quot;&gt;Greenplum&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;And today, we’ve started our look the &lt;a href=&quot;/technologies/hortonworks-data-platform&quot;&gt;Hortonworks Data Platform&lt;/a&gt;. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Of all the technologies we’ve looked at this week, &lt;a href=&quot;/technologies/hue&quot;&gt;Hue&lt;/a&gt; was the biggest surprise. It’s an open source project licensed under the Apache 2.0 licence, but is not an Apache Foundation project (it actually sits in a Cloudera github repository). It’s pitched as a general purpose user interface for Hadoop, and the range of functionality it includes was surprising - everything from managing data in HDFS to creating Ozzie workflows to monitoring YARN logs to running SQL and Solr queries. It’s not an analysis notebook ala Jupyter or Zeppelin (although it now has some basic functionality in this area), but a web front end onto all the common Hadoop components, and if you’re using Hadoop, I would strongly suggest it’s worth your time to take a look at it. Even Hortonworks bundle it (despite the fact it’s not an Apache project), although they obviously don’t advertise this.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/technologies/apache-zeppelin&quot;&gt;Apache Zeppelin&lt;/a&gt; is however an analytical notebook, with support for a wide range of languages and analytical tools. If you’re doing interactive or exploratory analytics it’s probably well worth a look.&lt;/p&gt; &lt;p&gt;The promise and potential of open source software is often overstated, but if you have a strong development team that are comfortable in opening and extending open source software then you can create fantastic capabilities specifically tuned to your business. &lt;a href=&quot;/technologies/quantcast-file-system&quot;&gt;Quantcast File System&lt;/a&gt; is a great example of this - an HDFS compatible filesystem based on an open source project (KFS) that allows Quantcast to operate at a scale that isn’t supported by other technologies. Companies that operate at extreme scale are fantastic breeding grounds for innovation, and we’ll definitely look at some of the technologies coming out of companies like Netflix, Facebook, eBay and LinkedIn in the future.&lt;/p&gt; &lt;p&gt;There are many reasons why companies open source projects - to pay back to the community, to accelerate development, and because a technology is no longer of significant commercial value. &lt;a href=&quot;/technologies/greenplum&quot;&gt;Greenplum&lt;/a&gt; feels like it falls into the final category - it’s been a commercial product since 2003 and has failed to gain any significant traction. It does however still seem to be under development (although primarily by Pivotal rather than by outside competitors), so maybe I’m misjudging this. I wouldn’t be &lt;a href=&quot;http://www.ness.com/big-data-101-the-rise-and-fall-of-greenplum-2/&quot;&gt;the only one&lt;/a&gt; however.&lt;/p&gt; &lt;p&gt;And today we looked at our second Hadoop distribution - the &lt;a href=&quot;/technologies/hortonworks-data-platform&quot;&gt;Hortonworks Data Platform&lt;/a&gt;. There’s a lot to like about HDP, especially it’s commitment to open source - I just hope that Hortonworks can work out a commercial model that makes them a sustainable business.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Week That Was - 24/02/2017</title><link>https://ondataengineering.net/blog/2017/02/24/the-week-that-was/</link><description> &lt;p&gt;And so we’ve started our foray into the technologies bundled with &lt;a href=&quot;/technologies/hortonworks-data-platform&quot;&gt;Hortonworks Data Platform&lt;/a&gt;. We’ve already looked at a large number of these technologies, but there’s a few here that are new.&lt;/p&gt; &lt;p&gt;First up this week were the Hortonworks candidates in the metadata management and security space - &lt;a href=&quot;/technologies/apache-atlas&quot;&gt;Apache Atlas&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-ranger&quot;&gt;Apache Ranger&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-knox&quot;&gt;Apache Knox&lt;/a&gt;. We’ve then finished the week with &lt;a href=&quot;/technologies/apache-falcon&quot;&gt;Apache Falcon&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-calcite&quot;&gt;Apache Calcite&lt;/a&gt;. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Everyone says they want metadata management, data catalogues and business glossaries, very few people actually build them, and they’re desperately un-sexy. Which is why you don’t often see open source technologies in this space. &lt;a href=&quot;/technologies/apache-atlas&quot;&gt;Apache Atlas&lt;/a&gt; is trying to buck that trend, with some significant commercial backers. It’s going to be interesting to see how far this gets and what level of adoptions it gets. At the moment it feels like it’s trailing Cloudera Navigator, but that’s a commercial product which perhaps gives Cloudera greater impetus to invest in it. One to come back to at some point I think.&lt;/p&gt; &lt;p&gt;We’ll also be coming back to look at the state of security in the Hadoop Ecosystem - Cloudera and MapR are supporting Apache Sentry, whereas Hortonworks are supporting &lt;a href=&quot;/technologies/apache-ranger&quot;&gt;Apache Ranger&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-knox&quot;&gt;Apache Knox&lt;/a&gt;. Competition and survival of the fittest in open source is one of it’s greatest strengths, however in this case it seems like the Cloudera Hortonworks rivalry (for want of a better word) is perhaps not helping to overall Hadoop ecosystem.&lt;/p&gt; &lt;p&gt;I really want to like &lt;a href=&quot;/technologies/apache-falcon&quot;&gt;Apache Falcon&lt;/a&gt;, however I think I need to get to know it better before I start professing any love. It’s trying to solve a real problem - managing and orchestrating your data pipelines and the data that moves between and through these - however it’s a difficult problem, and creating a reductive solution can create real limitations and constraints. Another one I’d like to return to in due course.&lt;/p&gt; &lt;p&gt;And finally &lt;a href=&quot;/technologies/apache-calcite&quot;&gt;Apache Calcite&lt;/a&gt;, here to claim the crown (from &lt;a href=&quot;/technologies/apache-zookeeper&quot;&gt;Apache ZooKeeper&lt;/a&gt;) of the best, most widely used technology you’ve probably never heard of. If you use an open source technology that has a SQL interface, you’re more than likely to be using Calcite - it provides SQL parsing, cost based optimisation and JDBC frameworks that are used in Hive, Drill, Storm, Apex, Druid, Kylin, Phoenix, Solr, Flink, Cascading and Samza amongst others. Creators of open source software often don’t get the acknowledgement they deserve, but Julian Hyde deserves our thanks and appreciation for creating what would become Apache Calcite.&lt;/p&gt; &lt;p&gt;Right - I’m done with this week. See you on the other side of the weekend.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Week That Was - 03/03/2017</title><link>https://ondataengineering.net/blog/2017/03/03/the-week-that-was/</link><description> &lt;p&gt;Right - we’re nearly at the end of the &lt;a href=&quot;/technologies/hortonworks-data-platform&quot;&gt;Hortonworks Data Platform&lt;/a&gt; technologies. Let’s summarise what we’ve looked at this week.&lt;/p&gt; &lt;p&gt;We started off with &lt;a href=&quot;/technologies/apache-slider&quot;&gt;Apache Slider&lt;/a&gt;, then looked at &lt;a href=&quot;/technologies/apache-storm&quot;&gt;Apache Storm&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-accumulo&quot;&gt;Apache Accumulo&lt;/a&gt;, before finishing off with &lt;a href=&quot;/technologies/livy&quot;&gt;Livy&lt;/a&gt; and &lt;a href=&quot;/technologies/hortonworks-data-platform-search/&quot;&gt;HDP Search&lt;/a&gt;. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/technologies/apache-slider&quot;&gt;Apache Slider&lt;/a&gt; is interesting. It feels like a technology that allows you to run any long running app on YARN and have it play nicely with other YARN apps should be something people care about, because isn’t the whole selling point of Hadoop that you can have on analytical cluster that supports multiple workloads that all play nicely together, but it looks like outside of Hortonworks there’s very take up. It’s been in incubation since April 2014, and it seems like the biggest barrier to graduation is that there simply aren’t any committers outside of Hortonworks.&lt;/p&gt; &lt;p&gt;If there’s any one technology that kick-started the rise of the streaming data engines is has to be &lt;a href=&quot;/technologies/apache-storm&quot;&gt;Apache Storm&lt;/a&gt; - the granddaddy of streaming technologies and still the 900 pound gorilla in the room. It’s not perfect, people have taken a lot of potshots at it over the years, and Twitter have now moved on (to Heron), however it’s been successful for a reason, and it looks like it’s been given a new lease of life after joining the Apache foundation, so if you’re looking at streaming use cases I don’t think you can afford not to look at it. Brush up on your micro batch vs record at a time considerations first however.&lt;/p&gt; &lt;p&gt;It want to look at &lt;a href=&quot;/technologies/apache-hbase&quot;&gt;Apache HBase&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-accumulo&quot;&gt;Apache Accumulo&lt;/a&gt; in more detail in the future, however Accumulo is gaining good adoption, is bundled with most Hadoop distributions, and has some interesting differentiations from HBase.&lt;/p&gt; &lt;p&gt;And on to &lt;a href=&quot;/technologies/livy&quot;&gt;Livy&lt;/a&gt; - a little piece of technology that’s come out of Cloudera that underpins the ability for analytical notebooks to run Spark code on remote clusters. I wonder how much it rankles Hortonworks to distribute Livy and Hue - both (open source Apache licenced) technologies that currently sit in a Cloudera repository in GitHub.&lt;/p&gt; &lt;p&gt;And last up for this week is &lt;a href=&quot;/technologies/hortonworks-data-platform-search/&quot;&gt;HDP Search&lt;/a&gt; - a custom bundling of Solr (along with a bunch of other technologies), built and maintained by Lucidworks and distributed as an add on to the Hortonworks Data Platform. It means that Solr doesn’t come out of the box with HDP (you have to download an extra Ambari management pack manually to install it), but it looks like a great partnership for Hortonworks - you get support from arguably the leading experts in Solr, and get Solr bundled with a bunch of other useful stuff that you don’t get with the other distributions.&lt;/p&gt; &lt;p&gt;Next week - the final HDP technologies. Have a great weekend.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Week That Was - 10/03/2017</title><link>https://ondataengineering.net/blog/2017/03/10/the-week-that-was/</link><description> &lt;p&gt;And so we come to the end of the &lt;a href=&quot;/technologies/hortonworks-data-platform&quot;&gt;Hortonworks Data Platform&lt;/a&gt; technologies. On Monday we’ll start looking at the remainder of the Hortonworks technology offerings (yes - I know we’re meant to be working our way through the Hadoop distributions - it’ll only be a short detour), but for now let’s summarise what we’ve looked at this week.&lt;/p&gt; &lt;p&gt;First up was the second add-on to HDP based on a partnership with another company - &lt;a href=&quot;/technologies/apache-hawq&quot;&gt;Hortonworks HBD&lt;/a&gt; (aka Pivotal HDB, aka Apache Hawq). We then looked at the management components of the HDP stack - &lt;a href=&quot;/technologies/apache-ambari&quot;&gt;Apache Ambari&lt;/a&gt;, &lt;a href=&quot;/technologies/cloudbreak&quot;&gt;Cloudbreak&lt;/a&gt; and &lt;a href=&quot;/technologies/hortonworks-smartsense&quot;&gt;Hortonworks SmartSense&lt;/a&gt;. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;So - &lt;a href=&quot;/technologies/apache-hawq&quot;&gt;Hortonworks HBD&lt;/a&gt;. It’s Pivotal HDB - you download it from Pivotal and the Hortonworks documentation links through to the Pivotal documentation, but with Hortonworks (according to the press release) providing customer support and professional implementation services. The press release is worth a look - as part of the deal Pivotal agreed to drop Pivotal HD (their Hadoop distribution) and resell HDP instead, and Hortonworks agreed to distribute HDB. But given their investment in Hive through the Stinger initiative, you have to wonder how interested Hortonworks are in pushing it. Which is possibly a shame, because Apache Hawq is probably the most mature SQL engine available on Hadoop today - whether that’s a good or bad thing, and how much traction it’s going to get I don’t know.&lt;/p&gt; &lt;p&gt;And on to &lt;a href=&quot;/technologies/apache-ambari&quot;&gt;Apache Ambari&lt;/a&gt; - Hortonworks competitor to Cloudera Manager. What I found most interesting about Ambari was the list of contributors - 50 from Hortonworks, 6 from IBM, 6 from Pivotal, 4 from RedHat plus some more - 82 in total. That’s a pretty significant development capacity, and probably goes to show how valuable having an easy way to provision and manage Hadoop clusters is to its adoption. What also struck me about Ambari was how much it didn’t feel like an open source technology - it only (realistically) supports the installation of HDP, the committers are all employees of large companies and the Apache documentation is pretty poor.&lt;/p&gt; &lt;p&gt;I split &lt;a href=&quot;/technologies/apache-ambari/ambari-views&quot;&gt;Ambari Views&lt;/a&gt; out from Ambari for a couple of reasons. Firstly, I ran out of time on Tuesday to include it in the Ambari technology summary, and there was probably too much to put in a single summary anyway, but also because I think Ambari Views is targeting a slightly different use case and group of users than Ambari. It feels like Hortonworks is lining this up as a competitor to Hue (they have a Hue to Ambari migration tool for starters), however it feels lightweight (in features) and heavyweight (in terms of hardware requirements) compared to Hue, and I don’t see it gaining the same traction. Perhaps if Cloudera had donated Hue to the Apache Foundation Ambari Views wouldn’t even exist.&lt;/p&gt; &lt;p&gt;And &lt;a href=&quot;/technologies/cloudbreak&quot;&gt;Cloudbreak&lt;/a&gt; - an extremely interesting technology where cloud infrastructure meets Docker meets Hadoop. It feels like early days for Cloudbreak, however Hadoop in the Cloud (on or off premesis) is seeing massive investment from both Cloudera and Hortonworks at the moment, and it’s a really interesting area that I’d love to come back to at some point.&lt;/p&gt; &lt;p&gt;Lastly to &lt;a href=&quot;/technologies/hortonworks-smartsense&quot;&gt;Hortonworks SmartSense&lt;/a&gt;. Hortonworks’ business model is interesting - their commitment to open source means you can use their entire technology stack for free, you only pay for support and professional services, but this means their support and services offering has to deliver value and be worth the money (which also means that their stack can’t be too reliable or easy to manage without their help). SmartSense is their only technology that isn’t open source, and is the key piece in the value of their support offering. And the fact it appears to contain a bunch of cluster analytics that aren’t available through Ambari is an interesting facet of that.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hortonworks</title><link>https://ondataengineering.net/blog/2017/03/13/hortonworks/</link><description> &lt;p&gt;Right - we’ve finished with our probing into the &lt;a href=&quot;/technologies/hortonworks-data-platform&quot;&gt;Hortonworks Data Platform&lt;/a&gt; technologies, it’s time to move on.&lt;/p&gt; &lt;p&gt;So today, we’re going to add &lt;a href=&quot;/tech-vendors/hortonworks/&quot;&gt;Hortonworks&lt;/a&gt; to our vendor catalogue, and over the next week and a bit have a look at some of the other technology offerings they have outside of HDP. Once we’re done with that, we’ll revert to our original course of looking at all the major Hadoop distributions.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Week That Was - 17/03/2017</title><link>https://ondataengineering.net/blog/2017/03/17/the-week-that-was/</link><description> &lt;p&gt;And another week passes…&lt;/p&gt; &lt;p&gt;This week we’ve wrapped up the &lt;a href=&quot;/tech-vendors/hortonworks&quot;&gt;Hortonworks&lt;/a&gt; technology stack (give or take). On Monday we’ll review what we’ve found, and look ahead to our next destination - Cloudera.&lt;/p&gt; &lt;p&gt;So what have we looked at this week? We took a spin through &lt;a href=&quot;/technologies/hortonworks-data-flow&quot;&gt;Hortonworks DataFlow&lt;/a&gt;, Hortonworks’ bundling of &lt;a href=&quot;/technologies/apache-nifi&quot;&gt;Apache NiFi&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-kafka&quot;&gt;Apache Kafka&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-storm&quot;&gt;Apache Storm&lt;/a&gt;. We’ve looked at Kafka and Storm previously, but we paused this week to look at &lt;a href=&quot;/technologies/apache-nifi&quot;&gt;Apache NiFi&lt;/a&gt; and it’s sub-project &lt;a href=&quot;/technologies/apache-nifi/minifi&quot;&gt;MiNiFi&lt;/a&gt; in more detail.&lt;/p&gt; &lt;p&gt;And we finished off by looking at &lt;a href=&quot;/technologies/hortonworks-data-cloud-for-aws/&quot;&gt;HDCloud for AWS&lt;/a&gt;. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/technologies/hortonworks-data-flow&quot;&gt;Hortonworks DataFlow&lt;/a&gt; (HDF to it’s friends) is Hortonworks’ big push into analytics on data in motion, and more specifically into analytics in the Internet of Things world (or Internet of Anything as they refer to it). It’s a compelling story - the ability to deploy key real time analytical technologies independently from your Hadoop cluster (which can now focus on the batch historical analytical use cases) - and comes with the introduction of a new technology - &lt;a href=&quot;/technologies/apache-nifi&quot;&gt;Apache NiFi&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;What to say about &lt;a href=&quot;/technologies/apache-nifi&quot;&gt;Apache NiFi&lt;/a&gt;? There’s a use case here that I think NiFi fills almost unapproachably well - specifically getting batch (and probably mini batch) data to your analytical cluster. Previously you’d be looking at a bunch of technologies - Sqoop for database unloads, and some combination of shell scripts, FTP transfers, custom jobs to pull data from queues etc. etc. NiFi wraps all of this up - giving you a single solution to bring data from anywhere to a place where you can exploit it. The visualisation of the data moving through your flows, the ability to view this data, to get detailed provenance of where every file came from and when, and to perform common file level transformations just make this a great fit for this use case (although I’m never entirely convinced by the develop, test, release and configuration management story of GUI based tools, but that’s a discussion for another day). Where I think it has stiffer competition, and where I’m not as wholly convinced, is in the high volume, low latency, real-time event data space. There are a lot of well established technologies in this space (Logstash, FluentD and Heka for starters), and I’m not entirely convinced that NiFi is well architected for this use case. Do I really want provenance and record level state tracking when I’m bringing in billions of records per day - that seems like a significant overhead to me. By it’s a space NiFi is targeting, both with &lt;a href=&quot;/technologies/apache-nifi/minifi&quot;&gt;MiNiFi&lt;/a&gt; (which supports collection, transformation and forwarding out at the edge), and with some bold claims about throughput. I’m happy to accept I’ve missed something here, and I’d love to hear from anyone that can talk to this with some experience and evidence…&lt;/p&gt; &lt;p&gt;I’m going to update the Hortonworks vendor page on Monday with more information about their product offerings, as their Cloud offerings are a little more complex and convoluted that I was expecting. However &lt;a href=&quot;/technologies/hortonworks-data-cloud-for-aws/&quot;&gt;HDCloud for AWS&lt;/a&gt; is their only Hortonworks branded cloud offering - a tool that allows you to deploy and resize HDP clusters in AWS, but with a limited set of technologies, focusing on Hive, Spark and Zeppelin. It’s brand new, only coming out at the end of 2016, and it appears to overlap with a more general capability that &lt;a href=&quot;/technologies/cloudbreak&quot;&gt;Cloudbreak&lt;/a&gt; is targeting. We’ll keep an eye on these, as it feels like next year is going to see a lot of movement in the Hadoop on Cloud space.&lt;/p&gt; &lt;p&gt;Right - back to the grindstone before the escape of the weekend. See you all next week for our first looks at Cloudera’s product offerings.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hortonworks to Cloudera</title><link>https://ondataengineering.net/blog/2017/03/20/hortonworks-to-cloudera/</link><description> &lt;p&gt;Right - I think we’re done with our trip through the &lt;a href=&quot;/tech-vendors/hortonworks&quot;&gt;Hortonworks&lt;/a&gt; technology stack, however there are two updates today before we move on. Apologies in advance if this screws up either the RSS feeds or e-mail newsletters - I’m not entirely sure how these will handle updates to the site, but there’s only one way to find out. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Firstly, I’ve re-worked the &lt;a href=&quot;/tech-vendors/hortonworks&quot;&gt;Hortonworks&lt;/a&gt; page to better describe their product offerings. The one thing I really like about Hortonworks is it’s openness. Normally when looking at commercial vendors there’s a whole pile of exaggeration and misdirection about what their product is and what it can do and how it’s so much better than any other option on the market, however because Hortonworks deal entirely in open source technologies they feel like a more open and transparent company, and actually understanding what they offer therefore feels a little more straightforward. There are some big commercial vendors coming up on my list however where I don’t have a huge amount of experience with their products, and I’m therefore holding out no hope of being able to understand the detail of what they offer, given the lack of any publicly available documentation and product websites that are little more than content-less flashy brochures. When I get to these, I’m definitely going to need some help.&lt;/p&gt; &lt;p&gt;Secondly, I’ve added a page for &lt;a href=&quot;/technologies/hortonworks-data-platform-for-windows&quot;&gt;HDP for Windows&lt;/a&gt;, which is now also included in our list of Hadoop Distributions. Hadoop itself, and a bunch of the other related Apache technologies support being built on Windows as a native Windows executable, however HDP is (as far as I’m aware) the only Hadoop distribution that supports Windows (presumably as part of Hortonworks’ deal with Microsoft for &lt;a href=&quot;/technologies/azure-hdinsight/&quot;&gt;Azure HDInsight&lt;/a&gt;). It’s not the full HDP distribution however - missing some key technologies including Ambari, Solr and Spark.&lt;/p&gt; &lt;p&gt;Up tomorrow then - our first look into the Cloudera technology stack. We’ll start with their distributions (CDH, Cloudera Express and Cloudera Enterprise), add these to our list of Hadoop distributions, have a look at the new Apache technologies that these include that we haven’t looked at yet, and then finish up with adding Cloudera to our list of technology vendors. At least that’s the plan - managing to stick to it will probably be a first for this site.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Week That Was - 24/03/2017</title><link>https://ondataengineering.net/blog/2017/03/24/the-week-that-was/</link><description> &lt;p&gt;So this week we started our journey into the Cloudera technology stack. I covered the final Hortonworks bits &lt;a href=&quot;/blog/2017/03/20/hortonworks-to-cloudera/&quot;&gt;on Monday&lt;/a&gt;, but what have we looked at since then?&lt;/p&gt; &lt;p&gt;We started off by looking at Cloudera’s Hadoop distribution &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt; and the technologies it bundles. We’ve covered a lot of these already (Hadoop being Hadoop there’s plenty of overlap between the various distributions), but there’s still plenty of new stuff here to keep us busy for a couple of weeks.&lt;/p&gt; &lt;p&gt;We then moved on to look at &lt;a href=&quot;/technologies/llama&quot;&gt;Llama&lt;/a&gt;, a small piece of open source technology created to support Impala running over YARN, &lt;a href=&quot;/technologies/apache-whirr&quot;&gt;Apache Whirr&lt;/a&gt;, a now retired Apache open source project for deploying a number of technologies onto cloud platforms, and &lt;a href=&quot;/technologies/apache-impala&quot;&gt;Apache Impala&lt;/a&gt;, Cloudera’s SQL on Hadoop engine for low latency interactive queries. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;I’m not sure there’s much to say about &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt; right now - it’s a great Hadoop distribution, and although it’s a significantly more commercial offering that Hortonworks, you can get most of it for free (including Cloudera Manager minus some enterprise bits). There’s pros and cons all over the stack if you’re comparing it against the other distributions, but we’ll take a deeper look at this in a couple of weeks time.&lt;/p&gt; &lt;p&gt;There’s not much to say about &lt;a href=&quot;/technologies/llama&quot;&gt;Llama&lt;/a&gt; - it was a piece of technology created by Cloudera to allow Impala to play nicely with YARN, but it’s not be adopted outside of Cloudera, who have themselves now deprecated it and will no longer be including it in CDH from v6.0 onwards (this wasn’t originally referenced in the technology summary so I’ve since updated it).&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/technologies/apache-whirr&quot;&gt;Apache Whirr&lt;/a&gt; feels like a bit of history. It’s well and truly dead now (with no development since September 2012, and having been moved into the Apache Attic in March 2015), however it was the first technology (I think I’m right in saying this) that allowed you to deploy Hadoop (and a whole pile of other technologies) into a cloud environment. It feels a little clunky and anachronistic now - there’s bespoke Java code written for each technology it supported to manage the software deployment and management using jclouds - but I’m guessing at the time it was almost revolutionary, paving the way for a whole pile of later technologies.&lt;/p&gt; &lt;p&gt;And on to &lt;a href=&quot;/technologies/apache-impala&quot;&gt;Apache Impala&lt;/a&gt;. At some point I want to dig into the whole Impala/Hive debate / religious war (because it’s a fascinating look into the relationship and cultural differences between Cloudera and Hortonworks as much as anything). Hive has always been a large batch data transformation engine (albeit executing SQL) - every query starting a new job (originally MapReduce but lately Spark or Tez) with all the associated overheads and latency, but with the ability to process as much data as your Hadoop cluster would hold. What it wouldn’t do is support large numbers of more targeted queries at a low latency, for example from a group of users running queries against a data mart (the so called interactive query use case) - this was the use case Impala (and Parquet - the columnar data format) were created to target. Impala was therefore never designed to replace or compete with Hive, it’s competition are the traditional OLAP database such as Greenplum, Netezza and Teradata, with Impala trying to deliver a roughly comparable capability at a much lower cost. Hortonworks of course felt that Hive should support both use cases, and its introduction of LLAP (which allows long running processes to execute multiple queries) was it’s answer to this.&lt;/p&gt; &lt;p&gt;And that’s us for this week - have a good weekend, and we’ll see you on the other side.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Week That Was - 31/03/2017</title><link>https://ondataengineering.net/blog/2017/03/31/the-week-that-was/</link><description> &lt;p&gt;No technology summary today for various reasons, one of which is that I’m taking a break next week and we’ve probably got to a pretty good place to pause. We’ve finished looking at the new open source technologies in the Cloudera stack this week, with their proprietary closed source technologies to come, but let’s save those for a fresh week.&lt;/p&gt; &lt;p&gt;So what exactly have we looked at this week? We started by looking at &lt;a href=&quot;/technologies/apache-sentry&quot;&gt;Apache Sentry&lt;/a&gt;, Cloudera’s competitor to &lt;a href=&quot;/technologies/apache-ranger&quot;&gt;Apache Ranger&lt;/a&gt; and &lt;a href=&quot;/technologies/cloudera-search/&quot;&gt;Cloudera Search&lt;/a&gt;, Cloudera’s competitor to Hortonworks’ &lt;a href=&quot;/technologies/hortonworks-data-platform-search/&quot;&gt;HDP Search&lt;/a&gt;. We then looked at &lt;a href=&quot;/technologies/apache-kudu/&quot;&gt;Apache Kudu&lt;/a&gt;, a structured data store that supports both updates and deletes by primary key as well as efficient analytical table scans, and &lt;a href=&quot;/technologies/recordservice/&quot;&gt;RecordService&lt;/a&gt;, a new technology that’s still in beta that provides an API for tools (such as Spark and MapReduce) to access structured data in Hadoop with fine grained access control. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;At some point, and I realise I keep saying this, I want to take a deeper look into the state of the security technologies in Hadoop, and more specifically a decent comparison of &lt;a href=&quot;/technologies/apache-sentry&quot;&gt;Apache Sentry&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-ranger&quot;&gt;Apache Ranger&lt;/a&gt;. There’s some history behind how we’ve ended up with two Apache technologies that are essentially trying to solve the same problem, each has their pros and cons but either will probably do what you need them two, with Ranger possibly looking the slightly further ahead functionality wise. Note that I’ve make some minor tweaks to the &lt;a href=&quot;/technologies/apache-ranger&quot;&gt;Apache Ranger&lt;/a&gt; technology summary this week.&lt;/p&gt; &lt;p&gt;I’m not entirely sure where the whole “wrap Solr up with some tools and utilities and give it a new name” came from, but there are some interesting differences between &lt;a href=&quot;/technologies/cloudera-search/&quot;&gt;Cloudera Search&lt;/a&gt; and &lt;a href=&quot;/technologies/hortonworks-data-platform-search/&quot;&gt;HDP Search&lt;/a&gt; in the wraps that they put around Solr. Hortonworks bundle Banana (the Solr port of Kibana), they both support utilities for loading data from HDFS and moving data from HBase to Solr, but Hortonworks also bundle integrations with Solr for Hive, Pig, Storm and Spark. Note again that I’ve made some minor tweaks to the HDP Search technology summary this week.&lt;/p&gt; &lt;p&gt;Cloudera position &lt;a href=&quot;/technologies/apache-kudu/&quot;&gt;Apache Kudu&lt;/a&gt; as the missing link between &lt;a href=&quot;/technologies/apache-hadoop/hdfs&quot;&gt;HDFS&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-hbase&quot;&gt;HBase&lt;/a&gt; - able to perform updates and deletes by primary key (ala HBase), as well as analytical queries performing column and table scans (ala HDFS). It’s not going to replace either for their respective specialisms, but the ability to run analytical workloads over mutable data feels like a bit of a gap in the Hadoop ecosystem at the moment (that probably led to the rise of the &lt;a href=&quot;http://lambda-architecture.net/&quot;&gt;lambda architecture&lt;/a&gt;). Kudu only provides the storage engine, but its combination with &lt;a href=&quot;/technologies/apache-impala&quot;&gt;Apache Impala&lt;/a&gt; to provide a SQL interface on top feels like yet more evidence that Cloudera is firmly targeting the established OLAP database vendors. The &lt;a href=&quot;http://kudu.apache.org/kudu.pdf&quot;&gt;Kudu whitepaper&lt;/a&gt; is worth a skim if you’re interested, including the performance comparison between Kudu, Parquet on HDFS and Phoenix on HBase.&lt;/p&gt; &lt;p&gt;And finally &lt;a href=&quot;/technologies/recordservice/&quot;&gt;RecordService&lt;/a&gt;, the benefits of which seem twofold. Firstly, it provides a unified data access path for Hadoop technologies - rather than every technology (MapReduce, Spark, etc.) having to implement their own reader for each file format, they can just integrate with RecordService. This then enables the second - fine grained access control to this data. At the moment, when tools like MapReduce and Spark read data from HDFS, access can only be granted at the file level - RecordService will enable finer grained access control in this scenario, which can only be a good thing. Two thoughts however - you can achieve this today by forcing these tools to read data via Hive, however my guess is that there are some performance limitations on this at scale that RecordService will address, and that at the moment it’s very early days for RecordService, plus it’s tied to Apache Sentry, so it’s going to be interesting to see how much traction this gains into the wider Hadoop ecosystem.&lt;/p&gt; &lt;p&gt;As mentioned above I’m taking a break next week so there’ll be no updates, but we’ll be back on Monday 10th with a look at Cloudera’s closed source commercial technologies.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Week That Was - 14/04/2017</title><link>https://ondataengineering.net/blog/2017/04/14/the-week-that-was/</link><description> &lt;p&gt;It’s the Easter holidays here in the UK, so no technology summary today, but let’s recap the last week before we forget everything we looked at.&lt;/p&gt; &lt;p&gt;This week, we’ve been looking at the Cloudera’s closed source products - &lt;a href=&quot;/technologies/cloudera-manager/&quot;&gt;Cloudera Manager&lt;/a&gt;, their tool for creating and managing CDH hadoop clusters, &lt;a href=&quot;/technologies/cloudera-navigator/&quot;&gt;Cloudera Navigator&lt;/a&gt;, a set of products for data management, data encryption and helping migrate SQL workloads to Hadoop, and &lt;a href=&quot;/technologies/cloudera-director/&quot;&gt;Cloudera Director&lt;/a&gt;, for doing CDH Hadoop in the cloud. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;The philosophical debate between Cloudera and Hortonworks on the used of closed source software is an interesting one, and lends almost a personal element to their commercial rivalry. The general view is that Cloudera have added closed source software into their offering to increase lock in, but I’m not entirely convinced by this - the cost of moving from Cloudera Manager/Navigator to an alternative doesn’t feel like it significantly impacts the fundamental costs of migrating from one distribution to another. Does it provide extra encouragement for companies to take up subscription licences - again I’m not entirely convinced, the risk with not having commercial support for critical systems is likely to be the driver here. Cloudera’s stated view is that it’s protection for their investment in Hadoop open source projects, to prevent a large company with deep pockets competing with them by distributing the open source software that they’ve put so much investment into. What’s clear is that most of the arguments are ideological in nature - I think if you look at it impartially the use of closed source software by Cloudera is unlikely to significantly impact any selection process.&lt;/p&gt; &lt;p&gt;And so on to the products themselves. I feel like I’ve probably done &lt;a href=&quot;/technologies/cloudera-manager/&quot;&gt;Cloudera Manager&lt;/a&gt; a slight disservice in it’s technology summary - cold hard facts on how it compares to &lt;a href=&quot;/technologies/apache-ambari/&quot;&gt;Apache Ambari&lt;/a&gt; seem to be pretty scarce (perhaps we’ll try and do something about this at some point), however the general consensus seems to be that Cloudera Manager was more mature and more capable, but that Ambari has been catching up rapidly, and there’s probably not a huge amount in it any more.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/technologies/cloudera-navigator/&quot;&gt;Cloudera Navigator&lt;/a&gt; started off as Cloudera’s data (or metadata) management tool, and is probably slightly more mature and capable in this space than &lt;a href=&quot;/technologies/apache-atlas/&quot;&gt;Apache Atlas&lt;/a&gt;. Metadata management in general still feels very immature however - most tools don’t deliver on their promises, and the investment, time and effort to get value out of them is often under estimated, leading to the infamous data swamp if you’re working in the Hadoop space.&lt;/p&gt; &lt;p&gt;Today, in addition to it’s original &lt;a href=&quot;/technologies/cloudera-navigator/data-management/&quot;&gt;data management&lt;/a&gt; elements, Cloudera Navigator also includes &lt;a href=&quot;/technologies/cloudera-navigator/data-encryption/&quot;&gt;data encryption&lt;/a&gt; capabilities following Cloudera’s acquisition of Gazzang, and &lt;a href=&quot;/technologies/cloudera-navigator/optimizer/&quot;&gt;Optimizer&lt;/a&gt; following their acqusition of Xplain.io, an interesting technology I know very little about for helping to migrate SQL workloads to Hadoop and to then optimise these.&lt;/p&gt; &lt;p&gt;And finally &lt;a href=&quot;/technologies/cloudera-director/&quot;&gt;Cloudera Director&lt;/a&gt; - Cloudera’s tool for creating, scaling and managing CDH clusters in all your favourite cloud environments. I’m not sure there’s a huge amount more to say than that to be honest.&lt;/p&gt; &lt;p&gt;Right - I’m off to eat chocolate. If I’ve not exploded we’ll resume next Tuesday with our summary of Cloudera themselves, and then maybe on Wednesday we’ll do our first news post to catch up on the many technologies that have had new releases since we did their initial technology summary.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 19/04/2017</title><link>https://ondataengineering.net/blog/2017/04/19/the-mid-week-news/</link><description> &lt;p&gt;Right, I’ve been slack in getting this out there, which means we’ve built up a nasty backlog, but it’s time to talk about what’s changed since we originally wrote some of our technology summaries. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Firstly, there are new releases of the &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt; and &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt;. &lt;a href=&quot;https://hortonworks.com/blog/announcing-the-general-availability-of-hortonworks-data-platform-2-6/&quot;&gt;Version 2.6 of HDP&lt;/a&gt; brings two main features - &lt;a href=&quot;https://hortonworks.com/blog/top-5-performance-boosters-with-apache-hive-llap/&quot;&gt;Hive LLAP&lt;/a&gt;, the ability for Hive to target the real time interactive query space, and &lt;a href=&quot;https://hortonworks.com/blog/apache-hive-moving-beyond-analytics-offload-with-sql-merge/&quot;&gt;Hive ACID Merges&lt;/a&gt;, allowing data to be transactionally loaded into Hive. &lt;a href=&quot;http://community.cloudera.com/t5/Community-News-Release/Announce-Cloudera-Enterprise-5-11-is-Now-Available/m-p/53808#M170&quot;&gt;Version 5.11 of CDH&lt;/a&gt; brings Navigator lineage support for Spark, the integration of Kudu with Kerberos, improvements to S3 support and support for Azure Data Lake Store.&lt;/p&gt; &lt;p&gt;There have also been a mass of projects that have seen new releases. Ordinarily I’d like to provide some sort of commentary on these, however given I’ve built up such a backlog we’ll just list them off this week. Each technology page includes a link to the relevent release announcement or details if you’re interested however. So, in no particular order the technologies that have seen new releases are: &lt;a href=&quot;/technologies/apache-ambari/&quot;&gt;Apache Ambari&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-apex/&quot;&gt;Apache Apex&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-atlas&quot;&gt;Apache Atlas&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-calcite&quot;&gt;Apache Calcite&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-crunch/&quot;&gt;Apache Crunch&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Apache Hadoop&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hbase/&quot;&gt;Apache HBase&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-ignite/&quot;&gt;Apache Ignite&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-knox/&quot;&gt;Apache Knox&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-mahout/&quot;&gt;Apache Mahout&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-parquet/&quot;&gt;Apache Parquet C++&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-phoenix/&quot;&gt;Apache Phoenix&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-ranger/&quot;&gt;Apache Ranger&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-solr/&quot;&gt;Apache Solr&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-storm/&quot;&gt;Apache Storm&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;In terms of other technologies news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sqoop2 has been deprecated by Cloudera as of CDH 5.9, and will be removed from CDH in version 6. Suggests that all is not well in Sqoop2 land.&lt;/li&gt; &lt;li&gt;Hadoop 3.0 is now into it’s second alpha release. Summary is &lt;a href=&quot;http://hadoop.apache.org/docs/r3.0.0-alpha2/index.html&quot;&gt;here&lt;/a&gt;, with some thoughts form &lt;a href=&quot;https://hortonworks.com/blog/data-lake-3-0-part-4-cutting-storage-overhead-in-half-with-hdfs-erasure-coding/&quot;&gt;Hortonworks&lt;/a&gt; and &lt;a href=&quot;http://blog.cloudera.com/blog/2017/02/apache-hadoop-3-0-0-alpha2-released/&quot;&gt;Cloudera&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-ranger/&quot;&gt;Apache Ranger&lt;/a&gt; has graduated to a top level Apache project. There’s an &lt;a href=&quot;https://www.infoq.com/news/2017/03/apache-ranger-top-level-project&quot;&gt;InfoQ write-up&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-gobblin/&quot;&gt;Gobblin&lt;/a&gt; has been donated to the Apache Foundation by LinkedIn - &lt;a href=&quot;https://github.com/linkedin/gobblin&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We’ll try and do this weekly going forward - let’s just hope keeping up to date with everything doesn’t prove to be unsustainable! And next week we’ll have a look at some of the interesting blog posts I’ve been accumulating.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Week That Was - 21/04/2017</title><link>https://ondataengineering.net/blog/2017/04/21/the-week-that-was/</link><description> &lt;p&gt;We’ve been a little bit all over the shop this week, but let’s try and summarise what we’ve looked at.&lt;/p&gt; &lt;p&gt;We started late having taken Monday off for Easter, with a look at &lt;a href=&quot;/tech-vendors/cloudera/&quot;&gt;Cloudera&lt;/a&gt; on Tuesday, closing out our review of them and their technologies. We then took another break on Wednesday to catchup on everything that’s changed in the technologies we’ve looked at to date, returning on Thursday with the start of our journey into MapR, the final Hadoop distribution we’re going to look at in detail. We’ve started by looking at their open source components, looking at the &lt;a href=&quot;/technologies/mapr-ecosystem-pack/&quot;&gt;MapR Ecosystem Pack&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-drill/&quot;&gt;Apache Drill&lt;/a&gt; &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;I think we’ve probably covered most of what there is to say on Cloudera in previous posts. At the moment they’re the biggest player in the Hadoop space, with a significant investment from Intel (an order of magnitude greater than any investment Hortonworks or MapR have secured), plus a recently announced IPO. What’s going to be interesting is how they transition from the “Hadoop’s the hot new thing come and get it from us” business model to one focused on specific business challenges and direct competition to more established BI and analytics vendors - you can see the start of that in the way they’ve recently re-organised their product offerings.&lt;/p&gt; &lt;p&gt;I don’t know a huge amount about MapR, so am looking forward to learning more. What I’ve looked at so far looks interesting - a commercial data repository that supports multiple interfaces (file orientated, database and messaging), addresses some of Hadoop’s limitations, and provides Hadoop compatible APIs (specifically HDFS, HBase and Kafka APIs), that’s blended with YARN to provide an Hadoop compatible analytics platform. It’s going to be interesting to dig into the detail and understand how this differentiates itself from vanilla Hadoop.&lt;/p&gt; &lt;p&gt;We’ve started off our look at MapR by looking at the &lt;a href=&quot;/technologies/mapr-ecosystem-pack/&quot;&gt;MapR Ecosystem Pack&lt;/a&gt; - a bundle of open source Apache Hadoop components that are certified to work together and to run on the MapR platform. All your favourite Hadoop technologies are in there (bar Solr), and MapR seem to have given themselves licence to pick and choose from technologies backed by both Cloudera and Hortonworks - an enviable position to be in.&lt;/p&gt; &lt;p&gt;There aren’t many technologies in the MapR Ecosystem Pack we’ve not looked at previously, the main one being &lt;a href=&quot;/technologies/apache-drill/&quot;&gt;Apache Drill&lt;/a&gt;. It’s another interactive (low latency high throughput) SQL query engine, with the big differentiator being that it can query and join across data in multiple datastores (including all your favourite filesystems, NoSQL and RDBMS databases) without first having to define a schema. There’s a lot of power here - being able to query data where it is without having to first bring it all together first or do lots of preparation delivers a huge range of benefits. Drill’s facing off against &lt;a href=&quot;/technologies/apache-impala/&quot;&gt;Impala&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Hive&lt;/a&gt; in the emerging interactive SQL on Hadoop space - there’s much more to dig into here in the future, however if you’re looking to get started there’s an interesting write up of Impala vs Drill from Rittman Mead &lt;a href=&quot;https://www.rittmanmead.com/blog/2017/04/sql-on-hadoop-impala-vs-drill/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;That’ll do for this week - have a lovely weekend and we’ll see you on the other side.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Week That Was - 28/04/2017</title><link>https://ondataengineering.net/blog/2017/04/28/the-week-that-was/</link><description> &lt;p&gt;&lt;em&gt;NOTE: this article was updated 23/02/18 to reflect changes to the MapR technology pages on this site&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Time for our weekly round up of the week again.&lt;/p&gt; &lt;p&gt;We finished off the new open source technologies in the MapR Ecosystem Pack at the beginning of the week, looking at &lt;a href=&quot;/technologies/apache-myriad&quot;&gt;Apache Myriad&lt;/a&gt;, which support running YARN over Mesos.&lt;/p&gt; &lt;p&gt;We then looked at &lt;a href=&quot;/technologies/mapr-monitoring/&quot;&gt;MapR Monitoring&lt;/a&gt;, a collection of open source components for collecting and visualising metrics and log files across the MapR ecosystem.&lt;/p&gt; &lt;p&gt;And we finished the week with a look at the meat of MapR’s offering - &lt;a href=&quot;/technologies/mapr-file-system&quot;&gt;MapR-FS&lt;/a&gt;, and the fist of it’s sub-projects &lt;a href=&quot;/technologies/mapr-file-system/mapr-db/&quot;&gt;MapR-DB&lt;/a&gt; &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/technologies/apache-myriad&quot;&gt;Apache Myriad&lt;/a&gt; provides connectivity between YARN and Mesos, effectively allowing YARN to run on a Mesos cluster, either dynamically or statically being assigned resources from Mesos that it then assigns to YARN jobs. It’s had some significant backing from eBay, MapR and Mesosphere, however the project seems to have gone very quiet with no commits since October 2016, so it’s either done, hit a brick wall or it’s going nowhere. If anyone knows, I’d like to hear.&lt;/p&gt; &lt;p&gt;I’d like to talk more about monitoring analytical systems and data transformation jobs in the future, however &lt;a href=&quot;/technologies/mapr-monitoring/&quot;&gt;MapR Monitoring&lt;/a&gt; is a really good reference example of what can be done. It combines collectd, OpenTSDB and Grafana for capturing, storing and visualising metrics and statistics, and fluentd, Elasticsearch and Kibana for capturing, storing and exploiting log files. It’s nice to see someone with an Hadoop distribution that uses the best of breed open source technologies rather than building yet more bespoke and custom solutions.&lt;/p&gt; &lt;p&gt;And so on to &lt;a href=&quot;/technologies/mapr-file-system&quot;&gt;MapR-FS&lt;/a&gt;. I have to admit to being surprised by MapR’s offerings. I’d always assumed they were a knock-off Hadoop distribution that was trying to find leverage by embedding a bunch of commercial components, however what’s become clear is that what they’re selling is not an Hadoop distribution but an enterprise data platform (based on MapR-FS) that just happens to have Hadoop compatibility. In short it’s highly resilient, scalable and performant, with support for full random read/write access, multi-tenancy, block level replication, snapshots, quotas, extensive and flexible access control, which supports a fully POSIX compliant filesystem with HDFS, NFS and FUSE APIs, a document and wide column datastore with OJAI and HBase APIs, a streaming data stores with Kafka compatible APIs, master-slave and master-master replication of database and streaming data stores, plus YARN support, meaning you can run any Hadoop compatible tool over the top. That’s a lot of capability in a single platform, which feels like it’s going to drive a strong TCO story.&lt;/p&gt; &lt;p&gt;I’ve broken out the database and streams capabilities into sub-projects, primarily because MapR talk about them as different products, even though they’re technically all part of the same solution. We’ve looked at &lt;a href=&quot;/technologies/mapr-file-system/mapr-db/&quot;&gt;MapR-DB&lt;/a&gt; this week, and will look at MapR-Streams first thing next week.&lt;/p&gt; &lt;p&gt;It’s another public holiday here in the UK on Monday, so no update then, however we’ll be back on Tuesday, and will hopefully round out our look at MapR next week. If you’re in the UK enjoy your long weekend, otherwise have fun at work on Monday.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 03/05/2017</title><link>https://ondataengineering.net/blog/2017/05/03/the-mid-week-news/</link><description> &lt;p&gt;So, I failed at the first hurdle in trying to do this weekly, however let’s carry on regardless.&lt;/p&gt; &lt;p&gt;This week - new products from Cloudera and Hortonworks, a bunch of Hortonworks and Cloudera releases that got missed last time, plus a collection of blog posts I’ve been collecting for a while. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;In terms of the new products from Cloudera and Hortonworks, we’ve seen &lt;a href=&quot;https://www.cloudera.com/products/data-science-and-engineering/data-science-workbench.html&quot;&gt;Cloudera Data Science Workbench&lt;/a&gt; and &lt;a href=&quot;https://hortonworks.com/apache/metron/&quot;&gt;Apache Metron&lt;/a&gt; formally released recently. I’m aiming to do tech summaries for both this week and we’ll look at these a bit closer.&lt;/p&gt; &lt;p&gt;Some Hortonworks updates that we missed last time - &lt;a href=&quot;/technologies/hortonworks-data-cloud-for-aws&quot;&gt;Hortonworks Data Cloud for AWS&lt;/a&gt; has seen a new release to 1.14, &lt;a href=&quot;/technologies/hortonworks-smartsense/&quot;&gt;Hortonworks SmartSense&lt;/a&gt; got a bump to 1.4, and it looks like &lt;a href=&quot;/technologies/hortonworks-data-platform-for-windows/&quot;&gt;HDP for Windows&lt;/a&gt; got discontinued whilst I wasn’t looking - 2.4 was the final version!&lt;/p&gt; &lt;p&gt;And on the Cloudera front, &lt;a href=&quot;/technologies/cloudera-manager/&quot;&gt;Cloudera Manager&lt;/a&gt;, &lt;a href=&quot;/technologies/cloudera-navigator/&quot;&gt;Cloudera Navigator&lt;/a&gt; and &lt;a href=&quot;/technologies/cloudera-director&quot;&gt;Cloudera Director&lt;/a&gt; have all seen version bumps as part of the CDH 5.11 release&lt;/p&gt; &lt;p&gt;And finally, some assorted blog posts that have caught my attention recently:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cloudera have released &lt;a href=&quot;http://blog.cloudera.com/blog/2017/04/apache-impala-leads-traditional-analytic-database/&quot;&gt;their last blog post&lt;/a&gt; on how much faster Impala is than anything else. Expect one from Hortonworks shortly that shows that Hive LLAP is actually the fastest.&lt;/li&gt; &lt;li&gt;From the ever excellent “The Morning Paper”, &lt;a href=&quot;https://blog.acolyer.org/2017/03/06/hopfs-scaling-hierarchical-file-system-metadata-using-newsql-databases/amp/&quot;&gt;a summary of a research paper on HopFS&lt;/a&gt;, a version of HDFS where the in-memory metadata database in the Name Node is replaced with a distributed database, allowing it to scale to much larger numbers of files and dramatically increase throughput.&lt;/li&gt; &lt;li&gt;An interesting &lt;a href=&quot;http://www.odbms.org/blog/2017/03/on-the-new-developments-in-apache-spark-and-hadoop-interview-with-amr-awadallah/&quot;&gt;interview with the CTO of Cloudera&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Merv Adrian’s latest &lt;a href=&quot;http://blogs.gartner.com/merv-adrian/2017/03/16/hadoop-tracker-march-2017/&quot;&gt;Hadoop tracker&lt;/a&gt; is out. I’m not sure you can directly compare the component versions in Hadoop distributions given how much each vendor pulls patches forward, but it’s an interesting analysis never-the-less.&lt;/li&gt; &lt;li&gt;The Flink blog has been busy with a &lt;a href=&quot;http://data-flair.training/blogs/apache-flink-ecosystem-components-tutorial/&quot;&gt;summary of the Flink ecosystem&lt;/a&gt; and a &lt;a href=&quot;http://data-flair.training/blogs/spark-vs-flink-vs-hadoop-comparison/&quot;&gt;comparison of Flink to Spark and MapReduce&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.datanami.com/2017/03/13/hadoop-failed-us-tech-experts-say/&quot;&gt;Hadoop has failed us&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Some &lt;a href=&quot;http://www.zdnet.com/article/at-analyst-conference-cloudera-focuses-message-pleads-the-fifth-on-ipo-rumors/&quot;&gt;analysis on Cloudera’s strategy&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Tech deep dives into &lt;a href=&quot;https://blogs.apache.org/hbase/entry/accordion-hbase-breathes-with-in&quot;&gt;HBase In-Memory Compaction&lt;/a&gt; and &lt;a href=&quot;http://blog.cloudera.com/blog/2017/04/apache-kudu-read-write-paths/&quot;&gt;Kudu read write paths&lt;/a&gt;&lt;/li&gt; &lt;li&gt;And &lt;a href=&quot;https://www.theregister.co.uk/2017/04/07/google_cloud_platform_partners_with_elastic_in_new_analytics_stretch/&quot;&gt;Elasticsearch is coming to Google’s Cloud Platform&lt;/a&gt;&lt;/li&gt; &lt;li&gt;And last but not least - Matt Turck’s monster &lt;a href=&quot;http://mattturck.com/bigdata2017/&quot;&gt;2017 Big Data Landscape&lt;/a&gt; - essential reading&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Week That Was - 05/05/2017</title><link>https://ondataengineering.net/blog/2017/05/05/the-week-that-was/</link><description> &lt;p&gt;It’s nearly the weekend, which means it’s time to summarise the week.&lt;/p&gt; &lt;p&gt;We started late this week (let’s hear it for public holidays), finishing off our look at MapR, before taking a quick look at Cloudera’s &lt;a href=&quot;/technologies/cloudera-data-science-workbench/&quot;&gt;Data Science Workbench&lt;/a&gt;. No technology summary today, but we’ll take a look at Apache Metron as part of this post. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;What I’ve really liked about MapR is their strategy around their common data platform to underpin a bunch of different data storage capabilities. I talked a little bit about their data platform &lt;a href=&quot;/blog/2017/04/28/the-week-that-was/&quot;&gt;last time&lt;/a&gt;, but this week as part of looking at &lt;a href=&quot;/technologies/mapr-file-system/mapr-db/&quot;&gt;MapR-DB&lt;/a&gt; and &lt;a href=&quot;/technologies/mapr-file-system/mapr-streams/&quot;&gt;MapR-Streams&lt;/a&gt; I’ve been thinking about how this compares and contrasts to Hadoop. Firstly, they’re both aiming to provide a common data platform that provides the ability to have a single cluster than can provide flexibility and value for money by allowing you to exploit the same infrastructure for multiple use cases. MapR appears to have fully embraced this, ensuring they support the ability to scale, partition and manage the platform in ways that Hadoop can’t yet, and by providing capabilities that Hadoop (and more specifically HDFS) doesn’t that actually make it work as a general purpose data platform - full random read and write access for starters. I’m also taken by MapR’s ability to provide access to the common data platform at different layers - rather than just build capabilities on top of their file system API, they’ve integrated (for example) MapR-DB at a much lower level, providing a range of benefits over HBase running over HDFS. It’s clear that Hadoop still has a long way to go to fulfil it’s potential, and without addressing some of it’s limitations we’re going to continue to see new technologies opting to implement their own storage systems from scratch (Kudu being a great example), leading to Hadoop clusters running multiple independent storage stacks on the same data nodes, which feels like it’s defeating the point.&lt;/p&gt; &lt;p&gt;I’ve also started wondering why there aren’t more common storage sub-system’s that multiple technologies leverage - not necessarily so that they could all co-exist on the same cluster (along this would be a side benefit), but just because storage systems are hard and complex, and it feels like there should be huge wins by having a strong and robust solution that can be leveraged for multiple capabilities. There are very few data platforms that don’t have some limitation or constraint, and that a world class storage system with a range of APIs implemented on top could be instantly competitive against a wide range of technologies. MapR are starting to demonstrate this - there’s certainly some evidence that MapR-Streams leverages their data platform and a Kafka compatible API to provide a solution that addresses a number of Kafka’s limitations.&lt;/p&gt; &lt;p&gt;Moving on, Cloudera’s &lt;a href=&quot;/technologies/cloudera-data-science-workbench/&quot;&gt;Data Science Workbench&lt;/a&gt; is now generally available. Their use of docker seems inspired - the flexibility this gives you to use different versions of different libraries in different notebooks, and to have this execution environment follow the notebook around feels like a huge win. It’s still early days for the product however - the number of interpreters seems light (not being able to run SQL or Solr searches directly in the notebook feels like a gap), and it remains to be seen how it will fair as a commercial product against the open source &lt;a href=&quot;/technologies/apache-zeppelin/&quot;&gt;Apache Zeppelin&lt;/a&gt; and Jupyer.&lt;/p&gt; &lt;p&gt;I said on Wednesday I’d do a technology summary of &lt;a href=&quot;http://metron.apache.org/&quot;&gt;Apache Metron&lt;/a&gt;, however I’ve decided as a packaged analytical application it’s probably slightly outside the conceptual remit of this site. It’s definitely worth digging into if you have time however, as it’s an interesting use case for what can be done with the Hadoop ecosystem, and an interesting capability in it’s own right. If you’re looking for reading material, there’s the &lt;a href=&quot;http://metron.apache.org/&quot;&gt;Apache Metron homepage&lt;/a&gt;, &lt;a href=&quot;https://hortonworks.com/apache/metron/&quot;&gt;Hortonwork’s overview of Metron&lt;/a&gt;, and their &lt;a href=&quot;http://docs.hortonworks.com/HDPDocuments/HCP1/HCP-1.1.0/index.html&quot;&gt;user documentation&lt;/a&gt;, but a good a place to start as any is the &lt;a href=&quot;https://cwiki.apache.org/confluence/display/METRON/Metron+Architecture&quot;&gt;architecture overview&lt;/a&gt; in the Apache Metron Wiki. In summary, the architectures probably fairly standard - Apache Kafka as an input point, fed by custom probes and Apache NiFi, with data then processed using Apache Storm supporting a level of data transformation and enrichment, real time alerting and built in scoring using machine learning models, with persistence of storage into HDFS and HBase, and then a range of dashboards and visualisation capabilities over the top. Apache Spark is in there somewhere as well.&lt;/p&gt; &lt;p&gt;That’s all (for this week) folks - see you on Monday.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hadoop Distros Week</title><link>https://ondataengineering.net/blog/2017/05/08/hadoop-distros-week/</link><description> &lt;p&gt;We’ve now finished looking at the major players in the Hadoop distributions space, so I’d like to take this week to look at Hadoop and Hadoop distributions in a bit more detail, and flesh out our &lt;a href=&quot;/tech-categories/hadoop-distributions/&quot;&gt;Hadoop Distributions&lt;/a&gt; page. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;There’s obviously a whole pile of technologies that sit in this space we’ve not looked at, so first steps will be to refresh our definition of an Hadoop distribution, and try and get a slightly more complete list in place, including the Cloud Hadoop-as-a-service offerings, the smaller more boutique vendors, the appliances and maybe a footnote on the distributions that are no longer with us.&lt;/p&gt; &lt;p&gt;I’d then like to look at the Hadoop ecosystem in a bit more detail, and try and map out which components sit where. I’d also like to try and do a comparison between the major offerings, focusing on the different technologies they include. And finally, assuming I’ve not run out of time, a quick look at ODPI (The Open Data Platform Initiative).&lt;/p&gt; &lt;p&gt;Right, time to crack on - see you shortly.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Why Choose Hadoop?</title><link>https://ondataengineering.net/blog/2017/05/12/why-choose-hadoop/</link><description> &lt;p&gt;So our Hadoop Distros week is going to be a bit longer than a week, but hey, we’re all flexible and adaptable right?&lt;/p&gt; &lt;p&gt;Today I’d like to spout some thoughts about why you might consider Hadoop, and I’d like to start but looking at it’s history. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Hadoop was originally designed for the single specific use case of doing aggregations over enormous volumes of data, and the combination of HDFS and MapReduce delivered this capability in a way that (at least at the very large scale) was not possible before and hasn’t really been superseded since. Pig and Hive were introduced as nicer ways to write MapReduce code, but didn’t fundamentally change anything. Hadoop, being open sourced, was then picked up a number of companies (both vendors and users) and taken in a couple of (largely complementary) directions.&lt;/p&gt; &lt;p&gt;First, HDFS was positioned as a great place to put all your data so that you could run a range of analytics over the top - the mythical Data Lake (although we’ll definitely talk about the challenges of building a Data Lake with Hadoop at some point). This required new technologies to bring data in (Flume and Sqoop for example), new technologies to exploit this data (Spark and Mahout for example), and a way to make all these technologies play nicely together (YARN). However there’s only so far that data in a filesystem will get you in terms of analytics - if you’re looking to do anything outside of batch appends and scanning workloads you’re going to struggle.&lt;/p&gt; &lt;p&gt;And so other technologies were introduced that used HDFS as an underlying storage technology to enable new data storage capabilities - HBase as a NoSQL database and Solr for search indexing for example. And what that created was not a single place to put all your data, but a range of complementary technologies that support different use cases, but that can share underlying infrastructure. All of which is good.&lt;/p&gt; &lt;p&gt;But there were always going to be challenges trying to move Hadoop from where it started to a more general purpose capability - you’re always going to be pushing against it’s original architectural and design decisions. HDFS is not a general purpose cluster filesystem - it was designed for a specific use case (for example it can only do file appends rather than random updates and has a hard limit on the number of files it can hold based on the memory capacity of the Name Node for), which can cause limitations when trying to use it for more general purpose analytics or to underpin other technologies (Kudu has chosen not run over HDFS for example). And YARN was a relatively late addition to Hadoop, meaning many Hadoop technologies don’t support it (including Flume, Solr and HBase, although Hortonworks is trying to address this through Slider). Which means we’re now in a position whereby you potentially have multiple technologies competing rather than co-existing on our Hadoop cluster, which feels like it’s starting to dilute some of the potential value. If you’re interested in which technologies do or don’t run over HDFS / YARN, I’ve tried to summarise it in diagrammatic form &lt;a href=&quot;/tech-categories/hadoop-distributions/ecosystem/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;So what are your options around Hadoop? Firstly, as an ecosystem it contains a good set of technologies. HDFS plus Hive/Spark/etc. is a great platform for doing batch scanning analytical workloads. HBase and Solr are great technologies that stand up well to their competition, and Hive and Impala are starting to provide some serious competition for the established MPP database vendors. Deploying any one of these technologies to fulfil a role in your wider analytical ecosystem will do you well, but if you’re going to use a commercial service you’ll need to find a cost effective way of doing this - you don’t want to be buying the entire ecosystem if you’re not going to use it all. This is where Cloudera are going - starting to offer tailor packages that include sub-sets of the components focusing on specific use cases, and most of the Cloud or Hadoop as a Service offerings allow you to only pay for what you use.&lt;/p&gt; &lt;p&gt;Or you can deploy Hadoop as a common analytical platform - a single set of infrastructure and a single purchase to give you a single platform that can deliver you a range of capabilities and fulfil a range of roles in a cost effective way. Note that this generally only works if you’re deploying on site rather than using the Cloud however. This is where Hortonworks and MapR are focusing - Hortonworks is investing in technologies such as Slider to allow everything to integrate with YARN, and MapR have built their entire offering around their own shared multi-tenancy storage capability (MapR-FS) that is designed and built for exactly this use case (and fulfils it better than HDFS).&lt;/p&gt; &lt;p&gt;In summary, Hadoop like any technology has it’s strengths and weaknesses. It’s not the be all and end all, it’s certainly not going to solve all your problems and magically make you a data driven organisation, it’s not going to dramatically decrease your costs, and deploying it is going to be as much hard work than deploying any other technology. However I’m hoping that the material I’ve added to this site over the last few months will help you start to understand what Hadoop is and how it might mean one or more of your use cases, whether that’s helping you start an evaluation of how one of it’s technologies stacks up against it’s competition, or to understand the ecosystem and how a specific distribution or offering can meet your range of use cases.&lt;/p&gt; &lt;p&gt;So that’s some thoughts on why Hadoop - on Monday we’ll summarise the options for how you can get it…&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hadoop Technology Options</title><link>https://ondataengineering.net/blog/2017/05/16/hadoop-technology-options/</link><description> &lt;p&gt;In the last post we looking at why you might want to use Hadoop - today I want to dig into the options for deploying or using Hadoop capabilities. Consider this a companion piece to the list of these options that’s now available on our &lt;a href=&quot;/tech-categories/hadoop-distributions/&quot;&gt;Hadoop Distributions&lt;/a&gt; page &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Broadly speaking, if you’re looking at deploying Hadoop the options you have fall into three categories.&lt;/p&gt; &lt;p&gt;Firstly, you can use an Hadoop managed service. This automates the provisioning and management of Hadoop, allowing you to easily create, scale and destroy clusters. You’ll obviously need to either have or get your data into the cloud, however this can be a great option for dipping your toe in the waters and exploring what Hadoop can do in a very cost effective way. However, if you want a persistent cluster, or if you’re doing anything at a significant scale, then this can get expensive very quickly, so it’s worth doing your sums first. Hadoop as a service does open up another use case however - if you already have you data in the cloud then you can use an Hadoop managed service to analyse that data in place using transient processing clusters - just spin up a cluster for a specific workload and then terminate it, meaning that you’re only paying whilst your workload is running. Note however that cloud object stores are significantly slower than HDFS running on local storage in a cloud based cluster.&lt;/p&gt; &lt;p&gt;Secondly, you can deploy and manage Hadoop yourself, either on your own infrastructure or on cloud infrastructure. The choice of your own vs cloud infrastructure is subject to all the usual considerations around TCO, manageability and having your data into the cloud, however both Cloudera and Hortonworks include tools (Cloudera Directory and Cloudbreak respectively) for managing cloud based deployments (including the provisioning of cloud infrastructure and the ability to scale up and down) that are also probably your prime options if you’re running an internal cloud such as OpenStack. Whatever infrastructure you use, you’re responsible for your Hadoop installation, which means you’ll need to make a decision about which Hadoop distribution to use, and whether you use a free open source version or purchase commercial support. You’ll also need to make sure you have access to all the specialist skills required to deploy, secure and manage your cluster - although the various management tools have improved significantly over the last few years, there are still significant decisions and configuration work to be done.&lt;/p&gt; &lt;p&gt;Lastly, you can use an Hadoop appliance - a prepackaged bundle of dedicated infrastructure and Hadoop software. There’s generally price premium for these, however they’re generally well architected for performance and can massively accelerate an onsite deployment, although they may or may not be easier to manage that a custom Hadoop deployment.&lt;/p&gt; &lt;p&gt;Selecting an Hadoop technology is never going to be a quick or easy process - the range of different options, the different ways it can be deployed, the range of different capabilities, the different cost models and level of support, the skills required and the management and maintenance costs make it a complex decision. However I’m hoping that the material on this site will give you a starting point for understanding the different options to help inform this decision making process. The &lt;a href=&quot;/tech-categories/hadoop-distributions/&quot;&gt;Hadoop Distributions&lt;/a&gt; page that lists the various options should be your first call, and I’ve started a comparison of the technologies bundled in the major distributions that we’ve looked at to date on an &lt;a href=&quot;/tech-categories/hadoop-distributions/distribution-comparison/&quot;&gt;Hadoop Distributions Comparison&lt;/a&gt; page. Neither of these are complete by a long stretch, however let me know of any obvious gaps, or even better send through a pull request with any new information.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 17/05/2017</title><link>https://ondataengineering.net/blog/2017/05/17/the-mid-week-news/</link><description> &lt;p&gt;Time for a short diversion from wrapping up our look at Hadoop to catch up on the news… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;New technology releases:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Version 1.2 of &lt;a href=&quot;/technologies/apache-nifi/&quot;&gt;Apache NiFi&lt;/a&gt; is out. See the &lt;a href=&quot;https://cwiki.apache.org/confluence/display/NIFI/Release+Notes#ReleaseNotes-Version1.2.0&quot;&gt;summary of new changes&lt;/a&gt; for details.&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-ignite&quot;&gt;Apache Ignite&lt;/a&gt; and it’s commercial edition GridGain Professional have seen a big 2.0 releases with a complete redesign of the off-heap memory architecture which should allow the extension of in-memory data structures to SSD disks. See the &lt;a href=&quot;https://blogs.apache.org/ignite/entry/apache-ignite-2-0-redesigned&quot;&gt;Ignite announcement&lt;/a&gt; and the &lt;a href=&quot;https://www.gridgain.com/resources/blog/gridgain-professional-edition-20-released-today&quot;&gt;GridGain Professional announcement&lt;/a&gt; for more details.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Interesting blog posts:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://www.ebaytechblog.com/2017/05/12/enhancing-the-user-experience-of-the-hadoop-ecosystem/&quot;&gt;eBay on their use of Hue, Zeppelin and Knox&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://dzone.com/articles/how-to-become-a-data-engineer&quot;&gt;Some thoughts on being a Data Engineer&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/blog/watermarks-tables-event-time-dataflow-model/&quot;&gt;A post from Confluent&lt;/a&gt; (who are therefore slightly biased) on why &lt;a href=&quot;/technologies/apache-kafka/kafka-streams/&quot;&gt;Kafka Streams&lt;/a&gt; Tables provide a better solution that windowing and watermarks (with specific reference to the Beam API) for calculating rolling aggregates of revisions or updates&lt;/li&gt; &lt;li&gt;Hortonworks have been on a bit of a roll with multi-part blog posts: &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/part-1-hortonworks-building-successful-streaming-analytics-platforms/&quot;&gt;Building streaming analytics platforms&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/part-5-of-data-lake-3-0-yarn-and-containerization-supporting-docker-and-beyond/&quot;&gt;Thoughts on Data Lake 3.0&lt;/a&gt;, including docker support in YARN&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/apache-hive-druid-part-1-3/&quot;&gt;OLAP analytics with Druid and Hive&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/enterprise-security-governance-part-1/&quot;&gt;Update on governance and security in HDP 2.6&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2017/05/how-to-backup-and-disaster-recovery-for-apache-solr-part-i/&quot;&gt;How to backup and recover Solr&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://data-artisans.com/blog/official-docker-images-apache-flink&quot;&gt;An official docker image for Flink&lt;/a&gt; is now available from Data Artisans&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/blog/how-get-started-spark-streaming-and-mapr-streams-using-kafka-api/&quot;&gt;Using Spark Streaming and MapR-Streams&lt;/a&gt; from the MapR blog&lt;/li&gt; &lt;li&gt;And a couple of interesting posts from Adrian Colyer’s The Morning Paper &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://blog.acolyer.org/2017/05/04/cherrypick-adaptively-unearthing-the-best-cloud-configurations-for-big-data-analytics/&quot;&gt;CherryPick - a system for determining the best cloud configurations for big data analytics&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blog.acolyer.org/2017/05/15/trajectory-recovery-from-ash-user-privacy-is-not-preserved-in-aggregated-mobility-data/&quot;&gt;Reconstructing individual user data from aggregated mobility data&lt;/a&gt; - well worth a quick look&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hadoop Wrap-Up</title><link>https://ondataengineering.net/blog/2017/05/19/hadoop-wrap-up/</link><description> &lt;p&gt;And so I think our current look into Hadoop is probably drawing to a close. That’s not because we’re run out of technologies to look at and thing to talk about, but because it’s time to move on. We’ll double back to this in the future - I’d like to dig into the major cloud technologies, and into the commercial world of Data Engineering in the future. I’m also hoping we’ll get contributions to help flesh out this information now we have a starting point.&lt;/p&gt; &lt;p&gt;But, let’s summarise where we’ve got to with Hadoop, and pass some comment on ODPi which we looked at yesterday. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Hadoop, it’s ecosystem, and the myriad of options for using it are a complex whirling maelstrom of sound and fury. Understanding it is like trying to catch a paper bag in a strong wind - no matter how close you might get, something is always changing, meaning you’re never going to get there. Fun hyperbole aside, my hope is that the content I’ve added to this site over the last few months becomes the starting point for a set of signposts to get you started - be that the &lt;a href=&quot;/tech-categories/hadoop-distributions/&quot;&gt;Hadoop Distributions&lt;/a&gt; page which provides a summary of the options for deploying or using Hadoop capabilities, the &lt;a href=&quot;/tech-categories/hadoop-distributions/ecosystem/&quot;&gt;Hadoop ecosystem&lt;/a&gt; page which tries to summarise the technologies that run over HDFS and YARN, the &lt;a href=&quot;/tech-categories/hadoop-distributions/distribution-comparison/&quot;&gt;Hadoop distributions comparison&lt;/a&gt; page which tries to summarise the different technologies bundled with different Hadoop distributions, or the myriad of technology summaries that might give you a starting point for understanding the different options and capabilities in the wider Hadoop ecosystem. A lot of this content isn’t finished or polished, so at this time this is very much a starting point, and some pages have big banners to clearly show they’re draft and not complete.&lt;/p&gt; &lt;p&gt;There’s been a lot written and said about ODPi, and for a long time I wasn’t sure I was ever going to understand what it was or what it was trying to do. Time however tends to make stuff clearer, and whereas originally ODP talked a lot about a common core for Hadoop, it seems to have settled down on defining some specifications for Hadoop compatibility to ensure that a single piece of software will run on any Hadoop distribution. That’s a noble goal, however I think there chances of achieving it are slim to none - they’re lacking platform vendor support (no Cloudera, MapR, Microsoft, Oracle), the specifications seem extreemly lightweight (meaning compliance to the spec is unlikely to mean compatibility with all certified platforms), there’s very little software vendor buy in (less than 10 software vendors at the time of writing have certified compliance) and it just doesn’t have the breadth (covering only HDFS, YARN, MapReduce and Hive at the moment). It feels like an extreemly expensive and challenging problem to solve, which which there appears to be little demand at the moment.&lt;/p&gt; &lt;p&gt;And just before we move on, I’m doing some minor tidying up as part of the last two weeks thinking, which I’ll now try to summarise:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Each distributions technology page now includes a link to the list of certified software that runs on it&lt;/li&gt; &lt;li&gt;I’ve update the Hadoop ecosystem page to remove Flume running over YARN (because it doesn’t)&lt;/li&gt; &lt;li&gt;Where technologies run over HDFS or YARN, I’ve make sure their technology summaries include this&lt;/li&gt; &lt;li&gt;I’ve moved some of the information on the technology vendor pages into the distribution pages, for example which cloud providers they run on&lt;/li&gt; &lt;li&gt;For technologies that are compatible with HDFS I’ve added a new technology relationship to show this&lt;/li&gt; &lt;li&gt;I’ve added links to the Hadoop ecosystem page to the YARN and HDFS pages&lt;/li&gt; &lt;li&gt;I’ve tweaked some of the descriptions on the Hadoop distributions page, for example to detail where there are free versions of a distribution&lt;/li&gt; &lt;li&gt;I’ve updated the Slider technology summary to reference Hoya and Koya&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So what’s next. I think it’s time to broaden out a little, and to start looking at some of the different types of technologies that might be of interest to us, and for each one look at the technology options available to us. So we’ll try that.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Chapter 2</title><link>https://ondataengineering.net/blog/2017/05/22/chapter-2/</link><description> &lt;p&gt;Given that I’m going a little short-sighted staring at Hadoop, I think it’s time to widen our gaze a little and to start chapter 2 of this site. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;In the recently added &lt;a href=&quot;/tech-categories/hadoop-distributions/distribution-comparison/&quot;&gt;Hadoop distributions comparison&lt;/a&gt; page, I introduced some categories by which I grouped and compared the technologies in the Cloudera, Hortonworks and MapR technology stacks, however at no point did I try and accurately define was I mean by each of these categories.&lt;/p&gt; &lt;p&gt;So that’s what I’m planning to do now - work through these categories (and some that aren’t on there) to create a technology category page for each that summarises the range of technologies available in that space. I’ll continue doing technology summaries for popular technologies (or anything that catches my eye), with a focus on open source and cloud technologies (with a smattering of commercial ones as and where there’s enough public information to do so).&lt;/p&gt; &lt;p&gt;We’ll kick off tomorrow with our first category, once I’ve rolled the dice and picked one!&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hadoop Compatible Filesystems</title><link>https://ondataengineering.net/blog/2017/05/26/hadoop-compatible-filesystems/</link><description> &lt;p&gt;So how did we do with our look at &lt;a href=&quot;/tech-categories/hadoop-compatible-filesystems/&quot;&gt;Hadoop Compatible Filesystems&lt;/a&gt; this week? Spoiler - not as well as I’d hoped!&lt;/p&gt; &lt;p&gt;In hindsight, picking this as the first technology category to do was a daft idea. Not only have I been trying to work out what I want to achieve with these technology category pages, and the level and information I want to capture, but my journey into Hadoop compatible storage has involved a myriad of rabbit holes from object stores to enterprise scale out storage to in-memory to every technology and its dog having an Hadoop compatible API. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;So I’m going to do my best to summarise, but this journey will probably continue next week - there’s some further detail I want to dig into, I’m not entirely happy with the technology category page, I’ve got a big pile of information that probably means I’m probably doing object stores and scale out storage as the next technology categories to close the circle.&lt;/p&gt; &lt;p&gt;Firstly, there are two use cases for an Hadoop Compatible Filesystem. The first is as a drop in replacement for HDFS - you run your filesystem and compute on the same nodes and utilise data locality to do local filesystem reads wherever possible. The second is as a way of reading and writing data from a remote data store using the standard Hadoop Filesystem API - great if you want to access and write data back to external storage, but not a replacement for your local HDFS filesystem that you’ll still be using for temporary and intermediate data.&lt;/p&gt; &lt;p&gt;If you want to swap out HDFS in your Hadoop cluster for something else, there are some technologies you can look at that are HDFS but better. The obvious one being &lt;a href=&quot;/technologies/mapr-file-system/&quot;&gt;MapR-FS&lt;/a&gt;, but the &lt;a href=&quot;/technologies/quantcast-file-system/&quot;&gt;Quantcast File System&lt;/a&gt; and Hops-HDFS are both interesting examples of open source projects that have taken HDFS and tried to improve it.&lt;/p&gt; &lt;p&gt;If you want to read and write data from Hadoop to an external system via the Hadoop Filesystem API, then the world’s your oyster, as pretty much every file or object based storage technology now allows you to do this, and generally in a (reasonably) high performance parallel way. This is the area I want to dig into in more detail next week however - what are you options if you want to store large volumes of data for analytics outside of an Hadoop cluster.&lt;/p&gt; &lt;p&gt;And as it feels like I’ve only scratched the surface of this area, I’m going to reserve the right to come and revise this information in the future. I wasn’t planning to dig so far into file and object storage at this time, but here we are so let’s see where it leads us.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Week That Was - 02/06/2017</title><link>https://ondataengineering.net/blog/2017/06/02/the-week-that-was/</link><description> &lt;p&gt;Just a very quick update this week.&lt;/p&gt; &lt;p&gt;It’s been a much shorter week that usual due to me being elsewhere at the beginning of the week and only starting our look into object stores on Wednesday. Because of that we’ll continue into next week, and I’m therefore not going to summarise any of the technologies we’ve looked at this week today. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;I’m starting to settle on a new rhythm for these technology category pages. I think I’m looking for broad coverage rather than depth - so although I would love to work through all the major technologies in each category I think my priority has to be getting good coverage of the various categories I want to cover so that there’s a framework that hopefully other people can help me build out. However, I don’t want to provide technology category pages that provide no value and don’t contain any useful or valuable information, but I don’t want to stop or reduce the number of technology summaries I’m doing - and that’s the balance I’m trying to strike. I’m not quite there yet, but I’m going to try and do technology summaries Monday to Thursday and a technology category page and some thoughts on a Friday, with the technology category page focusing on providing valuable links and very brief lists of potential technologies that fit in the category that require further analysis - a valuable starting point for understanding the potential technology options.&lt;/p&gt; &lt;p&gt;I’ve no idea if it’ll work, or if it’s achievable, but let’s find out. Next week we’ll finish of our last technology summaries for object stores, catch-up on the news, and maybe have a look at Cloudera Altus or Azure Data Lake Store if we have time.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 07/06/2017</title><link>https://ondataengineering.net/blog/2017/06/07/the-mid-week-news/</link><description> &lt;p&gt;We interupt the current broadcast for another (semi regular) catchup up on the news… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;New technology releases (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-apex/&quot;&gt;Apache Apex and DataTorrent RTS&lt;/a&gt; have both seen new releases&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-nifi/minifi&quot;&gt;MiNiFi&lt;/a&gt; has seen 0.2 releases of it’s Java and C++ versions&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-flink&quot;&gt;Apache Flink&lt;/a&gt; has seen a 1.3 release&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-hadoop&quot;&gt;Hadoop&lt;/a&gt; has seen it’s latest (alpha3) release of 3.0 - details &lt;a href=&quot;http://hadoop.apache.org/docs/r3.0.0-alpha3/index.html&quot;&gt;here&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cloudera have released Altus (&lt;a href=&quot;http://vision.cloudera.com/simplifying-big-data-in-the-cloud/&quot;&gt;blog post&lt;/a&gt;; &lt;a href=&quot;http://blog.cloudera.com/blog/2017/05/data-engineering-with-cloudera-altus/&quot;&gt;tech details&lt;/a&gt;; &lt;a href=&quot;http://community.cloudera.com/t5/Community-News-Release/Announce-Cloudera-Altus-is-now-available/m-p/55007#M175&quot;&gt;announcement&lt;/a&gt;, a service for running data engineering jobs (Spark, Hive and MapReduce) in the cloud on on-demand clusters. One for us to dig into further in the not too distant future I think.&lt;/li&gt; &lt;li&gt;Confluent have &lt;a href=&quot;https://www.infoq.com/news/2017/05/Confluent-Cloud-Kafka-AWS&quot;&gt;announced a cloud based offering&lt;/a&gt; of their &lt;a href=&quot;/technologies/apache-kafka&quot;&gt;Apache Kafka&lt;/a&gt; based solution, although it’s only in early access at the moment&lt;/li&gt; &lt;li&gt;Cockroach DB, a distributed SQL database that could be regarded as an open source version of Google Spanner &lt;a href=&quot;https://www.infoq.com/news/2017/06/Cockroach-DB-Production-Release&quot;&gt;has hit 1.0&lt;/a&gt;&lt;/li&gt; &lt;li&gt;AirBnB’s &lt;a href=&quot;https://github.com/airbnb/superset&quot;&gt;Superset&lt;/a&gt; has been donated to the Apache Foundation. This is well worth a look - it looks like an extreemly capable data exploration platform&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://pravega.io/&quot;&gt;Pravega&lt;/a&gt; is a new open source streaming storage system from Dell/EMC - see &lt;a href=&quot;https://siliconangle.com/blog/2017/04/17/dell-emc-takes-on-streaming-storage-with-open-source-solution-pravega-ffsf17/&quot;&gt;here&lt;/a&gt; for an introduction&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Technology updates:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A write up of the new capabilities in &lt;a href=&quot;/technologies/apache-nifi&quot;&gt;NiFi&lt;/a&gt; 1.2 for &lt;a href=&quot;https://blogs.apache.org/nifi/entry/record-oriented-data-with-nifi&quot;&gt;processing large volumes of records more efficiently&lt;/a&gt; and &lt;a href=&quot;https://blogs.apache.org/nifi/entry/real-time-sql-on-event&quot;&gt;running SQL on event streams&lt;/a&gt;&lt;/li&gt; &lt;li&gt;If you’re interested in &lt;a href=&quot;/technologies/apache-ignite&quot;&gt;Apache Ignite&lt;/a&gt;, there’s a two part getting starting set of blog posts from GridGain &lt;a href=&quot;https://www.gridgain.com/resources/blog/getting-started-apacher-ignitetm-part-i-0&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://www.gridgain.com/resources/blog/getting-started-apacher-ignitetm-part-2&quot;&gt;here&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-solr/&quot;&gt;Solr&lt;/a&gt; is getting a &lt;a href=&quot;https://sematext.com/blog/2017/05/10/solr-v2-api/&quot;&gt;new API&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2017/05/hdfs-maintenance-state/&quot;&gt;Options for doing system maintenance&lt;/a&gt; on &lt;a href=&quot;/technologies/apache-hadoop/hdfs/&quot;&gt;HDFS&lt;/a&gt; from Cloudera&lt;/li&gt; &lt;li&gt;Hortonworks are working on &lt;a href=&quot;/technologies/apache-spark/spark-sql&quot;&gt;Spark SQL&lt;/a&gt; &lt;a href=&quot;https://hortonworks.com/blog/row-column-level-control-apache-spark/&quot;&gt;integration with Apache Ranger&lt;/a&gt;, giving row/column level access control&lt;/li&gt; &lt;li&gt;Cloudera are trumpeting their work on &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Spark&lt;/a&gt; and some of the &lt;a href=&quot;http://vision.cloudera.com/new-capabilities-for-apache-spark-users/&quot;&gt;new features they’ve enabled&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2017/05/how-to-avoid-cloud-vendor-lock-in-to-minimize-cost-and-risk/&quot;&gt;Cloudera’s pitch&lt;/a&gt; for why you should use &lt;a href=&quot;/technologies/cloudera-director&quot;&gt;Cloudera Director&lt;/a&gt; to give you cloud independence&lt;/li&gt; &lt;li&gt;A &lt;a href=&quot;https://data-artisans.com/blog/apache-flink-1-3-0-evolution-stream-processing&quot;&gt;summary of the history&lt;/a&gt; of &lt;a href=&quot;/technologies/apache-flink&quot;&gt;Flink&lt;/a&gt; from dataArtisans&lt;/li&gt; &lt;li&gt;There’s a new &lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.1/bk_cloud-data-access/content/about.html&quot;&gt;cloud data access guide&lt;/a&gt; for &lt;a href=&quot;/technologies/hortonworks-data-platform&quot;&gt;Hortonwork’s Data Platform&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Interesting blog posts:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Automating testing of data pipelines and then doing continuous integration is definitely a topic I want to talk more about (but I say that of everything). In the meantime Databricks have an article on &lt;a href=&quot;https://databricks.com/blog/2017/06/02/integrating-apache-spark-cucumber-behavioral-driven-development.html&quot;&gt;using Cucumber with Spark&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Again from Databricks, and this feels topical for us - &lt;a href=&quot;https://databricks.com/blog/2017/05/31/top-5-reasons-for-choosing-s3-over-hdfs.html&quot;&gt;5 reasons for choosing S3 over HDFS&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Some thoughts on Open Source, licences and whether some commercial open source products are really open source from Bloor - &lt;a href=&quot;http://www.bloorresearch.com/analysis/the-open-source-dilemma/&quot;&gt;here&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A post from Cloudera on &lt;a href=&quot;http://blog.cloudera.com/blog/2017/05/bi-temporal-data-modeling-with-envelope/&quot;&gt;Envelope&lt;/a&gt;, a pre-developed Spark application for doing bi-temporal change management&lt;/li&gt; &lt;li&gt;Another one from The Morning Paper - &lt;a href=&quot;https://blog.acolyer.org/2017/05/30/mosaic-processing-a-trillion-edge-graph-on-a-single-machine/&quot;&gt;processing a trillion edge graph on a single machine&lt;/a&gt;&lt;/li&gt; &lt;li&gt;The latest update from Bloor on &lt;a href=&quot;http://www.bloorresearch.com/blog/im-blog/graph-update-4-performance-scalability-and-neo4j/&quot;&gt;graph technologies&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.infoq.com/news/2017/05/metrics-monitoring-robinhood&quot;&gt;A case study&lt;/a&gt; on the use of OpenTSDB, Grafana, Kafka and Riemann for metrics collection and monitoring at Robinhood Engineering&lt;/li&gt; &lt;li&gt;Confluent’s view on why &lt;a href=&quot;https://www.confluent.io/blog/the-future-of-etl-isnt-what-it-used-to-be/&quot;&gt;streaming is the new ETL&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Object Stores Wrap-Up</title><link>https://ondataengineering.net/blog/2017/06/09/object-stores-wrapup/</link><description> &lt;p&gt;I’ve not quite settled into our pivot to looking at technologies by category, but I think we’re getting somewhere. Again, I feel like I’ve picked some really nasty areas to start with, but I’m hoping what we’ve ended up with is going to provide some value.&lt;/p&gt; &lt;p&gt;So let’s summarise where we’ve got to with our list of &lt;a href=&quot;/tech-categories/object-stores/&quot;&gt;Object Store&lt;/a&gt; technologies, and ponder those we took a deeper dive into over the last week. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Firstly, I’m defining an object store technology as being object store like from top to bottom (whatever that means), but in summary that means I’ve excluded more general purpose storage technologies that provide an object store like API, but at their heart are a different type of technology. That’s horribly woolly, but let’s see how it holds up over time.&lt;/p&gt; &lt;p&gt;If there is a decent list of object store technologies out there on the web I’ve not been able to find it, however I’m therefore hoping that what I’ve managed to pull together will actually provide some value. Firstly a big shout out to Philippe Nicolas for his &lt;a href=&quot;http://www.theregister.co.uk/2016/07/15/the_history_boys_cas_and_object_storage_map/&quot;&gt;Object Storage Map&lt;/a&gt;, and Enrico Signoretti for his &lt;a href=&quot;http://www.juku.it&quot;&gt;juku.it&lt;/a&gt; blog and published research which were both extraordinary useful starting points. Oh, and to Gartner and IDC, for providing their view on the “Enterprise” market.&lt;/p&gt; &lt;p&gt;I’ve broken down the object store technologies into a bunch of categories. “Enterprise” services and products are those that the big analysts talk about, and there’s evaluations of most of these from analysts via the links I’ve included in the technology category page. However I’ve also tried to call out some of the technologies I’ve come access from smaller vendors that don’t or choose not to play in this space, but that still may have great technologies that may fit your use case. There’s also a surprising bunch of open source technologies - I’m sure I’m missing some, but I’ve tried to call these out as well.&lt;/p&gt; &lt;p&gt;And in terms of the specific technologies I’ve looked at, I’ve been surprised by the differences between the major cloud vendors. &lt;a href=&quot;/technologies/amazon-s3/&quot;&gt;Amazon S3&lt;/a&gt; is obviously the major player, but &lt;a href=&quot;/technologies/microsoft-azure-blob-storage/&quot;&gt;Azure Blob Storage&lt;/a&gt; seems to be doing a lot to differentiate itself, providing strong consistency and support for multiple object types (such as block, page and append which support different read/write models and different use cases). &lt;a href=&quot;/technologies/google-cloud-storage/&quot;&gt;Google Cloud Storage&lt;/a&gt; seems to provide less differentiation, but obviously gives you heavy integration into the Google Cloud ecosystem.&lt;/p&gt; &lt;p&gt;The only other technology we had time to look at was &lt;a href=&quot;/technologies/scality-ring&quot;&gt;Scality Ring&lt;/a&gt;, the current favoured technology by Gartner, making it (I assume) the pre-eminent on-site technology option. There’s no a lot to say about it, because as usual with commercial products the website it suitably vague - at some point we should look to integrate real world feedback into this site to actually get an understanding of how useful some of these technologies are.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Plan For This Week - 12/06/2017</title><link>https://ondataengineering.net/blog/2017/06/12/the-plan-for-this-week/</link><description> &lt;p&gt;It’s going to be a bit a a sweep up week this week - there are a number of (mostly new) technologies that have come up over the last few weeks I’d like to have a quick look at before we move onto our next technology catalogue.&lt;/p&gt; &lt;p&gt;So, in no particular order, the plan this week is to look at Azure Data Lake Store (scalable HDFS in the cloud - intriguing), Druid (now bundled with HDP in tech preview), Cloudera Altus (their new Data Engineering job execution service), Apache Superset (the new open source dashboard and reporting tool from AirBnB) and Pravega (the Kafka alternative from Dell EMC).&lt;/p&gt; &lt;p&gt;See you on Friday.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 14/06/2017</title><link>https://ondataengineering.net/blog/2017/06/14/the-mid-week-news/</link><description> &lt;p&gt;Right, let’s try and do this a bit more regularly (although it’s a bit late today!), especially as it seems to have been a busy news week… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/hortonworks-data-flow&quot;&gt;Hortonworks Data Flow&lt;/a&gt; has seen a 3.0 release, with the biggest changes being the introduction of two new products - Streaming Analytics Manager and Schema Registry - and a technical preview of SAM Stream Insights which bundles &lt;a href=&quot;/technologies/druid&quot;&gt;Druid&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-superset&quot;&gt;Apache Superset&lt;/a&gt;. We’ll talk more about this on Friday!&lt;/li&gt; &lt;li&gt;As part of it’s 2.6 release, &lt;a href=&quot;/technologies/hortonworks-data-platform&quot;&gt;Hortonworks Data Platform&lt;/a&gt; has deprecated a bunch of technologies that will be removed in HDP 3.0, including &lt;a href=&quot;/technologies/apache-falcon&quot;&gt;Falcon&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-flume&quot;&gt;Flume&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-mahout&quot;&gt;Mahout&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-slider&quot;&gt;Slider&lt;/a&gt; and &lt;a href=&quot;/technologies/hue&quot;&gt;Hue&lt;/a&gt;, and is moving &lt;a href=&quot;/technologies/apache-accumulo&quot;&gt;Accumulo&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-kafka&quot;&gt;Kafka&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-storm&quot;&gt;Storm&lt;/a&gt; out of HDP into other Hortonworks products. I’ll try and capture my thoughts on Friday.&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-falcon&quot;&gt;Apache Falcon&lt;/a&gt; now appears to be inactive, probably related to it’s deprecation from HDP&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-slider&quot;&gt;Apache Slider&lt;/a&gt; now also appears to be inactive, with a plan to fold support for long running services into YARN&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-nifi&quot;&gt;Apache NiFi&lt;/a&gt; continues it’s breakneck release schedule with a 1.3 release&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-solr/&quot;&gt;Apache Solr&lt;/a&gt; has seen a bump to 6.6&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/alluxio&quot;&gt;Alluxio&lt;/a&gt; has seen a 1.5 release, although details seem to be thin on the ground at the moment&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/hortonworks-data-cloud-for-aws/&quot;&gt;Hortonworks Data Cloud for AWS&lt;/a&gt; has skipped 1.15 and gone straight to 1.16&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudbreak&quot;&gt;Cloudbreak&lt;/a&gt;, Hortonworks’ Hadoop in the cloud orchestration tool, has jumped to 1.14&lt;/li&gt; &lt;li&gt;ZepplinHub (the &lt;a href=&quot;/technologies/apache-zeppelin&quot;&gt;Apache Zeppelin&lt;/a&gt; managed service) has changed it’s name to Zepl&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-livy&quot;&gt;Livy&lt;/a&gt; has been &lt;a href=&quot;http://incubator.apache.org/projects/livy.html&quot;&gt;donated to the Apache Foundation&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hortonworks and IBM &lt;a href=&quot;https://hortonworks.com/blog/data-met-science-anything-became-possible/&quot;&gt;have announced&lt;/a&gt; a partnership agreement, whereby IBM will distribute &lt;a href=&quot;/technologies/hortonworks-data-platform&quot;&gt;HDP&lt;/a&gt; as its official Hadoop product, and Hortonworks will resell IBM’s Data Science Experience (DSX) and BigSQL. Hortonworks now also &lt;a href=&quot;https://hortonworks.com/blog/hdp-ibm-spectrum-scale-brings-enterprise-class-storage-place-analytics/&quot;&gt;certify HDP to run on IBM Spectrum Scale&lt;/a&gt;. Good summary from ZDNet &lt;a href=&quot;http://www.zdnet.com/article/ibm-and-hortonworks-go-steady-with-oem-deal/&quot;&gt;here&lt;/a&gt;. Come back on Friday for some of my random thoughts.&lt;/li&gt; &lt;li&gt;Hortonworks have announced a new &lt;a href=&quot;https://hortonworks.com/blog/what-is-a-flexible-support-subscription-about/&quot;&gt;flex support subscription&lt;/a&gt; for &lt;a href=&quot;/technologies/hortonworks-data-platform&quot;&gt;HDP&lt;/a&gt; that covers the usage of HDP on-premise, on IaaS, when deployed using Cloudbreak, or when used as HDCloud on AWS.&lt;/li&gt; &lt;li&gt;Cloudera have a summary of how to &lt;a href=&quot;http://blog.cloudera.com/blog/2017/06/apache-solr-memory-tuning-for-production/&quot;&gt;tune the memory usage&lt;/a&gt; of &lt;a href=&quot;/technologies/apache-solr&quot;&gt;Apache Solr&lt;/a&gt;&lt;/li&gt; &lt;li&gt;On the subject of &lt;a href=&quot;/technologies/apache-solr&quot;&gt;Solr&lt;/a&gt;, see &lt;a href=&quot;http://www.codewrecks.com/blog/index.php/2017/06/06/running-solrmeter-without-a-ui/&quot;&gt;this article&lt;/a&gt; for information on Solrmeter (a tool for testing Solr performance under heavy load)&lt;/li&gt; &lt;li&gt;An update from Yahoo &lt;a href=&quot;http://yahoohadoop.tumblr.com/post/161742444781/hbase-goes-fast-and-lean-with-the-accordion&quot;&gt;on Accordian&lt;/a&gt;, an update to &lt;a href=&quot;/technologies/apache-hbase&quot;&gt;Apache HBase&lt;/a&gt; to improve performance by doing more work in memory.&lt;/li&gt; &lt;li&gt;Databricks have &lt;a href=&quot;https://databricks.com/blog/2017/06/07/databricks-serverless-next-generation-resource-management-for-apache-spark.html&quot;&gt;announced Databricks Serverless&lt;/a&gt;, a fully managed Databricks (built on &lt;a href=&quot;/technologies/apache-spark&quot;&gt;Apache Spark&lt;/a&gt;) service that manages it’s own (virtual) infrastructure&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Week That Was - 16/06/2017</title><link>https://ondataengineering.net/blog/2017/06/16/the-week-that-was/</link><description> &lt;p&gt;Let’s reminder ourselves of the plan for this week - &lt;a href=&quot;/technologies/microsoft-azure-data-lake-store&quot;&gt;Azure Data Lake Store&lt;/a&gt;, &lt;a href=&quot;/technologies/druid&quot;&gt;Druid&lt;/a&gt;, Cloudera Altus, &lt;a href=&quot;/technologies/apache-superset&quot;&gt;Apache Superset&lt;/a&gt; and Pravega. How did I do? Three out of five.&lt;/p&gt; &lt;p&gt;I’ll come back to Cloudera Altus first thing next week, and Pravega by looking at streaming data stores in the near future, but this week ended up being dominated by serendipity and Hortonworks’ &lt;a href=&quot;/technologies/hortonworks-data-flow&quot;&gt;HDF&lt;/a&gt; 3.0 release (and their two new technologies - &lt;a href=&quot;/technologies/schema-registry&quot;&gt;Schema Registry&lt;/a&gt; and &lt;a href=&quot;/technologies/streaming-analytics-manager&quot;&gt;Streaming Analytics Manager&lt;/a&gt;), and by a desire to have some content on some new and breaking stuff.&lt;/p&gt; &lt;p&gt;Oh, and this weeks news post ended up being a bit of a bumper post, with some stuff we need to dig into. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Let’s take the week in chronological order.&lt;/p&gt; &lt;p&gt;We started off by looking at &lt;a href=&quot;/technologies/microsoft-azure-data-lake-store&quot;&gt;Azure Data Lake Store&lt;/a&gt;, a wrap up from our lookup at &lt;a href=&quot;/tech-categories/hadoop-compatible-filesystems&quot;&gt;Hadoop Compatible Filesystems&lt;/a&gt;. If you’re working in the cloud, your options to date have been one of the big object stores (such as Amazon S3), but that’s going to give you limitations on the size of an individual file, and a performance hit based on the inability to read files in a massive parallel way. Azure Data Lake Store (appears to be) a pretty unique offering in the space, giving an HDFS compatible filesystem that addresses these limitations, but at huge scale in the cloud.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/technologies/druid&quot;&gt;Druid&lt;/a&gt; came next, by dint of it being included in tech preview in &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;HDP&lt;/a&gt; 2.6. It seems a far more popular open source project than I’d considered, with some serious deployments, and delivers on a use case that traditionally would have required significant hardware and software investment to do at scale. What’s interesting is Hortonworks interest with it having been significant committers for a while, their plan to integrate it with Hive, and the fact they’re now bundling it with both HDP and HDF (although it’s in tech preview in both). That means they see a significant future for it, and to be honest I think that’s a pretty good bet.&lt;/p&gt; &lt;p&gt;Which leads us on to &lt;a href=&quot;/technologies/apache-superset&quot;&gt;Apache Superset&lt;/a&gt;, recently donated to the Apache Foundation, and the tool of choice for use with Druid. Again, it hits a use case that’s traditionally been the preserve of commercial products, and again Hortonworks are going in, having been committers for a while and now bundling it (indirectly) with HDF (see later). Along with Druid, it’s got to be well worth a look if you have sort of requirement for delivering OLAP / cube type capabilities to end users.&lt;/p&gt; &lt;p&gt;As part of the mid week news, we took a quick stop off to look at the bucket load of technologies that have been deprecated as part of HDP 2.6. &lt;a href=&quot;/technologies/apache-accumulo&quot;&gt;Apache Accumulo&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-kafka&quot;&gt;Apache Kafka&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-storm&quot;&gt;Apache Storm&lt;/a&gt; are going as they’re being moved into other Hortonworks products - Kafka and Storm into HDF (one presumes), but I’m less sure about Accumulo. Perhaps it’s destined to become an add-on like Hawq and Solr - time will tell I guess. Then there’s &lt;a href=&quot;/technologies/apache-flume&quot;&gt;Apache Flume&lt;/a&gt; (advice is to consider &lt;a href=&quot;/technologies/hortonworks-data-flow&quot;&gt;HDF&lt;/a&gt; instead), &lt;a href=&quot;/technologies/apache-mahout&quot;&gt;Apache Mahout&lt;/a&gt; (advice is to consider &lt;a href=&quot;/technologies/apache-spark/mllib&quot;&gt;Spark MLLib&lt;/a&gt; instead), &lt;a href=&quot;/technologies/apache-slider&quot;&gt;Apache Slider&lt;/a&gt; (being folded into YARN) and &lt;a href=&quot;/technologies/hue&quot;&gt;Hue&lt;/a&gt; (advice is to consider &lt;a href=&quot;/technologies/apache-ambari/ambari-views&quot;&gt;Ambari Views&lt;/a&gt; instead). The more interesting one is &lt;a href=&quot;/technologies/apache-falcon&quot;&gt;Apache Falcon&lt;/a&gt;, which looks dead in the water with no commits for a number of months now, and with no clear replacement. The suggestion is that something’s coming, but it’s not clear what. If we get NiFi with intermediate files stored in HDFS and the ability to run arbitrary Spark / MapReduce jobs as processors then that would be lovely!&lt;/p&gt; &lt;p&gt;And so on to &lt;a href=&quot;/technologies/hortonworks-data-flow&quot;&gt;HDF&lt;/a&gt; 3.0. This looks like a big release for Hortonworks - they’re going all in with streaming data (IoT / analysis of data in movement / however else they’re selling it), and I can’t shake the feeling that they’re stealing a march on a bunch of competitors by selling an integrated set of technologies that fit this space, combined with the required security and governance bits. And yes, both MapR and Cloudera bundle Kafka and some sort of streaming tech (Storm for MapR, Spark Streaming for Cloudera), but it’s not a focus in the same way it is for Hortonworks. And then the announcements this week around the new technologies they’re adding to HDF just re-enforces the fact that they’re talking streaming analytics far more seriously that they’re competitors, and I think they’re going to reap the rewards.&lt;/p&gt; &lt;p&gt;So first up for HDF 3.0 was &lt;a href=&quot;/technologies/schema-registry&quot;&gt;Schema Registry&lt;/a&gt;. It fills a gap that Confluent had filled for Kafka with a commercial solution, but across all the technologies in the HDF stack. Having multiple jobs reading and writing the same data means that they all need to know the schema and understand / be updated as when the schema changes. You can solve this by bundling the schema in with the data (e.g. with &lt;a href=&quot;/technologies/apache-avro&quot;&gt;Avro&lt;/a&gt;), but the overhead of this when you’re dealing with individual records is huge. So you stick a schema version number of the record, and you go and get the actually schema from the registry. Interoperability between jobs and the ability to evolve schemas in streaming solutions - done. But it seems like Hortonworks have bigger plans for this product, with the idea that it could support other items such as business rules and machine learning models that need to be re-used in multiple places.&lt;/p&gt; &lt;p&gt;And then they’re their &lt;a href=&quot;/technologies/streaming-analytics-manager&quot;&gt;Streaming Analytics Manager&lt;/a&gt;) - a collection of bits that are designed to make building and operating streaming analytics solutions easier and simpler and to reduce the bar to entry. The operations stuff, and the bundling of &lt;a href=&quot;/technologies/druid&quot;&gt;Druid&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-superset&quot;&gt;Apache Superset&lt;/a&gt; to analyse the results of streaming analytics feels like a no brainer. The graphical GUI to building streaming apps over your streaming engine of choice (starting with Storm obviously) is going to be an interesting one to track to see what sort of uptake there is. I’m sure they’ll be views that say this is going to be a lowest common denominator solution, that it will lack flexibility and control, and although that’s true I’m not sure it matters. For 80% of streaming analytical use cases it will probably do the job, and allow you to deliver solutions in a fraction of the time it would take otherwise. For everything else - you can still drop down into the underlying tech just as you did before.&lt;/p&gt; &lt;p&gt;That’s pretty much it for this week (finally!), but one last thing I’d like to call out. Hortonworks’ other big announcement this week was their &lt;a href=&quot;https://hortonworks.com/blog/data-met-science-anything-became-possible/&quot;&gt;new partnership&lt;/a&gt;. IBM are going to drop their Hadoop distribution and resell HDP, and HDP will resell IBM’s BigQuery and Data Science Experience (DSX). This feels like the natural evolution of the market consolidation that’s been going on for a while, and is exactly the model that Pivotal took with their Hadoop distribution. Hortonworks get access to IBM’s Hadoop customer base, and access to a data science as a service solution, and IBM get to continue and enhance their Hadoop offering whilst reducing their costs. And it puts a new light on the Hortonworks’ announcements around support for PowerPC and IBM Spectrum Scale.&lt;/p&gt; &lt;p&gt;Right - I’m done with this week. See you back on Monday when I’ll summarise the plan for next week (that I then won’t keep to), but we’re definitely going to start the week by looking at Cloudera Altus.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Plan For This Week - 19/06/2017</title><link>https://ondataengineering.net/blog/2017/06/19/the-plan-for-this-week/</link><description> &lt;p&gt;As promised we’re going to start of this week by looking at Cloudera Altus, and then, despite the fact I’ve said we’ll be looking at different technology categories going forward, I want to send a few days summarising the capabilities available from AWS, Azure and Google Cloud (because I have a bunch of notes I want to write up). I then plan to close the week by actually doing what I said we’d be doing and adding a technology category, this time looking at Hadoop capabilities offered as a service (e.g. Altus), with a little rework on the &lt;a href=&quot;/tech-categories/hadoop-distributions/&quot;&gt;Hadoop Distributions&lt;/a&gt; page at the same time.&lt;/p&gt; &lt;p&gt;Have a great week, and if you’re in the UK, try to stay cool.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 21/06/2017</title><link>https://ondataengineering.net/blog/2017/06/21/the-mid-week-news/</link><description> &lt;p&gt;Time for some news, and only a week since we last did it! &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-pig&quot;&gt;Apache Pig&lt;/a&gt; has seen a 0.17 release, with support for using Spark as an execution engine introduced to complement the existing support for Tez and MapReduce&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-kudu&quot;&gt;Apache Kudu&lt;/a&gt; has seen a 1.4 release&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudbreak&quot;&gt;Cloudbreak&lt;/a&gt; has seen a 1.16 release, adding support for Hortonworks Flex Support Subscription&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-impala&quot;&gt;Apache Impala&lt;/a&gt; has seen a 2.9 release&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/hortonworks-data-cloud-for-aws&quot;&gt;Hortonworks Data Cloud for AWS&lt;/a&gt; has a tech preview of it’s 2.0 release&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://www.zdnet.com/article/kafka-the-story-so-far/&quot;&gt;Interesting post&lt;/a&gt; on the history of &lt;a href=&quot;/technologies/apache-kafka&quot;&gt;Kafka&lt;/a&gt; from ZDNet&lt;/li&gt; &lt;li&gt;Yahoo have &lt;a href=&quot;https://yahooeng.tumblr.com/post/161855616651/open-sourcing-bullet-yahoos-forward-looking&quot;&gt;open sourced Bullet&lt;/a&gt;, a “forward looking query engine” for streaming data&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.datanami.com/2017/06/14/hortonworks-shifts-focus-streaming-analytics/&quot;&gt;A view&lt;/a&gt; from Datanami on the latest &lt;a href=&quot;/technologies/hortonworks-data-flow&quot;&gt;HDF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/part-4-sams-stream-builder-building-complex-stream-analytics-apps-without-code/&quot;&gt;Part 4&lt;/a&gt; of Hortonworks intro to &lt;a href=&quot;/technologies/hortonworks-data-flow&quot;&gt;HDF&lt;/a&gt; 3.0 looking at stream builder (the GUI for building streaming flows)&lt;/li&gt; &lt;li&gt;A little old, but a still useful &lt;a href=&quot;http://www.dbms2.com/2016/08/21/introduction-to-data-artisans-and-flink/&quot;&gt;view&lt;/a&gt; on Data Artisans and &lt;a href=&quot;/technologies/apache-flink&quot;&gt;Flink&lt;/a&gt; from Curt Monash&lt;/li&gt; &lt;li&gt;A more recent post from Curt, with &lt;a href=&quot;http://www.dbms2.com/2017/06/14/cloudera-altus/&quot;&gt;his views&lt;/a&gt; on &lt;a href=&quot;/technologies/cloudera-altus&quot;&gt;Cloudera Altus&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blog.knoldus.com/2017/06/13/spark-streaming-vs-kafka-stream/&quot;&gt;A view&lt;/a&gt; on &lt;a href=&quot;/technologies/apache-spark/spark-streaming&quot;&gt;Spark Streaming&lt;/a&gt; vs &lt;a href=&quot;/technologies/apache-kafka/kafka-streams&quot;&gt;Kafka Streams&lt;/a&gt;&lt;/li&gt; &lt;li&gt;More views on the IBM-Hortonworks &lt;a href=&quot;/technologies/hortonworks-data-platform&quot;&gt;HDP&lt;/a&gt; deal from &lt;a href=&quot;https://www.theregister.co.uk/2017/06/15/ibm_adopts_hortonworks_for_hadoop_distribution/&quot;&gt;The Register&lt;/a&gt; and &lt;a href=&quot;http://blogs.gartner.com/merv-adrian/2017/06/21/ibm-ends-hadoop-distribution-hortonworks-expands-hybrid-open-source/&quot;&gt;Gartner&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Hortonworks are &lt;a href=&quot;https://hortonworks.com/blog/hortonworks-when-youre-hot-youre-really-hot/&quot;&gt;blowing their trumpet&lt;/a&gt; (no pun intended) on their &lt;a href=&quot;/technologies/hortonworks-data-flow&quot;&gt;HDF&lt;/a&gt; 3.0 release and their IBM &lt;a href=&quot;/technologies/hortonworks-data-platform&quot;&gt;HDP&lt;/a&gt; deal&lt;/li&gt; &lt;li&gt;Cloudera have published &lt;a href=&quot;http://blog.cloudera.com/blog/2017/06/solr-memory-tuning-for-production-part-2/&quot;&gt;part 2&lt;/a&gt; of their Solr memory tuning guide&lt;/li&gt; &lt;li&gt;And finally, Databricks’ &lt;a href=&quot;https://databricks.com/blog/2017/05/31/top-5-reasons-for-choosing-s3-over-hdfs.html&quot;&gt;view&lt;/a&gt; on &lt;a href=&quot;/tech-categories/object-stores&quot;&gt;object storage&lt;/a&gt; (specifically S3) vs &lt;a href=&quot;/tech-categories/hadoop-compatible-filesystems&quot;&gt;HDFS&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hadoop in the Cloud</title><link>https://ondataengineering.net/blog/2017/06/23/hadoop-in-the-cloud/</link><description> &lt;p&gt;So this week has been a bit of a scattergun - one technology (&lt;a href=&quot;/technologies/cloudera-altus&quot;&gt;Cloudera Altus&lt;/a&gt;), three vendors (&lt;a href=&quot;/tech-vendors/amazon-web-services&quot;&gt;Amazon Web Services&lt;/a&gt;, &lt;a href=&quot;/tech-vendors/microsoft-azure&quot;&gt;Microsoft Azure&lt;/a&gt; and &lt;a href=&quot;/tech-vendors/google-cloud-platform&quot;&gt;Google Cloud Platform&lt;/a&gt; - and yes, I know they’re not technically vendors) and one (refreshed) technology category (&lt;a href=&quot;/tech-categories/hadoop-distributions/&quot;&gt;Hadoop Distributions&lt;/a&gt;) - with the overriding theme of thinking about Hadoop in the cloud. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Every Hadoop distribution that you can install on your own physical hardware can also be installed on virtual infrastructure in a cloud in exactly the same way, however this is a spectacular way to avoid the benefits a cloud brings, and unless you’re going to be using your cluster all the time, probably an expensive way to do it given you’ll pay for your infrastructure by the hour.&lt;/p&gt; &lt;p&gt;So what every Hadoop vendor has done (with technologies like &lt;a href=&quot;/technologies/cloudera-director&quot;&gt;Cloudera Director&lt;/a&gt; and &lt;a href=&quot;/technologies/cloudbreak&quot;&gt;Cloudbreak&lt;/a&gt;), and what all the cloud providers now offer, is a way to automatically provision the infrastructure you need as part of deploying your Hadoop cluster, allowing you to spin up entire Hadoop clusters in minutes, and enabling the use of transient clusters which only exist for the period of time required to run a specific job or workload (although you’ll obviously need to persist your output into a persistent store). And we’ve now moved to Hadoop as a service, and we don’t care about the infrastructure anymore.&lt;/p&gt; &lt;p&gt;What Cloudera have done with &lt;a href=&quot;/technologies/cloudera-altus&quot;&gt;Cloudera Altus&lt;/a&gt;, and what Amazon did with Elastic Map Reduce a long time ago, is to elevate the stack one level higher to the Hadoop job - you can submit an Hadoop job (Hive, Spark, MapReduce) to Altus, and it will provision some infrastructure, create a cluster, run the job and then tear everything down. And we’ve now moved to Hadoop jobs as a service, and we don’t care about the clusters anymore.&lt;/p&gt; &lt;p&gt;If you’re working in the cloud this feels like the natural evolution of the cloud story, and to be blunt, even if you’re working on premises, having an on premises cloud that allows you to do this seems like an natural evolution (and in which case OpenStack Sahara might well be worth a look). And this feels like an area of differentiation between the various Hadoop cloud offerings at the moment - a lot of them allow you to programmatically run jobs via their API, but few allow the execution of jobs to automatically provision and tear down clusters, although you can obviously orchestrate this yourself.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Plan For This Week - 26/06/2017</title><link>https://ondataengineering.net/blog/2017/06/26/the-plan-for-this-week/</link><description> &lt;p&gt;So the plan for this week is to look at streaming data stores - Kafka, Pravega and maybe some of the cloud based services.&lt;/p&gt; &lt;p&gt;Message Ends.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 28/06/2017</title><link>https://ondataengineering.net/blog/2017/06/28/the-mid-week-news/</link><description> &lt;p&gt;News time, and it’s looking dangerously like this is becoming a weekly thing - it won’t last long. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-atlas&quot;&gt;Apache Atlas&lt;/a&gt; has graduated from the Apache incubator&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-calcite&quot;&gt;Apache Calcite&lt;/a&gt; has seen a 1.13 release&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/blog/building-real-time-streaming-etl-pipeline-20-minutes/&quot;&gt;More thoughts&lt;/a&gt; from Confluent on building ETL pipelines using &lt;a href=&quot;/technologies/apache-kafka&quot;&gt;Kafka&lt;/a&gt; and streaming technologies&lt;/li&gt; &lt;li&gt;An excellent &lt;a href=&quot;https://blog.bradfieldcs.com/you-are-not-google-84912cf44afb&quot;&gt;article&lt;/a&gt; from Ozan Onay at Bradfield School of Computer Science on why technologies built for the biggest companies in the world (Google, Amazon, LinkedIn) may not be right for you&lt;/li&gt; &lt;li&gt;From Cloudera - &lt;a href=&quot;http://blog.cloudera.com/blog/2017/06/offset-management-for-apache-kafka-with-apache-spark-streaming/&quot;&gt;how to manage your read position&lt;/a&gt; when reading from &lt;a href=&quot;/technologies/apache-kafka&quot;&gt;Kafka&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Two new updates from Cloudera on &lt;a href=&quot;/technologies/cloudera-altus&quot;&gt;Altus&lt;/a&gt; - one on &lt;a href=&quot;http://vision.cloudera.com/announcing-workload-analytics-for-cloudera-altus/&quot;&gt;workload analytics&lt;/a&gt; and the other on &lt;a href=&quot;http://blog.cloudera.com/blog/2017/06/announcing-support-for-spot-instances-in-cloudera-altus/&quot;&gt;support for AWS spot instances&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A post from Sematext on the &lt;a href=&quot;https://sematext.com/blog/2017/06/19/solr-vs-elasticsearch-differences/&quot;&gt;differences&lt;/a&gt; between &lt;a href=&quot;/technologies/apache-solr&quot;&gt;Solr&lt;/a&gt; and &lt;a href=&quot;/technologies/elasticsearch&quot;&gt;Elastic Search&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A &lt;a href=&quot;https://cloud.google.com/blog/big-data/2017/06/how-qubit-deduplicates-streaming-data-at-scale-with-google-cloud-platform&quot;&gt;reference case&lt;/a&gt; from Google on how Qubit use Google Cloud Bigtable to identify (and drop) duplicate records&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Streaming Data Stores</title><link>https://ondataengineering.net/blog/2017/06/30/streaming-data-stores/</link><description> &lt;p&gt;So this week we’ve been looking at &lt;a href=&quot;/tech-categories/streaming-data-stores&quot;&gt;streaming data stores&lt;/a&gt;, technologies for the buffering and long term storage of continuous data streams for consumption by multiple downstream consumers.&lt;/p&gt; &lt;p&gt;And as part of that look, I’ve updated our &lt;a href=&quot;/technologies/apache-kafka&quot;&gt;Apache Kafka&lt;/a&gt; pages, and we’ve taken a look at some new technologies - &lt;a href=&quot;/technologies/pravega&quot;&gt;Pravega&lt;/a&gt;, the new kid on the block, and &lt;a href=&quot;/technologies/confluent-open-source&quot;&gt;Confluent Open Source&lt;/a&gt; and &lt;a href=&quot;/technologies/confluent-enterprise&quot;&gt;Confluent Enterprise&lt;/a&gt;, Confluents offerings built around Kafka.&lt;/p&gt; &lt;p&gt;So let’s spout some thoughts on streaming data stores and the technologies we’ve looked at this week. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;I going to make a very bold statement - in a few years time, I have a sneaking suspicion that streaming data stores will be seen as the biggest shakeup of the technology space for getting data into analytical systems for thirty years. Part of this is driven by the rise of streaming analytics, where these technologies are pretty much a de-facto standard for connecting streaming data flows together, but I think they’re going to become the standard for connecting any data processing chains together (most of which I think will become more real-time continuous flows anyway). What I’m really interested about with these technologies is the way they address some of the issues in building batch pipelines - without significant engineering effort these are often extreemly tightly coupled together, causing significant issues if jobs fail or you want to change your pipeline by adding in new flows or re-generating data stores. Persistent data buffers or queues help solve a lot of these problems de-coupling jobs, and although this concept exists in a number of commercial data integration tools, it’s never been a mainstream concept until now.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/technologies/apache-kafka&quot;&gt;Apache Kafka&lt;/a&gt; is obviously the technology that’s driven most of this change, and is now seeing significant traction, with commercial support available from Confluent and inclusion in most Hadoop distributions. However the market for these technologies is extreemly immature, and Kafka itself has a number of potential limitations depending on your use case - the primary one being that it’s probably no as elastic as it could be.&lt;/p&gt; &lt;p&gt;Which is why it’s nice to see new technologies appearing in this space, showing there’s a healthy demand for these technologies, and providing competition and diversity. &lt;a href=&quot;/technologies/mapr-file-system/mapr-streams/&quot;&gt;MapR Streams&lt;/a&gt; is one, providing a Kafka compatible API over the MapR file system, and there are a range of cloud based services listed on the &lt;a href=&quot;/tech-categories/streaming-data-stores&quot;&gt;streaming data stores&lt;/a&gt; technology page, however the one we looked at this week was &lt;a href=&quot;/technologies/pravega&quot;&gt;Pravega&lt;/a&gt; - an open source product coming out of Dell EMC. They’re aiming to address what they see as the limitations of Kafka (summarised &lt;a href=&quot;https://www.slideshare.net/FlinkForward/flink-forward-sf-2017-srikanth-satya-tom-kaitchuck-pravega-storage-reimagined-for-streaming-world/50&quot;&gt;here&lt;/a&gt;), and I think they’re going to be an interesting technology to watch, but it’s still extreemly early days for them - they don’t have a production release yet, there is a significant capability gap to Kafka, and they’re going to need some significant commercial backing and vendor support if they’re going to be successful. What’s clear is that they’re off to a great start, and have already built a significant development community.&lt;/p&gt; &lt;p&gt;Confluent are the biggest backers of Kafka, and we looked at some of their offerings this week as well. They have what they call their Confluent Platform, which in two flavours - &lt;a href=&quot;/technologies/confluent-open-source&quot;&gt;Confluent Open Source&lt;/a&gt; and &lt;a href=&quot;/technologies/confluent-enterprise&quot;&gt;Confluent Enterprise&lt;/a&gt;. If you’re planning on using the open source version of Kafka, Confluent Open Source may well be worth a look, even if it’s just to adopt some of their open source components. Confluent Enterprise then gives you full support and extra management features, and what’s interesting here is that although the Hadoop vendors have started bundling Kafka, they don’t yet have an equivalent range of capabilities to Confluent.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Plan For This Week - 03/07/2017</title><link>https://ondataengineering.net/blog/2017/07/03/the-plan-for-this-week/</link><description> &lt;p&gt;So this week I’m planning to look at search technologies.&lt;/p&gt; &lt;p&gt;One of the things I want to dig into and explore is the role of search in analytics - search to support users finding stuff on your website, or to find stuff in your enterprise document management system is one thing, but what role does it play if you want to analyse and understand data.&lt;/p&gt; &lt;p&gt;Which means we’ll probably limit our technology list to those technologies that support analytics rather than just search, but let’s see how we go.&lt;/p&gt; &lt;p&gt;See you on Friday…&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 05/07/2017</title><link>https://ondataengineering.net/blog/2017/07/05/the-mid-week-news/</link><description> &lt;p&gt;Right - time for some updates on stuff that’s been happening over the last week. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-kafka&quot;&gt;Apache Kafka&lt;/a&gt; has seen a 0.11 release, with support for &lt;a href=&quot;https://www.confluent.io/blog/exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it/&quot;&gt;exactly once semantics&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Heron, the streaming technology that Twitter built to replace Storm has been donated to the Apache Foundation - see the &lt;a href=&quot;https://wiki.apache.org/incubator/HeronProposal&quot;&gt;proposal&lt;/a&gt; and &lt;a href=&quot;http://incubator.apache.org/projects/heron&quot;&gt;incubator page&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A couple of further bits on Heron from Adrian Colyer’s “the morning paper” - one on the &lt;a href=&quot;https://blog.acolyer.org/2017/06/29/twitter-heron-towards-extensible-streaming-engines/&quot;&gt;architecture&lt;/a&gt; and one on &lt;a href=&quot;https://blog.acolyer.org/2017/06/30/dhalion-self-regulating-stream-processing-in-heron/&quot;&gt;Dhalion&lt;/a&gt;, their tool for dynamically managing Heron flows&lt;/li&gt; &lt;li&gt;And &lt;a href=&quot;https://blog.acolyer.org/2017/07/03/spanner-becoming-a-sql-system/&quot;&gt;one more&lt;/a&gt; from “the morning paper” on Google Spanner&lt;/li&gt; &lt;li&gt;An article on &lt;a href=&quot;https://aws.amazon.com/blogs/big-data/seven-tips-for-using-s3distcp-on-amazon-emr-to-move-data-efficiently-between-hdfs-and-amazon-s3/&quot;&gt;S3DistCp&lt;/a&gt; for &lt;a href=&quot;/technologies/amazon-emr/&quot;&gt;Amazon EMR&lt;/a&gt; - their tool for moving data between &lt;a href=&quot;/technologies/amazon-s3/&quot;&gt;S3&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-hadoop/hdfs/&quot;&gt;HDFS&lt;/a&gt; that also supports a range of file manipulations.&lt;/li&gt; &lt;li&gt;And on the subject of &lt;a href=&quot;/technologies/amazon-s3/&quot;&gt;S3&lt;/a&gt;, Scality have a blog post on &lt;a href=&quot;http://www.scality.com/blog/ever-wanted-filesystem-s3-store/&quot;&gt;s3fs&lt;/a&gt;, an open source tool allowing you to mount S3 buckets as a filesystem&lt;/li&gt; &lt;li&gt;At at the risk of this becoming &lt;a href=&quot;/technologies/amazon-s3/&quot;&gt;S3&lt;/a&gt; week, a post from Hortonworks on &lt;a href=&quot;https://hortonworks.com/blog/s3guard-amazon-s3-consistency/&quot;&gt;using S3Guard&lt;/a&gt;, an extension to the Hadoop S3A FileSystem that uses Amazon DynamoDB to make access to S3 strongly consistent&lt;/li&gt; &lt;li&gt;Thoughts from Curt Monash on &lt;a href=&quot;http://www.dbms2.com/2017/06/30/analytics-on-the-edge/&quot;&gt;analytics on the edge&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.gridgain.com/resources/blog/getting-started-apacher-ignitetm-part-4&quot;&gt;Part 4&lt;/a&gt; of the getting started with &lt;a href=&quot;/technologies/apache-ignite&quot;&gt;Apache Ignite&lt;/a&gt; series is out&lt;/li&gt; &lt;li&gt;And because it’s important that open source software pro-actively notifies users around security vulnerabilities, let’s publicise some of them here, starting with &lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/www-announce/201706.mbox/%3CC6768663-E7C6-48CC-A480-400218F23486%40apache.org%3E&quot;&gt;an information disclosure&lt;/a&gt; announcement for &lt;a href=&quot;/technologies/apache-ignite&quot;&gt;Apache Ignite&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Thoughts on Analytical Search</title><link>https://ondataengineering.net/blog/2017/07/07/thoughts-on-analytical-search/</link><description> &lt;p&gt;Yes, the date on this post says Friday 7th, and it’s being published well after that, but let’s pretend it’s Friday and summarise some thoughts on analytical search. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Search is search right - you index some documents and call an API with some keywords and get back a list of documents that contain those keywords. You might also get back some counts of documents matches, get some matched text snippets back as well, and get support for stemming (so that searches for fox returns documents with foxes in as well), but fundamentally it’s about turning some search terms into a list of matching documents. Right?&lt;/p&gt; &lt;p&gt;What both &lt;a href=&quot;/technologies/elasticsearch&quot;&gt;Elasticsearch&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-solr&quot;&gt;Apache Solr&lt;/a&gt; have done is turn search into a much more general purpose analytical capability. In the same way that relational databases use indexes and full table scans as a starting point for the wide range of transformations, joins and aggregations supported by SQL, these search technologies now use their indexed lookups as the starting point for a similar range of capabilities.&lt;/p&gt; &lt;p&gt;So what can these technologies do? Aggregations were the first step, allowing results to be rolled up - great for understanding how many type X errors my system’s generated by hour over the last month. And this then lead to much more interesting aggregations, including those to support anomaly detection (are the number of errors I’m seeing now different from the usual count for this time of day) and graph analysis (what are the relationships between the terms I’m interested and the other terms in the same document).&lt;/p&gt; &lt;p&gt;And now both technologies are supporting configurable results analysis pipelines turning these into much more powerful analytical capabilities - in Solr’s case these support a range of other capabilities including MapReduce like transformations over results and the ability to execute SQL expressions over Solr indexes.&lt;/p&gt; &lt;p&gt;Which I think makes for a very interesting time to be looking at analysis of unstructured and semi-structured data, as we’re now starting to see similar analytical capabilities to those that have existed in the structured world for a long time, and they’re going to open up some extreemly interesting use cases.&lt;/p&gt; &lt;p&gt;And if &lt;a href=&quot;/technologies/elasticsearch&quot;&gt;Elasticsearch&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-solr&quot;&gt;Apache Solr&lt;/a&gt; are blazing this trail, which one should you choose for your use case? There are plenty of websites out there that will give you their thoughts or a feature by feature breakdown (although be wary of how old some of them are, as both technologies, and specifically Solr, have seen significant changes in the last couple of years), and although both technologies have slightly different focuses (Elasticsearch has a strong pedigree in doing log analysis for example) the strong likelihood is that unless you’re pushing the boundries they’ll both support your use case.&lt;/p&gt; &lt;p&gt;So my recommendation to you would be threefold:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Do some proof of concepts with both technologies to understand how well they fit your use case and how well they work for you&lt;/li&gt; &lt;li&gt;Look at the commercial model. Although Elastic is open source, there are considerable benefits to taking on one of Elastic’s commercial offerings, both in terms of extra functionality and having their support. With Solr all the functionality is in the open source product, but there’s still value in having commercial support, and there’s a much wider range of ways of getting this and a wider range of vendors that will provide this.&lt;/li&gt; &lt;li&gt;Make sure you have access to people that understand these technologies and what they can do. They are powerful and complex pieces of technology that are relatively new, and getting access to people that understand them will ensure you get the most out of them.&lt;/li&gt; &lt;/ol&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Plan For This Week - 10/07/2017</title><link>https://ondataengineering.net/blog/2017/07/10/the-plan-for-this-week/</link><description> &lt;p&gt;So we’re a little behind the curve this week, but the plan this week is to look at wide column NoSQL stores, and specifically how these can support analytical use cases.&lt;/p&gt; &lt;p&gt;Just as forewarning - it might be a slightly quieter week than usual for a number of reasons, but we’ll try and look at a couple of technologies and collect our thoughts for Friday…&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 12/07/2017</title><link>https://ondataengineering.net/blog/2017/07/12/the-mid-week-news/</link><description> &lt;p&gt;It’s a fairly light week this week, but let’s catch up on the news anyway. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-phoenix/&quot;&gt;Apache Phoenix&lt;/a&gt; has seen a 4.11 release&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/elasticsearch/&quot;&gt;Elasticsearch&lt;/a&gt;, &lt;a href=&quot;/technologies/elastic-x-pack/&quot;&gt;Elastic X-Pack&lt;/a&gt; and &lt;a href=&quot;/technologies/elasticsearch-hadoop/&quot;&gt;Elasticsearch Hadoop&lt;/a&gt; has all seen a 5.5 release&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The 3.0 alpha4 release of &lt;a href=&quot;/technologies/apache-hadoop&quot;&gt;Apache Hadoop&lt;/a&gt; is out - first beta release is next!&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blog.acolyer.org/2017/07/07/do-we-need-specialized-graph-databases-benchmarking-real-time-social-networking-applications/&quot;&gt;More&lt;/a&gt; from Adrian Colyer’s “the morning paper” - how relational databases can be faster than graph databases for some types of graph operations&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blog.acolyer.org/2017/07/04/azure-data-lake-store-a-hyperscale-distributed-file-service-for-big-data-analytics/&quot;&gt;And other one&lt;/a&gt; from Adrian - this time reviewing a paper on &lt;a href=&quot;/technologies/microsoft-azure-data-lake-store/&quot;&gt;Azure Data Lake Store&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;An &lt;a href=&quot;http://flink.apache.org/features/2017/07/04/flink-rescalable-state.html&quot;&gt;article&lt;/a&gt; on Rescalable State in Apache Flink&lt;/li&gt; &lt;li&gt;And I feel like we’ve committed to this now, but here’s &lt;a href=&quot;https://www.gridgain.com/resources/blog/getting-started-apacher-ignitetm-part-5&quot;&gt;part 5&lt;/a&gt; of GridGain’s introduction to &lt;a href=&quot;/technologies/apache-ignite/&quot;&gt;Apache Ignite&lt;/a&gt;. Should be over soon.&lt;/li&gt; &lt;li&gt;And given my comments on security vulnerabilities disclosures last week, there’s a job lot this week &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2017-5640&quot;&gt;CVE-2017-5640&lt;/a&gt; - Apache Impala (incubating) Information Disclosure&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2017-5652&quot;&gt;CVE-2017-5652&lt;/a&gt; - Apache Impala (incubating) Information Disclosure&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.cve.mitre.org/cgi-bin/cvename.cgi?name=2017-7660&quot;&gt;CVE-2017-7660&lt;/a&gt; - Security Vulnerability in secure inter-node communication in Apache Solr&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Thoughts on NoSQL Wide Column Stores</title><link>https://ondataengineering.net/blog/2017/07/14/thoughts-on-nosql-wide-column-stores/</link><description> &lt;p&gt;Right - it’s the end of another week and another (quick) review of a technology category is done - this time &lt;a href=&quot;/tech-categories/nosql-wide-column-stores/&quot;&gt;NoSQL Wide Column Stores&lt;/a&gt;. Let’s review what we’ve discovered. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;NoSQL Wide Column Stores are all based on the Bigtable paper from Google, and in terms of available technologies is not a huge pool, being dominated by &lt;a href=&quot;/technologies/apache-cassandra/&quot;&gt;Apache Cassandra&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hbase/&quot;&gt;Apache HBase&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-accumulo/&quot;&gt;Apache Accumulo&lt;/a&gt; in the open source space, and Azure Table Storage and Google Cloud Bigtable in the cloud space.&lt;/p&gt; &lt;p&gt;All the open source technologies are great technologies, firmly established, proven a huge scale, with significant backing, and with a range of commercial and managed service offerings available. ScyllaDB (&lt;a href=&quot;http://www.scylladb.com/&quot;&gt;http://www.scylladb.com/&lt;/a&gt;) is worth called out - a C++ re-write (effectively) of Apache Cassandra, giving 100% compatibility but performing a lower latencies with more consistency. Definitely one to look at if you use or are consider using Cassandra.&lt;/p&gt; &lt;p&gt;And then there are the multi-model databases - technologies that provide NoSQL wide column capabilities alongside other NoSQL capabilities (often document, graph and key-value) in the same technologies. A category we’ll come back to in the future, as well as having a proper look at the different types of NoSQL databases available.&lt;/p&gt; &lt;p&gt;In closing - why would you consider using a NoSQL Wide Column Store? The common use case is probably in the intersection of document (un-structured) and structured storage - within a column family, each record can reference a completely different set of columns and column data types, whilst retaining the ability to return structured data from queries and run (reasonably) efficient filtered scans or records by column. Add to this their massively scalability (up to thousands of nodes and petabytes of data) and high throughput low latency read/write/mutate operations, and these technologies have found themselves a useful niche.&lt;/p&gt; &lt;p&gt;In analytical terms, they’re often use to hold and serve the results of analytics (aggregations and metrics) for serving at low latencies to dashboards, but their use undoubtedly stretches way beyond that.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Plan For This Week - 17/07/2017</title><link>https://ondataengineering.net/blog/2017/07/17/the-plan-for-this-week/</link><description> &lt;p&gt;Right, it’s going a quiet week for me (but not the site) this week, as we have a bunch of technology summaries on Apache Mesos and some of it’s associated technologies from Jeff Moszuti.&lt;/p&gt; &lt;p&gt;Jeff also supplied the technology summary for Apache Cassandra last week, however I very rudely forgot to credit him in the wrap up post on Friday - apologies Jeff.&lt;/p&gt; &lt;p&gt;I’ll be back on Wednesday with an update on the news, and back on Friday with some thoughts on cluster management, but until then I’ll leave you in Jeff’s capable hands.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 19/07/2017</title><link>https://ondataengineering.net/blog/2017/07/19/the-mid-week-news/</link><description> &lt;p&gt;Right, time for the news again, and it feels like a busy release week… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It’s Cloudera release time, with lots of Cloudera software seeing new releases: &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt; is up to 5.12&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-manager/&quot;&gt;Cloudera Manager&lt;/a&gt; is also up to 5.12&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-navigator/&quot;&gt;Cloudera Navigator&lt;/a&gt; is up to 2.11&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-director/&quot;&gt;Cloudera Director&lt;/a&gt; is up to 2.5&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-data-science-workbench/&quot;&gt;Cloudera Data Science Workbench&lt;/a&gt; is up to 1.1&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt; has hit 2.2, and it seems like our technology summaries are a bit out of date, so we’ll revisit them soon&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-hawq/&quot;&gt;Apache HAWQ&lt;/a&gt; still seems to be going, with a 2.2 release&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/hue/&quot;&gt;Hue&lt;/a&gt; has hit 4.0, with a new UI&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;An &lt;a href=&quot;https://www.elastic.co/blog/a-practical-introduction-to-elasticsearch&quot;&gt;introduction&lt;/a&gt; to &lt;a href=&quot;/technologies/elasticsearch/&quot;&gt;Elasticsearch&lt;/a&gt; from Elastic&lt;/li&gt; &lt;li&gt;And also from Elastic on machine learning which is part of the &lt;a href=&quot;/technologies/elastic-x-pack/&quot;&gt;Elastic X-Pack&lt;/a&gt; - &lt;a href=&quot;https://www.elastic.co/blog/alerting-on-machine-learning-jobs-in-elasticsearch-v55&quot;&gt;Alerting on Machine Learning Jobs&lt;/a&gt; and &lt;a href=&quot;https://www.elastic.co/blog/using-elasticsearch-and-machine-learning-for-it-operations&quot;&gt;Using Machine Learning for IT Ops&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://yokota.blog/2017/07/12/hbase-application-archetypes-redux/&quot;&gt;Thoughts&lt;/a&gt; on how to store different types of data (entities, documents, graph, queue etc.) in &lt;a href=&quot;/technologies/apache-hbase/&quot;&gt;Hbase&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From ZDNet, their &lt;a href=&quot;http://www.zdnet.com/article/yahoos-bullet-looks-ahead-in-querying-streaming-data/&quot;&gt;introduction&lt;/a&gt; to Bullet, Yahoo’s recently open source tool for querying streams of data&lt;/li&gt; &lt;li&gt;It looks like Basho is no more &lt;a href=&quot;https://www.theregister.co.uk/2017/07/13/will_the_last_person_at_basho_get_the_lights_oh_too_late/&quot;&gt;according to The Register&lt;/a&gt;, however there’s still hope that Riak will live on&lt;/li&gt; &lt;li&gt;There’s now &lt;a href=&quot;https://www.swiftstack.com/blog/2017/07/11/swiftstack-client-new-easy-way-interact-swiftstack-storage/&quot;&gt;a web GUI&lt;/a&gt; “that an end user can use to view, upload, download, share, and manage their data in a SwiftStack cluster”&lt;/li&gt; &lt;li&gt;Oh my word, it never ends! &lt;a href=&quot;https://www.gridgain.com/resources/blog/getting-started-apacher-ignitetm-part-6&quot;&gt;Part 6&lt;/a&gt; of GridGain’s intro to &lt;a href=&quot;/technologies/apache-ignite/&quot;&gt;Apache Ignite&lt;/a&gt; is now up.&lt;/li&gt; &lt;li&gt;An &lt;a href=&quot;https://databricks.com/blog/2017/07/12/benchmarking-big-data-sql-platforms-in-the-cloud.html&quot;&gt;article from Databricks&lt;/a&gt; on how much faster Databrinks in the cloud is that vanilla &lt;a href=&quot;/technologies/apache-spark&quot;&gt;Spark&lt;/a&gt; on AWS&lt;/li&gt; &lt;li&gt;Via &lt;a href=&quot;https://www.theregister.co.uk/2017/07/13/scality_zenko/&quot;&gt;The Register&lt;/a&gt;, Scality have released &lt;a href=&quot;http://www.zenko.io/&quot;&gt;Zenko&lt;/a&gt;, an open source S3 gateway that can federate across multiple cloud and on premise object stores, with support to come for metadata attribute search and data management, replication and workflows. Sounds like we should look into &lt;a href=&quot;/tech-categories/object-stores/&quot;&gt;object storage&lt;/a&gt; gateways at some point.&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Thoughts on Compute Cluster Managers</title><link>https://ondataengineering.net/blog/2017/07/21/thoughts-on-compute-cluster-management/</link><description> &lt;p&gt;Let’s talk about compute clusters, and the management thereof, technologies that we’re snappily calling &lt;a href=&quot;/tech-categories/compute-cluster-managers/&quot;&gt;compute cluster managers&lt;/a&gt;… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;I’m assuming we’re all happy with database and application clusters - persistent processes running across a set of machines that co-ordinate with each other to serve requests and do work. The keyword here being persistent - you install the software on all the nodes, configure everything up, start it and it runs (hopefully forever).&lt;/p&gt; &lt;p&gt;Compute clusters are slightly different, in that work is not done by persistent processes, but instead by transitory jobs that start up, do some data processing work and then go away again. Some technologies implement their own clustering - Spark for example has a standalone mode with a simple cluster manager, and Hadoop 1.x MapReduce would manage it’s own distributed processing across an HDFS cluster. Of course you could run multiple technologies (or maybe even multiple jobs in the same technology) on a single cluster, however they’re going to compete for resources and if you’re being too ambitious, overwhelm your hardware.&lt;/p&gt; &lt;p&gt;What therefore emerged over the last 10 years are compute cluster management technologies. Rather than every technology having to implement their own clustering, they can talk to a cluster manager, ask for a bunch of processes to be started up, and relax safe in the knowledge that the cluster manager is going to take care of all the hard work of starting and managing these for them. And the key thing about these is that they can be used by multiple technologies - a single cluster can handle the execution of multiple simultaneous jobs in multiple technologies, with the cluster manager performing resource management to ensure that these multiple workloads execute in a harmonious and collaborative way.&lt;/p&gt; &lt;p&gt;The first of these was &lt;a href=&quot;/technologies/apache-hadoop/yarn/&quot;&gt;Apache YARN&lt;/a&gt;, the big addition in Hadoop 2.0. Once YARN support was added to the common Hadoop data processing frameworks, this allowed multiple jobs in different technologies to run on your Hadoop cluster in a controlled and managed way under the oversight of the YARN resource manager.&lt;/p&gt; &lt;p&gt;And then a few years later came &lt;a href=&quot;/technologies/apache-mesos/&quot;&gt;Apache Mesos&lt;/a&gt;, the technology (along with it’s ecosystem) that we’ve been looking at this week. Mesos was born out of UC Berkeley RAD Lab and has seen rapid growth, with a commercial backer now in place in Mesosphere, and adoption by some significant customers. It’s a much more general purpose capability than YARN, with support for long running containerised applications via &lt;a href=&quot;/technologies/mesosphere-marathon/&quot;&gt;Marathon&lt;/a&gt; also positioning it as an alternative to Kubernetes and Docker Swarm. It’s resource management model is also interestingly different to YARN, with Mesos making resource offers to applications which can then accept or reject them, as opposed to YARN which makes decisions about resources on behalf of applications, meaning applications can make decisions about data locality and resource usage, but removing the ability to manage this centrally.&lt;/p&gt; &lt;p&gt;YARN also has support for long running services coming, originally via &lt;a href=&quot;/technologies/apache-slider/&quot;&gt;Apache Slider&lt;/a&gt; but with this functionality now being added directly into YARN (and work on Slider stopped), although there’s no current work on making it able to run or manage generic docker images ala Mesos / Marathon (beyond an aborted attempt by Hortonworks a few years ago to support Kubernetes on YARN). But YARN does has some interesting new capabilities for running spawned processes inside containers, something that appears to require application level support on Mesos.&lt;/p&gt; &lt;p&gt;And then there’s &lt;a href=&quot;/technologies/apache-myriad/&quot;&gt;Apache Myriad&lt;/a&gt;, which allows you to run YARN jobs over Mesos that maybe allow you to have your cake and eat it if you have a Mesos cluster (although it’s probably not quite mature yet). And Apache REEF - a library that provides an abstraction layer over YARN and Mesos, allowing applications to support both (and future technologies) via a single API.&lt;/p&gt; &lt;p&gt;It’s an evolving area, and one we’ll track. And we’ll talk more about container management, and the use of containers for running data processing jobs and managing data platforms in the not too distant future as well.&lt;/p&gt; &lt;p&gt;Right - that’s us for the week. Thanks to Jeff again for the content this week, and we’ll see you after the weekend.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Plan For The Next Three Weeks - 24/07/2017</title><link>https://ondataengineering.net/blog/2017/07/24/the-plan-for-next-three-weeks/</link><description> &lt;p&gt;It’s time for me to take a short break, both to recharge and try and get ahead of the upcoming content for the site, but also to do some work on the site itself which is starting to accumulate a few to many bodges and hacks (and an exponentially increasing build time!)&lt;/p&gt; &lt;p&gt;It’ll be back in three weeks on Monday 14th August, and although there may be a few updates here and there between now and then, expect things to be generally quite.&lt;/p&gt; &lt;p&gt;Here’s hoping your having a nice summer wherever you are, and we’ll speak soon…&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Plan For This Week - 14/08/2017</title><link>https://ondataengineering.net/blog/2017/08/14/the-plan-for-this-week/</link><description> &lt;p&gt;And we’re back from our short break.&lt;/p&gt; &lt;p&gt;The plan for this week is to publish the list of Apache technologies that I think are of interest to us. I know we’re meant to be working through technology categories at the moment, but I’ve had this list lying around for a while and it’s time to get it out of my notes and onto the site.&lt;/p&gt; &lt;p&gt;So no update tomorrow, news on Wednesday, Apache technology list on Thursday and some thoughts on Friday - aka back to business as usual.&lt;/p&gt; &lt;p&gt;And I’m still working on tidying up some bits and pieces on the site - I’ll try and summarise these as and when they’re ready.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 16/08/2017</title><link>https://ondataengineering.net/blog/2017/08/16/the-mid-week-news/</link><description> &lt;p&gt;Bumper week this week given we’ve been off for a while - let’s crack on… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The &lt;a href=&quot;/technologies/apache-parquet&quot;&gt;Apache Parquet&lt;/a&gt; C++ library has hit 1.2&lt;/li&gt; &lt;li&gt;Confluent &lt;a href=&quot;/technologies/confluent-open-source/&quot;&gt;Open Source&lt;/a&gt; and &lt;a href=&quot;/technologies/confluent-enterprise&quot;&gt;Enterprise&lt;/a&gt; have hit 3.3&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-ignite&quot;&gt;Apache Ignite&lt;/a&gt; is up to 2.1&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-drill&quot;&gt;Apache Drill&lt;/a&gt; is up to 1.11&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-tez&quot;&gt;Apache Tez&lt;/a&gt; has hit 0.9&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-hive&quot;&gt;Apache Hive&lt;/a&gt; has seen 2.2 and 2.3 releases, with the 2.3 coming first. No idea what’s going on hear - if you can enlighten me please do!&lt;/li&gt; &lt;li&gt;There’s are also new links added to the &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt; 5.12, &lt;a href=&quot;/technologies/cloudera-director&quot;&gt;Cloudera Director&lt;/a&gt; 2.5, &lt;a href=&quot;/technologies/cloudera-data-science-workbench&quot;&gt;Cloudera Data Science Workbench&lt;/a&gt; 1.1, &lt;a href=&quot;/technologies/hue&quot;&gt;Hue&lt;/a&gt; 4, &lt;a href=&quot;/technologies/apache-kafka&quot;&gt;Kafka&lt;/a&gt; 0.11, &lt;a href=&quot;/technologies/apache-ignite&quot;&gt;Ignite&lt;/a&gt; 2.0 and &lt;a href=&quot;/technologies/apache-ambari&quot;&gt;Ambari&lt;/a&gt; 2.5 release entries from recent blog posts exploring the new functionality&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Apache Pulsar (&lt;a href=&quot;https://pulsar.incubator.apache.org/&quot;&gt;homepage&lt;/a&gt; has entered Apache incubation - looks like another potential Kafka alternative, this time from Yahoo. We’ll try and take a look at this next week.&lt;/li&gt; &lt;li&gt;It looks like &lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/Metastore+TLP+Proposal&quot;&gt;there are plans&lt;/a&gt; to split the &lt;a href=&quot;/technologies/apache-hive/hive-metastore/&quot;&gt;Hive Metastore&lt;/a&gt; off into it’s own top level Apache project&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://vision.cloudera.com/top-reasons-why-search-engines-and-big-data-go-hand-in-hand-part-1/&quot;&gt;Cloudera’s thoughts&lt;/a&gt; (part 1) on the role &lt;a href=&quot;/tech-categories/analytical-search/&quot;&gt;Analytical Search&lt;/a&gt; capabilities play in big data analytics&lt;/li&gt; &lt;li&gt;From DB-Engines, &lt;a href=&quot;https://db-engines.com/en/blog_post/71&quot;&gt;thoughts&lt;/a&gt; on time series databases&lt;/li&gt; &lt;li&gt;The Confluent blog has been busy, with a bunch of interesting posts &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/blog/simplest-useful-kafka-connect-data-pipeline-world-thereabouts-part-1/&quot;&gt;An introduction&lt;/a&gt; to creating a flows using &lt;a href=&quot;/technologies/apache-kafka/kafka-streams&quot;&gt;Kafka Streams&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/blog/apache-kafka-for-service-architectures/&quot;&gt;An excellent introduction&lt;/a&gt; to the architectural principles behind &lt;a href=&quot;/technologies/apache-kafka&quot;&gt;Kafka&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/blog/messaging-single-source-truth/&quot;&gt;Thoughts&lt;/a&gt; on the use of &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Kafka&lt;/a&gt; as the single source of truth (including history) of your data&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;From The Morning Paper, &lt;a href=&quot;https://blog.acolyer.org/2017/08/11/automatic-database-management-system-tuning-through-large-scale-machine-learning/&quot;&gt;an article&lt;/a&gt; on how machine learning can optimise database tuning, which probably speaks to the complexity of tuning databases as much as anything else&lt;/li&gt; &lt;li&gt;AWS Glue is &lt;a href=&quot;https://aws.amazon.com/blogs/aws/launch-aws-glue-now-generally-available/&quot;&gt;now generally available&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Another &lt;a href=&quot;https://blog.knoldus.com/2017/07/21/kafka-streams-more-than-just-a-dumb-storage/&quot;&gt;useful introduction&lt;/a&gt; to &lt;a href=&quot;/technologies/apache-kafka/kafka-streams/&quot;&gt;Kafka Streams&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Cloudera, a &lt;a href=&quot;http://blog.cloudera.com/blog/2017/07/quicker-insight-into-apache-solr-and-collection-health/&quot;&gt;post&lt;/a&gt; on monitoring Solr with &lt;a href=&quot;/technologies/cloudera-manager/&quot;&gt;Cloudera Manager&lt;/a&gt;&lt;/li&gt; &lt;li&gt;And another from Cloudera, &lt;a href=&quot;http://blog.cloudera.com/blog/2017/07/implyr-r-interface-for-apache-impala/&quot;&gt;querying&lt;/a&gt; &lt;a href=&quot;/technologies/apache-impala/&quot;&gt;Impala&lt;/a&gt; from R&lt;/li&gt; &lt;li&gt;And one more from Cloudera, detailing &lt;a href=&quot;http://blog.cloudera.com/blog/2017/08/introducing-s3guard-s3-consistency-for-apache-hadoop/&quot;&gt;S3Guard&lt;/a&gt;, providing consistency when running &lt;a href=&quot;/technologies/apache-hadoop/hdfs/&quot;&gt;HDFS&lt;/a&gt; over Amazon S3&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Apache Big Data Project List</title><link>https://ondataengineering.net/blog/2017/08/18/the-apache-big-data-project-list/</link><description> &lt;p&gt;It’s a day late, but the new and refreshed list of Apache technologies that I think are of interest to us is now live on the &lt;a href=&quot;/tech-vendors/apache/&quot;&gt;Apache Foundation&lt;/a&gt; vendor page. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;There’s 98 technologies in total, which talks to the breadth of projects that the Apache Foundation now supports, and the range of (just one small slice of) technology options in the space we’re looking at. And just for reference, we have technology summaries and technology pages for 53 (at the time of writing) of these technologies if you want more information on them.&lt;/p&gt; &lt;p&gt;I’ve split the technologies up into some very broad categories, but I’m looking to refine these as we continue our journey through the technology categories that I think are of interest to us. If you’re reading this in the future, the categories on the Apache technologies page should link to technology category pages that provide an insight into available technologies across the open source and commercial space.&lt;/p&gt; &lt;p&gt;I think that’s probably all for this week - see you after the weekend.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Plan For This Week - 21/08/2017</title><link>https://ondataengineering.net/blog/2017/08/21/the-plan-for-this-week/</link><description> &lt;p&gt;Something slightly different this week - I’ve got a bunch of technology summaries to put live, most of them from Jeff Moszuti (who contributed all the recent Mesos stuff), so we’re going to do a random week. Five technology summaries, with no relationship to each other and no overlying theme.&lt;/p&gt; &lt;p&gt;See you on Friday for the wrap-up…&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 23/08/2017</title><link>https://ondataengineering.net/blog/2017/08/23/the-mid-week-news/</link><description> &lt;p&gt;After last weeks monster update, it very slim pickings this week - two new technology releases and a couple of (semi) interesting blog posts… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-hbase/&quot;&gt;Apache HBase&lt;/a&gt; is prepping for it’s big 2.0 release, with a second alpha release of 2.0 announced this week. The HBase page includes a link to a presentation with more information on the 2.0 release&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-knox/&quot;&gt;Apache Knox&lt;/a&gt; has hit 0.13, with support for some new serivces and a bunch of improvements&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The &lt;a href=&quot;https://www.confluent.io/blog/leveraging-power-database-unbundled/&quot;&gt;latest&lt;/a&gt; post from Confluent in their series on how Kafka (and similar tools) can fundamentally re-shape the way you think about your architecture and the way you manage data. Well worth a read, and I’ve got a feeling we’ll be talking more about this in the future.&lt;/li&gt; &lt;li&gt;A post from Hortonworks on &lt;a href=&quot;https://hortonworks.com/blog/data-science-workbench-data-scientists-need-one/&quot;&gt;why you need a Data Science Workbench&lt;/a&gt;. There’s not a huge amount of content, and the phrasing echos Cloudera’s new product, so I’m wondering why they’ve pulished this…&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Week That Was - 25/08/2017</title><link>https://ondataengineering.net/blog/2017/08/25/the-week-that-was/</link><description> &lt;p&gt;So, a bit of a mixed bag this week, but what did we look at.&lt;/p&gt; &lt;p&gt;We started with three technologies summaries from Jeff Moszuti, looking at &lt;a href=&quot;/technologies/apache-kylin/&quot;&gt;Apache Kylin&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-beam/&quot;&gt;Apache Beam&lt;/a&gt; and &lt;a href=&quot;/technologies/rex-ray/&quot;&gt;Dell EMC REX-Ray&lt;/a&gt;. We then finished up the week by looking at &lt;a href=&quot;/technologies/zenko/&quot;&gt;Zenko&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-spark/structured-streaming/&quot;&gt;Spark Structured Streaming&lt;/a&gt;. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;We last looked at OLAP cube technologies with &lt;a href=&quot;/technologies/druid&quot;&gt;Druid&lt;/a&gt;, which like &lt;a href=&quot;/technologies/apache-kylin/&quot;&gt;Apache Kylin&lt;/a&gt; is tightly integrated with the Hadoop ecosystem. Unlike Druid however, Kylin doesn’t introduce new data storage, instead leveraging Hive and HBase, which potentially makes it more palatable if you don’t want more data management engines running on your cluster. What it lacks however is Druid’s support for combining streaming and batch data. And Kylin is an Apache project (if that’s important to you), however given Hortonwork’s recent commitment to Druid, it wouldn’t surprise me if Druid was heading that way as well.&lt;/p&gt; &lt;p&gt;I have to say I’m a little conflicted around &lt;a href=&quot;/technologies/apache-beam/&quot;&gt;Apache Beam&lt;/a&gt;. It aims to introduce a standard model for batch and stream processing that a range of different technologies can then support. The model feels ok, certainly Data Artisans liked it enough to completely rework the Flink API to be more aligned (&lt;a href=&quot;https://data-artisans.com/blog/why-apache-beam&quot;&gt;details here&lt;/a&gt;), but I’m struggling to see the value. I have to admin to be slightly biased against abstractions like these - my feeling is that they’re great in concept, however there’s always a cost associated with an abstraction layer, either in not being able to achieve something easily because you’re fighting the abstraction, or in performance overheads from the translations involved, and switching between back end runners will never be as easy as you hope. And does anyone really care about being able to take batch/streaming code and easily migrate it between different back end execution engines? I can see what’s in it for Google - having Google Cloud Dataflow being the de-facto runner for running Beam code in the cloud puts them in a good position. Perhaps I’m just being cynical.&lt;/p&gt; &lt;p&gt;I’ve not much to say about &lt;a href=&quot;/technologies/rex-ray/&quot;&gt;Dell EMC REX-Ray&lt;/a&gt;, but hopefully we’ll look more at containerisation technologies and how they support persistent storage and how you might use them for analytics at some point in the future.&lt;/p&gt; &lt;p&gt;We looked at Scality’s open sourced S3 Server back when we were looking at &lt;a href=&quot;/tech-categories/object-stores/&quot;&gt;object store&lt;/a&gt;. Just as a reminder, it was a Node based single process (i.e. not clustered or distributed) S3 compatible object store service, that could either proxy requests onto &lt;a href=&quot;/technologies/scality-ring/&quot;&gt;Scality Ring&lt;/a&gt; or &lt;a href=&quot;/technologies/amazon-s3/&quot;&gt;Amazon S3&lt;/a&gt;, or could serve them from local or in memory storage. Useful for development and test, but probably not anything significant in production. It seems like it was pretty successful (they keep banging on about how it was downloaded over 600,000 times), and they’re therefore trying to make something more significant of it.&lt;/p&gt; &lt;p&gt;The result is that they’ve renamed S3 Server into &lt;a href=&quot;/technologies/zenko/cloudserver/&quot;&gt;Zenko Cloudserver&lt;/a&gt;, and made it part of a new much larger open source project called &lt;a href=&quot;/technologies/zenko/&quot;&gt;Zenko&lt;/a&gt;. At the moment all Zenko does is provide a Docker Swarm stack definition that allows multiple Cloudservers to be clustered behind a load balancer, but what they have planned is more interesting. First up is support for more backend services, including Azure Blob Storage and potentially Google Storage, also support for other container management systems such as Kubernetes, and then new new sub-projects - Backbeat (which will provide policy-based data workflows such as replication or migration) and Clueso (which will provide object metadata search and analytics using Apache Spark). The aim is to provide a gateway into multiple back end object stores with federation capabilities over the top. Sounds like a nice idea, and probably one to track.&lt;/p&gt; &lt;p&gt;And finally, I’ve refreshed all our &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt; technology summaries (including all the sub-projects) to make sure they’re up to date (which is odd given there’s only been one point release since they were written). The big bit that was missing was information on &lt;a href=&quot;/technologies/apache-spark/structured-streaming/&quot;&gt;Spark Structured Streaming&lt;/a&gt;, which provides the ability to run a DataFrame or SQL query over streaming data (using the standard &lt;a href=&quot;/technologies/apache-spark/spark-sql/&quot;&gt;Spark SQL&lt;/a&gt; APIs), and have the result calculated and then updated/maintained as new data comes in. The result being that I think it’s probably time we took a deeper look into streaming technologies and understood the differences. It does feel like Spark is moving forward at a pace however, both the original Spark RDD API, and the original Spark Streaming API appear to now be effectively in maintenance mode, DataFrames being the future across both, including for machine learning with &lt;a href=&quot;/technologies/apache-spark/mllib/&quot;&gt;MLLib&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Right - enough for next week. See you after the weekend.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Plan For This Week - 29/08/2017</title><link>https://ondataengineering.net/blog/2017/08/29/the-plan-for-this-week/</link><description> &lt;p&gt;Yes - it’s a Tuesday and not a Monday. Apologies, forgot to say on Friday that it was a public holiday in the UK this Monday, and therefore they’d be no update. I hope you’ll forgive me.&lt;/p&gt; &lt;p&gt;So, a shorter week this week. The plan is to look at event series databases, both for analysing external event data, but also as a capability for analysing event logs and metrics that our analytical systems may generate.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 30/08/2017</title><link>https://ondataengineering.net/blog/2017/08/30/the-mid-week-news/</link><description> &lt;p&gt;Right, time for this weeks news… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We’ve only just covered it, but &lt;a href=&quot;/technologies/apache-beam/&quot;&gt;Beam&lt;/a&gt; has already seen a point release to 2.1&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A topical follow-up to our look at &lt;a href=&quot;/technologies/apache-spark/structured-streaming/&quot;&gt;Spark Structured Streaming&lt;/a&gt; last week - Confluent have &lt;a href=&quot;https://www.confluent.io/blog/ksql-open-source-streaming-sql-for-apache-kafka/&quot;&gt;just announced&lt;/a&gt; KSQ, which provides the ability to create continuously updated tables and new streams from data in Kafka using SQL&lt;/li&gt; &lt;li&gt;And on the subject of KSQL, &lt;a href=&quot;http://www.zdnet.com/article/ksql-kafka-gets-sql/&quot;&gt;an interview&lt;/a&gt; with Confluent on KSQL from ZDNet&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://vision.cloudera.com/search-engines-and-big-data-a-perfect-match-part-2/&quot;&gt;Part 2&lt;/a&gt; from Cloudera on the role &lt;a href=&quot;/tech-categories/analytical-search/&quot;&gt;Analytical Search&lt;/a&gt; capabilities play in big data analytics&lt;/li&gt; &lt;li&gt;LinkedIn have &lt;a href=&quot;https://engineering.linkedin.com/blog/2017/08/open-sourcing-kafka-cruise-control&quot;&gt;opened sourced Cruise Control&lt;/a&gt;, their technology for monitoring, balancing and managing their &lt;a href=&quot;/technologies/apache-kafka&quot;&gt;Apache Kafka&lt;/a&gt; clusters&lt;/li&gt; &lt;li&gt;An &lt;a href=&quot;https://www.theregister.co.uk/2017/08/25/bet365_to_buy_basho_release_code/&quot;&gt;update&lt;/a&gt; from The Register on Basho - it looks like Bet365 are planning to buy up Basho’s assets (including Riak), and open source all of their products&lt;/li&gt; &lt;li&gt;A &lt;a href=&quot;https://www.confluent.io/blog/blogthe-simplest-useful-kafka-connect-data-pipeline-in-the-world-or-thereabouts-part-2/&quot;&gt;follow up&lt;/a&gt; from Confluent on how to get started with &lt;a href=&quot;/technologies/apache-kafka/kafka-connect/&quot;&gt;Kafka Connect&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A &lt;a href=&quot;https://databricks.com/blog/2017/08/24/anthology-of-technical-assets-on-apache-sparks-structured-streaming.html&quot;&gt;big dump&lt;/a&gt; from DataBricks on useful links and information relating to &lt;a href=&quot;/technologies/apache-spark/structured-streaming/&quot;&gt;Spark Structured Streaming&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/hue/&quot;&gt;Hue&lt;/a&gt; is gaining the ability to do ad-hoc imports to HDFS from databases via &lt;a href=&quot;/technologies/apache-sqoop&quot;&gt;Sqoop&lt;/a&gt; in it’s next release (4.1), building on the existing functionality to be able to import flat files&lt;/li&gt; &lt;li&gt;A two part series from Hortonworks (&lt;a href=&quot;https://hortonworks.com/blog/update-hive-tables-easy-way/&quot;&gt;part 1&lt;/a&gt; and &lt;a href=&quot;https://hortonworks.com/blog/update-hive-tables-easy-way-2/&quot;&gt;part 2&lt;/a&gt;) on doing &lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Hive&lt;/a&gt; table updates, including how to do type 1, 2 and 3 slowly changing dimensions in Hive&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.infoq.com/presentations/contracts-streaming-microservices&quot;&gt;A presentation&lt;/a&gt; from Gwen Shapira at Confluent (via InfoQ) on schema management and the role of schema management tools such as the Confluent Schema Registry (bundled with &lt;a href=&quot;/technologies/confluent-open-source/&quot;&gt;Confluent Open Source&lt;/a&gt;) and Hortonworks Schema Registry (/technologies/schema-registry/)&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/blog/5-ways-business-cultivate-data-driven-culture/&quot;&gt;Thoughts&lt;/a&gt; from the MapR blog on how Businesses Can Cultivate a Data-Driven Culture. Normally I’d avoid these articles like the plague, but this one seems to keep it simple and the advice sensible.&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Thoughts on Time Series Databases</title><link>https://ondataengineering.net/blog/2017/09/01/thoughts-on-time-series-databases/</link><description> &lt;p&gt;It feels like it’s been a bit of a rush this week (probably because I lost a day due to public holidays), but let’s quickly summarise what we’ve learnt this week about &lt;a href=&quot;/tech-categories/time-series-databases/&quot;&gt;Time Series Databases&lt;/a&gt; &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Firstly, before you go any further, read &lt;a href=&quot;https://blog.outlyer.com/top10-open-source-time-series-databases&quot;&gt;Steven Acreman’s post&lt;/a&gt; from the Outlyer blog where he reviews a number of time series databases, provides some interesting commentary, and provides a link to his detailed analysis including some benchmark timing. And while you’reo outside the safe confines of this site, can I recommend DB Engines’ &lt;a href=&quot;https://db-engines.com/en/ranking/time+series+dbms&quot;&gt;list of time series databases&lt;/a&gt; (and their site in general).&lt;/p&gt; &lt;p&gt;So, first things first, what are time series databases and why might we be interested in them. Fundamentally, they’re databases optimised for storing point in time metrics and aggregating them by a range of dimensions (commonly called tags) to generate pretty graphs and support a range of analytics. Generally what you’re looking for is deviations from the norm, which suggests something interesting is happening in your sources, or you’re looking at trends over time. And with metrics you’re talking about potentially huge volumes of data, so we’re looking for databases that both store huge volumes of metrics efficiently on disk and can handle the ingest rates that we might through at it.&lt;/p&gt; &lt;p&gt;And why might we be interested in them? Well firstly because time series metrics are (and always have been) a big part of analytical systems (think financial transactions, invoices, call detail records and now the internet of things, and the functional similarities between time series databases and OLAP cube / star schema technologies), but also because one of the key use cases for this data (monitoring of infrastructure and applications) is a capability that we need for the infrastructure and applications in our analytical systems, and that’s before we start to talk about data quality and how we might track and manage data quality metrics. So for the analytical systems we build, we’re probably as much a consumer of this sort of analytics as we are a provider. We’ll come back and talk more about monitoring and data quality in the future, as they’re both huge topics.&lt;/p&gt; &lt;p&gt;So you want a time series database? What are the options…&lt;/p&gt; &lt;p&gt;Firstly, if you’re looking for a commercial option they there are some good ones. &lt;a href=&quot;/technologies/influxdb/&quot;&gt;InfluxDB&lt;/a&gt; seems to be the market leader, but there are some good alternatives as well, and a number of these have open source versions. TimescaleDB is an interesting one to call out, as this is a plugin to PostgreSQL to add time series data storage, which gives you a PostgreSQL database that supports both relational and time series data, potentially making this an interesting technology choice for star schema like use cases.&lt;/p&gt; &lt;p&gt;If you have a &lt;a href=&quot;/tech-categories/nosql-wide-column-stores/&quot;&gt;NoSQL Wide Column Stores&lt;/a&gt; already, then there are a number of open source options for leveraging these. &lt;a href=&quot;/technologies/opentsdb/&quot;&gt;OpenTSDB&lt;/a&gt; which runs over &lt;a href=&quot;/technologies/apache-hbase/&quot;&gt;HBase&lt;/a&gt; is tried and tested, but there are others that run over &lt;a href=&quot;/technologies/apache-cassandra/&quot;&gt;Cassandra&lt;/a&gt; and Google Cloud Bigtable, as well as one (Chronix) that runs over Solr. Warp 10 is an interesting one to call out in this space, as a time series database over HBase that also supports geo co-ordinates.&lt;/p&gt; &lt;p&gt;And there are also a range of open source technologies that implement their own datastores. RRDtool is the grandfather of time series databases, being created in 1999, &lt;a href=&quot;/technologies/druid/&quot;&gt;Druid&lt;/a&gt; we’ve talked about before, and both Facebook and Netflix have very high performance in memory solutions (Beringei and Atlas) that are probably way over the top for anyone else.&lt;/p&gt; &lt;p&gt;And then there are a bunch of full stack solutions, which I’ve lumped into this technology category page as well for the time being. Firstly, infrastructure/application monitoring stacks that include capabilities for hoovering up metrics, visualising these and alerting on them. Secondly, IoT stacks, which do much the same I guess, but differentiate themselves in some way I don’t understand yet. We’ll come back to these in the future…&lt;/p&gt; &lt;p&gt;And that will, I think, do for the week. Apologies this post is slightly late, but that’s life for you.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Plan For This Week - 04/09/2017</title><link>https://ondataengineering.net/blog/2017/09/04/the-plan-for-this-week/</link><description> &lt;p&gt;Good morning, and welcome back.&lt;/p&gt; &lt;p&gt;This week we will mostly be looking at in memory databases, grids and whatever else they want to call themselves. If I come across some interesting technologies I’ll try and knock out a couple of technology summaries, but my guess is that most of this week will be spent reading to try and get my head around this space.&lt;/p&gt; &lt;p&gt;So news on Wednesday, a technology category page and associated thoughts on Friday, and maybe a technology summary or two somewhere along the way.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 06/09/2017</title><link>https://ondataengineering.net/blog/2017/09/06/the-mid-week-news/</link><description> &lt;p&gt;Although it’s a quiet week this week, we’re still got time for the news… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;No new releases this week, but I’ve added more links to the &lt;a href=&quot;/technologies/apache-spark&quot;&gt;Spark&lt;/a&gt; 2.2 and &lt;a href=&quot;/technologies/apache-solr&quot;&gt;Solr&lt;/a&gt; 6.6 release information&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Facebook have &lt;a href=&quot;https://code.facebook.com/posts/357056558062811/logdevice-a-distributed-data-store-for-logs/&quot;&gt;posted about LogDevice&lt;/a&gt;, their distributed log data store, due to be open sourced later this year. Along with DistributedLog (Twitter), Pulsar (Yahoo) and BookKeeper (also Yahoo), it feels like the open source technologies in our &lt;a href=&quot;/tech-categories/streaming-data-stores/&quot;&gt;streaming data stores&lt;/a&gt; category are starting to multiply.&lt;/li&gt; &lt;li&gt;Analytics on video (and image and audio) sources is an interesting area (here I go again) that I’d like to come back to at some point, but in the meantime, from Amit Baghel via InfoQ, a post on &lt;a href=&quot;https://www.infoq.com/articles/video-stream-analytics-opencv&quot;&gt;using OpenCV, Kafka and Spark to do video stream analysis&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Following Confluent’s KSQL announcement last week, &lt;a href=&quot;https://data-artisans.com/blog/flink-streaming-sql-ksql-stream-processing&quot;&gt;Data Artisans’ thoughts and comparison&lt;/a&gt; to &lt;a href=&quot;/technologies/apache-flink/&quot;&gt;Flink&lt;/a&gt; SQL&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.theregister.co.uk/2017/08/30/open_source_ml_and_ai/&quot;&gt;Thoughts&lt;/a&gt; from The Register on machine learning, open source and the current skills gap&lt;/li&gt; &lt;li&gt;Interesting &lt;a href=&quot;https://beam.apache.org/blog/2017/08/28/timely-processing.html&quot;&gt;post&lt;/a&gt; on window processing in &lt;a href=&quot;/technologies/apache-beam/&quot;&gt;Apache Beam&lt;/a&gt; that’s a good introduction to the subject&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Thoughts on In Memory Databases</title><link>https://ondataengineering.net/blog/2017/09/08/thoughts-on-in-memory-databases/</link><description> &lt;p&gt;So this week has been a bit of a mess, with late posts and no technology summaries. My apologies - I’m going to blame technology issues, and promise that it won’t happen again (even though it undoubtedly will).&lt;/p&gt; &lt;p&gt;Anyway, this week I’ve been looking at &lt;a href=&quot;/tech-categories/in-memory-databases/&quot;&gt;In Memory Databases&lt;/a&gt;, and I’ll warn you in advance that there’s going to be heavy use of quotes around “in memory”… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;I guess my first thought is that this probably isn’t a technology category in it’s own right - all the technologies on the technology category page (&lt;a href=&quot;/tech-categories/in-memory-databases/&quot;&gt;here&lt;/a&gt; if you missed the link in the paragraph above) are in memory versions of other technology categories - primarily relational databases, NoSQL key value stores and Hadoop compatible filesystems. So, don’t be surprised at some point if this technology category disappears as all the technologies on it get added to other technology category pages.&lt;/p&gt; &lt;p&gt;So, why go “in memory”? Well, the short and simple answer is speed - memory is faster than SSDs which in turn is faster than disks, and if all your data is in memory, you’re going to be able to execute queries and return data an order of magnitude faster than if you were having to pull it off disk. But having an “in memory” database doesn’t mean that you don’t want your data persisted (in almost all use cases you do), and so even “in memory” databases need to support durable writes of new data to disk and restoration of state on startup. And so the conceptual difference between “on disk” and “in memory” databases is all about when and how much data is loaded from disk into memory - for “in memory” databases it’s all data on startup and sod the memory usage, for “on disk” databases it’s just enough data when required and we’ll be frugal with your memory sir. But this distinction has pretty significant architectural implications - just sticking a RAM disk underneath an “on disk” database, or throwing more memory at it, tends to have limited benefit - the entire data access path needs to change to reflect the fact the data is in memory rather than on disk (and doesn’t need to be fetched dynamically into memory when required for starters).&lt;/p&gt; &lt;p&gt;First up were the in memory caches (memcached, Redis and the like), simple key value data stores that allowed applications to cache the results of queries or operations or hold state, with their simplicity supporting robust high performance data stores. They’ve now been given fancy names (like data grids), but at the moment I’m not entirely convinced of the use case for these within an analytics ecosystem so am not proposing to create a technology category page for them, but I look forward to being corrected and educated.&lt;/p&gt; &lt;p&gt;The big technologies for us that have started moving to “in memory” are relational SQL databases. All the big enterprise vendors are investing in this space, allowing entire tables to be permanently cached into memory, and making much better use of any memory available., but the technologies that are probably more interesting are the new specialist “in memory” technologies. There’s a range of these listed on the technology page, but there’s a couple I want to call out.&lt;/p&gt; &lt;p&gt;Firstly, there are a some (&lt;a href=&quot;/technologies/apache-ignite&quot;&gt;GridGain/Apache Ignite&lt;/a&gt; and Oracle Times Ten for starters) that can use other relational databases for their backing storage (a range for Ignite, Oracle for Time Tens), meaning that can be used as a drop in cache between your application and your “on disk” database, providing performance improvements without significant application changes.&lt;/p&gt; &lt;p&gt;The others are MapD and Kinetica - not only do these run in memory but they can also exploit GPUs for even further acceleration. If you’re really looking at pushing the boundaries of relational database performance, this would be a fascinating place to start.&lt;/p&gt; &lt;p&gt;And finally we get to the Hadoop compatible / distributed parallel “in memory” filesystems - same principle, different use case. There are two technologies in this space - &lt;a href=&quot;/technologies/alluxio/&quot;&gt;Alluxio&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-ignite&quot;&gt;GridGain/Apache Ignite&lt;/a&gt;. Alluxio is focused on filesystems, supporting a FUSE driver (allowing an in memory filesystem to be mounted in Linux) as well as Hadoop compatibility, with the ability to tier data onto a range of different persistent storage technologies, allowing it to support larger filesystems that will fit in memory. Ignite seems to be the Swiss Army knife in the in memory space, supporting key value, SQL and filesystem APIs (amongst a raft of other capabilities).&lt;/p&gt; &lt;p&gt;Right - I’ve had enough of this week. See you on Monday for a fresh start.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Plan For This Week - 11/09/2017</title><link>https://ondataengineering.net/blog/2017/09/11/the-plan-for-this-week/</link><description> &lt;p&gt;Right - time for a new week.&lt;/p&gt; &lt;p&gt;The observant among you may have spotted that we’re working our way through data storage and database technologies before we make a start on data transformation. There’s not far to go - graph databases and analytical databases are the big ones, but we’ll touch on data format libraries (Avro, Parquet et al) as well before we finish, and I need to have a think about whether I want to cover any more NoSQL technologies.&lt;/p&gt; &lt;p&gt;But this week, I want to look at data virtualisation technologies - those that allow you to query over and across data stored in multiple underlying platforms. As always, we’ll publish a technology category page on Friday along with some thoughts, and try and crank out some technology summaries during the week.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 13/09/2017</title><link>https://ondataengineering.net/blog/2017/09/13/the-mid-week-news/</link><description> &lt;p&gt;It’s the middle of the week, which means it’s time for the news… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/elasticsearch/&quot;&gt;Elasticsearch&lt;/a&gt;, &lt;a href=&quot;/technologies/elastic-x-pack/&quot;&gt;Elastic X-Pack&lt;/a&gt; and &lt;a href=&quot;/technologies/elasticsearch-hadoop/&quot;&gt;Elasticsearch Hadoop&lt;/a&gt; have all seen a 5.6 release - a minor point release before the big upcoming 6.0 release&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Data Artisans have &lt;a href=&quot;https://data-artisans.com/blog/da-platform-2-stateful-stream-processing-with-apache-flink-made-easier&quot;&gt;announced dA Platform 2&lt;/a&gt; - which adds application lifecycle managment and components for logging and metrics to &lt;a href=&quot;/technologies/apache-flink/&quot;&gt;Apache Flink&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Amazon have &lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2017/09/new-quick-start-build-a-data-lake-foundation-on-the-aws-cloud-with-aws-services/&quot;&gt;announced&lt;/a&gt; a new full stack data lake quickstart that runs on &lt;a href=&quot;/tech-vendors/amazon-web-services/&quot;&gt;AWS&lt;/a&gt; - we should take a look at full stack solutions at some point&lt;/li&gt; &lt;li&gt;Amazon also have a &lt;a href=&quot;https://aws.amazon.com/blogs/big-data/unite-real-time-and-batch-analytics-using-the-big-data-lambda-architecture-without-servers/&quot;&gt;post&lt;/a&gt; on implementing the Lambda architecture on &lt;a href=&quot;/tech-vendors/amazon-web-services/&quot;&gt;AWS&lt;/a&gt; without having to deploy or manage any servers&lt;/li&gt; &lt;li&gt;And it’s &lt;a href=&quot;https://www.confluent.io/blog/simplest-useful-kafka-connect-data-pipeline-world-thereabouts-part-3/&quot;&gt;part 3&lt;/a&gt; from Confluent on how to get started with &lt;a href=&quot;/technologies/apache-kafka/kafka-connect/&quot;&gt;Kafka Connect&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Thoughts on Data Virtualization</title><link>https://ondataengineering.net/blog/2017/09/15/thoughts-on-data-virtualization/</link><description> &lt;p&gt;That’s another week down (don’t worry, there are plenty to go), so let’s talk about &lt;a href=&quot;/tech-categories/data-virtualization/&quot;&gt;Data Virtualization&lt;/a&gt;, our topic for the week… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;My personal view is that Data Virtualization is a really powerful component in your data integration story, but it’s probably not well known or understood. It’s also the first technology category we’ve looked at where the open source offerings significantly lag the commercial ones, with the first commercial technologies (Composite and Denodo) having been round since the turn of the millennium and a wide range of commercial offerings now available, whereas the open source offerings tend to focus on a slightly narrower use case and are relatively new.&lt;/p&gt; &lt;p&gt;So what is Data Virtualization (some times also known as Data Federation / Enterprise Information Integration)? It’s a technology that allows you to run analytics (queries) over data in multiple disparate sources, allowing that data to be exploited as an integrated set without having to physically move it into a single database or data store. Most technologies in this space also allow the definition of a logical schema or semantic model over the source data so that users don’t have to understand the complexities of the source data, in addition to being able to run ad-hoc queries over the raw source data. Fundamentally it’s a data integration tool - bringing multiple data sources together so they can be exploited as a single integrated data set.&lt;/p&gt; &lt;p&gt;So why are these tools useful?&lt;/p&gt; &lt;p&gt;Firstly, for some use cases you might be able to avoid having any analytical platform whatsoever - if you can generate all the analytics you want over the data in it’s original source, that’s a huge win. However, that obviously puts extra load on those sources, and all of the complexity in integrating the source data together is now embedded in your data virtualization layer and the cost of this is going to be incurred at query time.&lt;/p&gt; &lt;p&gt;The first of these issues (the load on the sources) can be addressed by moving the data from it’s original source into an analytical platform that can support this query load (let’s call this a Data Lake for arguments sake). No transformation or integration, but it’s now somewhere that can support the level and types of data access that our analytics via our virtualization layer requires.&lt;/p&gt; &lt;p&gt;The second of these issues can be addressed in two ways. Firstly, many of these technologies (including &lt;a href=&quot;/technologies/denodo-platform/&quot;&gt;Denodo&lt;/a&gt;, the technology we looked at in more detail this week), can cache tables defined within their semantic layer (and update these on an incremental or scheduled basis). So we now have a tool in which we define the rules and logic for integrating our set of data sources, which will then ensure that a materialised physical copy of the results of this logic is available at all times for querying. And that sounds exactly like what we would traditionally try to achieve with a traditional data integration tool.&lt;/p&gt; &lt;p&gt;And although there will be limits to the complexity of the data transformation and integration that a Data Virtualization tool can support that might require you to break out custom data transformation pipelines (and tools like Denodo support the use of external technologies for generating data in it’s materialised cache), it will generally always be quicker and easier to integrate data using a Data Virtualization tool, and therefore there is likely always a role for them to pay in either prototyping or the rapid development of some integrations (which some call Agile ETL).&lt;/p&gt; &lt;p&gt;By the second (and probably more important) reason that these tools are useful is that they allow you to run ad-hoc queries and analytics across data that hasn’t been integrated and prepared for analytics. No matter how much data preparation and integration you do, there will always be some ad-hoc questions you want to ask that aren’t supported by your integrated and prepared data. So what’s your option now - spend the time and effort integrating that data in (when perhaps you don’t yet know how valuable it’s going to be) or do a pile of bespoke one off integration work? And this is where (for me) a lot of the value of Data Virtualization tools lie - they allow analysts (for what of a better term) to run these exploratory ad-hoc analytics over raw (or rawer) source data without having to do a pile of painful and expensive integration work. And that can be an enormous win in exploring and understanding where the value is in your data, and where you should focus your efforts on pre integrating and preparing data.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Plan For This Week - 18/09/2017</title><link>https://ondataengineering.net/blog/2017/09/18/the-plan-for-this-week/</link><description> &lt;p&gt;So we’re got some big topics to look at before we finish our look through data storage and database technologies. This week, it’s time for a deep breath as we dive into graph technologies.&lt;/p&gt; &lt;p&gt;There’s a heady mix of different technologies in this space, from OLTP databases, to graph analytics frameworks, to RDF/triple technologies. Let’s see what sort of sense we can make of them all…&lt;/p&gt; &lt;p&gt;As usual, (at least one) technology category page on Friday with some associated thoughts, and maybe some technology summaries during the week.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 20/09/2017</title><link>https://ondataengineering.net/blog/2017/09/20/the-mid-week-news/</link><description> &lt;p&gt;Right - time for the mid week news again. There’s a few new software releases this week to cover, plus a bunch of interesting blog posts. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-hbase&quot;&gt;Apache HBase&lt;/a&gt; has got a 2.0 alpha-2 release&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-impala&quot;&gt;Apache Impala&lt;/a&gt; is up to 2.10&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-kudu&quot;&gt;Apache Kudu&lt;/a&gt; is up to 1.5&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/greenplum/&quot;&gt;Greenplum&lt;/a&gt; has hit the big 5.0&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/pravega/&quot;&gt;Pravega&lt;/a&gt; has hit 0.1&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://azure.microsoft.com/en-us/blog/events-data-points-and-messages-choosing-the-right-azure-messaging-service-for-your-data/&quot;&gt;Details&lt;/a&gt; from Microsoft on the different messaging services in &lt;a href=&quot;/tech-vendors/microsoft-azure/&quot;&gt;Azure&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Another from Confluent trying to explain how technologies like &lt;a href=&quot;/technologies/apache-kafka&quot;&gt;Kafka&lt;/a&gt; might change the analytics space, this time on &lt;a href=&quot;https://www.confluent.io/blog/okay-store-data-apache-kafka/&quot;&gt;why it’s ok to store data in Kafka long term&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Again from Confluent, and this is always interesting - details on &lt;a href=&quot;https://www.confluent.io/blog/apache-kafka-tested/&quot;&gt;how they test Kafka&lt;/a&gt;&lt;/li&gt; &lt;li&gt;So it’s an advert for Waterline, but I think the principles are probably sound. From MapR - &lt;a href=&quot;https://mapr.com/blog/why-atlas-and-navigator-dont-cut-it/&quot;&gt;Why Atlas and Navigator don’t cut it&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2017-9803&quot;&gt;CVE-2017-903&lt;/a&gt; - Solr Kerberos issues with delegation tokens when using SecurityAwareZkACLProvider type of ACL provider e.g. SaslZkACLProvider&lt;/li&gt; &lt;li&gt;The &lt;a href=&quot;https://hortonworks.com/downloads/&quot;&gt;Hortonworks download page&lt;/a&gt; now includes links for IBM Data Science and IBM Big SQL&lt;/li&gt; &lt;li&gt;Fight time! Hortonworks have &lt;a href=&quot;https://hortonworks.com/blog/hbase-cassandra-benchmark/&quot;&gt;YCSB benchmark results&lt;/a&gt; for HBase vs Cassandra&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.zdnet.com/article/pivotal-greenplum-is-alive-and-kicking/&quot;&gt;Thoughts from ZDNet&lt;/a&gt; on &lt;a href=&quot;/technologies/greenplum&quot;&gt;Greenplum&lt;/a&gt; (which has just seen a 5.0 release)&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Week That Was</title><link>https://ondataengineering.net/blog/2017/09/22/the-week-that-was/</link><description> &lt;p&gt;When I decided this week was graph technologies week, I think I probably knew that it was too big a topic to do in one week, and so it’s proved. So, so new content this week, but to sweeten the blow, next week there should be four technology categories and at least three technology summaries coming down the pipe.&lt;/p&gt; &lt;p&gt;So apologies there’s not much this week, but next week should be a bumper week!&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 27/09/2017</title><link>https://ondataengineering.net/blog/2017/09/27/the-mid-week-news/</link><description> &lt;p&gt;It’s news time again, and there are big announcements from Cloudera and Hortonworks this week… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-solr/&quot;&gt;Apache Solr&lt;/a&gt; has hit 7.0&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/alluxio/&quot;&gt;Alluxio&lt;/a&gt; is up to 1.6&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-ignite/&quot;&gt;Apache Ingite&lt;/a&gt; is up to 2.2&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-parquet/&quot;&gt;Apache Parquet&lt;/a&gt; C++ is up to 1.3&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Cloudera/Hortonworks technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The big news this week is the simultaneous big product announcements from Hortonworks and Cloudera that look like they might be similar capabilities, but I think are probably trying to solve subtly different problems - we’ll revisit these in a few weeks once there’s more information available and do some technology summaries.&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/products/sdx.html&quot;&gt;Cloudera SDX&lt;/a&gt; (Shared Data Experience, coming in CDH 5.13) appears to be trying to enable the “one” data platform experience that you get with an on premesis CDH cluster in the cloud, specifically a persistent shared storage layer with shared metadata, security and governance and a range of workloads on top. That looks different in the cloud - you probably don’t want persistent Cloudera cluster that you’re paying for by the hour even if you’re not using it - so SDX gives you a shared storage layer using cloud object storage, a shared metadata and management layer, and then the ability to run compute workloads in isolated transient workload clusters managed through &lt;a href=&quot;/technologies/cloudera-altus/&quot;&gt;Cloudera Altus&lt;/a&gt;. The original sales pitch of a single shared Hadoop data platform re-imagined for the cloud. More details via a &lt;a href=&quot;http://vision.cloudera.com/introducing-cloudera-sdx-a-shared-data-experience-for-the-hybrid-cloud/&quot;&gt;Cloudera VISION blog post&lt;/a&gt; and a &lt;a href=&quot;http://blog.cloudera.com/blog/2017/09/cloudera-sdx-under-the-hood/&quot;&gt;Cloudera Engineering blog post&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/products/data-management/dataplane-service/&quot;&gt;Hortonworks Data Plane&lt;/a&gt; is again all about shared metadata, security and data management, but this time across a range of different data platforms - Hadoop, relational databases and your EDW, either on-premesis or in the cloud, and for data in motion or at rest. It’s open source, extensible for adding new services, with data lifecycle management being first up, allowing you to replicate, backup &amp;amp; restore and tier your data across your data platforms. It’s another cloud service (because obviously), and they talk about it as a Global Data Management Platform. More details via a &lt;a href=&quot;https://hortonworks.com/blog/category-emerges-introducing-hortonworks-dataplane-service/&quot;&gt;Hortonworks blog post&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/mapr-file-system/mapr-db/&quot;&gt;MapR DB&lt;/a&gt; 6.0 has &lt;a href=&quot;https://community.mapr.com/community/products/blog/2017/09/25/mapr-db-60-the-modern-database-for-global-data-intensive-applications&quot;&gt;been announced&lt;/a&gt; and will be available Q4 2017. There’s been a bunch of changes in the MapR stack over the last couple of months that I’ve not been keeping up to date with (the introduction of MapR XD for starters), so we’ll loop back round in a couple of weeks to refresh our MapR information.&lt;/li&gt; &lt;li&gt;Hortonworks are &lt;a href=&quot;https://hortonworks.com/blog/3x-faster-interactive-query-hive-llap/&quot;&gt;crowing&lt;/a&gt; about the increase in &lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Hive&lt;/a&gt; performance in HDP 2.6 and its support for the full suite of 99 TPC-DS queries&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://kudu.apache.org/2017/09/18/kudu-consistency-pt1.html&quot;&gt;Part 1&lt;/a&gt; on the &lt;a href=&quot;/technologies/apache-kudu/&quot;&gt;Apache Kudu&lt;/a&gt; consitency model&lt;/li&gt; &lt;li&gt;Looks like Hortonworks’ &lt;a href=&quot;https://hortonworks.com/blog/yinception-yarn-based-container-cloud-certify-hadoop-hadoop/&quot;&gt;are proud&lt;/a&gt; of the fact they run docker containers on &lt;a href=&quot;/technologies/apache-hadoop/yarn/&quot;&gt;YARN&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.influxdata.com/blog/introduction-to-influxdatas-influxdb-and-tick-stack/&quot;&gt;An introduction&lt;/a&gt; from InfluxData on &lt;a href=&quot;/technologies/influxdb/&quot;&gt;InfluxDB&lt;/a&gt; and the TICK stack&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Thoughts on Graph Technologies</title><link>https://ondataengineering.net/blog/2017/09/29/thoughts-on-graph-technologies/</link><description> &lt;p&gt;So, I said four technology categories and three technology summaries this week. One day you’ll learn.&lt;/p&gt; &lt;p&gt;But let’s talk about what we did manage to achieve this week, specifically technology category pages on &lt;a href=&quot;/tech-categories/rdf-databases/&quot;&gt;RDF Databases&lt;/a&gt;, &lt;a href=&quot;/tech-categories/graph-databases/&quot;&gt;Graph Databases&lt;/a&gt; and &lt;a href=&quot;/tech-categories/graph-analytics/&quot;&gt;Graph Analytics&lt;/a&gt;… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;So I’m not going to go over what a graph is, but suffice it to say that if you have data that’s modelled (or can be modelled) as a graph, then you might want to consider using some graph technologies.&lt;/p&gt; &lt;p&gt;However, given it’s still just data, the primary use cases are exactly the same as any other type of data. We want to manage and master the data in some sort of operational system (giving us fine grained ACID transactions at the entity/relationship level - the so-called OLTP use case), and we want to analytics over the data (large scans over lots of data to generates insight - the so-called OLAP use case).&lt;/p&gt; &lt;p&gt;And as per other operational databases, we need to be able to query the data as well as create/update it. For operational graph databases this focuses on graph traversals - essentially finding a set of nodes and relationships that match a pattern (all books by an author named John Smith) by finding an initial subset of nodes (the name John Smith), and then following relationships to match the pattern (named and then wrote), with maybe some aggregations at the end.&lt;/p&gt; &lt;p&gt;For the operational management of graph data there are two primary technology categories:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/tech-categories/rdf-databases/&quot;&gt;RDF Databases&lt;/a&gt; implement the W3C RDF data model standard that describes data as subject–predicate–object expressions (or triples), with support for ontologies that define the list of valid subject/object and predicate types. The benefit of RDF databases is their maturity (they’ve been around forever in technology terms, with a wide range of commercial and open source technologies), but also the standardisation driven by the W3C. There’s a W3C standard query language (SPARQL) that all RDF databases support, there are standard ontologies (OWL and RDFS), and there are a vast range of RDF creation, extraction, processing and visualisation tools that will work with your data. There’s a vast amount more here around RDF and the semantic web (that allows exploitation of text content as RDF data) that I’d love to come back to one day.&lt;/p&gt; &lt;p&gt;Then you have your &lt;a href=&quot;/tech-categories/graph-databases/&quot;&gt;Graph Databases&lt;/a&gt;. Again, these are operational databases, but they have a slightly more expressive data model that RDF databases by supporting both labels (or types, although RDF nodes are also typed) and properties (name/value pairs) against nodes and link, resulting in the term “labelled property graph”. That’s not to say you can’t do properties in RDF graphs (they’re just more relationships and nodes), however there are a number of RDF databases that explicitly support properties, as well as a number of graph databases that also support RDF/SPARQL. There’s no one standard query language for graph databases, however there are two popular options. The first is Cypher, the language used by Neo4j, which now has an open source specification (&lt;a href=&quot;http://www.opencypher.org/&quot;&gt;http://www.opencypher.org/&lt;/a&gt;) and has been adopted by a number of graph databases. The second is TinkerPop Gremlin (part of the Apache TinkerPop project), however rather than a language specification this is an entire abstraction layer that can be bolted on top of a graph database, with all the associated performance implications. Neo4j is the big cheese in the graph databases space, but it’s an active thriving technology area with a wide range of commercial and open source technologies to choose from.&lt;/p&gt; &lt;p&gt;However, before you piling into graph databases, have a look at &lt;a href=&quot;https://blog.acolyer.org/2017/07/07/do-we-need-specialized-graph-databases-benchmarking-real-time-social-networking-applications/&quot;&gt;The Morning Paper review of the “Do we need specialized graph databases? Benchmarking real-time social networking applications” paper from the University of Waterloo, Ontario from May 2017&lt;/a&gt;. Considering the technologies you already have before introducing a new one is never bad advice, and the paper looks at a number of graph use cases where relational databases actually perform better than dedicated graph databases. It also looks at the performance impact of using TinkerPop Gremlin over a native API, and the results aren’t good.&lt;/p&gt; &lt;p&gt;And so on to graph analytics. As per other types of data, there are options here for analytical databases (that focus on large scanning aggregation rather than transactional workloads) as well as analytical processing engines (batch engines that run over external data, for example MapReduce/Spark over HDFS for structured data).&lt;/p&gt; &lt;p&gt;Pragel feels like the originator here - Google’s technology that executed its PageRank algorithm. This implemented (and probably popularised) the BSP execution model (which can very crudely be describes as an equivalent of MapReduce for graph data). It’s another iterative model that can be distributed across a cluster, with each iteration generating “messages” between nodes that are used as the input for the next iteration.&lt;/p&gt; &lt;p&gt;Unlike batch analytics over structured data however, the use case for batch analytics over graph data is less clear. The specialist tools that have been created that implement the BSP modek (Giraph, Hama, GraphX) have never really taken off, and all are seeing limited active development. There are a number of analytical databases (Greenplum and Aster for starters) that support graph queries using a BSP execution model, but again this hasn’t seen widespread adoption in this space. And although TinkerPop now has a graph compute model, this is only supported by a limited number of databases.&lt;/p&gt; &lt;p&gt;It feels like time will tell in this space - there doesn’t appear to be clear use cases driving new technical capabilities at the moment, but maybe machine learning will change that.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Plan For This Week - 02/10/2017</title><link>https://ondataengineering.net/blog/2017/10/02/the-plan-for-this-week/</link><description> &lt;p&gt;It’s a guest publication week this week - our old friend Jeff Moszuti has a bunch of technology summaries on record format libraries (specifically Apache Arrow, CarbonData and ORCFile), and some thoughts to share with us at the end of the week.&lt;/p&gt; &lt;p&gt;We’ll start today with Apache Arrow…&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 04/10/2017</title><link>https://ondataengineering.net/blog/2017/10/04/the-mid-week-news/</link><description> &lt;p&gt;Time for the news again, with all our updates on new technology releases and interesting things to read… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-calcite/&quot;&gt;Apache Calcite&lt;/a&gt; is up to 1.14&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-nifi/&quot;&gt;Apache NiFi&lt;/a&gt; is up to 1.4&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/elastic-cloud/&quot;&gt;Elastic Cloud&lt;/a&gt; is now generally available on GCP (after being in beta since April)&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-altus/&quot;&gt;Cloudera Altus&lt;/a&gt; now supports Azure&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/elasticsearch/&quot;&gt;Elasticsearch&lt;/a&gt; 6.0 release candidate 1 (RC1) (not for production) is out&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;From MapR, and these are always good for a read, a &lt;a href=&quot;https://mapr.com/blog/database-comparison-an-in-depth-look-at-mapr-db/&quot;&gt;post&lt;/a&gt; on why &lt;a href=&quot;/technologies/mapr-file-system/mapr-db/&quot;&gt;MapR-DB&lt;/a&gt; is better than Cassandra, HBase and others&lt;/li&gt; &lt;li&gt;And another face-off - this time a &lt;a href=&quot;https://brewing.codes/2017/09/25/flink-vs-spark/&quot;&gt;post&lt;/a&gt; on &lt;a href=&quot;/technologies/apache-flink/&quot;&gt;Flink&lt;/a&gt; vs &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Spark&lt;/a&gt;&lt;/li&gt; &lt;li&gt;It’s &lt;a href=&quot;https://www.confluent.io/blog/build-deploy-scalable-machine-learning-production-apache-kafka/&quot;&gt;another interesting Confluence post&lt;/a&gt; (YMMV), this time on machine learning with &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Kafka&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.zdnet.com/article/strata-nyc-2017-to-hadoop-go-jump-in-a-data-lake/&quot;&gt;Thoughts from ZDNet&lt;/a&gt; on the recent Strata NY event&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/azure-hdinsight/&quot;&gt;Azure HDInsight&lt;/a&gt; now supports &lt;a href=&quot;https://azure.microsoft.com/en-us/blog/general-availability-of-hdinsight-interactive-query-blazing-fast-data-warehouse-style-queries-on-hyper-scale-data-2/&quot;&gt;Interactive Query&lt;/a&gt;, aka Hive on LLAP as a service&lt;/li&gt; &lt;li&gt;There are a bunch of security vulnerability announcements this week &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2017-9792&quot;&gt;CVE-2017-9792&lt;/a&gt; - malicious user with “ALTER” permissions on an Impala table can access any other Kudu table data&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2017-9797&quot;&gt;CVE-2017-9797&lt;/a&gt; - unauthenticated client can enter multi-user authentication mode in Apache Geode&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2017-9794&quot;&gt;CVE-2017-9794&lt;/a&gt; - user with read privileges can use the gfsh command line utility to execute queries with Apache Geode&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Thoughts on Hadoop Data Formats</title><link>https://ondataengineering.net/blog/2017/10/06/thoughts-on-hadoop-data-formats/</link><description> &lt;p&gt;This week we looked at two file formats for Hadoop; &lt;a href=&quot;/technologies/apache-orc&quot;&gt;ORC&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-carbondata&quot;&gt;CarbonData&lt;/a&gt; and a new in-memory data structure specification; &lt;a href=&quot;/technologies/apache-arrow&quot;&gt;Arrow&lt;/a&gt;. Earlier on this year we looked at data serialisation frameworks - &lt;a href=&quot;/technologies/apache-avro&quot;&gt;Avro&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-parquet&quot;&gt;Parquet&lt;/a&gt;. File formats, data serialisation frameworks, specifications… Ahh, what a minefield! Today I’d like to try and make sense of it all by looking at the evolution of these various data formats to see how we got here. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Back in the old days, it wasn’t uncommon to process gzipped plain text delimited formatted data using MapReduce. However, if the data was supplied as lots of &lt;a href=&quot;https://blog.cloudera.com/blog/2009/02/the-small-files-problem&quot;&gt;small files&lt;/a&gt;, eventually, pressure would be put on HDFS because of a hard limit on the number of files it can physically track. At the same time if too few large files were supplied, MapReduce wouldn’t be able to efficiently carve up the data for parallel processing. SequenceFile was the first file format to address both of these issues by providing a container in which many small files could be put into a larger single file with synchronisation markers to permit efficient splitting of files to distribute the workload. The file format also allowed for different types of compressions to be used inside of the file to provide a finer level of compression control whilst still maintaining its splitability characteristics. And for a while, SequenceFile was fine if you were just using it to do large-scale distributed batch processing or building a self-contained system using Java, such a &lt;a href=&quot;/technologies/apache-hbase&quot;&gt;storage manager&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;As a file format, &lt;a href=&quot;/technologies/apache-avro&quot;&gt;Avro&lt;/a&gt; builds on top of the &lt;a href=&quot;http://avro.apache.org/docs/current/spec.html#Object+Container+Files&quot;&gt;container file format&lt;/a&gt; and synchronisation marker ideas but adds the ability to model relational and complex data. It achieves this through the concept of a schema that allows you to specify the structure, type and the meaning of your data, and when distributed together with your data, you’ll have self describing data that can be used as both a wire format for communication and a serialisation format for persistent data. But Avro’s creator - Doug Cutting didn’t just stop there, Avro had a much grander aspiration. Its design accommodated schema evolution which allowed your data to be long lived and reusable by other applications outside of the Hadoop and Java ecosystem.&lt;/p&gt; &lt;p&gt;So far, when it comes to serialising data for persistence, our file formats have been writing consecutive elements of a row next to each other on disk. Around 2010/2011, there were several research projects experimenting with column-oriented data layout designs on HDFS. The idea behind a columnar data structure was to lay out your data so that column values were adjacent to one another. For read queries that only process a small subset of columns but over a large number of rows at a time (the so called typical analytical workload), it was possible to read only the required column values off disk, thereby minimising disk I/O. Contrary to the more traditional row-oriented data layout for the same type of query, you were forced to read all the other columns off disk, keep the columns required by the query and discard the unused ones. Column data being uniform in type, also had the additional benefit of being highly compressible, allowing the same data to be stored on disk in a smaller form and further reducing I/O. This approach to optimise data placement was popularised by databases such as &lt;a href=&quot;https://www.monetdb.org&quot;&gt;MonetDB&lt;/a&gt; and &lt;a href=&quot;https://www.vertica.com&quot;&gt;HP Vertica&lt;/a&gt;. The tradeoff would be slower writes, but this proved a good optimisation for analytical workloads. The record columnar file format (RCFile) came out of one of these research projects and became widely adopted in the Hadoop ecosystem.&lt;/p&gt; &lt;p&gt;By 2013, there was gathering interest towards using Hadoop for interactive, data warehouse-style SQL queries, and combining Hive with RCFile for data storage was a popular choice. &lt;a href=&quot;/tech-vendors/hortonworks&quot;&gt;Hortonworks&lt;/a&gt; launched the &lt;a href=&quot;https://hortonworks.com/blog/100x-faster-hive&quot;&gt;Stinger initiative&lt;/a&gt; with the goal to dramatically speed up Hive and make it more enterprise-ready. Building on top of the columnar file format of RCFile, the team introduced Avro’s schema concept to allow the format to model complex nested structures which wasn’t previously supported by RCFile. Armed with this metadata, it was also possible to intelligently select an appropriate compression schema based on a column’s data type. The format also allowed additional metadata such as min and max indexes to be collected which could then be later used to intelligently skip irrelevant parts of the data without the need for large, complex, or manually maintained indexes. Other improvements introduced over Avro and RCFile was the ability to identify the boundaries on which files could be split with having to scan for synchronisation markers. This new file format was known as the &lt;a href=&quot;/technologies/apache-orc&quot;&gt;Optimized Record Columnar (ORC)&lt;/a&gt; File. The Avro team had also been experimenting with a columnar file format design called Trevni which was picked up by &lt;a href=&quot;/tech-vendors/cloudera&quot;&gt;Cloudera&lt;/a&gt; and Twitter to develop &lt;a href=&quot;/technologies/apache-parquet&quot;&gt;Parquet&lt;/a&gt;. On the other hand, ORC was spun out of Hive into a separate project but was initially kept closely integrated. Parquet was positioned as a more general-purpose columnar file format for use with any Hadoop framework, although essentially both projects shared the same fundamental ideas. Today, they both stand in their own right and are integrated with a number of different frameworks.&lt;/p&gt; &lt;p&gt;Up until now we’ve being looking at data formats that are primarily optimised for a single type of query analysis. This is not ideal if you want your data processing platform to support a wide spectrum of different types of query analysis. &lt;a href=&quot;/technologies/apache-carbondata&quot;&gt;CarbonData&lt;/a&gt; from Huawei aims to tackle the ‘one format to rule them all’ idea heads on. Building on the previous formats, CarbonData introduces multi-dimensional key indexes inspired from the likes of Mondrian (an early open source OLAP server) and Apache &lt;a href=&quot;/technologies/apache-kylin&quot;&gt;Kylin&lt;/a&gt; to support multi-dimensional OLAP style queries, inverted indexes for count distinct like operations, and the ability to group columns together to support detailed queries which fetch many columns out of a wide table.&lt;/p&gt; &lt;p&gt;Finally, we come to &lt;a href=&quot;/technologies/apache-arrow&quot;&gt;Apache Arrow&lt;/a&gt;. Unlike the previous data formats we’ve discussed, Arrow isn’t about serialising data to disk, but is an in-memory data format that focuses on CPU throughput for efficient processing and for data exchange between process/systems without serialisation and deserialisation. Similar to ORC and Parquet, data is structured in a columnar structure, so when it comes to analytical workloads, only the required data can be supplied to the CPU. This data placement strategy takes full advantage of the on-chip cache storage (which is 100x faster to access than main memory), pipelining, and SIMD (Single Instruction Multiple Data) instructions which work on multiple data values simultaneously in a single CPU clock cycle. As an in-memory data format, this concept wasn’t new and had already been implemented in both &lt;a href=&quot;/technologies/apache-drill&quot;&gt;Drill&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-hive/hive-server&quot;&gt;Hive&lt;/a&gt;. What is special about Arrow is its goal to define a standard interchange format to allow sharing of data between processes without the overhead of moving or transforming the data. This is important when you want to put together a data processing platform that isn’t limited only to Hadoop.&lt;/p&gt; &lt;p&gt;In summary, we’ve seen that the original Hadoop data formats were designed to solve very specific use cases. As the drive to develop Hadoop into a more general purpose analytics platform and expand its use outside of the Java ecosystem, the data formats have rapidly evolved, with each new format building on top of the ideas of its predecessor. While the Hadoop ecosystem continues to evolve to cater for new use cases, I expect to see continued innovation in this space.&lt;/p&gt; &lt;p&gt;So I hope this little journey has helped you navigate your way through the complex and rapidly evolving collection of Hadoop data formats - on Monday I’ll hand you back over to Peter.&lt;/p&gt; </description> <discourse_author>moszutij</discourse_author> </item> <item><title>The Plan For This Week - 10/10/2017</title><link>https://ondataengineering.net/blog/2017/10/10/the-plan-for-this-week/</link><description> &lt;p&gt;First up, yes, we’re a day late on this post - apologies. Don’t get confused - it is Tuesday today and not Monday.&lt;/p&gt; &lt;p&gt;So I think we have one last technology category to see off before we move on from data storage and database technologies, and that’s analytical databases - those that specialise in large scanning analytical queries and that combine support for SQL with a range of other analytical capabilities.&lt;/p&gt; &lt;p&gt;This feels like another monster category, so don’t expect much content this week, and I have a feeling it’s going to be next week before we can wrap everything up.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 11/10/2017</title><link>https://ondataengineering.net/blog/2017/10/11/the-mid-week-news/</link><description> &lt;p&gt;It’s news time again - come and see what new tech releases and interesting reading we have for you this week! &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/mesosphere-dcos/&quot;&gt;Mesosphere DC/OS&lt;/a&gt; is up to 1.10&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/mesosphere-marathon&quot;&gt;Mesosphere Marathon&lt;/a&gt; is up to 1.5&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/rex-ray/&quot;&gt;REX-Ray&lt;/a&gt; is up to 0.10&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/hue&quot;&gt;Hue&lt;/a&gt; is up to 4.1&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-flume&quot;&gt;Flume&lt;/a&gt; is up to 1.8&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;From ZDNet, &lt;a href=&quot;http://www.zdnet.com/article/towards-a-unifying-data-theory-and-practice-combining-operations-analytics-and-streaming/&quot;&gt;thoughts&lt;/a&gt; on SnappyData, and the convergence of OLTP, OLAP and streaming analytics&lt;/li&gt; &lt;li&gt;From The Register, &lt;a href=&quot;/tech-vendors/microsoft-azure/&quot;&gt;Azure&lt;/a&gt; now supports a dedicated tool for provisioning Spark based on Azure Batch&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/amazon-emr/&quot;&gt;Amazon EMR&lt;/a&gt; now supports &lt;a href=&quot;/technologies/apache-livy&quot;&gt;Livy&lt;/a&gt;, as well as new versions of Hue, Presto, Flink and Pig.&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://content.pivotal.io/blog/apache-madlib-comes-of-age&quot;&gt;Thoughts&lt;/a&gt; on MADLib from Pivotal following it’s graduation from the Apache incubator. We’ll be looking more at capabilities like this over this week and next.&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-nifi/&quot;&gt;Apache Nifi&lt;/a&gt; has security vulnerablity - &lt;a href=&quot;http://www.cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2017-12623&quot;&gt;CVE-2017-12623&lt;/a&gt; - authorized user could upload a template which contained malicious code and accessed sensitive files via an XML External Entity (XXE) attack&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.crail.io/&quot;&gt;Crail&lt;/a&gt; has been &lt;a href=&quot;https://wiki.apache.org/incubator/CrailProposal&quot;&gt;submitted&lt;/a&gt; to the Apache Incubator - looks like a high performance distributed and tiered (in memory, flash and disk) storage layer for temporary data that provides memory, storage and network access that bypasses the JVM and OS, and with integration to Spark (as a custom Spark Suffler that improves sort performance by a factor of five) and Hadoop (via an HDFS adaptor).&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Week That Was</title><link>https://ondataengineering.net/blog/2017/10/13/the-week-that-was/</link><description> &lt;p&gt;As suspected, no updates this week - I’m still trying to work out how to make sense of analytical database technologies and present these in a useful way.&lt;/p&gt; &lt;p&gt;So hold tight - a bunch (few) technology categories and some thoughts will be coming next week.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 18/10/2017</title><link>https://ondataengineering.net/blog/2017/10/18/the-mid-week-news/</link><description> &lt;p&gt;News news news! A bunch of new technology releases and interested blog posts for your purusal this week… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt; has seen a 5.13 release, with Kudu now fully bundled and Spark 1.x deprecated&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-manager/&quot;&gt;Cloudera Manager&lt;/a&gt; is up to 5.13&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-navigator/&quot;&gt;Cloudera Navigator&lt;/a&gt; is up to 2.12&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-director/&quot;&gt;Cloudera Director&lt;/a&gt; is up to 2.6&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-phoenix/&quot;&gt;Apache Phoenix&lt;/a&gt; is up to 4.12&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-solr&quot;&gt;Apache Solr&lt;/a&gt; is up to 7.1&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Microsoft have released/updated &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/architecture/aws-professional/services&quot;&gt;their Azure to AWS services comparison&lt;/a&gt;&lt;/li&gt; &lt;li&gt;LinkedIn have posted about &lt;a href=&quot;https://engineering.linkedin.com/blog/2017/10/streaming-data-pipelines-with-brooklin&quot;&gt;Brooklin&lt;/a&gt; - their internal product (planned to be open sourced in 2018) for moving streaming data around and performing change data capture on source databases&lt;/li&gt; &lt;li&gt;Uber have posted about &lt;a href=&quot;https://eng.uber.com/athenax/&quot;&gt;AthenaX&lt;/a&gt; their technology for running SQL analytics over streaming data using Flink&lt;/li&gt; &lt;li&gt;An interesting post from DB Engines on &lt;a href=&quot;https://db-engines.com/en/blog_post/72&quot;&gt;multi-model databases&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Hortonworks have posted on &lt;a href=&quot;https://hortonworks.com/blog/automated-validation-apache-hadoop-ecosystem/&quot;&gt;how they test their Hadoop distribution&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A post from Google on BigQuery, and how it’s separation of data and processing &lt;a href=&quot;https://cloud.google.com/blog/big-data/2017/10/separation-of-compute-and-state-in-google-bigquery-and-cloud-dataflow-and-why-it-matters.html&quot;&gt;gives near linear scalability&lt;/a&gt;, comparing it’s performance to Impala, Spark, Hive and Presto&lt;/li&gt; &lt;li&gt;Cloudera have been looking at &lt;a href=&quot;/technologies/microsoft-azure-data-lake-store/&quot;&gt;Azure Data Lake Store&lt;/a&gt;, concluding that [performance “compares favourably” to using network-attached Azure disk storage - &lt;a href=&quot;http://blog.cloudera.com/blog/2017/10/a-look-at-adls-performance-throughput-and-scalability/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;And another benchmark - this time DataBricks claiming that Spark &lt;a href=&quot;/technologies/apache-spark/structured-streaming/&quot;&gt;Structured Streaming&lt;/a&gt; is 5 times faster than Flink and Kafka Streams - &lt;a href=&quot;https://databricks.com/blog/2017/10/11/benchmarking-structured-streaming-on-databricks-runtime-against-state-of-the-art-streaming-systems.html&quot;&gt;link&lt;/a&gt; - UPDATE 2018-01-05: See dataArtisans response &lt;a href=&quot;https://data-artisans.com/blog/curious-case-broken-benchmark-revisiting-apache-flink-vs-databricks-runtime&quot;&gt;here&lt;/a&gt;&lt;/li&gt; &lt;li&gt;And finally, there’s a new security vulnerability in Solr - &lt;a href=&quot;http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2017-12629&quot;&gt;CVE-2017-12629&lt;/a&gt; - a remote code execution issue&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Week That Was</title><link>https://ondataengineering.net/blog/2017/10/20/the-week-that-was/</link><description> &lt;p&gt;Another week, another apology.&lt;/p&gt; &lt;p&gt;For various reasons the new analytical databases technology category pages aren’t ready yet. They’re almost done however, and will be going up all next week.&lt;/p&gt; &lt;p&gt;Thank you for your patience.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 25/10/2017</title><link>https://ondataengineering.net/blog/2017/10/25/the-mid-week-news/</link><description> &lt;p&gt;It’s news time again… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/greenplum/&quot;&gt;Greenplum&lt;/a&gt; has hit 5.1&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-data-science-workbench/&quot;&gt;Cloudera Data Science Workbench&lt;/a&gt; has hit 1.2&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Some interesting slides from Todd Lipcon on the Kudu blog on Hybrid Transactional/Analytic Processing (HTAP) and how &lt;a href=&quot;/technologies/apache-kudu/&quot;&gt;Kudu&lt;/a&gt; compares to Google Spanner - &lt;a href=&quot;https://kudu.apache.org/2017/10/23/nosql-kudu-spanner-slides.html&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A couple of updates from Neo4j from its GraphConnect conference in New York, including the contribution of a Cypher interface for &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt;, and new analytical functions in Neo4J - &lt;a href=&quot;http://www.zdnet.com/article/sparkier-faster-more-graph-databases-and-neo4j-moving-on/&quot;&gt;ZDNet&lt;/a&gt;; &lt;a href=&quot;https://www.theregister.co.uk/2017/10/24/neo4j_native_graph_platform/&quot;&gt;The Register&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Comparison of &lt;a href=&quot;/technologies/apache-cassandra/&quot;&gt;Apache Cassandra&lt;/a&gt; and ScyllaDB (the Cassandra port to C++) from ZDNet - &lt;a href=&quot;http://www.zdnet.com/article/a-rock-and-a-hard-place-between-scylladb-and-cassandra/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From The Register - The Linux Foundation have a new open-data licence - &lt;a href=&quot;https://www.theregister.co.uk/2017/10/23/linux_foundation_community_data_license_agreement/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From the Influx blog - getting started with &lt;a href=&quot;/technologies/influxdb/&quot;&gt;InfluxDB&lt;/a&gt; in 5 minutes - &lt;a href=&quot;https://www.influxdata.com/blog/zero-awesome-in-5-minutes/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;The John Snow Labs have released an NLP library for &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt; - &lt;a href=&quot;https://databricks.com/blog/2017/10/19/introducing-natural-language-processing-library-apache-spark.html&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Gartner have released their latest Magic Quadrant for Distributed File Systems and Object Storage. No significant changes, thoughts from The Register are &lt;a href=&quot;https://www.theregister.co.uk/2017/10/19/gartner_2017_object_storage_magic_quadrant/&quot;&gt;here&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Thoughts on Query Engines and Analytical Databases</title><link>https://ondataengineering.net/blog/2017/10/27/thoughts-on-query-engines-and-analytical-databases/</link><description> &lt;p&gt;Right, we’ve finally completed our look at &lt;a href=&quot;/tech-categories/analytical-databases/&quot;&gt;analytical databases&lt;/a&gt;, with a short stop to look at &lt;a href=&quot;/tech-categories/query-engines&quot;&gt;query engines&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;So let’s summarise and spew some thoughts… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Let’s start with &lt;a href=&quot;/tech-categories/analytical-databases/&quot;&gt;analytical databases&lt;/a&gt;, with a bit of noddy pseudo-history tale telling. Once upon a time the relational database was invented, but these initially focused on transactional use creates - creating, finding, updating and deleting records - what’s now sometimes referred to as OLTP (online transaction processing). However SQL was a great and so people started using it to try and generate reports on the data in their relational databases - queries that focused on aggregating and joining significant portions of their data. And that was generally fine, until data volumes increased to the point where databases designed for transactional workloads struggled to handle the loads generated from these reports or analytics - fetching lots of data off disk and aggregating it was expensive.&lt;/p&gt; &lt;p&gt;And so the analytical OLAP (online analytical processing) databases were born - designed to primarily support the large full table scan aggregation queries, while still retaining the core relational database support for transactions. These were marketed as data warehouse or analytical databases, and with them came a bunch of new technologies - parallelism and the invention of the MPP (massively parallel processing) database, as full table scans and aggregations lend themselves well to partitioning and parallelisation (even if joins do not); columnar compression, which enables faster and more efficient table scans of columns from database tables; pre-aggregation of data, through materialized views and pre-generated cubes; and a range of new functionality designed to complement SQL as an analytical tool, such as support for machine learning, geographical analytics, map reduce and custom analytical functions. Today, these are often sold as appliances, bundling clusters of compute and storage servers with huge bandwidth interconnects between them, however many have now also started embracing the cloud, being available as a cloud service but also to a lesser extents as cloud native software. Open source projects in this space however are scarce.&lt;/p&gt; &lt;p&gt;Instead, the the charge of open source software into the space ended up being spearheaded by Hadoop. Whilst analytical databases have long separated storage and compute (with some interesting abilities to push some parts of the query down to the storage layer), it was Hadoop that’s formalised this by separating them into completely separate and interchangeable components. This can be seen from the very first versions of Hadoop, in that it was made up of two products - HDFS and MapReduce - storage and compute, and over time there’s been evolution on both sides. In storage, HDFS has remained a constant, but there’s been huge innovation in the storage formats of data (see our &lt;a href=&quot;/tech-categories/data-storage-formats/&quot;&gt;Data Storage Formats&lt;/a&gt; page). On the compute side, there’s been a veritable explosion of &lt;a href=&quot;/tech-categories/query-engines&quot;&gt;query engines&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;These started out as batch query engines - every query starting up a new job (initial MapReduce) to go and read all the data and execute the query. However there’s been significant push into low latency high concurrency query engines, led by the big Hadoop vendors - Cloudera with &lt;a href=&quot;/technologies/apache-impala/&quot;&gt;Impala&lt;/a&gt; and Hortonworks with &lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Hive&lt;/a&gt; - both trying to make Hadoop a realistic competitor in the analytical database market. Compared to analytical databases Hadoop query engines are new technologies - their query optimisers and SQL compliance generally lag the mature analytical databases, however they’re seeing significant investment, and they’re not at the point where they’ll probably support most of your use cases.&lt;/p&gt; &lt;p&gt;And this split of storage and compute has brought some interesting benefits. This first is support for the whole “schema-on-read” shtick - the idea that you can query your raw data without having to do any preparation first. It’s slow, and painful, but for an initial exploratory analysis it’s a valuable tool. And of course if there’s value in the data, you’re still feel to do some work that makes it quicker, easier and more efficient to query. The second is that these tools have naturally evolved some level of query federation - if I want to exploit raw un prepared data I have to accept that this data may be in multiple places in a range of formats, and if I have a query engine that’s separated from the underlying storage, why not make that storage pluggable, and support multiple storage platforms. And so many of these tools support querying over a range of data sources, from HDFS, to S3, to HBase, to relational and NoSQL database, and (interestingly) some emerging support for Kafka . They don’t have the level of sophistication around semantic layers and caching and materialisation of data the &lt;a href=&quot;/tech-categories/data-virtualization/&quot;&gt;Data Virtualization&lt;/a&gt; technologies do, but it can still be a hugely valuable capability.&lt;/p&gt; &lt;p&gt;And now we’re starting to see commercial vendors get involved, both large established vendors (Teradata, IBM, Oracle) who are updating their products to run over HDFS and external data stores, but also new vendors in this space who’ve seen an opportunity to sell commercial products in this space that offer a level of functionality and maturity that maybe some of the open source products can’t match. If you’re running open source Hadoop, some of these may well be worth a look.&lt;/p&gt; &lt;p&gt;Finally - a couple of footnotes…&lt;/p&gt; &lt;p&gt;Although many of these tools support SQL, there are many that have their own query languages - Pig has Pig Latin, MRQL has it’s own language - and of course you could probably count most of the graph and machine learning projects as query languages, which suggests I’ve probably not named this category particularly well. SQL is always going to be the dominant language in this space however, and anything with it’s own query language is unlikely to make an impact.&lt;/p&gt; &lt;p&gt;It’s also worth commenting on the role of the Hive metadata in the Hadoop query engine ecosystem. If you’re separating compute and storage, you need some way of telling the compute what data’s available for query and what format it’s in. Within Hive, this was the Hive Metastore, and this is now gradually being adopted as the standard in this space, giving some interesting interoperability options, in that if you define a table in the Hive Metastore, you can query that with either Hive or Impala (or any other technology that uses the Metastore). And now there’s a proposal to break the Metastore out of the Hive project into it’s own top level Apache project, to reflect the wider role it has in the Hadoop ecosystem.&lt;/p&gt; &lt;p&gt;Right - that will do, and hopefully brings the protracted birth of our analytical databases content to and end. Have a good weekend everyone, and we’ll see you on Monday for a week or two of random catch up and clean up.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Plan For This Week - 30/10/2017</title><link>https://ondataengineering.net/blog/2017/10/30/the-plan-for-this-week/</link><description> &lt;p&gt;So, I think we’re probably done with looking at data storage and databases, however before we move on to data acquisition, processing and transformation tools, and the interesting set of supporting capabilities that live within a data ecosystem, there’s a bit of housekeeping to do.&lt;/p&gt; &lt;p&gt;So over this week (and maybe next), expect some random updates. I want to update and rework the technology categories home page to provide a bit more structure, and there are a few technologies I’ve been meaning to double back on now there should be some more documentation around beyond a press release (Hortonworks Data Plane, Cloudera SDX, MapR-XD/ES for starters). Plus I need to work out what technology categories might be coming up.&lt;/p&gt; &lt;p&gt;So hang in there - new content coming, but what and when might be a little random for the next few weeks…&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 01/11/2017</title><link>https://ondataengineering.net/blog/2017/11/01/the-mid-week-news/</link><description> &lt;p&gt;It’s Wednesday, which means it’s time for the news… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-ambari/&quot;&gt;Ambari&lt;/a&gt; has hit 2.6&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Hadoop&lt;/a&gt; has hit 2.8.2, the first 2.8 GA release for production use&lt;/li&gt; &lt;li&gt;Cloudera have finally published their blog post on &lt;a href=&quot;/technologies/cloudera-data-science-workbench/&quot;&gt;Data Science Workbench&lt;/a&gt; version 1.2 - &lt;a href=&quot;http://blog.cloudera.com/blog/2017/10/new-in-cloudera-data-science-workbench-1-2-usage-monitoring-for-administrators/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hortonworks have announced the general availability of their first plugin for their new DataPlane Service - Data Lifecycle Manager (DLM) - &lt;a href=&quot;https://hortonworks.com/blog/disasters-can-instant-takes-village-build-hybrid-cloud-based-recovery-solution/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Influx have a blog post on the internals of &lt;a href=&quot;/technologies/influxdb/&quot;&gt;InfluxDB&lt;/a&gt; - &lt;a href=&quot;https://www.influxdata.com/blog/influxdb-internals-101-part-one/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Following up on the Linux Foundation open data licence last week, some expert views from The Register - &lt;a href=&quot;https://www.theregister.co.uk/2017/10/25/linux_foundation_data_licence_analysis/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Amazon - how to build a Data Lake on &lt;a href=&quot;/tech-vendors/amazon-web-services/&quot;&gt;AWS&lt;/a&gt; with AWS Glue and S3 - &lt;a href=&quot;https://aws.amazon.com/blogs/big-data/build-a-data-lake-foundation-with-aws-glue-and-amazon-s3/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Thoughts from O’Reilly (via the Cloudera Vision Blog) on (Hadoop) data marts in the cloud - &lt;a href=&quot;https://www.oreilly.com/ideas/rethinking-data-marts-in-the-cloud&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Following up on our look at analytical databases, an interview from ODBMS Industry Watch with Colin Mahony from Vertica - &lt;a href=&quot;http://www.odbms.org/blog/2017/10/on-vertica-and-the-new-combined-micro-focus-company-interview-with-colin-mahony/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;There’s a new vulnerability in Apache Tika - CVE-2016-6809 - allows Java code execution for serialized objects embedded in MATLAB files - &lt;a href=&quot;http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-6809&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;And one in &lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Apache Hive&lt;/a&gt; - CVE-2017-12625 - issues with column masking over views - &lt;a href=&quot;http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2017-12625&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Thoughts on Hortonworks DataPlane and Cloudera SDX</title><link>https://ondataengineering.net/blog/2017/11/03/thoughts-on-hortonworks-dataplane-and-cloudera-sdx/</link><description> &lt;p&gt;So I’ve spent most of this week reviewing and making some minor refreshes to our &lt;a href=&quot;/tech-vendors/hortonworks/&quot;&gt;Hortonworks&lt;/a&gt; and &lt;a href=&quot;/tech-vendors/cloudera/&quot;&gt;Cloudera&lt;/a&gt; information, and taking a further look at &lt;a href=&quot;/technologies/hortonworks-dataplane-service/&quot;&gt;Hortonworks DataPlane Service&lt;/a&gt; and Cloudera SDX. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Firstly, let’s review what I said in &lt;a href=&quot;/blog/2017/09/27/the-mid-week-news/&quot;&gt;The Mid Week News - 27/09/2017&lt;/a&gt;:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;The big news this week is the simultaneous big product announcements from Hortonworks and Cloudera that look like they might be similar capabilities, but I think are probably trying to solve subtly different problems - we’ll revisit these in a few weeks once there’s more information available and do some technology summaries.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.cloudera.com/products/sdx.html&quot;&gt;Cloudera SDX&lt;/a&gt; (Shared Data Experience, coming in CDH 5.13) appears to be trying to enable the “one” data platform experience that you get with an on premesis CDH cluster in the cloud, specifically a persistent shared storage layer with shared metadata, security and governance and a range of workloads on top. That looks different in the cloud - you probably don’t want a persistent Cloudera cluster that you’re paying for by the hour even if you’re not using it - so SDX gives you a shared storage layer using cloud object storage, a shared metadata and management layer, and then the ability to run compute workloads in isolated transient workload clusters managed through Cloudera Altus. The original sales pitch of a single shared Hadoop data platform re-imagined for the cloud. More details via a &lt;a href=&quot;http://vision.cloudera.com/introducing-cloudera-sdx-a-shared-data-experience-for-the-hybrid-cloud/&quot;&gt;Cloudera VISION blog post&lt;/a&gt; and a &lt;a href=&quot;http://blog.cloudera.com/blog/2017/09/cloudera-sdx-under-the-hood/&quot;&gt;Cloudera Engineering blog post&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://hortonworks.com/products/data-management/dataplane-service/&quot;&gt;Hortonworks Data Plane&lt;/a&gt; is again all about shared metadata, security and data management, but this time across a range of different data platforms - Hadoop, relational databases and your EDW, either on-premesis or in the cloud, and for data in motion or at rest. It’s open source, extensible for adding new services, with data lifecycle management being first up, allowing you to replicate, backup &amp;amp; restore and tier your data across your data platforms. It’s another cloud service (because obviously), and they talk about it as a Global Data Management Platform. More details via a &lt;a href=&quot;https://hortonworks.com/blog/category-emerges-introducing-hortonworks-dataplane-service/&quot;&gt;Hortonworks blog post&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;With Cloudera SDX, CDH 5.13 has come and gone, and there’s almost no new information about SDX. The 5.13 &lt;a href=&quot;http://community.cloudera.com/t5/Community-News-Release/ANNOUNCE-Cloudera-Enterprise-5-13-is-Now-Available/m-p/60879#M200&quot;&gt;announcement&lt;/a&gt; name checks SDX as the “SDX Cloud Reference Architecture”, which I think probably sums up what it is as much as anything, especially given there’s absolutely no reference to SDX in the Cloudera documentation, and there’s nothing on their site beyond the product page and two blog posts linked above. It feels like this is Cloudera pushing the traditional Hadoop one platform, lots of different workloads message, but now applying it to the cloud as well.&lt;/p&gt; &lt;p&gt;Hortonworks on the other hand seem to be heading in a slightly different direction with the new &lt;a href=&quot;/technologies/hortonworks-dataplane-service/&quot;&gt;Hortonworks DataPlane Service&lt;/a&gt;. The premise for this is that it becomes a single place to understand, manage and govern all the data your enterprise holds, wherever it may be - it’s a big ask, but it feels like there’s value there. Saying that it’s early days for this product is an understatement - it’s now had it’s first generally available release (see &lt;a href=&quot;https://hortonworks.com/blog/hdp-2-6-3-dataplane-service/&quot;&gt;this post&lt;/a&gt;) and there’s a big pile of documentation on the Hortonworks site, but the functionality at the moment is pretty limited, and there’s no visibility yet of plugin services coming from any of the Hortonworks partners. And this is an interesting change for Hortonworks, in that this is a commercial managed service offering and not open source software (although there’s no public sign up process yet and the documentation talks about how to install it), and it only works with Ambari managed clusters and you have to have a SmartSense ID. Which makes you wonder whether this a response to challenges in generating revenue from support and consultancy from fully open source software. It will also be interesting to see how this will impact Atlas and Ranger - you could easily see a world where a lot of the end user functionality in these products migrates into the DataPlane service. One to watch I think - it’ll be interesting to see where this goes.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 08/11/2017</title><link>https://ondataengineering.net/blog/2017/11/08/the-mid-week-news/</link><description> &lt;p&gt;It’s time to take my head out of MapR (updates and post hopefully coming tomorrow if I can push on though) and break for the news, although it’s a bit light this week… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The big one this week is that &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt; has hit 1.0&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-ignite/&quot;&gt;Apache Ignite&lt;/a&gt; has hit 2.3&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-kylin/&quot;&gt;Apache Kylin&lt;/a&gt; has hit 2.2&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Another benchmark to take with a pinch of salt - &lt;a href=&quot;/technologies/mapr-file-system/mapr-db/&quot;&gt;MapR-DB&lt;/a&gt; is faster than both Cassandra and HBase - &lt;a href=&quot;https://mapr.com/whitepapers/mike-leone-esg-lab-nosql-benchmark/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Hortonworks on extensibility in &lt;a href=&quot;/technologies/streaming-analytics-manager/&quot;&gt;Streaming Analytics Manager&lt;/a&gt; - &lt;a href=&quot;https://hortonworks.com/blog/streaming-analytics-manager-extensibility/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From The Register - it looks like the IBM Cloud is undergoing another name change - &lt;a href=&quot;https://www.theregister.co.uk/2017/11/02/ibm_renames_bluemix_ibm_cloud/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Thoughts on MapR</title><link>https://ondataengineering.net/blog/2017/11/10/thoughts-on-mapr/</link><description> &lt;p&gt;I’ve somehow spent (pretty much) all of these week taking another look at MapR - the time taken being a combination of me getting sucked down some rabbit holes, and the contradictory and confusing nature of MapR’s public material. But I think we’ve got somewhere, so let’s renew what we already thought about MapR and what might have changed recently… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Firstly, let’s review what I said in &lt;a href=&quot;/blog/2017/04/28/the-week-that-was/&quot;&gt;The Week That Was - 28/04/2017&lt;/a&gt;:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;I have to admit to being surprised by MapR’s offerings. I’d always assumed they were a knock-off Hadoop distribution that was trying to find leverage by embedding a bunch of commercial components, however what’s become clear is that what they’re selling is not an Hadoop distribution but an enterprise data platform (based on MapR-FS) that just happens to have Hadoop compatibility. In short, &lt;a href=&quot;/technologies/mapr-file-system&quot;&gt;MapR-FS&lt;/a&gt; is a highly resilient, scalable and performant, with support for full random read/write access, multi-tenancy, block level replication, snapshots, quotas, extensive and flexible access control, which supports a fully POSIX compliant filesystem with HDFS, NFS and FUSE APIs, a document and wide column datastore with OJAI and HBase APIs, a streaming data stores with Kafka compatible APIs, master-slave and master-master replication of database and streaming data stores, plus YARN support, meaning you can run any Hadoop compatible tool over the top. That’s a lot of capability in a single platform, which feels like it’s going to drive a strong TCO story.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;And in &lt;a href=&quot;/blog/2017/05/05/the-week-that-was/&quot;&gt;The Week That Was - 05/05/2017&lt;/a&gt;:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;What I’ve really liked about MapR is their strategy around their common data platform to underpin a bunch of different data storage capabilities. I talked a little bit about their data platform &lt;a href=&quot;/blog/2017/04/28/the-week-that-was/&quot;&gt;last time&lt;/a&gt;, but this week as part of looking at &lt;a href=&quot;/technologies/mapr-file-system/mapr-db/&quot;&gt;MapR-DB&lt;/a&gt; and &lt;a href=&quot;/technologies/mapr-file-system/mapr-streams/&quot;&gt;MapR-Streams&lt;/a&gt; I’ve been thinking about how this compares and contrasts to Hadoop. Firstly, they’re both aiming to provide a common data platform that provides the ability to have a single cluster than can provide flexibility and value for money by allowing you to exploit the same infrastructure for multiple use cases. MapR appears to have fully embraced this, ensuring they support the ability to scale, partition and manage the platform in ways that Hadoop can’t yet, and by providing capabilities that Hadoop (and more specifically HDFS) doesn’t that actually make it work as a general purpose data platform - full random read and write access for starters. I’m also taken by MapR’s ability to provide access to the common data platform at different layers - rather than just build capabilities on top of their file system API, they’ve integrated (for example) MapR-DB at a much lower level, providing a range of benefits over HBase running over HDFS. It’s clear that Hadoop still has a long way to go to fulfil it’s potential, and without addressing some of it’s limitations we’re going to continue to see new technologies opting to implement their own storage systems from scratch (Kudu being a great example), leading to Hadoop clusters running multiple independent storage stacks on the same data nodes, which feels like it’s defeating the point.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Looking back, that feels like it all still holds true. So what’s changed recently?&lt;/p&gt; &lt;p&gt;Well, the answer is fundamentally nothing (and yet I’ve spent almost an entire week looking at this!). In the time since we last looked at MapR there hasn’t been even a minor release (5.2 came out in August 2016) so there’s no major new functionality or product changes - my guess is they’re gearing up for a big 6.0 release given they’ve started to talk about some elements of this.&lt;/p&gt; &lt;p&gt;However, it looks like they’re having a push around widening the use cases for their &lt;a href=&quot;/technologies/mapr-converged-data-platform/&quot;&gt;Converged Data Platform&lt;/a&gt; and the number of ways they bundle and market this. When we last looked this was already also available as MapR Edge (a version designed to run on low power devices at the edge of the network) and MapR Converged Data Platform for Docker (a version designed to provide persistent storage for docker containers), however they’ve now introduce a new version - MapR-XD - a version built around MapR-FS bundled with their monitoring and management software (and one would assume the new Orbit Cloud Suite - details below), that’s being positioned as a cloud scale data store / fabric which will provide a single layer over on-premises and cloud storage, with support for automatic tiering, mirroring and replication.&lt;/p&gt; &lt;p&gt;(NOTE: MapR’s material is inconsistent in whether MapR-XD is a re-branding of MapR-FS or a new product that includes MapR-FS, however I’m going with &lt;a href=&quot;https://community.mapr.com/message/59688-what-is-the-difference-between-mapr-fs-and-mapr-xd&quot;&gt;this&lt;/a&gt; and the fact that MapR-XD has it’s own installation page separate from MapR under &lt;a href=&quot;https://maprdocs.mapr.com/home/install.html&quot;&gt;here&lt;/a&gt; and say that it’s a new product.)&lt;/p&gt; &lt;p&gt;This has then been supplemented via a couple of new add-ons.&lt;/p&gt; &lt;p&gt;Firstly the MapR Orbit Cloud Suite, which adds full cloud support to the MapR Converged Data Platform (and appears to provide a bunch of the functionality of MapR-XD), including support for deployment of cloud infrastructure along with MapR, integration with cloud object stores, plus mirroring and replication, with support for multi-tenancy, object tiering and with OpenStack integration announced.&lt;/p&gt; &lt;p&gt;The second is the MapR Data Science Refinery, a docker based analytics notebook powered by Apache Zeppelin that fully integrates with the MapR Converged Data Platform. MapR have been pushing the use of the Converged Data Platform as a data science and machine learning platform for a while now, and this feels like it supports that.&lt;/p&gt; &lt;p&gt;Oh, and they’ve renamed MapR Streams to MapR-ES (Event Streams), as &lt;a href=&quot;https://community.mapr.com/thread/21827-what-is-mapr-es-event-data-streams&quot;&gt;apparently&lt;/a&gt; people were often wrongly assuming it was a streaming engine like Storm or Flink.&lt;/p&gt; &lt;p&gt;So that’s that. If you haven’t already, I strongly suggest you have a read through the information we have on MapR on this site, starting with our &lt;a href=&quot;/tech-vendors/mapr/&quot;&gt;MapR&lt;/a&gt; vendor page and following the links through to the &lt;a href=&quot;/technologies/mapr-converged-data-platform/&quot;&gt;MapR Converged Data Platform&lt;/a&gt; and onwards to the products it packages and is managed by. It’ll only take you 10 minutes I promise!&lt;/p&gt; &lt;p&gt;We’ll be back next week with our final catch up week before we launch into some new technology categories. See you then.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 15/11/2017</title><link>https://ondataengineering.net/blog/2017/11/15/the-mid-week-news/</link><description> &lt;p&gt;News time again, let’s catch up on what’s new… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/elasticsearch/&quot;&gt;Elasticsearch&lt;/a&gt; has hit the big 6.0, along with &lt;a href=&quot;/technologies/elastic-x-pack/&quot;&gt;X-Pack&lt;/a&gt; and &lt;a href=&quot;/technologies/elasticsearch-hadoop/&quot;&gt;Elasticsearch Hadoop&lt;/a&gt;, with &lt;a href=&quot;/technologies/elastic-cloud/&quot;&gt;Elastic Cloud&lt;/a&gt; up to 1.1&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-apex/&quot;&gt;Apache Apex&lt;/a&gt; has seen a 3.8 release of it’s Malhar library&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-phoenix/&quot;&gt;Apache Phoenix&lt;/a&gt; has seen a 4.13 release&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/influxdb/&quot;&gt;InfluxDB&lt;/a&gt; is up to 1.4&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/zenko/cloudserver/&quot;&gt;Zenko CloudServer&lt;/a&gt; is up to 7.1&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ZDNet have an interview with the CEO of Elastic - &lt;a href=&quot;http://www.zdnet.com/article/elasticsearch-6-0-not-that-new-but-quite-improved/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;IBM Data Science Experience (DSX) is now certified on &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;HDP&lt;/a&gt; - &lt;a href=&quot;https://hortonworks.com/blog/certification-ibm-data-science-experience-dsx-hdp-win-win-customers/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;An interview from ODBMS Industry Watch with Head of Data Technical Field for Pivotal and the founder and CEO Datometry. There’s some significant product plugs in there, but it’s worth a read - &lt;a href=&quot;http://www.odbms.org/blog/2017/11/on-the-future-of-data-warehousing-interview-with-jacque-istok-and-mike-waas/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From DataArtisans - an updated on CEP with &lt;a href=&quot;/technologies/apache-flink/&quot;&gt;Apache Flink&lt;/a&gt; - &lt;a href=&quot;https://data-artisans.com/blog/complex-event-processing-flink-cep-update&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;More from Confluent for using &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt; and KSQL in micro services architectures - &lt;a href=&quot;https://www.confluent.io/blog/building-a-microservices-ecosystem-with-kafka-streams-and-ksql/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Plan For This Week - 20/11/2017</title><link>https://ondataengineering.net/blog/2017/11/20/the-plan-for-this-week/</link><description> &lt;p&gt;So I said last week was going to be the last catch up week, but (as is becoming a depressingly predictable pattern recently) I’m going to overrun.&lt;/p&gt; &lt;p&gt;However, I’ve been taking the time to do some well needed structural updates to the site - hopefully sorting my SEO problem, and allowing me to publish new custom technology, vendor and category index pages this week.&lt;/p&gt; &lt;p&gt;So, bear with me please. Some final updates this week and then next we’ll start on our final set of technology categories before we move onto Chapter 3.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 22/11/2017</title><link>https://ondataengineering.net/blog/2017/11/22/the-mid-week-news/</link><description> &lt;p&gt;It’s Wednesday, which means it’s time for the news, and there’s big MapR and Microsoft Azure announcements this week… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Hadoop&lt;/a&gt; version 2.9 is out&lt;/li&gt; &lt;li&gt;The &lt;a href=&quot;/technologies/apache-nifi/minifi/&quot;&gt;MiNiFi&lt;/a&gt; C++ version has seen a 0.3 release&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/greenplum/&quot;&gt;Greenplum&lt;/a&gt; continues it’s regular release schedule with a 5.2 release&lt;/li&gt; &lt;li&gt;Hortonworks &lt;a href=&quot;/technologies/cloudbreak/&quot;&gt;Cloudbreak&lt;/a&gt; has a 2.1 technical preview out with all new documentation&lt;/li&gt; &lt;li&gt;The &lt;a href=&quot;/technologies/elasticsearch/&quot;&gt;Elasticsearch&lt;/a&gt; 6.0 details have been updated with a link to their blog post on the removal of mapping types&lt;/li&gt; &lt;li&gt;The &lt;a href=&quot;/technologies/cloudera-data-science-workbench/&quot;&gt;Cloudera Data Science Workbench&lt;/a&gt; 1.2 details have been updated with a link to the latest Cloudera blog post&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The &lt;a href=&quot;/technologies/mapr-converged-data-platform/&quot;&gt;MapR Converged Data Platform&lt;/a&gt; v6.0 is out - we’ll wait for the dust to settle a bit before updating the site - &lt;a href=&quot;https://mapr.com/products/whats-new/6-0/&quot;&gt;what’s new&lt;/a&gt;; &lt;a href=&quot;http://www.zdnet.com/article/mapr-6-0-converges-control-of-data-at-rest-and-in-motion-on-the-same-pane-of-glass/&quot;&gt;ZDNet write-up&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Microsoft’s big Connect event has thrown out a bunch of announcements: &lt;ul&gt; &lt;li&gt;Azure Databricks is a new service jointly developed with Databricks that brings Spark as a service as a first class citizen into Azure - we’ll look more at this in the coming weeks I think - &lt;a href=&quot;https://azure.microsoft.com/en-gb/blog/a-technical-overview-of-azure-databricks/&quot;&gt;link&lt;/a&gt;; &lt;a href=&quot;https://databricks.com/blog/2017/11/15/introducing-azure-databricks.html&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Azure Cosmos DB (their NoSQL database) now supports an Apache Cassandra compatible API to join the existing SQL, Gemlin (Neo4j), MongoDB and Azure Table Store APIs - making it a true multi model database supporting wide column storage, graph, relational, document and key value store use cases - &lt;a href=&quot;https://azure.microsoft.com/en-gb/blog/azure-cosmosdb-microsoft-connect-2017/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Azure Time Series Insights (TSI) - their time series databases has hit general availability - &lt;a href=&quot;https://azure.microsoft.com/en-gb/blog/microsoft-announces-the-general-availability-of-azure-time-series-insights/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Azure Database for MariaDB has been announced in preview - &lt;a href=&quot;https://azure.microsoft.com/en-gb/blog/mariadb-postgresql-and-mysql-more-choices-on-microsoft-azure/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;ZDNet have a good write up - &lt;a href=&quot;http://www.zdnet.com/article/microsoft-gets-data-fabulous-at-nyc-event/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;There’s also a summary from Microsoft themselves - &lt;a href=&quot;https://azure.microsoft.com/en-gb/blog/connect-2017-gettopannouncementslist-cloud-data-ai/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;From DZone - an introduction to Pulsar - &lt;a href=&quot;https://dzone.com/articles/a-developers-introduction-to-the-pulsar-streaming&quot;&gt;link&lt;/a&gt;, the Kafka alternative from Yahoo&lt;/li&gt; &lt;li&gt;A view from Kognitio on why it’s not open source (although it can be used over Hadoop for free), and what their differentiators are - &lt;a href=&quot;https://kognitio.com/blog/why-isnt-kognitio-open-source/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A good write up from the Knoldus blog (which is always good value for money) on the architecture of &lt;a href=&quot;/technologies/apache-storm/&quot;&gt;Apache Storm&lt;/a&gt; - &lt;a href=&quot;https://blog.knoldus.com/2017/11/14/apache-storm-architecture/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Influx have announced IFQL - a new query language for &lt;a href=&quot;/technologies/influxdb/&quot;&gt;InfluxDB&lt;/a&gt; - &lt;a href=&quot;https://www.influxdata.com/blog/announcing-ifql-a-new-query-language-and-engine-for-influxdb/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;An introduction from Cloudera to &lt;a href=&quot;/technologies/cloudera-data-science-workbench/&quot;&gt;Cloudera Data Science Workbench&lt;/a&gt; and &lt;a href=&quot;/technologies/cloudera-altus/&quot;&gt;Cloudera Altus&lt;/a&gt; - &lt;a href=&quot;http://blog.cloudera.com/blog/2017/11/cloud-scale-modeling-with-cloudera-altus/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Plan For This Week - 27/11/2017</title><link>https://ondataengineering.net/blog/2017/11/27/the-plan-for-this-week/</link><description> &lt;p&gt;So having (pretty much) finished my extended and rather more painful tidy up of what we’ve done so far, I’ve got a bunch of content to put live this week, before we move on to a brand new technology category next week. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;For the record, the &lt;a href=&quot;/technologies/&quot;&gt;technology&lt;/a&gt;, &lt;a href=&quot;/tech-vendors/&quot;&gt;technology vendor&lt;/a&gt; and &lt;a href=&quot;/tech-categories/&quot;&gt;technology category&lt;/a&gt; index pages are now part of the content site, meaning that these are now editable as per any other content page. They’ll be a bunch of changes to these going live this week.&lt;/p&gt; &lt;p&gt;I’ve also slightly changed the structure of technology category pages (and will do a similar thing for technology and technology vendor pages in due course). The description of these pages is now a meta description of the page rather than a summary of the technology category, with this moved down to be the initial text in the content.&lt;/p&gt; &lt;p&gt;Oh, add the draft banner has now moved to the left hand sidebar.&lt;/p&gt; &lt;p&gt;The content this week is coming from me digging through my old notes I made when planning the site, which includes a few technology summaries I did whilst planning what I wanted to do. We’ll stick these live this week, along with a bunch of updates to the technology category pages, and a new re-designed technology category index page.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 29/11/2017</title><link>https://ondataengineering.net/blog/2017/11/29/the-mid-week-news/</link><description> &lt;p&gt;To tide you over whilst the content I promised never arrives, let’s look at the news, and it feels like a interesting crop this week… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The Confluent platform - &lt;a href=&quot;/technologies/confluent-open-source/&quot;&gt;Confluent Open Source&lt;/a&gt; and &lt;a href=&quot;/technologies/confluent-enterprise/&quot;&gt;Confluent Enterprise&lt;/a&gt; - has hit 4.0 based on the Apache Kafka 1.0 release&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cloudera have announced &lt;a href=&quot;/technologies/cloudera-altus/analytical-db/&quot;&gt;Altus Analytical DB&lt;/a&gt; - their cloud based SQL analytics service. We’ll take a deeper look at this in a few week. &lt;a href=&quot;http://vision.cloudera.com/introducing-cloudera-altus-analytic-db-beta-for-cloud-based-data-warehousing/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zenko Orbit has been announced - a web based portal for managing object storage data across multiple clouds, for example when used behind &lt;a href=&quot;/technologies/zenko/cloudserver/&quot;&gt;Zenko CloudServer&lt;/a&gt; - &lt;a href=&quot;https://www.zenko.io/blog/introducing-zenko-orbit/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Confluent Cloud is now generally available on AWS - &lt;a href=&quot;https://www.confluent.io/blog/confluent-cloud-enterprise-ready-hosted-apache-kafka/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;If you’ve upgraded to Elastic Cloud 1.1 and Kibana’s not available - Elastic have a workaround for you! &lt;a href=&quot;https://www.elastic.co/blog/elastic-cloud-enterprise-1-1-0-upgrade-issues&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;This is pretty neat - from “the morning paper”, a in-memory key value store implemented directly on NIC FPGAs that can do 1.22 billion KV operations per second when running on 10 NICs - &lt;a href=&quot;https://blog.acolyer.org/2017/11/23/kv-direct-high-performance-in-memory-key-value-store-with-programmable-nic/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Confluent, an update on transactions in &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt; - &lt;a href=&quot;https://www.confluent.io/blog/transactions-apache-kafka/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;An update on the upcoming features in &lt;a href=&quot;/technologies/apache-flink/&quot;&gt;Apache Flink&lt;/a&gt; 1.4 and 1.5 - &lt;a href=&quot;http://flink.apache.org/news/2017/11/22/release-1.4-and-1.5-timeline.html&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-impala/&quot;&gt;Apache Impala&lt;/a&gt; has graduated from the Apache Incubator - &lt;a href=&quot;https://blogs.apache.org/foundation/entry/the-apache-software-foundation-announces24&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Technology Categories</title><link>https://ondataengineering.net/blog/2017/12/01/technology-categories/</link><description> &lt;p&gt;So this has taken far too long, and some proper updates to this site are long overdue, but let’s summarise what I’ve been looking at for the last three weeks. And yes, I said they’d be new content last week, but in the end the stuff I had (which was old from when I was planning the site) just wasn’t up to scratch, so it’s been binned rather than published. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;So the big update has been some (minor) structural changes. Firstly, the &lt;a href=&quot;/technologies/&quot;&gt;technology&lt;/a&gt;, &lt;a href=&quot;/tech-categories/&quot;&gt;technology category&lt;/a&gt; and &lt;a href=&quot;/tech-vendors/&quot;&gt;technology vendor&lt;/a&gt; index pages are now part of the open source content repository for this site, meaning that they’re editable just like any of the technology, vendor or category pages. And secondly, the technology category pages now have a meta description under the header, with the summary of the technology category now moved into the body of the content.&lt;/p&gt; &lt;p&gt;And with the new index pages has come a completely redesigned &lt;a href=&quot;/tech-categories/&quot;&gt;technology category index page&lt;/a&gt;, with all the technology categories now organised into sections (categories!), with a bit more description and structure. And as part of doing this I’ve taken the opportunity to review the technology categories we have and make some tweaks and adjustments - I’m still not entirely happy, but I’ve spent far too much time thinking about it, and it’s time to move on.&lt;/p&gt; &lt;p&gt;But I have made some updates to the technology categories as follows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I’ve moved/copied a bunch of commercial and open source Hadoop based technologies from the &lt;a href=&quot;/tech-categories/query-engines/&quot;&gt;query engines&lt;/a&gt; page to the &lt;a href=&quot;/tech-categories/analytical-databases/&quot;&gt;analytical databases&lt;/a&gt; page. I’ve debated for far to long what the different between these categories are, but I think I’ve got to somewhere I’m comfortable with, and hopefully the technology category descriptions now reflect this.&lt;/li&gt; &lt;li&gt;I’ve added RecordService and the Hive Metastore to the Data Storage Services section of the &lt;a href=&quot;/tech-categories/data-storage-formats/&quot;&gt;data storage formats&lt;/a&gt; page. I’m thinking about splitting this section out into it’s own category at some point, as the storage of structured data that can then be exploited by multiple tools is an area that requires more consideration.&lt;/li&gt; &lt;li&gt;I’ve moved the analytical graph databases from the &lt;a href=&quot;/tech-categories/graph-analytics/&quot;&gt;graph analytics&lt;/a&gt; page into the &lt;a href=&quot;/tech-categories/analytical-databases/&quot;&gt;analytical databases&lt;/a&gt; page, leaving the graph analytics page to focus on graph analytics run over external data. I’ve also broken out the &lt;a href=&quot;/tech-categories/graph-databases/&quot;&gt;graph databases&lt;/a&gt; that also support analytics into their own section.&lt;/li&gt; &lt;li&gt;The &lt;a href=&quot;/tech-categories/in-memory-databases/&quot;&gt;in memory databases&lt;/a&gt; page is no longer included on the index page, as all the information is now replicated to other pages with the exception of some operational relational database stuff. Once this has a new home, we’ll drop this category.&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Plan For This Week - 04/12/2017</title><link>https://ondataengineering.net/blog/2017/12/04/the-plan-for-this-week/</link><description> &lt;p&gt;Right, time to push on with more technology categories and actually add some meaningful content to this site, and we’re going to focus on data movement, ingestion and transformation tools for a little while.&lt;/p&gt; &lt;p&gt;First up this week, technologies for the continuous ingestion of event data.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 06/12/2017</title><link>https://ondataengineering.net/blog/2017/12/06/the-mid-week-news/</link><description> &lt;p&gt;Right - time for your weekly updates on new software releases and interesting new information and posts, with a big dump from AWS re:Invent this week… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-beam/&quot;&gt;Apache Beam&lt;/a&gt; has hit 2.2&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/druid/&quot;&gt;Druid&lt;/a&gt; has hit 0.11&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;After the Azure product dump a few weeks ago, it’s Amazon’s turn via AWS re:Invent: &lt;ul&gt; &lt;li&gt;Amazon Neptune - a graph/RDF database as a service with support for TinkerPop Gremlin and RDF SPARQL - &lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2017/11/amazon-neptune-fast-reliable-graph-database-built-for-the-cloud/&quot;&gt;announcement&lt;/a&gt; and &lt;a href=&quot;https://aws.amazon.com/blogs/aws/amazon-neptune-a-fully-managed-graph-database-service/&quot;&gt;blog&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Amazon SageMaker - service for building, training and deploying machine learning at scale - &lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2017/11/introducing-amazon-sagemaker/&quot;&gt;announcement&lt;/a&gt; and &lt;a href=&quot;https://aws.amazon.com/blogs/aws/sagemaker/&quot;&gt;blog&lt;/a&gt;&lt;/li&gt; &lt;li&gt;AWS Fargate - provisioning of containers on AWS without managing servers or clusters - &lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2017/11/introducing-aws-fargate-a-technology-to-run-containers-without-managing-infrastructure/&quot;&gt;announcement&lt;/a&gt; and &lt;a href=&quot;https://aws.amazon.com/blogs/aws/aws-fargate/&quot;&gt;blog&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Elastic Kubernetes Service (EKS) - Kubernetes as a service - &lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2017/11/introducing-amazon-elastic-container-service-for-kubernetes/&quot;&gt;announcement&lt;/a&gt; and &lt;a href=&quot;https://aws.amazon.com/blogs/aws/amazon-elastic-container-service-for-kubernetes/&quot;&gt;blog&lt;/a&gt;&lt;/li&gt; &lt;li&gt;S3 Select and Glacier Select - retrieve subsets of stored objects by running select queries server side - &lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2017/11/amazon-s3-select-is-now-available-in-limited-preview/&quot;&gt;S3 announcement&lt;/a&gt;, &lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2017/11/amazon-glacier-select-makes-big-data-analytics-of-archive-data-possible/&quot;&gt;Glacier announcement&lt;/a&gt; and &lt;a href=&quot;https://aws.amazon.com/blogs/aws/s3-glacier-select/&quot;&gt;blog&lt;/a&gt;&lt;/li&gt; &lt;li&gt;See also summaries from &lt;a href=&quot;https://www.theregister.co.uk/2017/11/29/amazon_aws_kubernetes/&quot;&gt;The Register&lt;/a&gt;, from &lt;a href=&quot;https://www.infoq.com/news/2017/12/aws-reinvent-day-one&quot;&gt;InfoQ&lt;/a&gt;, and the &lt;a href=&quot;https://aws.amazon.com/blogs/aws/category/events/reinvent/&quot;&gt;motherlist of blog posts&lt;/a&gt; relating to re:Invent from Amazon&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;From Cloudera, infrastructure considerations for deploying &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt; - &lt;a href=&quot;http://blog.cloudera.com/blog/2017/11/deploy-cloudera-edh-clusters-like-a-boss-revamped-part-1/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;MapR have posted their thoughts on &lt;a href=&quot;/technologies/apache-drill/&quot;&gt;Apache Drill&lt;/a&gt; as part of the &lt;a href=&quot;/technologies/mapr-converged-data-platform/&quot;&gt;MapR Converged Data Platform&lt;/a&gt;, and their view of it as “a unified SQL access layer across files, tables and streams”, along (of course) with some new benchmarks - &lt;a href=&quot;https://community.mapr.com/community/products/blog/2017/11/29/apache-drill-version-111-on-mapr-release-overview&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;An interesting post of MariaDB AX, the data warehouse solution from MariaDB that’s built on MariaDB ColumnStore, on bulk and streaming ingestion of data - &lt;a href=&quot;https://mariadb.com/resources/blog/real-time-data-streaming-mariadb-ax&quot;&gt;link&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;AtScale now runs over Amazon RedShift - &lt;a href=&quot;http://blog.atscale.com/atscale_aws_redshift&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Confluent have a new blog post on Confluent Platform 4.0 (&lt;a href=&quot;/technologies/confluent-open-source/&quot;&gt;Confluent Open Source&lt;/a&gt; and &lt;a href=&quot;/technologies/confluent-enterprise/&quot;&gt;Confluent Enterprise&lt;/a&gt;) - &lt;a href=&quot;https://www.confluent.io/blog/introducing-confluent-platform-4-0/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From ZDNet, an interview on &lt;a href=&quot;/technologies/apache-flink/&quot;&gt;Apache Flink&lt;/a&gt; and thoughts on the wider ecosystem - &lt;a href=&quot;http://www.zdnet.com/article/real-time-applications-are-going-places/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Google, another post on the separation of storage and compute with BigQuery - &lt;a href=&quot;https://cloud.google.com/blog/big-data/2017/11/separation-of-storage-and-compute-in-bigquery&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.crail.io/&quot;&gt;Crail&lt;/a&gt; has been accepted to the Apache Incubator - we last saw this in October when it was submitted, so that’s a pretty quick turn around. As a recap, this looks like a high performance distributed and tiered (in memory, flash and disk) storage layer for temporary data that provides memory, storage and network access that bypasses the JVM and OS, and with integration to Spark (as a custom Spark Suffler that improves sort performance by a factor of five) and Hadoop (via an HDFS adaptor).&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Thoughts on Data Ingestion Technologies</title><link>https://ondataengineering.net/blog/2017/12/08/thoughts-on-data-ingestion-technologies/</link><description> &lt;p&gt;So we’ve been looking at &lt;a href=&quot;/tech-categories/data-ingestion/&quot;&gt;data ingestion&lt;/a&gt; technologies - let’s talk about it… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Before you can exploit data, you need to get it into a position where you can exploit it. In some limited cases that might be where it currently sites, but in most cases that means moving it to somewhere you can run more expansive and expensive analytics.&lt;/p&gt; &lt;p&gt;The challenge here is the potential range of different sources you might want to acquire data from - event logs, relational databases, NoSQL databases, mainframes, application APIs, cloud platforms, file based feeds etc. And although any good data integration or transformation tool will include adapters to read data from a wide range of sources, reliably getting data in is always a challenge, and you’ll quickly end up either building a framework for ingesting data or with huge amounts of copied and pasted code.&lt;/p&gt; &lt;p&gt;So what are these challenges? Let’s brain dump:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supporting both batch and continuous ingestion of data&lt;/li&gt; &lt;li&gt;If you’re pulling data, working out what data needs acquiring (that hasn’t been acquired before), and if it’s being pushed to you, making sure that you’re not missing anything&lt;/li&gt; &lt;li&gt;Support for acquiring full snapshots, new data and CDC feeds (containing new, changed and deleted records)&lt;/li&gt; &lt;li&gt;Capturing data continuously for systems that aren’t designed for it (mainframes, databases, applications)&lt;/li&gt; &lt;li&gt;Distributed collection - do you need to run agents on remote machines to capture data, and maybe pre-do some processing nearer the edge before forwarding it on&lt;/li&gt; &lt;li&gt;For file based feeds, file based checks, including headers and footers, completeness and format&lt;/li&gt; &lt;li&gt;Resilience and reliability - if you’re receiving a million records a minute what happens when you’re down&lt;/li&gt; &lt;li&gt;Tracking what data you’ve received and when you received it&lt;/li&gt; &lt;li&gt;Checking that there are things changing in your source that’s going to impact your analytics - has it introduced new fields, new values for fields you’re not expecting, changed data types; do the fields still contain what they contained when you originally wrote your analytics?&lt;/li&gt; &lt;li&gt;Tracking metrics of the data you’re receiving - do you have stats (and graphs) or how much data you’re receiving, what the profile of the data looks like so you can spot trends and deviations?&lt;/li&gt; &lt;li&gt;Acquiring new data without spending weeks writing, debugging and testing lots of code&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;TLDR - it’s easy to acquire data, it’s really difficult to do this in a reliable and robust way that doesn’t include huge amounts of development and maintenance.&lt;/p&gt; &lt;p&gt;And this is where specialist data ingestion tools come in. They don’t do anything a general purpose data transformation / ingestion tool can’t do - they just come with a huge amount of the functionality you need for ingesting data out of the box that you’d otherwise have to build yourself, and focusing on making this as simple, quick and robust as possible.&lt;/p&gt; &lt;p&gt;If I need to spell this out any more clearly - if you’re not using a data ingestion tool to capture your data, then it’ll be well worth your time looking at one.&lt;/p&gt; &lt;p&gt;And there’s a really interesting range to choose from now. If you’re looking for a generally purpose tool, there’s a range of open source and commercial offers available, from &lt;a href=&quot;/technologies/apache-nifi/&quot;&gt;Apache Nifi&lt;/a&gt; which we’ve looked at both, to &lt;a href=&quot;/technologies/streamsets-data-collector/&quot;&gt;StreamSets Data Collector&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-gobblin/&quot;&gt;Apache Gobblin (Incubating)&lt;/a&gt; that we looked at this week.&lt;/p&gt; &lt;p&gt;And then there’s a range of capabilities that have different focuses, from continuous event ingestion (or log shipping), to database unloads and continuous ingestion from databases.&lt;/p&gt; &lt;p&gt;And please remember - this is about the aquisition of data and not transformation or aggregation for analytics. You’re trying to get it to your analytics platform in a state where it’s ready for onward processing, but without doing so much work that the chance of a failure means you’ve got lots of data lying on the floor. We’ll look at tools for doing the analytics soon…&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Plan For This Week - 11/12/2017</title><link>https://ondataengineering.net/blog/2017/12/11/the-plan-for-this-week/</link><description> &lt;p&gt;So we looked at data ingestion tools last week, however there was once type that we didn’t include that I want to look at this week, and that’s data wrangling tools - tools designed for the one off manual ingestion of data, focusing on quickly and easily cleaning and standardising data from whatever form it’s in into a standard form for onwards processing.&lt;/p&gt; &lt;p&gt;See you at the end of the week…&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 13/12/2017</title><link>https://ondataengineering.net/blog/2017/12/13/the-mid-week-news/</link><description> &lt;p&gt;It’s time for your weekly dose of the news, but don’t worry, it’s a fairly light one this week… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-calcite/&quot;&gt;Apache Calcite&lt;/a&gt; has hit 1.5&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-flink/&quot;&gt;Apache Flink&lt;/a&gt; has hit 1.4&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-kudu/&quot;&gt;Apache Kudu&lt;/a&gt; has hit 1.6&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/greenplum/&quot;&gt;Greenplum&lt;/a&gt; has hit 5.3&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;From Cloudera, a deep dive into &lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Hadoop&lt;/a&gt; security and delegation tokens - &lt;a href=&quot;http://blog.cloudera.com/blog/2017/12/hadoop-delegation-tokens-explained/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From The Register, thoughts on &lt;a href=&quot;/tech-categories/object-stores/&quot;&gt;object storage&lt;/a&gt; and the increasing challenge of scale out file systems - &lt;a href=&quot;https://www.theregister.co.uk/2017/12/11/the_failure_of_object_storage/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Thoughts on choosing an &lt;a href=&quot;/tech-categories/hadoop-distributions/&quot;&gt;Hadoop Distribution&lt;/a&gt; from Kognitio - &lt;a href=&quot;https://kognitio.com/blog/which-hadoop-distribution-is-right-for-you/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From the SmartCat blog (via DZone), problems with &lt;a href=&quot;/technologies/apache-kafka/kafka-streams/&quot;&gt;Kafka Stream&lt;/a&gt; - &lt;a href=&quot;https://www.smartcat.io/blog/2017/problem-with-kafka-streams&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From InfoQ - thoughts on Amazon Nepture, their new cloud graph database - &lt;a href=&quot;https://www.infoq.com/news/2017/12/amazon-neptune?utm_campaign=infoq_content&amp;amp;utm_source=infoq&amp;amp;utm_medium=feed&amp;amp;utm_term=Database&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Week That Was</title><link>https://ondataengineering.net/blog/2017/12/15/the-week-that-was/</link><description> &lt;p&gt;So I’ve been looking at self service data preparation tools this week, and it’s fair to say that once again the topic at hand has turned out to be far much more that I expected… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Last week we looked at data ingest - getting data to your analytical platform from which point you could then conform, standardise, integrate and otherwise prepare it for analytics. This is difficult - the complexity of this preparation, the variety and volume of input data, the potentially widely varying levels of data quality, and the range of analytics you might want to do make this extreemly challenging - just look at every failed or massively overrun BI or analytics project.&lt;/p&gt; &lt;p&gt;This is going to be a really important area for us to look at one this site - perhaps the most important one. And these week I’ve stumbled into it by accident before I was ready.&lt;/p&gt; &lt;p&gt;Self service data preparation tools - I’ve seen some of these before I thought - they’re basically tools that allow you to do basic ingestion and transformation of ad-hoc data sources, often targeting power users or analysts rather than data engineers.&lt;/p&gt; &lt;p&gt;And that may have been true a few years ago, but it’s clear this is an area that’s seen massive change over the last few years, to the point where there are now a huge range of tools covering a range of capabilities including data cataloging (crawling your data sources and constructing models of how it all fits together, often supported by machine learning), data profiling, test data management, data preparation (targeting both analysts and power users with user friendly and powerful graphical user interfaces and data engineers via extensions to existing and established data integration and transformation tools) as well as all the follow up stuff including workflow management, data quality management, metadata management and data governance.&lt;/p&gt; &lt;p&gt;And these tools all cover different capabilities - although there are some stand alone tools there’s a huge range that cover multiple capabilities, from traditional data integration tools that have added new functionality, data lake management tools, analytics tools that including data ingest/preparation functionality, semantic web technologies, data warehouse automation tools, and all in one end to end analytical tools.&lt;/p&gt; &lt;p&gt;So it’s going to take me a while to get to the bottom of these, and I feel like I have a lot of reading to do.&lt;/p&gt; &lt;p&gt;For now, we’re going to take a three week break for Christmas, back on the 8th of January. When we come back we might take a quick look at streaming analytics, maybe a bit of a review of commercial analyst reporting, and then we’ll dive headlong into this.&lt;/p&gt; &lt;p&gt;Have a good holiday everyone, and we’ll see you soon…&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Plan For This Week - 08/01/2018</title><link>https://ondataengineering.net/blog/2018/01/08/the-plan-for-this-week/</link><description> &lt;p&gt;Welcome back - I hope you’ve had some time off and a relaxing break, I certainly have.&lt;/p&gt; &lt;p&gt;It’s time to get back into it, however before we push on I’m going to take this week just to catch up with a bunch of minor updates to existing technology and technology category pages based on stuff that’s happened over the last few months that I’ve not had time to fold into the site.&lt;/p&gt; &lt;p&gt;Expend a bumper news update on Wednesday, and a post on Friday with some details of the changes.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 10/01/2018</title><link>https://ondataengineering.net/blog/2018/01/10/the-mid-week-news/</link><description> &lt;p&gt;It’s the first news back after the Christmas break, so brace yourself - it’s a massive bumper jam packed edition… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The big one this week is &lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Apache Hadoop&lt;/a&gt; 3.0 - there’s links to the release note on our &lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Hadoop&lt;/a&gt; page and some links below to some commentry&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/elasticsearch/&quot;&gt;Elasticsearch&lt;/a&gt; has hit 6.1, along with &lt;a href=&quot;/technologies/elastic-x-pack/&quot;&gt;X-Pack&lt;/a&gt; and &lt;a href=&quot;/technologies/elasticsearch-hadoop/&quot;&gt;Elasticsearch Hadoop&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-solr/&quot;&gt;Apache Solr&lt;/a&gt; 7.2 is out&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-hbase/&quot;&gt;Apache HBase&lt;/a&gt; 1.4 is out&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-drill/&quot;&gt;Apache Drill&lt;/a&gt; is up to 1.12 - Kafka support is interesting&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-knox/&quot;&gt;Apache Knox&lt;/a&gt; has hit 0.14&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-arrow/&quot;&gt;Apache Arrow&lt;/a&gt; has hit 0.8&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/pravega/&quot;&gt;Pravega&lt;/a&gt; - the Kafka challenger - has hit 0.2&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-nifi/minifi&quot;&gt;MiNiFi&lt;/a&gt; has seen 0.3 releases of it’s Java version&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudbreak/&quot;&gt;Cloudbreak&lt;/a&gt; has seen it’s second 2.x technology preview release - 2.2&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Both ZDNet and Datanami have posts on &lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Hadoop&lt;/a&gt; 3.0 and what the roadmap past this looks like - &lt;a href=&quot;http://www.zdnet.com/article/hadoop-3-confronts-the-realities-of-storage-growth/&quot;&gt;ZDNet&lt;/a&gt;; &lt;a href=&quot;https://www.datanami.com/2017/12/15/hadoop-3-0-ships-roadmap-reveal/&quot;&gt;Datanami&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Blog posts have appeared for &lt;a href=&quot;/technologies/apache-kudu/&quot;&gt;Kudu&lt;/a&gt; 1.6 and &lt;a href=&quot;/technologies/greenplum/&quot;&gt;Greenplum&lt;/a&gt; 5.3 that have been added to their technology pages. Greenplum is looking to move to a fully containerised deployment model - which is interesting.&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/azure-hdinsight/&quot;&gt;Azure HDInsight&lt;/a&gt; has seen a big price reduction and a bunch of new announcements - &lt;a href=&quot;https://azure.microsoft.com/en-gb/blog/azure-hdinsight-announcements-significant-price-reduction-and-amazing-new-capabilities/&quot;&gt;link&lt;/a&gt;; &lt;a href=&quot;http://www.zdnet.com/article/microsofts-cloud-big-data-service-cuts-prices-up-to-52/&quot;&gt;ZDNet commentary&lt;/a&gt;&lt;/li&gt; &lt;li&gt;An excellent article from Ehud Kaldor and SwiftStack on the differences between NFS and &lt;a href=&quot;/tech-categories/object-stores/&quot;&gt;Object Storage&lt;/a&gt; - &lt;a href=&quot;https://www.swiftstack.com/blog/2018/01/04/nasing-object-storage/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Hortonworks have published a set of pre-canned streaming analytics projects using &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;HDP&lt;/a&gt; and &lt;a href=&quot;/technologies/hortonworks-data-flow/&quot;&gt;HDF&lt;/a&gt;, including Ad Serving, Clickstream Analysis and Predictive Maintenance - &lt;a href=&quot;https://hortonworks.com/blog/applying-big-data-streaming-analytics-in-real-world/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A couple of old Databricks announcements we didn’t cover at the time for some reason &lt;ul&gt; &lt;li&gt;Databricks Unified Analytics Platform - Databricks runtime + interactive collaborative notebooks and dashboards + production job / notebook scheduling + enterprise security - &lt;a href=&quot;https://databricks.com/product/unified-analytics-platform&quot;&gt;homepage&lt;/a&gt;; &lt;a href=&quot;https://databricks.com/blog/2017/10/05/build-complex-data-pipelines-with-unified-analytics-platform.html&quot;&gt;blog&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Databricks Delta - a service over cloud blog stores like S3 that adds ACID transactions and support for automatic data indexing - &lt;a href=&quot;https://databricks.com/product/databricks-delta&quot;&gt;homepage&lt;/a&gt;; &lt;a href=&quot;https://databricks.com/blog/2017/10/25/databricks-delta-a-unified-management-system-for-real-time-big-data.html&quot;&gt;blog&lt;/a&gt;&lt;/li&gt; &lt;li&gt;And some thoughts from ZDNet - &lt;a href=&quot;http://www.zdnet.com/article/the-future-of-the-future-spark-big-data-insights-streaming-and-deep-learning-in-the-cloud/&quot;&gt;Spark in the cloud&lt;/a&gt;; &lt;a href=&quot;http://www.zdnet.com/article/databricks-is-no-longer-playing-david-and-goliath/&quot;&gt;Databricks strategy&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Merv Adrian’s latest Hadoop tracker is up detailing the component versions used by the major &lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Hadoop&lt;/a&gt; vendors - &lt;a href=&quot;https://blogs.gartner.com/merv-adrian/2018/01/03/january-2018-hadoop-tracker/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;If you’ve got some time for reading, AtScale have a list of their top 10 posts and articles from 2017 - &lt;a href=&quot;http://blog.atscale.com/what-youve-missed-in-2017&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From ZDNet, their thoughts on big data in 2018 and the move to the cloud - &lt;a href=&quot;http://www.zdnet.com/article/big-data-2018-cloud-storage-becomes-the-de-facto-data-lake/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;The excellent db-engines site have announced their database of the year - &lt;a href=&quot;https://db-engines.com/en/blog_post/76&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;DZone have published a Refcard for &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Kafka&lt;/a&gt; covering a whole pile of useful getting started information - &lt;a href=&quot;https://dzone.com/refcardz/apache-kafka&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A good write up of the features in &lt;a href=&quot;/technologies/elasticsearch/&quot;&gt;Elasticsearch&lt;/a&gt; 6.0 from Logz.io - &lt;a href=&quot;https://logz.io/blog/elastic-stack-6-new/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Are you running &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Kafka&lt;/a&gt; - we have a couple of posts this week from NewRelic and Confluent on monitoring it - &lt;a href=&quot;https://blog.newrelic.com/2017/12/12/new-relic-kafkapocalypse/&quot;&gt;NewRelic&lt;/a&gt;; &lt;a href=&quot;https://www.confluent.io/blog/blog-post-on-monitoring-an-apache-kafka-deployment-to-end-most-blog-posts&quot;&gt;Confluent&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/microsoft-azure-blob-storage/&quot;&gt;Azure Blob Storage&lt;/a&gt; now supports an archive level tier - &lt;a href=&quot;https://azure.microsoft.com/en-au/blog/cloud-storage-now-more-affordable-announcing-general-availability-of-azure-archive-storage/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A deep drive into the &lt;a href=&quot;/technologies/apache-hadoop/yarn/&quot;&gt;YARN&lt;/a&gt; capacity scheduler from Hortonworks - &lt;a href=&quot;https://hortonworks.com/blog/yarn-capacity-scheduler/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Apache &lt;a href=&quot;/technologies/apache-flink/&quot;&gt;Flink&lt;/a&gt; - 2017 in review and plans for 2018 - &lt;a href=&quot;http://flink.apache.org/news/2017/12/21/2017-year-in-review.html&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;dataArtisans have responded to the Databricks Spark Streaming vs Flink benchmark - &lt;a href=&quot;https://data-artisans.com/blog/curious-case-broken-benchmark-revisiting-apache-flink-vs-databricks-runtime&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Apache Mnemonic and Trafodion have graduated from the Apache Incubator - &lt;a href=&quot;https://blogs.apache.org/foundation/entry/the-apache-software-foundation-announces25&quot;&gt;link&lt;/a&gt;; &lt;a href=&quot;http://incubator.apache.org/projects/trafodion&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;The &lt;a href=&quot;/technologies/apache-nifi/&quot;&gt;Apache Nifi&lt;/a&gt; project has released the first (0.1) version of the NiFi registry for the configuration management of flows - &lt;a href=&quot;https://nifi.apache.org/registry.html&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A write-up from ZDNet on &lt;a href=&quot;/technologies/streamsets-data-collector/&quot;&gt;Streamsets&lt;/a&gt; - &lt;a href=&quot;http://www.zdnet.com/article/streamsets-updates-etl-to-the-cloud-data-pipeline/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;It’s an old article, but still interesting - ZDNet looked at &lt;a href=&quot;/tech-categories/graph-databases/&quot;&gt;graph&lt;/a&gt; vs &lt;a href=&quot;/tech-categories/rdf-databases/&quot;&gt;rdf&lt;/a&gt; databases - &lt;a href=&quot;http://www.zdnet.com/article/graph-databases-and-rdf-its-a-family-affair/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;By comparison this is ancient (from 2015), but looks like a really good intro the the &lt;a href=&quot;/technologies/apache-hbase/&quot;&gt;HBase&lt;/a&gt; architecture from MapR - &lt;a href=&quot;https://mapr.com/blog/in-depth-look-hbase-architecture/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;At the risk of this becoming a ZDNet fest - their views on big data in 2017 and 2018 - &lt;a href=&quot;http://www.zdnet.com/article/big-data-crystal-balls-and-looking-glasses-reviewing-2017-predicting-2018/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;An update from the &lt;a href=&quot;/technologies/pravega/&quot;&gt;Pravega&lt;/a&gt; blog on their architecture and design principles - &lt;a href=&quot;http://blog.pravega.io/2017/12/14/i-have-a-stream/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;For the deeply technical - how to build a distributed log (&lt;a href=&quot;/tech-categories/streaming-data-stores/&quot;&gt;streaming data store&lt;/a&gt;) - &lt;a href=&quot;https://bravenewgeek.com/building-a-distributed-log-from-scratch-part-1-storage-mechanics/?0&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;And last but not least, from Sonra - dimensional modelling on Hadoop - &lt;a href=&quot;https://sonra.io/2017/05/15/dimensional-modeling-and-kimball-data-marts-in-the-age-of-big-data-and-hadoop/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 17/01/2018</title><link>https://ondataengineering.net/blog/2018/01/17/the-mid-week-news/</link><description> &lt;p&gt;After last weeks bumper edition, we’re back to normal this week… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-nifi/&quot;&gt;Apache NiFi&lt;/a&gt; has hit 1.5&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Teradata have spun off the support and development of Presto (originally developed at Facebook) into a new company called Starburst data - &lt;a href=&quot;https://www.bloorresearch.com/2017/12/teradata-spins-off-starburst/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;It looks like Lucene 7.1 (and therefore &lt;a href=&quot;/technologies/elasticsearch/&quot;&gt;ElasticSearch&lt;/a&gt; and presumably at some point &lt;a href=&quot;/technologies/apache-solr/&quot;&gt;Solr&lt;/a&gt;) can now support Attibute Based Access Control - &lt;a href=&quot;https://www.elastic.co/blog/attribute-based-access-control-with-xpack&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;An update from ZDNet on &lt;a href=&quot;/tech-vendors/mapr/&quot;&gt;MapR&lt;/a&gt; - &lt;a href=&quot;http://www.zdnet.com/article/mapr-midcourse-correction-puts-original-ceo-back-in-the-drivers-seat/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>An Update</title><link>https://ondataengineering.net/blog/2018/01/19/an-update/</link><description> &lt;p&gt;So if you’ve been following this site, it’s probably become clear that the pace of updates has slowed quite considerably - unfortunately at this moment I don’t have the same time available on a regular basis that I did a few months ago.&lt;/p&gt; &lt;p&gt;So for the next few months until things quieten down I’m going to change my strategy a little. There will still be (semi-regular) updates, however these will appear as and when they’re ready, and they’ll be no posts on a Monday with any sort of commitment for what will happen that week. We will keep up with the weekly news updates however, to make sure we’re staying on top of the technologies we’ve looked at to date.&lt;/p&gt; &lt;p&gt;And we’ll try and get a few more contributors involved - if you’re interested in contributing of helping in any way then please do get in touch.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 24/01/2018</title><link>https://ondataengineering.net/blog/2018/01/24/the-mid-week-news/</link><description> &lt;p&gt;And it’s time for the news again… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-impala/&quot;&gt;Apache Impala&lt;/a&gt; has hit 2.11&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/greenplum/&quot;&gt;Greenplum&lt;/a&gt; continues it’s relentless development march to 5.4&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/alluxio/&quot;&gt;Alluxio&lt;/a&gt; is up to 1.7&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-hbase/&quot;&gt;Apache HBase&lt;/a&gt; has it’s first 2.0 beta out&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;There’s an update from LinkedIn on the latest with &lt;a href=&quot;/technologies/apache-gobblin/&quot;&gt;Apache Gobblin&lt;/a&gt; including new features added since 0.11 - &lt;a href=&quot;https://engineering.linkedin.com/blog/2018/01/gobblin-enters-apache-incubation&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 2 of Cloudera’s &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt; deployment blog posts is up, focusing on “where Cloudera services and roles should be placed across the various nodes in your clusters” - &lt;a href=&quot;http://blog.cloudera.com/blog/2018/01/deploy-cloudera-edh-clusters-like-a-boss-revamped-part-2/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Data Artisans - information on checkpointing in &lt;a href=&quot;/technologies/apache-flink/&quot;&gt;Apache Flink&lt;/a&gt; - &lt;a href=&quot;https://data-artisans.com/blog/managing-large-state-apache-flink-incremental-checkpointing-overview&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From ZDNet - an interview with Marko A. Rodriguez - the creator of the Titan graph database (now JanusGraph), DSE Graph and TinkerPop - &lt;a href=&quot;http://www.zdnet.com/article/from-graph-to-the-world-pioneering-a-database-virtual-machine/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Bloor have issued a Market Report Paper on SQL Engines on Hadoop - covering Hadoop &lt;a href=&quot;/tech-categories/query-engines/&quot;&gt;query engines&lt;/a&gt; capabilities, transactional (operational) capabilities and some &lt;a href=&quot;/tech-categories/analytical-databases/&quot;&gt;analytical database&lt;/a&gt; - &lt;a href=&quot;https://www.bloorresearch.com/research/sql-engines-hadoop/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 31/01/2018</title><link>https://ondataengineering.net/blog/2018/01/31/the-mid-week-news/</link><description> &lt;p&gt;It’s Wednesday, which means it’s news time… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt; has seen a 5.14 release&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-manager/&quot;&gt;Cloudera Manager&lt;/a&gt; is up to 5.14&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-navigator/&quot;&gt;Cloudera Navigator&lt;/a&gt; is up to 2.13&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-director/&quot;&gt;Cloudera Director&lt;/a&gt; is up to 2.7&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-data-science-workbench/&quot;&gt;Cloudera Data Science Workbench&lt;/a&gt; has hit 1.3&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-nifi/minifi/&quot;&gt;Apache NiFi MiNiFi&lt;/a&gt; has seen 0.4 releases of it’s C++ and Java versions&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-atlas/&quot;&gt;Apache Atlas&lt;/a&gt; has a 1.0 alpha release out&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hortonworks have an update on &lt;a href=&quot;/technologies/apache-hadoop/yarn/&quot;&gt;YARN&lt;/a&gt; support for long running applications coming in Hadoop 3.1 - &lt;a href=&quot;https://hortonworks.com/blog/first-class-support-long-running-services-apache-hadoop-yarn/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Something interesting from Gartner - some thoughts on the Data Warehouse, the Data Lake, and what roles each of them play in a larger logical data warehouse - &lt;a href=&quot;https://blogs.gartner.com/henry-cook/2018/01/28/the-logical-data-warehouse-and-its-jobs-to-be-done/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;An introduction to Apache Pulsar, the latest Kafka competitor - &lt;a href=&quot;http://furkankamaci.com/apache-pulsar-distributed-pub-sub-messaging-system/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Neo4J - a comparison of &lt;a href=&quot;/tech-categories/rdf-databases/&quot;&gt;RDF Triple Stores&lt;/a&gt; and &lt;a href=&quot;/tech-categories/graph-databases/&quot;&gt;Labeled Property Graphs&lt;/a&gt; - &lt;a href=&quot;https://neo4j.com/blog/rdf-triple-store-vs-labeled-property-graph-difference/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Thoughts on Informatica’s entry to the cloud from Bloor - &lt;a href=&quot;https://www.bloorresearch.com/2018/01/informatica-pale-pink-maybe-orange/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Grab yourself a free copy of the &lt;a href=&quot;/technologies/apache-nifi/&quot;&gt;Apache NiFi&lt;/a&gt; For Beginners book from Hortonworks - &lt;a href=&quot;https://hortonworks.com/blog/introduction-apache-nifi/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 07/02/2018</title><link>https://ondataengineering.net/blog/2018/02/07/the-mid-week-news/</link><description> &lt;p&gt;No news next week, but back the week after, hopefully with a resumption of updates.&lt;/p&gt; &lt;p&gt;In the meantime, let’s have this weeks news… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/hortonworks-data-flow/&quot;&gt;Hortonworks Data Flow (HDF)&lt;/a&gt; has seen a 3.1 release&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-carbondata/&quot;&gt;Apache CarbonData&lt;/a&gt; has seen a 1.3 release&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Nemo has entered the Apache Incubator - it’s a runtime for data processing languages (currently supporting &lt;a href=&quot;/technologies/apache-beam/&quot;&gt;Apache Beam&lt;/a&gt; with &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt; support coming) that dynamically adjusts to the runtime environment and uses Apache REEF to run over &lt;a href=&quot;/technologies/apache-hadoop/yarn/&quot;&gt;YARN&lt;/a&gt; or &lt;a href=&quot;/technologies/apache-mesos/&quot;&gt;Mesos&lt;/a&gt; - &lt;a href=&quot;https://snuspl.github.io/nemo/&quot;&gt;homepage&lt;/a&gt;; &lt;a href=&quot;https://wiki.apache.org/incubator/NemoProposal&quot;&gt;Apache proposal&lt;/a&gt;. &lt;em&gt;NOTE: this was originally called and submitted as Coral, but was subsequently renamed&lt;/em&gt;&lt;/li&gt; &lt;li&gt;From MapR on &lt;a href=&quot;/tech-vendors/mapr/&quot;&gt;MapR&lt;/a&gt;, part 1 of 4 on why their technologies are the best - &lt;a href=&quot;https://mapr.com/blog/mapr-data-technologies-gloves-off-series-1-4-storage/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Datanami - 2018 will be the year of the Data Engineer - &lt;a href=&quot;https://www.datanami.com/2018/02/05/2018-will-year-data-engineer/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;An intro to NiFi Registry (that enables configuration management of flows) from Hortonworks - &lt;a href=&quot;https://hortonworks.com/blog/hdf-3-1-blog-series-part-2-introducing-nifi-registry/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;There’s a new Gartner Critical Capabilities for Object Storage report out, and The Register have a summary - &lt;a href=&quot;https://www.theregister.co.uk/2018/02/02/gartner_object_storage_report/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Pivotal, graph processing on Greenplum using MADLib - &lt;a href=&quot;https://content.pivotal.io/blog/graph-processing-on-greenplum-database-using-apache-madlib&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 21/02/2018</title><link>https://ondataengineering.net/blog/2018/02/21/the-mid-week-news/</link><description> &lt;p&gt;Right, following our week off (the snow was lovely thank you), we’re back with a bumper edition of the news… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-knox/&quot;&gt;Apache Knox&lt;/a&gt; has hit 1.0&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-beam/&quot;&gt;Apache Beam&lt;/a&gt; has hit 2.3&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-storm/&quot;&gt;Apache Storm&lt;/a&gt; has hit 1.2&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-phoenix/&quot;&gt;Apache Phoenix&lt;/a&gt; has released a 5.0 alpha&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/elasticsearch/&quot;&gt;Elasticsearch&lt;/a&gt; has hit 6.2, along with &lt;a href=&quot;/technologies/elastic-x-pack/&quot;&gt;X-Pack&lt;/a&gt; and &lt;a href=&quot;/technologies/elasticsearch-hadoop/&quot;&gt;Elasticsearch Hadoop&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-oozie/&quot;&gt;Apache Oozie&lt;/a&gt; has security vulnerablity - &lt;a href=&quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=2017-15712&quot;&gt;CVE-2017-15712&lt;/a&gt; - a user can expose private files on the Oozie server process by constructing a workflow that references sensitive files&lt;/li&gt; &lt;li&gt;Trifacta have just announced a new funding round, including funding from Google that resell their software as Google Cloud Dataprep - &lt;a href=&quot;https://www.datanami.com/2018/02/06/trifacta-cashing-cloud-analytics/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A couple of updates on the benefits of &lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Hadoop&lt;/a&gt; 3 - &lt;a href=&quot;https://hortonworks.com/blog/hadoop-3-adds-value-hadoop-2/&quot;&gt;Hortonworks link&lt;/a&gt;; &lt;a href=&quot;https://www.datanami.com/2018/02/07/erasure-coding-changes-hadoop-storage-economics/&quot;&gt;erasure coding thoughts from Datanami&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Data Artisans, a view on end to end exactly once processing in &lt;a href=&quot;/technologies/apache-flink/&quot;&gt;Apache Flink&lt;/a&gt; - &lt;a href=&quot;https://data-artisans.com/blog/end-to-end-exactly-once-processing-apache-flink-apache-kafka&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A bit batch of updates from Hortonworks on new features in &lt;a href=&quot;/technologies/hortonworks-data-flow/&quot;&gt;HDF&lt;/a&gt; following on from last weeks GA announcement &lt;ul&gt; &lt;li&gt;NiFi Registry - &lt;a href=&quot;https://hortonworks.com/blog/hdf-3-1-blog-series-part-2-introducing-nifi-registry/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Kafka&lt;/a&gt; 1.0 support - &lt;a href=&quot;https://hortonworks.com/blog/hdf-3-1-blog-series-part-3-kafka-1-0-support-powerful-hdf-integrations/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Unit testing and continuous integration in &lt;a href=&quot;/technologies/streaming-analytics-manager/&quot;&gt;Hortonworks Streaming Analytics Manager&lt;/a&gt; - &lt;a href=&quot;https://hortonworks.com/blog/hortonworks-dataflow-hdf-3-1-blog-series-part-4-unit-testing-continuous-integration-delivery-streaming-analytics-apps/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-nifi/&quot;&gt;NiFi&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-atlas/&quot;&gt;Atlas&lt;/a&gt; integration - &lt;a href=&quot;https://hortonworks.com/blog/hdf-3-1-blog-series-part-6-introducing-nifi-atlas-integration/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Couple of big updates from InfluxData (creators of &lt;a href=&quot;/technologies/influxdb/&quot;&gt;InfluxDB&lt;/a&gt;) - &lt;a href=&quot;https://www.influxdata.com/blog/adventures-in-building-a-modern-time-series-platform/&quot;&gt;new funding round&lt;/a&gt; and &lt;a href=&quot;https://www.influxdata.com/blog/ifql-and-the-future-of-influxdata/&quot;&gt;IFQL as their new standard query language&lt;/a&gt;&lt;/li&gt; &lt;li&gt;An article from Datanami on Scylla, the C++ re-write of &lt;a href=&quot;/technologies/apache-cassandra/&quot;&gt;Cassandra&lt;/a&gt; - &lt;a href=&quot;https://www.datanami.com/2018/02/13/scylla-eyes-cassandras-nosql-workloads/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;An update from Cloudera on new features in &lt;a href=&quot;/technologies/hue/&quot;&gt;Hue&lt;/a&gt; in CDH 5.14 - &lt;a href=&quot;http://blog.cloudera.com/blog/2018/02/new-in-cloudera-5-14-query-assistance-improvements-and-adls-integration-for-the-self-service-analytic-database/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From InfoQ - implementing a resilient &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Kafka&lt;/a&gt; cluster at Goldman Sachs - &lt;a href=&quot;https://www.infoq.com/articles/resilient-kafka-goldman-sachs&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A technical overview of &lt;a href=&quot;/technologies/cloudera-altus/analytical-db/&quot;&gt;Cloudera Altus Analytical DB&lt;/a&gt; from Cloudera - &lt;a href=&quot;http://blog.cloudera.com/blog/2018/02/a-technical-overview-of-cloudera-altus-analytic-db/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;This is always interesting - LinkedIn’s experiences of Hadoop failures at extreme scale and how they test for them at smaller scale - &lt;a href=&quot;https://engineering.linkedin.com/blog/2018/02/dynamometer--scale-testing-hdfs-on-minimal-hardware-with-maximum&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Should you migrate all your batch ETL jobs to streaming jobs - probably not, but here is Netflix’s experiences courtesy of InfoQ - &lt;a href=&quot;https://www.infoq.com/articles/netflix-migrating-stream-processing&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 28/02/2018</title><link>https://ondataengineering.net/blog/2018/02/28/the-mid-week-news/</link><description> &lt;p&gt;News time again, and it’s a bit quieter than the monster last week… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-datafu/&quot;&gt;Apache DataFu&lt;/a&gt; has graduated from the Apache Incubator&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudbreak/&quot;&gt;Hortonworks Cloudbreak&lt;/a&gt; has seen it’s first 2.x GA release - 2.4&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/greenplum/&quot;&gt;Greenplum&lt;/a&gt; has hit 5.5&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;DataTorrent (the authors of &lt;a href=&quot;/technologies/apache-apex/&quot;&gt;Apache Apex&lt;/a&gt;) have released version 3.1 of their DataTorrent DTS product, that looks like they’re expanding their product a bit - &lt;a href=&quot;https://www.datatorrent.com/blog/technical-details-for-datatorrent-rts-3-10-release/&quot;&gt;post1&lt;/a&gt;; &lt;a href=&quot;https://www.datatorrent.com/blog/datatorrent-online-analytics-service/&quot;&gt;post2&lt;/a&gt;; &lt;a href=&quot;http://www.zdnet.com/article/datatorrents-hard-code-around-streaming-data-philosophy-in-90-days/&quot;&gt;ZDNet view&lt;/a&gt;; &lt;a href=&quot;https://www.datanami.com/2018/02/22/datatorrent-glues-open-source-componentry-apoxi/&quot;&gt;Datanami view&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-altus/&quot;&gt;Cloudera Atlus&lt;/a&gt; has a new Java SDK - &lt;a href=&quot;http://blog.cloudera.com/blog/2018/02/altus-sdk-for-java-blog/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/google-cloud-storage/&quot;&gt;Google Cloud Storage&lt;/a&gt; now offers strongly consistent object operations through the use of Spanner - I’ve got some updates to the site around object storage consistency I’ll put live shortly - &lt;a href=&quot;https://cloudplatform.googleblog.com/2018/02/how-Google-Cloud-Storage-offers-strongly-consistent-object-listing-thanks-to-Spanner.html&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A write up from Bloor on Snowflake DB - &lt;a href=&quot;https://www.bloorresearch.com/2018/02/cloud-data-warehousing-snowflake-old-ideas-shot-ribbons/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Thoughts on open source software and it’s impact on Big Data from Datanami - &lt;a href=&quot;https://www.datanami.com/2018/02/26/weighing-open-sources-worth-future-big-data/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Looks like TigerGraph - the hybrid OLTP/OLAP massively scalable graph database - has launched it’s 2.0 platform. Thoughts from &lt;a href=&quot;https://www.theregister.co.uk/2018/02/27/tigergraph_launches_realtime_graph_analytics_collaboration/&quot;&gt;The Register&lt;/a&gt; and &lt;a href=&quot;https://www.datanami.com/2018/02/27/tigergraph-serves-collaboration-security-multi-graph/&quot;&gt;Datanami&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Week That Was - 02/03/2018</title><link>https://ondataengineering.net/blog/2018/03/02/the-week-that-was/</link><description> &lt;p&gt;Ok, it’s been a while, but there’s been a bunch of (small) updates to the site, and it’s time to summarise. I’ve got a few more random updates next week, and then we’ll get back into a groove of new content.&lt;/p&gt; &lt;p&gt;Back to those updates, and they are, in no particular order… &lt;!--more--&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Updated the &lt;a href=&quot;/technologies/mapr-converged-data-platform/&quot;&gt;MapR Converged Data Platform&lt;/a&gt; and associated technology pages for MapR 6.0 and MEP 4.0 and 4.1, including: &lt;ul&gt; &lt;li&gt;Renamed MapR-Streams to &lt;a href=&quot;/technologies/mapr-es/&quot;&gt;MapR-ES&lt;/a&gt; and MapR File System to &lt;a href=&quot;/technologies/mapr-fs/&quot;&gt;MapR-FS&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Moved &lt;a href=&quot;/technologies/mapr-es/&quot;&gt;MapR-ES&lt;/a&gt; and &lt;a href=&quot;/technologies/mapr-db/&quot;&gt;MapR-DB&lt;/a&gt; to top level projects&lt;/li&gt; &lt;li&gt;Renamed MapR Ecosystem Pack to &lt;a href=&quot;/technologies/mapr-expansion-pack/&quot;&gt;MapR Expansion Pack&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Added references to Apache Fluo, Omid and Tephra to the &lt;a href=&quot;/tech-categories/nosql-wide-column-stores/&quot;&gt;NoSQL Wide Column Stores&lt;/a&gt; technology category page - these are projects that add functionality on top of other NoSQL Wide Column Store databases.&lt;/li&gt; &lt;li&gt;Added a bunch of related technologies to the &lt;a href=&quot;/tech-categories/compute-cluster-managers/&quot;&gt;Compute Cluster Manager&lt;/a&gt; page&lt;/li&gt; &lt;li&gt;Added tech category links to &lt;a href=&quot;/tech-categories/hadoop-distributions/distribution-comparison/&quot;&gt;Hadoop Distributions comparison&lt;/a&gt; page&lt;/li&gt; &lt;li&gt;Refreshed the technology pages for &lt;a href=&quot;/technologies/cloudera-navigator/&quot;&gt;Cloudera Navigator&lt;/a&gt; and it’s sub-projects&lt;/li&gt; &lt;li&gt;Updates to reflect the rename of Apache Coral to Apache Nemo, including the &lt;a href=&quot;/tech-vendors/apache/&quot;&gt;Apache&lt;/a&gt; vendor page&lt;/li&gt; &lt;li&gt;Added references to Uno and Muchos to the &lt;a href=&quot;/technologies/apache-accumulo/&quot;&gt;Accumulo&lt;/a&gt; page&lt;/li&gt; &lt;li&gt;Added a list of Kafka management tools to the &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Kafka&lt;/a&gt; page&lt;/li&gt; &lt;li&gt;Added links to the Cloudera and Hortonworks documentation and product pages to a bunch of open source Hadoop related technology pages&lt;/li&gt; &lt;li&gt;Added Mnemonic to the &lt;a href=&quot;/tech-categories/data-storage-formats/&quot;&gt;data storage formats&lt;/a&gt; page&lt;/li&gt; &lt;li&gt;Refreshed the &lt;a href=&quot;/tech-categories/object-stores/&quot;&gt;object stores&lt;/a&gt; page, and added Wasabi&lt;/li&gt; &lt;li&gt;Refreshed the add-on information on the &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Updated the &lt;a href=&quot;/technologies/elasticsearch/&quot;&gt;Elasticsearch&lt;/a&gt; page with a link to blog entry from Sematext listing alternative add-ons to the Elastic X-Pack&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 07/03/2018</title><link>https://ondataengineering.net/blog/2018/03/07/the-mid-weeks-news/</link><description> &lt;p&gt;It’s time for the news again… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-kylin/&quot;&gt;Apache Kylin&lt;/a&gt; has hit 2.3&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt; has also hit 2.3&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/druid/&quot;&gt;Druid&lt;/a&gt; has been donated to the Apache Incubator - &lt;a href=&quot;https://wiki.apache.org/incubator/DruidProposal&quot;&gt;proposal&lt;/a&gt;; &lt;a href=&quot;http://incubator.apache.org/projects/druid&quot;&gt;incubator page&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Elastic have announced that they’ll be open sourcing their &lt;a href=&quot;/technologies/elastic-x-pack/&quot;&gt;Elastic X-Pack&lt;/a&gt; as of Elastic 6.3. The code will be moved into the public repos for their other products (but under the Elastic EULA), and the free elements will be pre-bundled with those products rather than requiring a separate download - &lt;a href=&quot;https://www.elastic.co/blog/doubling-down-on-open&quot;&gt;accouncement&lt;/a&gt;; &lt;a href=&quot;https://www.elastic.co/products/x-pack/open&quot;&gt;details&lt;/a&gt;; &lt;a href=&quot;https://www.datanami.com/2018/02/28/elastic-make-entire-stack-open-source/&quot;&gt;Datanami view&lt;/a&gt;&lt;/li&gt; &lt;li&gt;An excellent article on Data Warehouse Automation - we’ll get to talking more about this soon - &lt;a href=&quot;http://www.sspaeti.com/blog/why-data-warehouse-automation-is-not-more-popular/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;MapR have announced “MapR Data Fabric for Kubernetes” - persistent storage for containers running on Kubernetes - &lt;a href=&quot;https://community.mapr.com/community/products/blog/2018/03/06/announcing-mapr-data-fabric-for-kubernetes&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;https://mapr.com/solutions/data-fabric/kubernetes/&quot;&gt;homepage&lt;/a&gt;; &lt;a href=&quot;https://www.datanami.com/2018/03/06/inside-maprs-support-kubernetes/&quot;&gt;Datanami view&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Hortonworks have blogged about what’s new in Cloudbreak 2.4 - &lt;a href=&quot;https://hortonworks.com/blog/announcing-cloudbreak-2-4/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;The latest Hortonworks blog post on HDF 3.1 is up, this time on the &lt;a href=&quot;/technologies/apache-nifi/minifi/&quot;&gt;MiNiFi&lt;/a&gt; C++ agent - &lt;a href=&quot;https://hortonworks.com/blog/hortonworks-dataflow-hdf-3-1-blog-series-part-6-introducing-apache-minifi-c-agent/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;AWS have published their best practice for running &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Kafka&lt;/a&gt; on AWS - &lt;a href=&quot;https://aws.amazon.com/blogs/big-data/best-practices-for-running-apache-kafka-on-aws/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Datanami have covered Cloudera’s announcement of &lt;a href=&quot;//technologies/cloudera-altus/&quot;&gt;Altus&lt;/a&gt; Data Science (R and Python-based machine learning workloads based on their &lt;a href=&quot;/technologies/cloudera-data-science-workbench/&quot;&gt;Data Science Workbench&lt;/a&gt;) coming to beta soon, with an operational database build on HBase coming as the fourth package in the future - &lt;a href=&quot;https://www.datanami.com/2018/03/06/clouderas-vision-cloud-coming-focus/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Again from Datanami, a report that Streamlio is claiming up to 150% performance advantage of Apache Pulsar vs Apacke Kafka as a &lt;a href=&quot;/tech-categories/streaming-data-stores/&quot;&gt;Streaming Data Store&lt;/a&gt; - &lt;a href=&quot;https://www.datanami.com/2018/03/06/streamlio-claims-pulsar-performance-advantages-kafka/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From ZDNet, this is a well worth a read if you have an interest in &lt;a href=&quot;/tech-categories/graph-databases/&quot;&gt;Graph Databases&lt;/a&gt; or &lt;a href=&quot;/tech-categories/rdf-databases/&quot;&gt;RDF Databases&lt;/a&gt; that’s dense with information - &lt;a href=&quot;http://www.zdnet.com/article/back-to-the-future-does-graph-database-success-hang-on-query-language/&quot;&gt;link&lt;/a&gt; &lt;ul&gt; &lt;li&gt;Cypher (the open source Graph query language from Neo4J) now has adapters to allow Cypher jobs to be run over Spark and TinkerPop Gremlin compatible databases&lt;/li&gt; &lt;li&gt;There’s a SPARQL Gremlin bridge, allowing you to run SPARQL queries over TinkerPop Gremlin compatible databases&lt;/li&gt; &lt;li&gt;Amazone Neptune (which supports both Gremlin and SPARQL), is apparently built on BlazeGraph&lt;/li&gt; &lt;li&gt;There’s a new massively parallel distributed graph database from Cambridge Semantics (CS) called AnzoGraph, which they compare to TigerGraph&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Looks like I missed the donation of this to the Apache Foundation, but Apache Hivemall is a scalable machine learning library implemented as Hive UDFs/UDAFs/UDTFs - &lt;a href=&quot;http://hivemall.incubator.apache.org/&quot;&gt;home page&lt;/a&gt;&lt;/li&gt; &lt;li&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;LinkedIn have proposed DrElephant to the Apache Foundation -&lt;/td&gt; &lt;td&gt;their performance monitoring and tuning service for jobs and workflows that run on Apache Hadoop and Apache Spark - &lt;a href=&quot;https://wiki.apache.org/incubator/DrElephantProposal&quot;&gt;proposal&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 14/03/2018</title><link>https://ondataengineering.net/blog/2018/03/14/the-mid-week-news/</link><description> &lt;p&gt;It’s Wednesday, and it’s time for the weekly news… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-falcon/&quot;&gt;Apache Falcon&lt;/a&gt;, which I thought was dead, has just hit 0.11&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-hbase/&quot;&gt;Apache HBase&lt;/a&gt; has seen it’s second 2.0 beta release&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-parquet/&quot;&gt;Apache Parquet&lt;/a&gt; has seen a 1.4 release of it’s C++ library&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/influxdb/&quot;&gt;InfluxDB&lt;/a&gt; has hit 1.5&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Datanami have the lowdown on StreamSets’ new Data Protection product out that supports automatic identification of personally identifiable information and a range of options for obfuscating or encrypting it - &lt;a href=&quot;https://www.datanami.com/2018/03/07/streamsets-balances-streaming-data-demands-for-security-access/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Databricks have blogged about two key new features in Spark 2.3 - &lt;a href=&quot;https://databricks.com/blog/2018/03/06/apache-spark-2-3-with-native-kubernetes-support.html&quot;&gt;Kubernetes support&lt;/a&gt; and &lt;a href=&quot;https://databricks.com/blog/2018/03/13/introducing-stream-stream-joins-in-apache-spark-2-3.html&quot;&gt;stream to stream joins&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From ODBMS Industry Watch, and interesting interview with John Ryan, Data Warehouse Solution Architect (Director) at UBS, on database technologies (RDBMS, NoSQL and NewSQL) with some thoughts on the Lambda architecture thrown in at the end - &lt;a href=&quot;http://www.odbms.org/blog/2018/03/on-rdbms-nosql-and-newsql-databases-interview-with-john-ryan/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Again from Datanami, some interesting thoughts on Elastic and where they’re going - &lt;a href=&quot;https://www.datanami.com/2018/03/12/elastic-plots-its-own-course-to-big-data-success/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 21/03/2018</title><link>https://ondataengineering.net/blog/2018/03/21/the-mid-week-news/</link><description> &lt;p&gt;It’s time for the news… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-ranger/&quot;&gt;Apache Ranger&lt;/a&gt; has hit the big 1.0&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-calcite/&quot;&gt;Apache Calcite&lt;/a&gt; has hit 0.16&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-drill/&quot;&gt;Apache Drill&lt;/a&gt; has hit 1.13&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-ignite/&quot;&gt;Apache Ignite&lt;/a&gt; has hit 2.4&lt;/li&gt; &lt;li&gt;Databricks have published some more blog posts on 2.3 features for &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Influx have published some more blog posts on 1.5 features for &lt;a href=&quot;/technologies/influxdb/&quot;&gt;InfluxDB&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Datanami have a summary of the new features in &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Spark&lt;/a&gt; 2.3 - &lt;a href=&quot;https://www.datanami.com/2018/03/14/top-3-new-features-in-apache-spark-2-3/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Confluent have a post of how to continuously extract data from your database using &lt;a href=&quot;/technologies/apache-kafka/kafka-connect/&quot;&gt;Kafka Connect&lt;/a&gt; - &lt;a href=&quot;https://www.confluent.io/no-more-silos-how-to-integrate-your-databases-with-apache-kafka-and-cdc&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/tech-vendors/hortonworks/&quot;&gt;Hortonworks&lt;/a&gt; have announced Hortonworks Operational Services - their new on site or in the cloud managed service for HDP or HDF - &lt;a href=&quot;https://hortonworks.com/blog/operational-services-embark-big-data-journey-confidence/&quot;&gt;blog&lt;/a&gt;; &lt;a href=&quot;https://hortonworks.com/services/support/operational-services/&quot;&gt;homepage&lt;/a&gt;&lt;/li&gt; &lt;li&gt;ZDNet have their view on Hortonworks Operational Services, KSQL and Waterline - &lt;a href=&quot;http://www.zdnet.com/article/hortonworks-confluent-and-waterline-make-big-data-easier/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Week That Was - 23/03/2018</title><link>https://ondataengineering.net/blog/2018/03/23/the-week-that-was/</link><description> &lt;p&gt;There have been a few minor updates to the site this week… &lt;!--more--&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;There’s a new information on &lt;a href=&quot;/technologies/openlink-virtuoso-universal-server/&quot;&gt;OpenLink Virtuoso Universal Server&lt;/a&gt; and updates to the &lt;a href=&quot;/tech-categories/data-virtualization/&quot;&gt;Data Virtualization&lt;/a&gt; tech category page for this and OpenLink Software Universal Data Access Suite courtesy of TallTed&lt;/li&gt; &lt;li&gt;I’ve added SQData CDC and Debezium to the &lt;a href=&quot;/tech-categories/data-ingestion/&quot;&gt;Data Ingestion&lt;/a&gt; page. Debezium looks interesting - an open source package of Kafka and Kafka Connect for streaming data out of databases&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 28/03/2018</title><link>https://ondataengineering.net/blog/2018/03/28/the-mid-week-news/</link><description> &lt;p&gt;Wednesday’s come round again, so let’s catch up on the news… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-arrow/&quot;&gt;Apache Arrow&lt;/a&gt; has hit 0.9&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-beam/&quot;&gt;Apache Beam&lt;/a&gt; is up to 2.4&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-datafu/&quot;&gt;Apache DataFu&lt;/a&gt; has hit a symbolic 1.4 after graduating&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-hawq/&quot;&gt;Apache HAWQ&lt;/a&gt; has hit 2.3&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-kudu/&quot;&gt;Apache Kudu&lt;/a&gt; has hit 1.7&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/greenplum/&quot;&gt;Greenplum&lt;/a&gt; has hit 5.6&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;There’s a summary on the &lt;a href=&quot;/technologies/apache-accumulo/&quot;&gt;Accumulo&lt;/a&gt; blog on how to export metrics to Grafana (and other metrics tools) - &lt;a href=&quot;http://accumulo.apache.org/blog/2018/03/22/view-metrics-in-grafana.html&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;An update from Datanami on Presto and Starburst (the commercial company that now backs it) - &lt;a href=&quot;https://www.datanami.com/2018/03/21/building-presto-business-no-magic-trick-for-starburst/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;More from Databricks and Microsoft on Azure Databricks - &lt;a href=&quot;https://databricks.com/blog/2018/03/22/azure-databricks-industry-leading-analytics-platform-powered-by-apache-spark.html&quot;&gt;Databricks&lt;/a&gt;; &lt;a href=&quot;https://azure.microsoft.com/en-us/blog/implementation-patterns-for-big-data-and-data-warehouse-on-azure/&quot;&gt;Azure&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Cloudera, deploying &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt; using &lt;a href=&quot;/technologies/cloudera-director/&quot;&gt;Cloudera Director&lt;/a&gt; and Ansible - &lt;a href=&quot;http://blog.cloudera.com/blog/2018/03/automated-provisioning-of-cdh-in-the-cloud-with-cloudera-director-and-ansible/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;It looks like &lt;a href=&quot;/technologies/apache-arrow/&quot;&gt;Apache Arrow&lt;/a&gt; has a new native Go implementation in the works, donated by InfluxDB - &lt;a href=&quot;https://arrow.apache.org/blog/2018/03/22/go-code-donation/&quot;&gt;Arrow blog&lt;/a&gt;; &lt;a href=&quot;https://www.influxdata.com/blog/influxdata-apache-arrow-go-implementation/&quot;&gt;Influx blog&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Datanami, ZEPL have moved their analytics tool based on &lt;a href=&quot;/technologies/apache-zeppelin/&quot;&gt;Apache Zeppelin&lt;/a&gt; out of beta - &lt;a href=&quot;https://www.datanami.com/2018/03/23/apache-zeppelin-launches-latest-data-science-notebook/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From ZDNet, thoughts on GPU accelerated databases - &lt;a href=&quot;http://www.zdnet.com/article/gpu-databases-are-coming-of-age/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 11/04/2018</title><link>https://ondataengineering.net/blog/2018/04/11/the-mid-week-news/</link><description> &lt;p&gt;It was half term holiday week last week (sorry - forgot to say) which meant no news, but we’re back this week… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Apache Hadoop&lt;/a&gt; has hit 3.1, with what looks like some significant new functionality&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-solr/&quot;&gt;Apache Solr&lt;/a&gt; has hit 7.3&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudbreak/&quot;&gt;Cloudbreak&lt;/a&gt; has a 2.5 tech preview release with support for HDF&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-parquet/&quot;&gt;Apache Parquet&lt;/a&gt; has hit a 1.10 release of it’s Map Reduce implementation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt; has hit 1.1&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-nifi/&quot;&gt;Apache NiFi&lt;/a&gt; has hit 1.6&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/mapr-expansion-pack/&quot;&gt;MapR Expansion Pack&lt;/a&gt; has hit 5.0&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/hue/&quot;&gt;Hue&lt;/a&gt; has hit 4.2&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/greenplum/&quot;&gt;Greenplum&lt;/a&gt; has hit 5.7&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Amazon have announced a new cheaper One Zone-IA Storage Class for &lt;a href=&quot;/technologies/amazon-s3/&quot;&gt;S3&lt;/a&gt; that doesn’t support geo redundancy, and the general availability of S3 select - &lt;a href=&quot;https://aws.amazon.com/blogs/aws/amazon-s3-update-new-storage-class-general-availability-of-s3-select/&quot;&gt;summary&lt;/a&gt;; &lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2018/04/announcing-s3-one-zone-infrequent-access-a-new-amazon-s3-storage-class/&quot;&gt;One Zone-IA&lt;/a&gt;; &lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2018/04/amazon-s3-select-is-now-generally-available/&quot;&gt;S3 Select&lt;/a&gt;; &lt;a href=&quot;https://www.theregister.co.uk/2018/04/05/aws_drops_s3_storage_costs/&quot;&gt;TheRegister view&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Hortonworks, a view on the performance of &lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Hive&lt;/a&gt; over local disk vs S3 cloud storage - &lt;a href=&quot;https://hortonworks.com/blog/cloud-architectures-interactive-analytics-apache-hive/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Hardly surprise, but from Datanami - Excel remains the go to Data Prep tool - &lt;a href=&quot;https://www.datanami.com/2018/04/02/survey-excel-remains-go-to-data-prep-tool/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;An update on Oracle’s new “automated” database / data warehouse strategy - &lt;a href=&quot;https://www.enterprisetech.com/2018/03/28/oracle-ellison-seek-path-around-aws/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Microsoft have announced a public preview of soft deletes for &lt;a href=&quot;/technologies/microsoft-azure-blob-storage/&quot;&gt;Azure Storage Blobs&lt;/a&gt; - &lt;a href=&quot;https://azure.microsoft.com/en-us/blog/soft-delete-for-azure-storage-blobs-now-in-public-preview/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;MapR are now also talking about “Streams of Record” with &lt;a href=&quot;/technologies/mapr-es/&quot;&gt;MapR-ES&lt;/a&gt; - &lt;a href=&quot;https://mapr.com/blog/extending-your-stream-of-record-mapr-6-0-1-mep-5-0/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;We have a bunch of security announcements this week: &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-1308&quot;&gt;CVE-2018-1308&lt;/a&gt; - XXE attack through Apache Solr’s DIH’s dataConfig request parameter&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-1284&quot;&gt;CVE-2018-1284&lt;/a&gt; - Hive UDF series UDFXPathXXXX allow users to pass carefully crafted XML to access local files&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-1315&quot;&gt;CVE-2018-1315&lt;/a&gt; - Hive ‘COPY FROM FTP’ statement in HPL/SQL can write to arbitrary location if the FTP server is compromised&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-1282&quot;&gt;CVE-2018-1282&lt;/a&gt; - Hive JDBC driver is susceptible to SQL injection attack if the input parameters are not properly cleaned&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 18/04/2018</title><link>https://ondataengineering.net/blog/2018/04/18/the-mid-week-news/</link><description> &lt;p&gt;It’s a very slow news week this week… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/druid/&quot;&gt;Druid&lt;/a&gt; is up to 0.12&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Spark 2.3 is now available for &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;Cloudera CDH&lt;/a&gt; - &lt;a href=&quot;http://community.cloudera.com/t5/Community-News-Release/ANNOUNCE-CDS-2-3-Release-2-Powered-by-Apache-Spark-Released/td-p/66412&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Hortonworks, a summary of running IBM Big SQL on HDP - &lt;a href=&quot;https://hortonworks.com/blog/access-data-anywhere-using-db2-big-sqls-federation/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From The Register, Actian has been sold - &lt;a href=&quot;http://www.theregister.co.uk/2018/04/13/actian_acquired/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A good summary of &lt;a href=&quot;/technologies/mesosphere-dcos/&quot;&gt;DC/OS&lt;/a&gt; - &lt;a href=&quot;https://blog.knoldus.com/2018/04/14/dc-os-the-architecture-1/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News - 25/04/2018</title><link>https://ondataengineering.net/blog/2018/04/25/the-mid-week-news/</link><description> &lt;p&gt;It’s another slow news week - is everyone on holiday of something? Read on for what we have… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-oozie/&quot;&gt;Apache Oozie&lt;/a&gt; has hit 5.0 - it’s first release for over a year&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-accumulo/&quot;&gt;Apache Accumulo&lt;/a&gt; is up to 1.9&lt;/li&gt; &lt;li&gt;Confluent &lt;a href=&quot;/technologies/confluent-open-source/&quot;&gt;Open Source&lt;/a&gt; and &lt;a href=&quot;/technologies/confluent-enterprise&quot;&gt;Enterprise&lt;/a&gt; have hit 4.1&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/zenko/cloudserver/&quot;&gt;Zenko Cloudserver&lt;/a&gt; is up to 7.4&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hortonworks has formally announced Data Steward Studio for &lt;a href=&quot;/technologies/hortonworks-dataplane-service/&quot;&gt;Hortonworks DataPlane&lt;/a&gt;, although it’s been in tech preview for a while - &lt;a href=&quot;https://www.datanami.com/2018/04/17/hortonworks-upgrades-data-plane-service/&quot;&gt;Datanami&lt;/a&gt;; &lt;a href=&quot;https://www.theregister.co.uk/2018/04/18/hortonworks_data_steward_studio_release/&quot;&gt;The Register&lt;/a&gt;&lt;/li&gt; &lt;li&gt;The Register also has a longer look at &lt;a href=&quot;/tech-vendors/hortonworks/&quot;&gt;Hortonworks&lt;/a&gt; and where it’s going - &lt;a href=&quot;https://www.theregister.co.uk/2018/04/18/hortonworks_data_steward_studio_release/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;And again from The Register, thoughts on Scality, it’s new funding round for doing multi-cloud storage via &lt;a href=&quot;/technologies/zenko/&quot;&gt;Zenko&lt;/a&gt; - &lt;a href=&quot;https://www.theregister.co.uk/2018/04/18/scality_zenko_60m_funding/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Datanami - Presto looks like it’s catching on - &lt;a href=&quot;https://www.datanami.com/2018/04/18/presto-use-surges-qubole-finds/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Again from Datanami - apparently a Teradata survey says that cloud analytics engines are falling short - &lt;a href=&quot;https://www.datanami.com/2018/04/24/cloud-analytics-engines-falling-short-survey-finds/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From ZDNet, is PostgreSQL about to have it’s day - &lt;a href=&quot;https://www.zdnet.com/article/has-the-time-finally-come-for-postgresql/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News 02/05/2018</title><link>https://ondataengineering.net/blog/2018/05/02/the-mid-week-news/</link><description> &lt;p&gt;Wednesday is news day… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-hbase/&quot;&gt;Apache HBase&lt;/a&gt; has seen it’s 2.0 GA release&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-impala/&quot;&gt;Apache Impala&lt;/a&gt; is up to 2.12&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Azure SQL Data Warehouse is seen a major Generation 2 upgrade, it still uses Azure Blob Storage (decoupling compute from storage), but can now cache to local SSD disks, meaning performance is now on par with RedShift - &lt;a href=&quot;https://azure.microsoft.com/en-us/blog/blazing-fast-data-warehousing-with-sql-data-warehouse-gen2/&quot;&gt;link&lt;/a&gt;; &lt;a href=&quot;https://www.zdnet.com/article/azure-sql-data-warehouse-gen-2-microsofts-shot-across-amazons-bow/&quot;&gt;ZDNet view&lt;/a&gt;&lt;/li&gt; &lt;li&gt;And while we’re talking about Cloud Analytical Databases - Datanami have an article on the growth of Google BigQuery - &lt;a href=&quot;https://www.datanami.com/2018/04/24/googles-bigquery-gaining-steam-as-cloud-warehouse-wars-heat-up/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Thoughts from ZDNet on the maturity of Hadoop, including new features in Hadoop 3.x, &lt;a href=&quot;/technologies/hortonworks-dataplane-service/&quot;&gt;Hortonworks DataPlane&lt;/a&gt;, IBM and Hortonworks pushing &lt;a href=&quot;/technologies/apache-atlas/&quot;&gt;Atlas&lt;/a&gt; as a standard Enterprise metadata repository, and their new plans for &lt;a href=&quot;/tech-vendors/odpi/&quot;&gt;ODPi&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Dremio has a 2.0 version out - I’d not heard of this before but it’s worth a look if you want to be able to exploit data from multiple sources and create (and automatically materialise) derived tables - &lt;a href=&quot;https://www.zdnet.com/article/dremio-2-0-adds-data-reflections-improvements-support-for-looker-and-connectivity-to-azure-data-lake/&quot;&gt;ZDNet link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Couple of interesting paper reviews from The Morning Papers this week &lt;ul&gt; &lt;li&gt;Firstly - Skyway, which allows objects to be moved directly between JVM heaps on different machines avoiding the need for serialisation and de-serialization in distributed systems, apparently delivering a 16% performance increase over Spark using Kyro - &lt;a href=&quot;https://blog.acolyer.org/2018/04/26/skyway-connecting-managed-heaps-in-distributed-big-data-systems/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;And secondly, and I love this sort of stuff - using the ASICs on network switches to deliver a co-ordination service (ala ZooKeeper) at ridiculous throughput and latency - &lt;a href=&quot;https://blog.acolyer.org/2018/04/30/netchain-scale-free-sub-rtt-coordination/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;There are a bunch of CVE vulnerabilities announced for Apache Tika - &lt;a href=&quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-1338&quot;&gt;CVE-2018-1338&lt;/a&gt;; &lt;a href=&quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-1339&quot;&gt;CVE-2018-1339&lt;/a&gt;; &lt;a href=&quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-1335&quot;&gt;CVE-2018-1335&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News 09/05/2018</title><link>https://ondataengineering.net/blog/2018/05/09/the-mid-week-news/</link><description> &lt;p&gt;It’s time for the news again. One one tech release this week, but we have some reading for you… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-apex/&quot;&gt;Apache Apex&lt;/a&gt; Core is up to 3.7&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Google Cloud Composer, and orchestration service based on Apache Airflow is now available on the &lt;a href=&quot;/tech-vendors/google-cloud-platform/&quot;&gt;Google Cloud Platform&lt;/a&gt; - &lt;a href=&quot;https://cloud.google.com/blog/big-data/2018/05/cloud-composer-is-now-in-beta-build-and-run-practical-workflows-with-minimal-effort&quot;&gt;link&lt;/a&gt;; [Datanami view]9https://www.datanami.com/2018/05/01/apache-airflow-to-power-googles-new-workflow-service/)&lt;/li&gt; &lt;li&gt;Confluent now support running their &lt;a href=&quot;/technologies/confluent-enterprise/&quot;&gt;Confluent Platform&lt;/a&gt; (bsaed on &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Kafka&lt;/a&gt; on Kubernetes, although it’s only available in their commercial version - &lt;a href=&quot;https://www.confluent.io/blog/introducing-the-confluent-operator-apache-kafka-on-kubernetes/&quot;&gt;link&lt;/a&gt;; &lt;a href=&quot;https://www.datanami.com/2018/05/03/want-kafka-on-kubernetes-confluent-has-it-made/&quot;&gt;Datanami view&lt;/a&gt;&lt;/li&gt; &lt;li&gt;I’d like to talk about how to test data pipelines in more detail at some point, but this is right up my ally - a new framework called Great Expectations for defining tests or checks that run as part of the pipeline - &lt;a href=&quot;https://medium.com/@expectgreatdata/down-with-pipeline-debt-introducing-great-expectations-862ddc46782a&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Cloudera, how to backup and recover data in &lt;a href=&quot;/technologies/apache-solr&quot;&gt;Apache Solr&lt;/a&gt; - &lt;a href=&quot;http://blog.cloudera.com/blog/2018/05/backup-and-disaster-recovery-for-cloudera-search/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Datanami - DataTorrent, the company behind &lt;a href=&quot;/technologies/apache-apex/&quot;&gt;Apache Apex&lt;/a&gt; and their commercial version of it (DataTorrent RTS) has gone under - &lt;a href=&quot;https://www.datanami.com/2018/05/08/datatorrent-stream-processing-startup-folds/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Again from Datanami - looks like Netflix has moved its Keystone data pipeline from Samza to &lt;a href=&quot;/technologies/apache-flink/&quot;&gt;Apache Flink&lt;/a&gt;&lt;/li&gt; &lt;li&gt;An update from Microsoft on CosmosDB - &lt;a href=&quot;https://azure.microsoft.com/en-us/blog/azure-cosmosdb-build-2018-the-catalyst-for-next-generation-apps/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News 16/05/2018</title><link>https://ondataengineering.net/blog/2018/05/16/the-mid-week-news/</link><description> &lt;p&gt;It’s Wednesday, it’s the news (and it’s very quiet again).. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-accumulo/&quot;&gt;Apache Accumulo&lt;/a&gt; has seen a high priority security patch release (1.9.1)&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-impala/&quot;&gt;Apache Impala&lt;/a&gt; is up to 3.0&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-orc/&quot;&gt;Apache ORC&lt;/a&gt; is up to 1.5&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/greenplum/&quot;&gt;Greenplum&lt;/a&gt; continues it’s relentless release march with 5.8&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Google have announced Google Cloud Memorystore for &lt;a href=&quot;/tech-vendors/google-cloud-platform/&quot;&gt;Google Cloud Platform&lt;/a&gt; - a NoSQL key value store with Redis compatibilitiy - &lt;a href=&quot;https://cloudplatform.googleblog.com/2018/05/Introducing-Cloud-Memorystore-A-fully-managed-in-memory-data-store-service-for-Redis.html&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Microsoft have announced Kafka compatibility for Azure Event Hub - &lt;a href=&quot;https://azure.microsoft.com/en-us/blog/azure-event-hubs-for-kafka-ecosystems-in-public-preview/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Although it’s an advert for Snowflake, it’s also a passable summary of the evolution of data warehouse data platforms - &lt;a href=&quot;https://www.linkedin.com/pulse/data-warehousing-evolution-frank-bell/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News 23/05/2018</title><link>https://ondataengineering.net/blog/2018/05/23/the-mid-week-news/</link><description> &lt;p&gt;It’s time for the news again - let’s go… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-data-science-workbench/&quot;&gt;Cloudera Data Science Workbench&lt;/a&gt; 1.4 has been announced - due out in June&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;Cloudera CDH&lt;/a&gt; 6.0 has been announced, out now in beta&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-altus/&quot;&gt;Cloudera Altus&lt;/a&gt; Data Engineering is now GA and Analytics DB is now beta on Azure&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Apache Hive&lt;/a&gt; 3.0 is out&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/hortonworks-dataplane-service/&quot;&gt;Hortonworks DataPlane Services&lt;/a&gt; is up to 1.1&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/streamsets-data-collector&quot;&gt;StreamSets Data Collector&lt;/a&gt; is up to 3.2&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Datanami and ZDNet have published their views on CDH 6.0 - &lt;a href=&quot;https://www.datanami.com/2018/05/22/cloudera-gives-data-scientists-more-options-for-ml/&quot;&gt;Datanami&lt;/a&gt;; &lt;a href=&quot;https://www.zdnet.com/article/cloudera-enterprise-6-hits-the-streets/&quot;&gt;ZDNet&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Hortonworks has published it’s latest blog entry on using Docker on &lt;a href=&quot;/technologies/apache-hadoop/yarn/&quot;&gt;YARN&lt;/a&gt; - &lt;a href=&quot;https://hortonworks.com/blog/trying-containerized-applications-apache-hadoop-yarn-3-1/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Nexla has published a whitepaper on Hadoop &lt;a href=&quot;/tech-categories/data-storage-formats/&quot;&gt;Data Storage Formats&lt;/a&gt; - &lt;a href=&quot;https://www.nexla.com/new-whitepaper-understanding-avro-parquet-orc/&quot;&gt;link&lt;/a&gt; (via &lt;a href=&quot;https://www.datanami.com/2018/05/16/big-data-file-formats-demystified/&quot;&gt;Datanami&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Couple of security announcements this week &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=2018-8012&quot;&gt;CVE-2018-8012&lt;/a&gt; - no authentication/authorization is enforced when a server attempts to join a quorum in &lt;a href=&quot;/technologies/apache-zookeeper/&quot;&gt;Apache ZooKeeper&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=2018-8010&quot;&gt;CVE-2018-8010&lt;/a&gt; - XML external entity expansion (XXE) in &lt;a href=&quot;/technologies/apache-solr/&quot;&gt;Apache Solr&lt;/a&gt; config files&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;And we like these - Data Engineers are more in demand than Data Scientists - &lt;a href=&quot;https://www.infoworks.io/2018/04/23/data-engineers-greater-demand-data-scientists/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News 06/06/2018</title><link>https://ondataengineering.net/blog/2018/06/06/the-mid-week-news/</link><description> &lt;p&gt;It’s a bumper week this week as there was no news last week ‘cos I was on holiday! Lucky me. So here we go… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-atlas/&quot;&gt;Apache Atlas&lt;/a&gt; has hit 1.0&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-carbondata/&quot;&gt;Apache CarbonData&lt;/a&gt; has hit 1.4&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-flink/&quot;&gt;Apache Flink&lt;/a&gt; has hit 1.5&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-ignite/&quot;&gt;Apache Ignite&lt;/a&gt; has hit 2.5&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-slider/&quot;&gt;Apache Slider&lt;/a&gt; has been retired from the Apache Incubator&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/scality-ring/&quot;&gt;Scality RING&lt;/a&gt; has hit 7.4&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/streamsets-data-collector/&quot;&gt;StreamSets Data Collector&lt;/a&gt; has hit 3.3&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ZDNet have a good summary of the upcoming &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Spark&lt;/a&gt; Summit - &lt;a href=&quot;https://www.zdnet.com/article/spark-summit-2018-preview-putting-ai-up-front-and-giving-r-and-python-programmers-more-respect/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Cloudera have published their latest post on deploying &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt;, this time looking at the cloud - &lt;a href=&quot;http://blog.cloudera.com/blog/2018/05/deploy-cloudera-edh-clusters-like-a-boss-revamped-part-3-cloud-considerations/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Hortonworks have published their lastest post on &lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Hadoop&lt;/a&gt; 3 looking at running &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Spark&lt;/a&gt; in Docker containers on Hadoop - &lt;a href=&quot;https://hortonworks.com/blog/containerized-apache-spark-yarn-apache-hadoop-3-1/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;And also from Hortonworks - their latest view on Data Steward Studio, part of the &lt;a href=&quot;/technologies/hortonworks-dataplane-service/&quot;&gt;DataPlane Service&lt;/a&gt; - &lt;a href=&quot;https://hortonworks.com/blog/discover-data-steward-studio-dss-understand-hybrid-data-lakes-exploit-business-value/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Amazon Neptune is now generally available - &lt;a href=&quot;https://aws.amazon.com/blogs/aws/amazon-neptune-generally-available/&quot;&gt;link&lt;/a&gt;; &lt;a href=&quot;https://www.zdnet.com/article/aws-neptune-going-ga-the-good-the-bad-and-the-ugly-for-graph-database-users-and-vendors/&quot;&gt;ZDNet view&lt;/a&gt;&lt;/li&gt; &lt;li&gt;TheRegister have a thought provoking piece on the rise of open source databases and what this means for Oracle - &lt;a href=&quot;https://www.theregister.co.uk/2018/05/31/rise_of_the_open_source_data_strategies/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;InfoQ have an interesting article on columnar databases, vectorisation and &lt;a href=&quot;/technologies/apache-arrow/&quot;&gt;Apache Arrow&lt;/a&gt; - &lt;a href=&quot;https://www.infoq.com/articles/columnar-databases-and-vectorization?utm_campaign=infoq_content&amp;amp;utm_source=infoq&amp;amp;utm_medium=feed&amp;amp;utm_term=global&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From ZDNet - how different data and analytics vendors are supporting GDPR - &lt;a href=&quot;https://www.zdnet.com/article/gdpr-what-the-data-companies-are-offering/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Databricks have announced Databricks Runtime 4.1 - &lt;a href=&quot;https://databricks.com/blog/2018/05/23/announcing-databricks-runtime-4-1.html&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;It looks like Google have just acquired Cask - &lt;a href=&quot;https://www.datanami.com/2018/05/22/google-cloud-adds-cask-data/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Confluent Cloud is now available on Google Cloud as well as AWS - &lt;a href=&quot;https://cloud.google.com/blog/big-data/2018/05/google-cloud-platform-and-confluent-partner-to-deliver-a-managed-apache-kafka-service&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;And rounding us out, a munch of vulnerability announcements this week &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2014-0114&quot;&gt;CVE-2014-0114&lt;/a&gt; - execution of arbitrary code with Apache Commons BeanUtils now impacts &lt;a href=&quot;/technologies/apache-ignite/&quot;&gt;Apache Ignite&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-1309&quot;&gt;CVE-2018-1309&lt;/a&gt; - &lt;a href=&quot;/technologies/apache-nifi/&quot;&gt;Apache NiFi&lt;/a&gt; External XML Entity issue in SplitXML processor&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-1310&quot;&gt;CVE-2018-1310&lt;/a&gt; - &lt;a href=&quot;/technologies/apache-nifi/&quot;&gt;Apache NiFi&lt;/a&gt; Apache NiFi JMS Deserialization issue&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2017-8028&quot;&gt;CVE-2017-8028&lt;/a&gt; - Spring Security LDPA issue impacts &lt;a href=&quot;/technologies/apache-nifi/&quot;&gt;Apache NiFi&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-1324&quot;&gt;CVE-2018-1324&lt;/a&gt; - Apache Commons Compress issue impacts &lt;a href=&quot;/technologies/apache-nifi/&quot;&gt;Apache NiFi&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Week That Was - 08/06/2018</title><link>https://ondataengineering.net/blog/2018/06/08/the-week-that-was/</link><description> &lt;p&gt;There are more major site updates coming, hopefully next week, but in the meantime there’s been some minor updates this week… &lt;!--more--&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Added Azure Time Series Insights to the &lt;a href=&quot;/tech-categories/time-series-databases/&quot;&gt;Time Series Databases&lt;/a&gt; tech category from which it was missing&lt;/li&gt; &lt;li&gt;Added Kafka Streams to the Streaming Analytics section of the &lt;a href=&quot;/tech-vendors/apache/&quot;&gt;Apache&lt;/a&gt; tech vendor page from which it was missing&lt;/li&gt; &lt;li&gt;Finally updated the &lt;a href=&quot;/technologies/apache-hadoop/hdfs/&quot;&gt;HDFS&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-hadoop/yarn/&quot;&gt;YARN&lt;/a&gt; pages for Hadoop 3.0 and 3.1, although both these summaries could do with more work&lt;/li&gt; &lt;li&gt;Finally updated the &lt;a href=&quot;/technologies/apache-druid/&quot;&gt;Druid&lt;/a&gt; page to rename this to Apache Druid following it’s donation to the Apache Foundation&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News 13/06/2018</title><link>https://ondataengineering.net/blog/2018/06/13/the-mid-week-news/</link><description> &lt;p&gt;After a double bumper week last week, it’s a little quieter on the news front this week… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-phoenix/&quot;&gt;Apache Phoenix&lt;/a&gt; has hit 4.14&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-nifi/minifi/&quot;&gt;MiNiFi&lt;/a&gt; C++ has hit 0.5&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/microsoft-azure-blob-storage/&quot;&gt;Azure Blob Storage&lt;/a&gt; now supports soft deletes - &lt;a href=&quot;https://azure.microsoft.com/en-us/blog/soft-delete-for-azure-storage-blobs-ga/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;If you’re into &lt;a href=&quot;/technologies/apache-hadoop/yarn/&quot;&gt;YARN&lt;/a&gt;, maybe a deep dive into the FairScheduler from Cloudera would be of interest - &lt;a href=&quot;http://blog.cloudera.com/blog/2018/06/yarn-fairscheduler-preemption-deep-dive/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;TheRegister has a story on SQream - purveyors of a GPU accelerated analytical database - and their latest funding round - &lt;a href=&quot;https://www.theregister.co.uk/2018/05/30/sqream_b_round_alibaba/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From DZone, monitoring &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt; using Burrow - &lt;a href=&quot;https://dzone.com/articles/kafka-monitoring-with-burrow&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;It looks like Cloudera pulled &lt;a href=&quot;/technologies/recordservice/&quot;&gt;RecordService&lt;/a&gt; from public view at the end of last year - the website and all GitHub repos have gone, replaced with a holding &lt;a href=&quot;https://www.cloudera.com/downloads/beta/record-service.html&quot;&gt;link&lt;/a&gt; saying it’s in development&lt;/li&gt; &lt;li&gt;Hortonworks have an interview on Data Lifecycle Manager, one of the services currently supported by &lt;a href=&quot;/technologies/hortonworks-dataplane-service/&quot;&gt;Hortonworks DataPlane Service&lt;/a&gt; - &lt;a href=&quot;https://hortonworks.com/blog/data-replication-hadoop/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News 20/06/2018</title><link>https://ondataengineering.net/blog/2018/06/20/the-mid-week-news/</link><description> &lt;p&gt;Strap yourself in, here’s the news… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cloudera have a bunch of new releases this week &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt; is up to 5.15&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-manager/&quot;&gt;Cloudera Manager&lt;/a&gt; is also up to 5.15&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-navigator/&quot;&gt;Cloudera Navigator&lt;/a&gt; is up to 2.14&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-director/&quot;&gt;Cloudera Director&lt;/a&gt; is up to 2.8&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-data-science-workbench/&quot;&gt;Cloudera Data Science Workbench&lt;/a&gt; is up to 1.4&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/elasticsearch/&quot;&gt;Elasticsearch&lt;/a&gt; has hit 6.3, along with &lt;a href=&quot;/technologies/elasticsearch-hadoop/&quot;&gt;Elasticsearch Hadoop&lt;/a&gt;, with &lt;a href=&quot;/technologies/elastic-x-pack/&quot;&gt;X-Pack&lt;/a&gt; now deprecated&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudbreak/&quot;&gt;Cloudbreak&lt;/a&gt; 2.7 is out, supporting Hortonworks new multi-cloud strategy&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-nifi/registry/&quot;&gt;NiFi Registry&lt;/a&gt; is up to 0.2&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hortonworks have announced &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;HDP&lt;/a&gt; 3.0, due out Q3 2018 - &lt;a href=&quot;https://hortonworks.com/blog/announcing-hdp-3-0-faster-smarter-hybrid-data/&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;https://www.datanami.com/2018/06/18/hortonworks-looks-to-expand-hybrid-cloud-footprints/&quot;&gt;Datanami&lt;/a&gt;; &lt;a href=&quot;https://www.zdnet.com/article/hortonworks-data-platform-turns-3-0-new-cloud-partnerships-announced/&quot;&gt;ZDNet&lt;/a&gt;; &lt;a href=&quot;https://www.theregister.co.uk/2018/06/18/hortonworks_enables_containerisation_in_latest_data_platform_release/&quot;&gt;The Register&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/microsoft-azure-blob-storage/&quot;&gt;Azure Blob Storage&lt;/a&gt; now has immutable storage support and lifecycle management in public preview - &lt;a href=&quot;https://azure.microsoft.com/en-us/blog/azure-immutable-blob-storage-now-in-public-preview/&quot;&gt;link&lt;/a&gt;; &lt;a href=&quot;https://azure.microsoft.com/en-us/blog/azure-blob-storage-lifecycle-management-public-preview/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Always interesting - from Datanami, with reference from Hortonworks - does open source make software more secure - &lt;a href=&quot;https://www.datanami.com/2018/06/19/does-open-source-boost-security-hortonworks-says-yes/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Elastic, on how they do one click rolling upgrades with their &lt;a href=&quot;/technologies/elastic-cloud/&quot;&gt;Elastic Cloud&lt;/a&gt; - &lt;a href=&quot;https://www.elastic.co/blog/upgrading-your-hosted-cluster-with-our-elasticsearch-service&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From ZDNet, an update on Teradata, and how restructuring their offerings and moving to the cloud is paying off - &lt;a href=&quot;https://www.zdnet.com/article/teradatas-bet-on-subscriptions-is-paying-off/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;More from Hortonworks on Hadoop 3.1 - this time their experiences on running Hive LLAP as a YARN service - &lt;a href=&quot;https://hortonworks.com/blog/apache-hive-llap-as-a-yarn-service/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From The Morning Paper - a deep dive on Medea, a scheduler with optimisations for long running applications now built into YARN in Hadoop 3.1 - &lt;a href=&quot;https://blog.acolyer.org/2018/06/13/medea-scheduling-of-long-running-applications-in-shared-production-clusters/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;GridGain have announced GridGain cloud, based on &lt;a href=&quot;/technologies/apache-ignite/&quot;&gt;Apache Ignite&lt;/a&gt; - &lt;a href=&quot;https://www.gridgain.com/resources/blog/gridgain-cloud-beta-in-memory-cache-service-sql-key-value-and-rest-apis&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Week That Was - 22/06/2018</title><link>https://ondataengineering.net/blog/2018/06/22/the-week-that-was/</link><description> &lt;p&gt;Some more minor updates whilst you wait patiently for new content… &lt;!--more--&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Added information on Greeplum GPText (analytical search in Greenplum using Solr) to &lt;a href=&quot;/technologies/greenplum/&quot;&gt;Greenplum&lt;/a&gt; and &lt;a href=&quot;/tech-categories/analytical-search/&quot;&gt;Analytical Search&lt;/a&gt; pages&lt;/li&gt; &lt;li&gt;Refreshed the Teradata information on the &lt;a href=&quot;/tech-categories/analytical-databases/&quot;&gt;Analytical Database&lt;/a&gt; page&lt;/li&gt; &lt;li&gt;Updated the &lt;a href=&quot;/technologies/elasticsearch/&quot;&gt;ElasticSearch&lt;/a&gt; page to reflect the fact that &lt;a href=&quot;/technologies/elastic-x-pack/&quot;&gt;X-Pack&lt;/a&gt; components are now bundled&lt;/li&gt; &lt;li&gt;Updated &lt;a href=&quot;/tech-vendors/google-cloud-platform/&quot;&gt;Google Cloud Platform&lt;/a&gt; page for Google Kubernetes Engine, and &lt;a href=&quot;/tech-vendors/microsoft-azure/&quot;&gt;Microsoft Azure&lt;/a&gt; for Azure Kubernetes Service (and to add in Azure Analysis Services and Azure Log Analytics)&lt;/li&gt; &lt;li&gt;Added Azure Data Factory and AWS Data Pipelines to &lt;a href=&quot;/tech-categories/data-ingestion/&quot;&gt;Data Ingestion&lt;/a&gt; page&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;As always, you can get to the page history from the link on the left on an site page.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News 27/06/2018</title><link>https://ondataengineering.net/blog/2018/06/27/the-mid-week-news/</link><description> &lt;p&gt;After a busy few weeks, there’s a few tumbleweeds running through this weeks news… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-kylin/&quot;&gt;Apache Kylin&lt;/a&gt; is up to 2.4&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-nifi/&quot;&gt;Apache NiFi&lt;/a&gt; is up to 2.7&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/greenplum/&quot;&gt;Greenplum&lt;/a&gt; is up to 5.9&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/pravega/&quot;&gt;Pravega&lt;/a&gt; is up to 0.3&lt;/li&gt; &lt;li&gt;There are new links for the recent releases of &lt;a href=&quot;/technologies/cloudera-director/&quot;&gt;Cloudera Director&lt;/a&gt; &amp;amp; &lt;a href=&quot;/technologies/cloudera-data-science-workbench/&quot;&gt;Cloudera Data Science Workbench&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Confluent have announced Confluent Hub - a place to get (and share) connectors, transformations and converters for &lt;a href=&quot;/technologies/apache-kafka/kafka-connect/&quot;&gt;Kafka Connect&lt;/a&gt; - &lt;a href=&quot;https://www.confluent.io/blog/introducing-confluent-hub/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;(via DZone) a tongue-in-cheek history of of Big Data - &lt;a href=&quot;https://hackernoon.com/story-of-big-data-a-technical-comedy-98630e9bd6bf&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News 04/07/2018</title><link>https://ondataengineering.net/blog/2018/07/04/the-mid-week-news/</link><description> &lt;p&gt;ok - time for the news again… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-beam/&quot;&gt;Apache Beam&lt;/a&gt; has hit 2.5&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-gobblin/&quot;&gt;Apache Gobblin&lt;/a&gt; has hit 0.11&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-solr/&quot;&gt;Apache Solr&lt;/a&gt; has hit 7.4&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-zeppelin/&quot;&gt;Apache Zeppelin&lt;/a&gt; has hit 0.8&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hortonworks have the latest in their long set of posts on &lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Hadoop&lt;/a&gt; 3.x - this time on federated name nodes - &lt;a href=&quot;https://hortonworks.com/blog/increasing-hadoop-storage-scale-4x/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/tech-vendors/google-cloud-platform/&quot;&gt;Google&lt;/a&gt; have announced a NAS storage offering - Google Cloud Filestore - &lt;a href=&quot;https://cloudplatform.googleblog.com/2018/06/New-Cloud-Filestore-service-brings-GCP-users-high-performance-file-storage.html&quot;&gt;link&lt;/a&gt;; &lt;a href=&quot;https://www.theregister.co.uk/2018/06/28/goog_cloud_platform_filestore_beta/&quot;&gt;The Register&lt;/a&gt;&lt;/li&gt; &lt;li&gt;There’s a new Apache incubator proposal - this time for Doris, and MPP interative SQL data warehouse built on &lt;a href=&quot;/technologies/apache-impala/&quot;&gt;Apache Impala&lt;/a&gt; with a storage layer inspired by Google Mesa that’s come from Baidu where it was known as Palo - &lt;a href=&quot;https://wiki.apache.org/incubator/DorisProposal&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;MapR have starting talking about version 6.1 of the &lt;a href=&quot;/technologies/mapr-converged-data-platform/&quot;&gt;MapR Data Platform&lt;/a&gt; - &lt;a href=&quot;https://mapr.com/blog/mapr-6-1-new-horizons/&quot;&gt;link1&lt;/a&gt;; &lt;a href=&quot;https://mapr.com/blog/data-tiering-capacity-performance-juxtaposition/&quot;&gt;link2&lt;/a&gt;; &lt;a href=&quot;https://www.datanami.com/2018/06/26/mapr-makes-platform-more-cloud-like/&quot;&gt;Datanami view&lt;/a&gt;&lt;/li&gt; &lt;li&gt;And a couple of big updates for &lt;a href=&quot;/tech-vendors/microsoft-azure/&quot;&gt;Azure&lt;/a&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/azure-hdinsight/&quot;&gt;Azure HDInsight&lt;/a&gt; has had an upgrade and is now 50% cheaper - &lt;a href=&quot;https://azure.microsoft.com/en-us/blog/enterprises-get-deeper-insights-with-hadoop-and-spark-updates-on-azure-hdinsight/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;And &lt;a href=&quot;/technologies/microsoft-azure-data-lake-store/&quot;&gt;Azure Data Lake Storage&lt;/a&gt; has had a Gen2 upgrade, which integrates it with &lt;a href=&quot;/technologies/microsoft-azure-blob-storage/&quot;&gt;Azure Blob Storage&lt;/a&gt; with a shared hierarchical name space - &lt;a href=&quot;https://azure.microsoft.com/en-us/blog/a-closer-look-at-azure-data-lake-storage-gen2/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;IDC have their latest &lt;a href=&quot;/tech-categories/object-stores/&quot;&gt;object storage&lt;/a&gt; marketscape out - &lt;a href=&quot;https://www.theregister.co.uk/2018/06/28/ddn_idc_marketplace/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;And Matt Turck has his 2018 Big and AI Landscape out - well worth a read - &lt;a href=&quot;http://mattturck.com/bigdata2018/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News 11/07/2018</title><link>https://ondataengineering.net/blog/2018/07/11/the-mid-week-news/</link><description> &lt;p&gt;It’s the middle of the week, and it’s time for the news… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-ranger/&quot;&gt;Apache Ranger&lt;/a&gt; has hit 1.1&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/alluxio/&quot;&gt;Alluxio&lt;/a&gt; has hit 1.8&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-nifi/minifi/&quot;&gt;MiNiFi&lt;/a&gt; Java has hit 0.5&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/influxdb/&quot;&gt;InfluxDB&lt;/a&gt; has hit 1.5&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;From Datanami, an article on how Neustar used Arcadia Data (now added to our &lt;a href=&quot;/tech-categories/query-engines/&quot;&gt;Query Engines&lt;/a&gt; page) to better exploit data in Hadoop - &lt;a href=&quot;https://www.datanami.com/2018/07/09/neustar-finds-the-last-mile-for-hadoop-analytics/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Confluent have a post on the upcoming features for their &lt;a href=&quot;/technologies/confluent-open-source&quot;&gt;Confluent Open Source&lt;/a&gt; and &lt;a href=&quot;/technologies/confluent-enterprise&quot;&gt;Confluent Enterprise&lt;/a&gt;] products - &lt;a href=&quot;https://www.confluent.io/blog/june-preview-release-confluent-plaform/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From DZone - is ETL still relevant in the new world - &lt;a href=&quot;https://dzone.com/articles/transforming-etl-in-data-driven-age&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;There’s a new Solr vulnarability announcement - &lt;a href=&quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-8026&quot;&gt;CVE-2018-8026&lt;/a&gt; - XML external entity expansion (XXE) in Solr config files&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News 18/07/2018</title><link>https://ondataengineering.net/blog/2018/07/18/the-mid-week-news/</link><description> &lt;p&gt;Ok - news time again, and this weeks sees the release of Hortonworks Data Platform (HDP) 3.0… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-phoenix/&quot;&gt;Apache Phoenix&lt;/a&gt; has hit 5.0&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt; has hit the big 3.0 - lots of info after the link&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-ambari/&quot;&gt;Apache Ambari&lt;/a&gt; has hit 2.7&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/hortonworks-smartsense/&quot;&gt;Hortonworks SmartSense&lt;/a&gt; has hit 1.5&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;From DataBricks - why their platform is the best for running Spark due to the level of automation of scaling, management and monitoring - &lt;a href=&quot;https://databricks.com/blog/2018/07/17/apache-spark-clusters-in-autopilot-mode.html&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From AWS - running Spark pipelines using Apache Airflow and Apach Livy on &lt;a href=&quot;/technologies/amazon-emr/&quot;&gt;Amazon EMR&lt;/a&gt; - &lt;a href=&quot;https://aws.amazon.com/blogs/big-data/build-a-concurrent-data-orchestration-pipeline-using-amazon-emr-and-apache-livy/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From ZDNet - views on Kafka and it’s uptake - &lt;a href=&quot;https://www.zdnet.com/article/kafka-is-establishing-its-toehold/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Looks like Azure SQL Data Warehouse just got a performance boost, now making it (allegedly) cheaper and faster than Redshift - &lt;a href=&quot;https://azure.microsoft.com/en-us/blog/lightning-fast-query-performance-with-azure-sql-data-warehouse/&quot;&gt;link&lt;/a&gt;; &lt;a href=&quot;https://azure.microsoft.com/en-us/blog/azure-sets-new-performance-benchmarks-with-sql-data-warehouse/&quot;&gt;benchmark summary&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Snowflake is now available on Azure (as well as AWS) - &lt;a href=&quot;https://www.zdnet.com/article/snowflakes-cloud-data-warehouse-comes-to-microsoft-azure/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Tamr has secured a new funding round - &lt;a href=&quot;https://www.datanami.com/2018/07/11/tamr-preps-for-growth-with-18m-round/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Benchmarking for &lt;a href=&quot;/technologies/elasticsearch/&quot;&gt;ElasticSearch&lt;/a&gt; - Rally has hit 1.0 - &lt;a href=&quot;https://www.elastic.co/blog/rally-1-0-0-released-benchmark-elasticsearch-like-we-do&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News 25/07/2018</title><link>https://ondataengineering.net/blog/2018/07/25/the-mid-week-news/</link><description> &lt;p&gt;Time for the news again. Next news will be in three weeks as I’m taking some time off… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-calcite/&quot;&gt;Apache Calcite&lt;/a&gt; has hit 1.17&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-hbase/&quot;&gt;Apache HBase&lt;/a&gt; has hit 2.1&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-ignite/&quot;&gt;Apache Ignite&lt;/a&gt; has hit 2.6&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-altus/&quot;&gt;Cloudera Altus&lt;/a&gt; SDX is now in beta&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;From Elastic - an Introduction to &lt;a href=&quot;/technologies/elasticsearch/&quot;&gt;Elasticsearch&lt;/a&gt; SQL - &lt;a href=&quot;https://www.elastic.co/blog/an-introduction-to-elasticsearch-sql-with-practical-examples-part-1&quot;&gt;part 1&lt;/a&gt;; &lt;a href=&quot;https://www.elastic.co/blog/an-introduction-to-elasticsearch-sql-with-practical-examples-part-2&quot;&gt;part 2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From The Register - what does 11-nines availablity (looking at &lt;a href=&quot;/tech-categories/object-stores/&quot;&gt;Object Stores&lt;/a&gt;) actually mean - &lt;a href=&quot;https://www.theregister.co.uk/2018/07/19/data_durability_statements/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From InfoQ, an interview with Hortonworks on &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;HDP&lt;/a&gt; 3.0 - &lt;a href=&quot;https://www.infoq.com/news/2018/07/hdp-30-ga&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From the Apache Kylin blog - benchmarking &lt;a href=&quot;/technologies/apache-kylin/&quot;&gt;Kylin&lt;/a&gt; with a modified version of the TPC-H benchmark - &lt;a href=&quot;http://kylin.apache.org/blog/2018/07/16/Star-Schema-Benchmark-on-Apache-Kylin/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Google - running JanusGraph (nee Titan) on Google Cloud over BigTable and Elasticsearch - &lt;a href=&quot;https://cloud.google.com/blog/big-data/2018/07/developing-a-janusgraph-backed-service-on-google-cloud-platform&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Databricks 4.2 is announced - &lt;a href=&quot;https://databricks.com/blog/2018/07/18/announcing-databricks-runtime-4-2.html&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Confluent - the difference between a &lt;a href=&quot;/tech-categories/streaming-data-stores/&quot;&gt;Streaming Data Store&lt;/a&gt; (specifically &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Kafka&lt;/a&gt;) and an Enterprise Service Bus - &lt;a href=&quot;https://www.confluent.io/blog/apache-kafka-vs-enterprise-service-bus-esb-friends-enemies-or-frenemies/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From the Pravega blog - streaming data in and out of &lt;a href=&quot;/technologies/pravega/&quot;&gt;Pravega&lt;/a&gt; - &lt;a href=&quot;http://blog.pravega.io/2018/02/12/streams-in-and-out-of-pravega/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From ZDNet - &lt;a href=&quot;/technologies/apache-cassandra/&quot;&gt;Apache Cassandra&lt;/a&gt; turns 10 - &lt;a href=&quot;https://www.zdnet.com/article/apache-cassandra-turns-10/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-ignite/&quot;&gt;Apache Ignite&lt;/a&gt; has a couple of security announcements - &lt;a href=&quot;http://www.cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-1273&quot;&gt;CVE-2018-1273&lt;/a&gt; and &lt;a href=&quot;http://www.cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-8018&quot;&gt;CVE-2018-8018&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News 15/08/2018</title><link>https://ondataengineering.net/blog/2018/08/15/the-mid-week-news/</link><description> &lt;p&gt;So we’re back, and given it’s been three weeks, there’s a bumper load of news this week… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-arrow/&quot;&gt;Apache Arrow&lt;/a&gt; has hit 0.10&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-beam/&quot;&gt;Apache Beam&lt;/a&gt; is up to 2.6&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-drill/&quot;&gt;Apache Drill&lt;/a&gt; is up to 1.14&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-flink/&quot;&gt;Apache Flink&lt;/a&gt; is now 2.6&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Apache Hive&lt;/a&gt; is up to 3.1&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt; has hit 2.0&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-knox/&quot;&gt;Apache Knox&lt;/a&gt; is up to 1.1&lt;/li&gt; &lt;li&gt;Confluent &lt;a href=&quot;/technologies/confluent-open-source/&quot;&gt;Open Source&lt;/a&gt; and &lt;a href=&quot;/technologies/confluent-enterprise&quot;&gt;Enterprise&lt;/a&gt; have hit 5.0&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/greenplum/&quot;&gt;Greenplum&lt;/a&gt; has hit 5.10&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/streamsets-data-collector/&quot;&gt;Streamsets Data Collector&lt;/a&gt; has hit 3.4&lt;/li&gt; &lt;li&gt;Cloudera Altus Analytical DB is now &lt;a href=&quot;/technologies/cloudera-altus/data-warehouse/&quot;&gt;Cloudera Altus Data Warehouse&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;From DataNami - storage, and how the future of storage is software - &lt;a href=&quot;https://www.datanami.com/2018/08/03/the-future-of-storage-software/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Hortonworks - GPU support in Hadoop 3.1 - &lt;a href=&quot;https://hortonworks.com/blog/gpus-support-in-apache-hadoop-3-1-yarn-hdp-3/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From ZDNet - advances in self tuning data structures - &lt;a href=&quot;https://www.zdnet.com/article/zen-and-the-art-of-data-structures-from-self-tuning-to-self-designing-data-systems/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Cloudera have a three part blog on how to roll your own schema management tool for &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Kafka&lt;/a&gt; using a Kafka topic to store your schemas - &lt;a href=&quot;http://blog.cloudera.com/blog/2018/07/robust-message-serialization-in-apache-kafka-using-apache-avro-part-1/&quot;&gt;part1&lt;/a&gt;; &lt;a href=&quot;http://blog.cloudera.com/blog/2018/07/robust-message-serialization-in-apache-kafka-using-apache-avro-part-2/&quot;&gt;part2&lt;/a&gt;; &lt;a href=&quot;http://blog.cloudera.com/blog/2018/08/robust-message-serialization-in-apache-kafka-using-apache-avro-part-3/&quot;&gt;part3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Datnami - a view on parallel graph databases (which we list under our &lt;a href=&quot;/tech-categories/analytical-databases/&quot;&gt;Analytical Databases&lt;/a&gt;) category - &lt;a href=&quot;https://www.datanami.com/2018/08/09/is-golap-the-next-wave-for-big-data-warehousing/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Elastic have issues a security warning for &lt;a href=&quot;/technologies/elasticsearch/&quot;&gt;Elastic&lt;/a&gt; 6.3.0 and 6.3.1 which “May Disable Security for Trial Licenses” - &lt;a href=&quot;https://www.elastic.co/blog/elastic-stack-6-3-0-and-6-3-1-may-disable-security-for-trial-licenses&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Hortonworks financial results are out - &lt;a href=&quot;https://www.datanami.com/2018/08/07/hortonworks-handily-beats-analyst-estimates/&quot;&gt;Dataname&lt;/a&gt;; &lt;a href=&quot;https://www.theregister.co.uk/2018/08/08/hortonworks_revenues_up_losses_down_q2_2018/&quot;&gt;The Register&lt;/a&gt;&lt;/li&gt; &lt;li&gt;GridGain have a two part blog on how to use &lt;a href=&quot;/technologies/apache-ignite/&quot;&gt;Apache Ignite&lt;/a&gt; to accelerate &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Spark&lt;/a&gt; - &lt;a href=&quot;https://www.gridgain.com/resources/blog/apacher-ignitetm-and-apacher-sparktm-integration-using-ignite-rdds&quot;&gt;part1&lt;/a&gt;; &lt;a href=&quot;https://www.gridgain.com/resources/blog/apacher-ignitetm-and-apacher-sparktm-integration-using-ignite-dataframes&quot;&gt;oart2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;DLab has been submitted to the Apache Incubator - “a platform for creating self-service, exploratory data science environments in the cloud using best-of-breed data science tools” - &lt;a href=&quot;https://wiki.apache.org/incubator/DLabProposal&quot;&gt;proposal&lt;/a&gt;&lt;/li&gt; &lt;li&gt;An in depth piece from Elastic on designing and sizing &lt;a href=&quot;/technologies/elasticsearch/&quot;&gt;Elasticsearch&lt;/a&gt; clusters, with a specific focus on &lt;a href=&quot;/technologies/elastic-cloud/&quot;&gt;Elastic Cloud&lt;/a&gt; - &lt;a href=&quot;https://www.elastic.co/blog/sizing-hot-warm-architectures-for-logging-and-metrics-in-the-elasticsearch-service-on-elastic-cloud&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;This is always fun - thoughts from Microsoft on running a 50 thousand node Hadoop cluster - &lt;a href=&quot;https://azure.microsoft.com/blog/how-microsoft-drives-exabyte-analytics-on-the-world-s-largest-yarn-cluster/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Amazon Redshift Spectrum now supports querying nested data structured - &lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2018/08/amazon-redshift-announces-support-for-nested-data-with-redshift-spectrum/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt; has a couple of security announcements - &lt;a href=&quot;http://www.cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-1288&quot;&gt;CVE-2018-1288&lt;/a&gt; and &lt;a href=&quot;http://www.cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2017-12610&quot;&gt;CVE-2017-12610&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News 22/08/2018</title><link>https://ondataengineering.net/blog/2018/08/22/the-mid-week-news/</link><description> &lt;p&gt;It’s Wednesday, so it’s time for the news again… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/hortonworks-data-flow/&quot;&gt;Hortonworks Data Flow&lt;/a&gt; is up to 3.2&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Looks like Hortonworks have a new &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Kafka&lt;/a&gt; monitoring and management solution coming out - &lt;a href=&quot;https://hortonworks.com/blog/kafka-blindness/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Looks like Cloudera have a new cloud service for getting insights into your Cloudera Spark, MapReduce and Impala workloads - &lt;a href=&quot;https://www.cloudera.com/products/workload-xm.html&quot;&gt;product page&lt;/a&gt;; &lt;a href=&quot;https://www.cloudera.com/documentation/wxm/latest.html&quot;&gt;docs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/zenko/&quot;&gt;Zenko&lt;/a&gt; has an update on it’s multi cloud object store controller ahead of it’s 1.0 release - &lt;a href=&quot;https://www.zenko.io/blog/bright-future-multi-cloud/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From the very excellent “the morning paper” - Sparser delivers an order of magnitude increase in JSON parsing - &lt;a href=&quot;https://blog.acolyer.org/2018/08/20/filter-before-you-parse-faster-analytics-on-raw-data-with-sparser/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Confluent - building UDFs in KSQL - &lt;a href=&quot;https://www.confluent.io/blog/build-udf-udaf-ksql-5-0&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;New features for customising your infrastructure when using &lt;a href=&quot;/technologies/elastic-cloud/&quot;&gt;Elastic Cloud&lt;/a&gt; - &lt;a href=&quot;https://www.elastic.co/blog/stretching-the-cloud-flexibility-in-elastic-cloud-deployments&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Airbnd - details on their enterprise Change Data Capture solution SpinalTap - &lt;a href=&quot;https://medium.com/airbnb-engineering/capturing-data-evolution-in-a-service-oriented-architecture-72f7c643ee6f&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From NewRelic - best practice for working with &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Kafka&lt;/a&gt; at scale - &lt;a href=&quot;https://blog.newrelic.com/engineering/kafka-best-practices/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Only last week we talked about DLab (“a platform for creating self-service, exploratory data science environments in the cloud using best-of-breed data science tools”) being submitted to the Apache Incubator - well it’s now been accepted into Incubation - &lt;a href=&quot;http://incubator.apache.org/projects/dlab.html&quot;&gt;incubator page&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News 29/08/2018</title><link>https://ondataengineering.net/blog/2018/08/29/the-mid-week-news/</link><description> &lt;p&gt;Right - time for the news again… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/hortonworks-dataplane-service/&quot;&gt;Hortonworks DataPlane Service&lt;/a&gt; is up to 1.2&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/elasticsearch/&quot;&gt;Elasticsearch&lt;/a&gt; has hit 6.4, along with &lt;a href=&quot;/technologies/elasticsearch-hadoop/&quot;&gt;Elasticsearch Hadoop&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-hawq/&quot;&gt;Apache HAWQ&lt;/a&gt; has graduated from the Apache Incubator - &lt;a href=&quot;https://blogs.apache.org/foundation/entry/the-apache-software-foundation-announces38&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Hortonworks have formally announced their new &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Kafka&lt;/a&gt; monitoring and management solution called Hortonworks Streams Messaging Manager, and we’ll try and do a tech summary soon - &lt;a href=&quot;https://hortonworks.com/blog/introducing-hortonworks-streams-messaging-manager-smm/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Datanami - their view on the latest Gartner Hype Cycle - &lt;a href=&quot;https://www.datanami.com/2018/08/27/gartner-sees-ai-democratized-in-latest-hype-cycle/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Databricks now have a built in connector to Snowflake - &lt;a href=&quot;https://databricks.com/blog/2018/08/27/by-customer-demand-databricks-and-snowflake-integration.html&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News 05/09/2018</title><link>https://ondataengineering.net/blog/2018/09/05/the-mid-week-news/</link><description> &lt;p&gt;It’s a slower news week this week, but there’s a big release from Cloudera… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cloudera have released v6.0 of &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt;, &lt;a href=&quot;/technologies/cloudera-manager/&quot;&gt;Cloudera Manager&lt;/a&gt; and &lt;a href=&quot;/technologies/cloudera-navigator/&quot;&gt;Cloudera Navigator&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;There’s a whole pile of analyst thoughts on Cloudera’s rename of Altus Analytical DB to &lt;a href=&quot;/technologies/cloudera-altus/data-warehouse/&quot;&gt;Altus Data Warehouse&lt;/a&gt; - &lt;a href=&quot;https://www.datanami.com/2018/08/28/cloudera-pivots-from-zoo-animals-to-data-warehousing/&quot;&gt;Datanami&lt;/a&gt;; &lt;a href=&quot;https://www.zdnet.com/article/clouderas-a-data-warehouse-player-now/&quot;&gt;ZDNet&lt;/a&gt;; &lt;a href=&quot;https://www.theregister.co.uk/2018/08/29/cloudera_launches_data_warehouse_as_a_service/&quot;&gt;The Register&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Hortonworks have a post on disaster recovery with Data Lifecycle Manager, part of their &lt;a href=&quot;/technologies/hortonworks-dataplane-service/&quot;&gt;Hortonworks DataPlane Service&lt;/a&gt; - &lt;a href=&quot;https://hortonworks.com/blog/painless-disaster-recovery-using-hortonworks-data-lifecycle-manager/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;SwiftStack have a post on how &lt;a href=&quot;/technologies/openstack-swift/&quot;&gt;OpenStack Swift&lt;/a&gt; can now support billions of objects - &lt;a href=&quot;https://www.swiftstack.com/blog/2018/08/30/billions-and-billions-of-objects/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Datanami have an interesting post on Octopai - a product allegedly capable of reverse engineering ETL processes to provide data lineage - &lt;a href=&quot;https://www.datanami.com/2018/08/31/reverse-engineering-etl-jobs-for-fun-and-profit/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-flink/&quot;&gt;Apache Flink&lt;/a&gt; now supports serializable ACID transactions on streaming data - &lt;a href=&quot;https://data-artisans.com/blog/serializable-acid-transactions-on-streaming-data&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News 12/09/2018</title><link>https://ondataengineering.net/blog/2018/09/12/the-mid-week-news/</link><description> &lt;p&gt;Can I help you? You’re here for the news? Let’s crack on then… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;None this week&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The big news this week is Hortonworks Open Hybrid Architecture Initiative, and I’ll do a post soon to dig into this a big more - &lt;a href=&quot;https://hortonworks.com/blog/bringing-cloud-native-architecture-to-big-data-in-the-data-center/&quot;&gt;link&lt;/a&gt;; &lt;a href=&quot;https://www.datanami.com/2018/09/10/open-hybrid-initiative-targets-big-data-workloads/&quot;&gt;Datanami view&lt;/a&gt;; &lt;a href=&quot;https://www.zdnet.com/article/hortonworks-unveils-roadmap-to-make-hadoop-cloud-native/&quot;&gt;ZDNet view&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Hortonworks have anther post on their new new &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Kafka&lt;/a&gt; monitoring and management solution called Hortonworks Streams Messaging Manager, part of their DataPlane offering - &lt;a href=&quot;https://hortonworks.com/blog/solving-needs-devops-developers-streams-messaging-manager/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;And on the subjects of Hortonworks DataPlane, they also seem to have snuck out a new module - Data Analytics Studio - &lt;a href=&quot;https://hortonworks.com/products/dataplane/data-analytics-studio/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Netflix have a blog on Keystone - their streaming analytics architecture - &lt;a href=&quot;https://medium.com/netflix-techblog/keystone-real-time-stream-processing-platform-a3ee651812a&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Datanami, and interview with the CEO of Mindbreeze on insight engines - &lt;a href=&quot;https://www.datanami.com/2018/09/06/what-is-an-insight-engine-and-other-questions/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Amazon S3 Select (server side filtering of data) now supports Parquet, CSV, JSON and BZIP2 - &lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2018/09/amazon-s3-announces-new-features-for-s3-select/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Confluent - building a streaming data ingestion architecture using &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Kafka&lt;/a&gt;, Kafka Connect and KSQL - &lt;a href=&quot;https://www.confluent.io/blog/data-wrangling-apache-kafka-ksql&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/azure-hdinsight/&quot;&gt;Azure HDInsight&lt;/a&gt; has a bunch of new capabilities - &lt;a href=&quot;https://azure.microsoft.com/en-gb/blog/exciting-new-capabilities-on-azure-hdinsight/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From ZDNet - Google can now find you datasets - &lt;a href=&quot;https://www.zdnet.com/article/google-can-now-search-for-datasets-first-research-then-the-world/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;We had the Datanami view last week, this week it’s The Registers view on the latest Gartner Hype Cycle - &lt;a href=&quot;https://www.theregister.co.uk/2018/09/11/gartner_dataops/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News 19/09/2018</title><link>https://ondataengineering.net/blog/2018/09/19/the-mid-week-news/</link><description> &lt;p&gt;There’s some new content coming this week, but in the mean time lets check in with the news… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/greenplum/&quot;&gt;Greenplum&lt;/a&gt; has hit 5.11&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-altus/director/&quot;&gt;Cloudera Altus Director&lt;/a&gt; (née Cloudera Director) has hit the big 6.0&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Datanami have their 2018 Readers and Editors awards out - &lt;a href=&quot;https://www.datanami.com/2018/09/14/datanami-reveals-winners-of-2018-readers-and-editors-choice-awards/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Hortonworks have formally announced their new Data Analytics Studio model for DataPlane - [https://hortonworks.com/blog/announcing-general-availability-data-analytics-studio/]&lt;/li&gt; &lt;li&gt;Looks like Streamsets (makes of &lt;a href=&quot;/technologies/streamsets-data-collector/&quot;&gt;StreamSets Data Collector&lt;/a&gt; have raised some more money - &lt;a href=&quot;https://streamsets.com/blog/series-c-funding-right-where-we-knew-wed-be/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qubole have published their 2018 Big Data Trends and Challenges Survery Report, with &lt;a href=&quot;/technologies/apache-flink/&quot;&gt;Flink&lt;/a&gt; and Presto apparently seeing a big increase in uptake - &lt;a href=&quot;https://www.qubole.com/blog/big-data-projects-are-increasing-in-breadth-depth-and-inclusivity/&quot;&gt;link&lt;/a&gt;; &lt;a href=&quot;https://www.datanami.com/2018/09/11/increased-complexity-is-dragging-on-big-data/&quot;&gt;Datanami view&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Databricks have a post on Flint, a time-series library for &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Spark&lt;/a&gt; - &lt;a href=&quot;https://databricks.com/blog/2018/09/11/introducing-flint-a-time-series-library-for-apache-spark.html&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Confluent have another blog post on how Kafka and streaming pipelines are going to revolutionise ETL - &lt;a href=&quot;https://www.confluent.io/blog/changing-face-etl&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;It looks like Hortonworks &lt;a href=&quot;/technologies/hortonworks-data-cloud-for-aws/&quot;&gt;HDCloud for AWS&lt;/a&gt; has been quietly dropped with their new &lt;a href=&quot;/technologies/cloudbreak/&quot;&gt;Cloudbreak&lt;/a&gt; powered multi-cloud strategy with the documentation now gone&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Thoughts on Hortonwork's Open Hybrid Architecture Initiative</title><link>https://ondataengineering.net/blog/2018/09/21/thoughts-on-hortonworks-open-hybrid-architecture-initiative/</link><description> &lt;p&gt;So last week Hortonworks announced their Open Hybrid Architecture Initiative, and I said we’d take a look, so let’s take a look… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;So to start with, this is another “big” initiative from Hortonworks ala their Stinger initiative (which aimed to turn Hive into an interative analytical database platform), looking to drive a bunch of significant architectural changes into the core Hadoop platform.&lt;/p&gt; &lt;p&gt;But before we start, let’s do a quick potted history of Hadoop. Version 1 was about allowing companies run batch analytics (using MapReduce) over vast amounts of data using very large numbers of inexpensive machines. It started off as a pretty niche technology - you only needed it if you had the data volumes (and the skills) that meant there was no other alternative technology you could cost effectively use. However, version 2 brought YARN, allowing multiple different analytical tools (hello Spark, Hive on Tez, Impala etc.) to be used and nicely co-exist, and suddenly the Hadoop proposition changed from being about massive scale to being about flexibility - it was now a general purpose platform that gave you storage and compute, over which you can run a wide range of analytical workloads.&lt;/p&gt; &lt;p&gt;And as great as that was on-premises (I can use new analytical tools without deploying new infrastructure), in the cloud it doesn’t work (why would I provision a persistent Hadoop cluster when I’m not using it half the time) and isn’t needed (because storage and compute is already abstracted away). In the cloud &lt;a href=&quot;/tech-categories/object-stores/&quot;&gt;Object Stores&lt;/a&gt; are the de-facto standard for storage, and Kubernetes (and maybe a bit of Mesos) the emerging standard for compute. And if you need to abstract away storage and compute on-premises, it’s going to make sense to standardise on those technologies going forward. This moves us away from the original Hadoop premise of having the compute and storage co-located, but with the explosion of high bandwidth network connectivity that doesn’t feel like a huge issue any more.&lt;/p&gt; &lt;p&gt;So what does this mean for Hadoop? Well firstly it will have to evolve if it wants to remain relevant as a shared infrastructure platform, and it feels like this will involve support for an object store interface and the integration of Kubernetes, so workloads you run in the cloud using these technologies can also be run on your local Hadoop cluster. But Hadoop (and more specifically the Hadoop ecosystems sold by the big vendors) are more than just the underlying Infrastructure - they’re a set of analytical tools bound together by some shared standards - the Hadoop Compatible Filesystem specification, YARN, the Hive Metastore, and common security, audit and metadata integrations (although these aren’t standard across the Hadoop vendors). And there will still be lasting and enduring value in this, however these tools and workloads will need to evolve to exist in the new world.&lt;/p&gt; &lt;p&gt;And this is what Hortonworks are planning to do via their three point plan:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Containerisation of workloads - this means you’ll be able to spin up individual services (HBase, Hive etc.) or transient workloads (e.g. Spark) on demand, when you need, for as long as you need, and with exactly the dependencies you need. For Hortonworks this will be managed through their &lt;a href=&quot;/technologies/hortonworks-dataplane-service/&quot;&gt;DataPlane Service (DPS)&lt;/a&gt;, and one presumes you’ll (eventually) be able to do this on-site on your persistent Hadoop cluster or in the cloud on dynamically provisioned clusters.&lt;/li&gt; &lt;li&gt;Add an object store interface to HDFS - this is the new Hadoop &lt;a href=&quot;/technologies/apache-hadoop/ozone/&quot;&gt;Ozone&lt;/a&gt; project, which is introducing a new more scalable block storage layer (the &lt;a href=&quot;/technologies/apache-hadoop/hdds/&quot;&gt;Hadoop Distributed Data Store&lt;/a&gt; that will underpin both Ozone and HDFS. Have a read of our new technology summarises on the other side of these links for more details. However it’s worth noting here that the move from HDFS to Object Stores is not trivial, there there are still plenty of issues to address before object stores support the range of features that HDFS does, such as consistency and fine grained access control. And it’s also worth noting that Ozone will be an object store optimised for fast IO scanes (as it uses the HDFS storage layer), which makes it unusual in the object store world.&lt;/li&gt; &lt;li&gt;Add support for container management to Hadoop, with the suggestion that this means making Kubernetes into your on-premises Hadoop cluster, turning it into a platform that gives you object storage and Kubernetes, making it your on site cloud platform.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;So that’s Hortonworks, but what are the other big Hadoop vendors doing. Cloudera have already started their move to the cloud with &lt;a href=&quot;/technologies/cloudera-altus/&quot;&gt;Cloudera Altus&lt;/a&gt; which allows you to programatically spin up Hadoop workloads on transient clusters. And although they’re not outwardly supporting this Hortonworks initiative, you can bet they’re interested in containerisation of Hadoop workloads. ZDNet have a good interview with Mike Olsen &lt;a href=&quot;https://www.datanami.com/2018/09/19/mike-olson-on-zoo-animals-object-stores-and-the-future-of-cloudera/&quot;&gt;here&lt;/a&gt; that’s worth a read.&lt;/p&gt; &lt;p&gt;And MapR have kind of been here for a while. &lt;a href=&quot;/technologies/mapr-fs/&quot;&gt;MapR-FS&lt;/a&gt; is pretty much a cloud scale storage solution already, and they’re heavy sponsors of &lt;a href=&quot;/technologies/apache-myriad/&quot;&gt;Apache Myriad&lt;/a&gt; for running YARN jobs on Mesos clusters, so they’re heading in the same direction.&lt;/p&gt; &lt;p&gt;If you fancy some more reading, Hortonworks announcement is &lt;a href=&quot;https://hortonworks.com/blog/bringing-cloud-native-architecture-to-big-data-in-the-data-center/&quot;&gt;here&lt;/a&gt;, and ZDNet have a good write up &lt;a href=&quot;https://www.zdnet.com/article/hortonworks-unveils-roadmap-to-make-hadoop-cloud-native/&quot;&gt;here&lt;/a&gt; that is worth a read. There are also views from &lt;a href=&quot;https://www.datanami.com/2018/09/10/open-hybrid-initiative-targets-big-data-workloads/&quot;&gt;Datanami&lt;/a&gt;, &lt;a href=&quot;https://www.geekwire.com/2018/hortonworks-plans-revamp-hadoop-big-data-tools-cloud-best-practices-mind/&quot;&gt;GeekWire&lt;/a&gt;, &lt;a href=&quot;https://siliconangle.com/2018/09/10/hortonworks-launches-initiative-define-unified-hybrid-cloud-platform-big-data/&quot;&gt;Silicon Angle&lt;/a&gt; and &lt;a href=&quot;https://searchdatamanagement.techtarget.com/news/252448640/Containers-key-for-Hortonworks-alliance-on-big-data-hybrid&quot;&gt;Search Data Management&lt;/a&gt; if you want more reading.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News 26/09/2018</title><link>https://ondataengineering.net/blog/2018/09/26/the-mid-week-news/</link><description> &lt;p&gt;The news is looking a bit deeper this week, so let’s dive in… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-atlas/&quot;&gt;Apache Atlas&lt;/a&gt; has hit 1.1&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-gobblin/&quot;&gt;Apache Gobblin&lt;/a&gt; has hit 0.13&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-hawq/&quot;&gt;Apache Hawq&lt;/a&gt; has hit 2.4&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-kylin&quot;&gt;Apache Kylin&lt;/a&gt; has hit 2.5&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-parquet/&quot;&gt;Apache Parquet&lt;/a&gt; C++ has hit 1.5&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-solr/&quot;&gt;Apache Solr&lt;/a&gt; has hit 7.5&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudbreak/&quot;&gt;Cloudbreak&lt;/a&gt; has hit 2.8 (a Technical Preview release)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Apache Pulsar has graduated to a top level Apache project - &lt;a href=&quot;https://blogs.apache.org/foundation/entry/the-apache-software-foundation-announces39&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Looks like Cloudera have a new cloud service (Workload XM) for getting insights into your Cloudera Spark, MapReduce and Impala workloads - &lt;a href=&quot;https://www.cloudera.com/products/workload-xm.html&quot;&gt;product page&lt;/a&gt;; &lt;a href=&quot;https://www.cloudera.com/documentation/wxm/latest.html&quot;&gt;docs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Facebook have open sourced LogDevice (a distributed data store for sequential data) as promise - &lt;a href=&quot;https://code.fb.com/core-data/open-sourcing-logdevice-a-distributed-data-store-for-sequential-data/&quot;&gt;link&lt;/a&gt;; &lt;a href=&quot;https://logdevice.io/&quot;&gt;homepage&lt;/a&gt;&lt;/li&gt; &lt;li&gt;This is potentially interesting - Couchbase have bolted an Analytics Service onto the side of their database based on AsterixDB and SQL++ that supports analytical queries over JSON data - &lt;a href=&quot;&quot;&gt;link&lt;/a&gt;; &lt;a href=&quot;https://www.datanami.com/2018/09/20/couchbase-to-deliver-parallel-json-analytics-without-the-etl/&quot;&gt;Datanami view&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Hortonworks have a guide on upgrading your &lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Hadoop&lt;/a&gt; cluster to Hadoop 3 - &lt;a href=&quot;https://hortonworks.com/blog/upgrading-clusters-workloads-hadoop-2-hadoop-3/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From ZDNet - on &lt;a href=&quot;/tech-categories/graph-databases/&quot;&gt;graph databases&lt;/a&gt;, and specifcally TigerGraph and Memgraph (winner and runner up for Most Disruptive Startup at Strata) - &lt;a href=&quot;https://www.zdnet.com/article/knowledge-graphs-beyond-the-hype-getting-knowledge-in-and-out-of-graphs-and-databases/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Benchmark time! According to Fivetran, Azure SQL Data Warehouse is faster than Redhift, Snowflake, Presto and BigQuery - &lt;a href=&quot;https://fivetran.com/blog/warehouse-benchmark&quot;&gt;link&lt;/a&gt;; &lt;a href=&quot;https://azure.microsoft.com/en-us/blog/redefine-data-analytics-with-modern-data-warehouse-on-azure/&quot;&gt;Microsoft view&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Big bunch of updates from Microsoft &lt;ul&gt; &lt;li&gt;Azure Databricks now supports Databricks Delta - &lt;a href=&quot;https://databricks.com/blog/2018/09/24/databricks-delta-now-available-in-preview-as-part-of-microsoft-azure-databricks.html&quot;&gt;Databricks&lt;/a&gt;; &lt;a href=&quot;https://azure.microsoft.com/en-gb/blog/azure-databricks-delta-in-preview-9-regions-added-and-other-exciting-announcements/&quot;&gt;Microsoft&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/azure-hdinsight/&quot;&gt;Azure HDInsight&lt;/a&gt; 4.0 is in public preview based on HDP 3 - &lt;a href=&quot;https://azure.microsoft.com/en-gb/blog/azure-hdinsight-brings-next-generation-hadoop-3-0-and-enterprise-security-to-the-cloud/&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;https://azure.microsoft.com/en-us/blog/deep-dive-into-azure-hdinsight-4-0/&quot;&gt;deep dive&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Immutable storage for &lt;a href=&quot;/technologies/microsoft-azure-blob-storage/&quot;&gt;Azure Blob Storage&lt;/a&gt; is now generally available - &lt;a href=&quot;https://azure.microsoft.com/en-gb/blog/immutable-storage-for-azure-storage-blobs-now-generally-available/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;And on the subject of &lt;a href=&quot;/technologies/microsoft-azure-blob-storage/&quot;&gt;Azure Blob Storage&lt;/a&gt;, it now has a new Premium storage tier - &lt;a href=&quot;https://azure.microsoft.com/en-gb/blog/introducing-azure-premium-blob-storage-limited-public-preview/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;And from Google, Data Studio and Cloud Dataprep are now generally available - &lt;a href=&quot;https://cloud.google.com/blog/products/data-analytics/unlock-insights-with-ease-data-studio-and-cloud-dataprep-are-now-generally-available&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Thoughts on Hortonworks DataPlane</title><link>https://ondataengineering.net/blog/2018/09/28/thoughts-on-hortonworks-dataplane/</link><description> &lt;p&gt;&lt;em&gt;Updated 05/10/18 to reflect the fact that the DataPlane source code has been publshed on GitHub - thanks to Olivier from the comments&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I’ve spent the last week digging a little further into &lt;a href=&quot;/technologies/hortonworks-dataplane/&quot;&gt;Hortonworks DataPlane&lt;/a&gt;, and here are my thoughts.. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;We talked last week about how Hadoop was changing away from being a single shared physical platform to a more logical platform that’s better aligned with the emerging standard cloud technologies for storage and compute. What we didn’t talk about was the other shared goodness that went alongside the shared compute and storage - the shared metadata and single management, security and governance points that the Hadoop vendors have baked into their offerings. Because the move to the cloud and being a more logical platform is going to impact this as well - they will need to adapt to this change.&lt;/p&gt; &lt;p&gt;Cloudera have addressed this with SDX - a conceptual architecture (although now baked into &lt;a href=&quot;/technologies/cloudera-altus/&quot;&gt;Altus&lt;/a&gt; and &lt;a href=&quot;/technologies/cloudera-altus/director/&quot;&gt;Altus Director&lt;/a&gt;) that delivers a single instance of all their governance, management and security products across Hadoop jobs and services running on multiple clusters over a multiple heterogenous storage.&lt;/p&gt; &lt;p&gt;And as as we noted when we last looked at DataPlane (and SDX) &lt;a href=&quot;/blog/2017/11/03/thoughts-on-hortonworks-dataplane-and-cloudera-sdx/&quot;&gt;late last year&lt;/a&gt;, Hortonworks were positioning DataPlane to be the same, however their stated ambition was much grander - to be the standard management layer across your entire data ecosystem by being a pluggable framework that other vendors could exploit. It feels like that vision is a little way off however - at the moment the platform only supports Hortonworks Hadoop clusters (and only generally only Hive on those), and although there was a suggestion at one point that DataPlane would run as a cloud service, this doesn’t currently appear to be the case.&lt;/p&gt; &lt;p&gt;And let’s just note that the idea of having a single point of governance, management and security across your entire data ecosystem is not new - the big Data Integration vendors (Informatica, IBM etc.) have been banging on about elements of this for ever, and Forrester have now started talking about what they call the Big Data Fabric (score one for analysts introducing new terminology to make themselves sound clever). Definitely something for us to dive into in a big for detail in the future.&lt;/p&gt; &lt;p&gt;Back to DataPlane then, and the new applications are coming thick and fast. &lt;a href=&quot;/technologies/hortonworks-dataplane/data-lifecycle-manager/&quot;&gt;Data Lifecycle Manager&lt;/a&gt; looks like a pretty convenient way of configuring backup and replication of Hive and HDFS data, &lt;a href=&quot;/technologies/hortonworks-dataplane/data-steward-studio/&quot;&gt;Data Steward Studio&lt;/a&gt; could be the start of an interesting metadata management solution, &lt;a href=&quot;/technologies/hortonworks-dataplane/data-analytics-studio/&quot;&gt;Data Analytics Studio&lt;/a&gt; looks useful for enabling users to run queries and understand performance and for techies to understand the load on their Hive tables and what needs optimising, and &lt;a href=&quot;/technologies/hortonworks-dataplane/streams-messaging-manager/&quot;&gt;Streams Messaging Manager&lt;/a&gt; is undoubtedly going to be invaluable if you’re running Kafka with HDF.&lt;/p&gt; &lt;p&gt;And since we last looked, a bunch of DataPlane source code has been published on GitHub. They’re only publishing (development looks like it’s still being done internally) and some of the source code is missing, but they’re holding up their commitment to open source.&lt;/p&gt; &lt;p&gt;It’s still early days, and as I noted when we last looked at DataPlane, it’s going to be interesting to see where this goes, both in terms of what it means for Atlas and parts of Ambari, but also whether Hortonworks are really serious about this being a wider data ecosystem tool.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News 04/10/2018</title><link>https://ondataengineering.net/blog/2018/10/04/the-mid-week-news/</link><description> &lt;p&gt;We’re a little bit later than usual this week, but there’s some big Cloudera/Hortonworks news to make up for it… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;None this week!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;First up, Cloudera and Hortonworks have announced their plans to merge - I’ll try and capture my thoughts on this tomorrow - &lt;a href=&quot;http://vision.cloudera.com/cloudera-hortonworks-from-the-edge-to-ai/&quot;&gt;Cloudera&lt;/a&gt;; &lt;a href=&quot;https://hortonworks.com/blog/joining-forces-cloudera/&quot;&gt;Hortonworks&lt;/a&gt;; &lt;a href=&quot;https://www.zdnet.com/article/cloudera-hortonworks-merge-in-deal-valued-at-5-2-billion/&quot;&gt;ZDNet view&lt;/a&gt;; &lt;a href=&quot;https://www.datanami.com/2018/10/03/cloudera-and-hortonworks-to-merge/&quot;&gt;Datanami view&lt;/a&gt;; &lt;a href=&quot;https://blogs.gartner.com/svetlana-sicular/the-big-data-obituary/&quot;&gt;Gartner view&lt;/a&gt;; &lt;a href=&quot;https://www.theregister.co.uk/2018/10/04/cloudera_hortonworks_merger/&quot;&gt;The Register view&lt;/a&gt;&lt;/li&gt; &lt;li&gt;ZDNet have a post on processing time series data - &lt;a href=&quot;https://www.zdnet.com/article/processing-time-series-data-what-are-the-options/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Hortonworks have a reminder that Hive in HDP now integrates with Druid for cube based queries, and it’s faster in HDP 3.0 - &lt;a href=&quot;https://hortonworks.com/blog/benchmark-update-apache-hive-druid-integration-hdp-3-0/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A view from Datanami on Apache Pulsar - &lt;a href=&quot;https://www.datanami.com/2018/09/25/apache-pulsar-ready-for-prime-time/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;An update on &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Spark&lt;/a&gt; on Kubernetes from DataBricks - &lt;a href=&quot;https://databricks.com/blog/2018/09/26/whats-new-for-apache-spark-on-kubernetes-in-the-upcoming-apache-spark-2-4-release.html&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From The Morning Papers - “the design and implementation of modern column-oriented database systems” - &lt;a href=&quot;https://blog.acolyer.org/2018/09/26/the-design-and-implementation-of-modern-column-oriented-database-systems/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Thoughts on the Cloudera Hortonworks merger...</title><link>https://ondataengineering.net/blog/2018/10/05/thoughts-on-the-cloudera-hortonworks-merger/</link><description> &lt;p&gt;Well this is going to generate a bunch of updates to the site… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;So first some facts - it’s a “merger of equals” (although slightly skewed towards Cloudera in terms of stakeholder equity in the final company and makeup of the new board), there’s no new name announced (although Datanami are suggesting they’ll use the Cloudera “banner”), and it’s obviously going to be a few months until everything is signed and sealed.&lt;/p&gt; &lt;p&gt;Existing product lines will all be supported for at least three years, and there’ll be an upgrade path from all the current product lines to a new unified product line. In the short term, they’ll look to cross port unique offerings to each others stacks - &lt;a href=&quot;/technologies/hortonworks-data-flow/&quot;&gt;HDF&lt;/a&gt; from Hortonworks, and &lt;a href=&quot;/technologies/cloudera-data-science-workbench/&quot;&gt;Data Science Workbench&lt;/a&gt; from Cloudera are name checked.&lt;/p&gt; &lt;p&gt;So is this a good thing?&lt;/p&gt; &lt;p&gt;Well for some it probably won’t. If you’re a user of open source Hadoop, and if we assume the new company will inherit Cloudera’s leverage of commercial product elements (given this seemed to be the more successful business model), then there’s likely to be less investment in some key open source components, less diversity, and you’re going to have to pick up the slack or pony up.&lt;/p&gt; &lt;p&gt;However if you’re a Hortonworks or Cloudera customer then this is probably good news. Sure, there’s going to be some short term pain as products get rationalised, but you’re going to get access to a much broader and deeper product line.&lt;/p&gt; &lt;p&gt;But surely this will reduce competition I hear you say. Firstly, I’m not entirely convinced that competition within the Hadoop distributions market has been great - it’s enabled some customers to drastically reduce their Hadoop licence costs by playing companies against each other, but the rush to differentiate each distribution and develop commercial levers has lead to some fairly significant schisms in the Hadoop ecosystem.&lt;/p&gt; &lt;p&gt;And the reality is that the market segment Hadoop is playing in isn’t the “Hadoop” market - the distro vendors haven’t really been competing against themselves, they’ve been competing against the established data integration and analytics vendors trying to carve out some market space, and the battles with each other have been a distraction from that. And this merge represents the final cessation of those battles - IBM, Microsoft and Teradata threw their lot in with Hortonworks a while ago, Intel and Oracle with Cloudera, meaning we’ve now seen the coalescence of all those distributions that sprang up.&lt;/p&gt; &lt;p&gt;Because today the battle front has moved. Whereas before the competition was the established (and potentially vulnerable) established analytics vendors, today the big cloud vendors are starting to eat everyone’s lunch - winning on speed to market, price (in most situations) and flexibility. Going forward, you’re not going to be choosing a distribution, you’re going to be choosing a service provider, and the market for that looks really strong. Cloudera and Hortonworks are going to require their combined resources to compete.&lt;/p&gt; &lt;p&gt;If you’re looking for some more reading can I suggest the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.zdnet.com/article/cloudera-hortonworks-merge-in-deal-valued-at-5-2-billion/&quot;&gt;ZDNet&lt;/a&gt; have probably the best summary of the merger, however &lt;a href=&quot;https://www.datanami.com/2018/10/03/cloudera-and-hortonworks-to-merge/&quot;&gt;Datanami&lt;/a&gt; also has some good stuff&lt;/li&gt; &lt;li&gt;Plus &lt;a href=&quot;https://www.datanami.com/2018/10/04/reaction-to-hortonworks-cloudera-mega-merger/&quot;&gt;Datanami&lt;/a&gt; has a great summary of the reactions to the announcement&lt;/li&gt; &lt;li&gt;The &lt;a href=&quot;http://vision.cloudera.com/cloudera-hortonworks-from-the-edge-to-ai/&quot;&gt;Cloudera Vision blog&lt;/a&gt; has the details on the future of the stacks, plus there’s the &lt;a href=&quot;https://hortonworks.com/blog/joining-forces-cloudera/&quot;&gt;Hortonworks blog&lt;/a&gt; and the &lt;a href=&quot;https://www.cloudera.com/more/news-and-blogs/press-releases/2018-10-03-cloudera-and-hortonworks-announce-merger-to-create-worlds-leading-next-generation-data-platform-and-deliver-industrys-first-enterprise-data-cloud.html&quot;&gt;press release&lt;/a&gt;&lt;/li&gt; &lt;li&gt;If you care, both &lt;a href=&quot;https://blogs.gartner.com/svetlana-sicular/the-big-data-obituary/&quot;&gt;Gartner&lt;/a&gt; and &lt;a href=&quot;https://go.forrester.com/blogs/cloudera-and-hortonworks-merger-a-win-win-for-all/&quot;&gt;Forrester&lt;/a&gt; are positive&lt;/li&gt; &lt;li&gt;And &lt;a href=&quot;https://mapr.com/blog/in-a-consolidating-market-mapr-delivers-today/&quot;&gt;MapR&lt;/a&gt; have of course piped up about how they’ve already achieved what Hortonworks and Cloudera are proposing to do&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Mid Week News 10/10/2018</title><link>https://ondataengineering.net/blog/2018/10/10/the-mid-week-news/</link><description> &lt;p&gt;Right, we interupt all the news and thoughts about Hadoop for some more news… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/mapr-converged-data-platform/&quot;&gt;MapR Converged Data Platform&lt;/a&gt; is up to 6.1&lt;/li&gt; &lt;li&gt;And the &lt;a href=&quot;/technologies/mapr-expansion-pack/&quot;&gt;MapR Expansion Pack&lt;/a&gt; is up to 6.0&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/streamsets-data-collector/&quot;&gt;Streamsets Data Collector&lt;/a&gt; has hit 3.5&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-arrow/&quot;&gt;Apache Arrow&lt;/a&gt; has hit 0.11&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-beam/&quot;&gt;Apache Beam&lt;/a&gt; has hit 2.7&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-ranger/&quot;&gt;Apache Ranger&lt;/a&gt; has hit 1.2&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-nifi/registry/&quot;&gt;NiFi Registry&lt;/a&gt; is up to 0.3&lt;/li&gt; &lt;li&gt;And &lt;a href=&quot;/technologies/apache-hadoop/ozone/&quot;&gt;Apache Hadoop Ozone&lt;/a&gt; has it’s first release - 0.2.1-alpha&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Elastic has become a public company - &lt;a href=&quot;https://www.elastic.co/blog/ze-bell-has-rung-thank-you-users-customers-and-partners&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;An introduction to &lt;a href=&quot;/technologies/apache-hadoop/ozone/&quot;&gt;Ozone&lt;/a&gt; from Hortonworks - &lt;a href=&quot;https://hortonworks.com/blog/introducing-apache-hadoop-ozone-object-store-apache-hadoop/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Cloudera - how to secure your compute and knowledge with &lt;a href=&quot;/technologies/cloudera-altus/&quot;&gt;Cloudera Altus&lt;/a&gt; - &lt;a href=&quot;http://blog.cloudera.com/blog/2018/10/cloudera-altus-cloud-services-youre-in-control-part1/&quot;&gt;part1&lt;/a&gt;; &lt;a href=&quot;http://blog.cloudera.com/blog/2018/10/cloudera-altus-cloud-services-youre-in-control-part2/&quot;&gt;part2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;An updated from ZDNet on where SAP are going - &lt;a href=&quot;https://www.zdnet.com/article/saps-road-to-cloud-native-data-platforms-in-a-hybrid-world/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Datanami - Diffbot, the graph of all human knowledge - &lt;a href=&quot;https://www.datanami.com/2018/10/02/the-graph-that-knows-the-world/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hadoop Compatible Filesystems</title><link>https://ondataengineering.net/tech-categories/hadoop-compatible-filesystems/</link><description> &lt;p&gt;Our list of and information on commercial, open source and cloud based Hadoop compatible filesystems, including HDFS, MapR-FS, Alluxio, Ignite, Azure Data Lake Store and alternatives to these.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Category Definition&lt;/h2&gt; &lt;!-- Tech Vendor metadata --&gt; &lt;p&gt;A parallel distributed filesystem that implements the Hadoop FileSystem API and conforms to the Hadoop Compatible Filesystem specification, allowing it to be used in place of HDFS. The use of the FileSystem API (over native filesystem access) allows for parallel reads and location aware block placement, with the HCFS specification covering runtime behaviour. Note that Hadoop Compatible Filesystems (as per HDFS) are not fully POSIX compliant, there is no formal compatibility test suite (although a test suite that will highlight potential issues is available), and that some implementations (for example object stores) do not (and cannot) fully conform to the specification. &lt;!--more--&gt;&lt;/p&gt; &lt;h2 id=&quot;further-information&quot;&gt;Further Information&lt;/h2&gt; &lt;p&gt;The specification for Hadoop compatible filesystems is included in the Hadoop documentation &lt;a href=&quot;https://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-common/filesystem/introduction.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;h2 id=&quot;specialist-hadoop-compatible-filesystems&quot;&gt;Specialist Hadoop Compatible Filesystems&lt;/h2&gt; &lt;p&gt;The following technologies are all designed specifically to be Hadoop compatible and to be drop in replacements for HDFS within an Hadoop cluster, meaning that they can co-exist with YARN and other analytical compute workloads on the same nodes:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/hdfs/&quot;&gt;HDFS&lt;/a&gt;&lt;/td&gt; &lt;td&gt;The Hadoop Distributed Filesystem, bundled with Hadoop&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/mapr-fs/&quot;&gt;MapR-FS&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Hadoop compatible, highly resilient and scalable distributed filesystem that also supports NoSQL table and streaming data native storage&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/quantcast-file-system/&quot;&gt;Quantcast File System&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Open source HDFS replacement, which focuses on improving performance and scalability over HDFS&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Hops-HDFS&lt;/td&gt; &lt;td&gt;Experimental solution based on HDFS but using a distributed MySQL cluster for metadata to increase performance and scalability - &lt;a href=&quot;http://www.hops.io/?q=content/hdfs&quot;&gt;http://www.hops.io/?q=content/hdfs&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;See our &lt;a href=&quot;/tech-categories/hadoop-distributions/&quot;&gt;Hadoop Distributions&lt;/a&gt; page for options on deploying Hadoop clusters utilising these technologies.&lt;/p&gt; &lt;h2 id=&quot;in-memory-hadoop-compatible-filesystems&quot;&gt;In Memory Hadoop Compatible Filesystems&lt;/h2&gt; &lt;p&gt;There are also a number of in memory data grids / storage layers that provide Hadoop compatibility and the option to run Hadoop jobs entirely in memory or across tiered storage:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/alluxio/&quot;&gt;Alluxio&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Distributed virtual storage layer over memory and tiered storage with support for a range of interfaces. Previously known as Tachyon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-ignite&quot;&gt;GridGain/Apache Ignite&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Distributed in-memory data fabric/grid, including support for an in-memory Hadoop compatible filesystem&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;cloud-hadoop-compatible-filesystems&quot;&gt;Cloud Hadoop Compatible Filesystems&lt;/h2&gt; &lt;p&gt;Azure has an Hadoop compatible filesystem as a service:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/microsoft-azure-data-lake-store&quot;&gt;Azure Data Lake Store&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Cloud based massively scalable HDFS compatible filesystem based on Microsoft Cosmos&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;other-technologies&quot;&gt;Other Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;DataStax Enterprise file system&lt;/td&gt; &lt;td&gt;Distributed Hadoop compatible filesystem that runs on DataStax Enterprise, replacing the now deprecated Cassandra File System (CFS) - &lt;a href=&quot;http://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/analytics/dsefsTOC.html&quot;&gt;http://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/analytics/dsefsTOC.html&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;parallel-distributed-filesystems&quot;&gt;Parallel Distributed Filesystems&lt;/h2&gt; &lt;p&gt;Parallel distributed filesystems provide similar capabilities to HDFS, including the ability to scale horizontally and to read a file in parallel from multiple nodes. Their general focus is on providing direct filesystem access plus NFS and object store APIs, and although most offer an Hadoop compatible API this is generally just to allow data to be exploited by Hadoop compatible workloads as a remote filesystem. Some may support installation on an Hadoop cluster as a drop in replacement for HDFS, however there are often compatibility issues and performance is often not as good as HDFS.&lt;/p&gt; &lt;h2 id=&quot;object-stores&quot;&gt;Object Stores&lt;/h2&gt; &lt;p&gt;Most object stores also provide Hadoop compatible APIs, and although this means that Hadoop can natively read and write from them using the Hadoop Filesystem API, they are not considered Hadoop Compatible Filesystems due to their lack of compliance to the compliance specification. More details can be found in the “Object Stores vs. Filesystems” section of the &lt;a href=&quot;https://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-common/filesystem/introduction.html&quot;&gt;specification page&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;One of the potential differences is the use of an eventual consistency model by object stores, which particularly affects &lt;a href=&quot;/technologies/amazon-s3/&quot;&gt;Amazon S3&lt;/a&gt;. The Hadoop S3A client now includes S3Guard, a feature which uses a database such as DynamoDB to provide a consistent view of the object store - see the &lt;a href=&quot;https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/s3guard.html&quot;&gt;Hadoop docs page for S3Guard&lt;/a&gt; for more details.&lt;/p&gt; &lt;p&gt;See our &lt;a href=&quot;/tech-categories/object-stores/&quot;&gt;Object Stores&lt;/a&gt; page for our list of object store technologies.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Object Stores</title><link>https://ondataengineering.net/tech-categories/object-stores/</link><description> &lt;p&gt;Our list of and information on commercial, open source and cloud based object stores, including Amazon S3, Azure Blob Storage, Google Cloud Storage, OpenStack Swift, Scality, Caringo and alternatives to these.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Category Definition&lt;/h2&gt; &lt;!-- Tech Vendor metadata --&gt; &lt;p&gt;Storage solutions whereby data is stored without any concept of folders or organisational structure, instead being referenced by a unique identifier, allowing for massively parallel and scalable solutions. Generally access via a REST API, with Amazon S3 the defacto standard, although many also support a range of file based interfaces as well, simulating a file system using the underlying object storage. Common features include support for multiple storage tiers, storage of custom metadata against data, replication of data for redundancy, and object versioning. &lt;!--more--&gt;&lt;/p&gt; &lt;h2 id=&quot;further-information&quot;&gt;Further Information&lt;/h2&gt; &lt;p&gt;Good explanation of object stores in comparison to file and block stores: &lt;a href=&quot;http://searchcloudstorage.techtarget.com/feature/How-an-object-store-differs-from-file-and-block-storage&quot;&gt;http://searchcloudstorage.techtarget.com/feature/How-an-object-store-differs-from-file-and-block-storage&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Excellent list of object stores including their history: &lt;a href=&quot;http://www.theregister.co.uk/2016/07/15/the_history_boys_cas_and_object_storage_map/&quot;&gt;http://www.theregister.co.uk/2016/07/15/the_history_boys_cas_and_object_storage_map/&lt;/a&gt;&lt;/p&gt; &lt;h2 id=&quot;object-store-services&quot;&gt;Object Store Services&lt;/h2&gt; &lt;p&gt;The three major cloud vendors all have object store services as follows:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/amazon-s3/&quot;&gt;Amazon S3&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Eventually consistent object store service&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/microsoft-azure-blob-storage/&quot;&gt;Azure Blob Storage&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Strongly consistent object store service, with support for multiple object types (block, page and append)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/google-cloud-storage/&quot;&gt;Google Cloud Storage&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Strongly consistent object store service&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;All other “Enterprise” cloud vendors will also have a comparable offering, generally including Amazon S3 compatibility. Many are based on &lt;a href=&quot;/technologies/openstack-swift&quot;&gt;OpenStack Swift&lt;/a&gt;, including IBM Bluemix Object Storage and Rackspace Cloud Files.&lt;/p&gt; &lt;p&gt;Other object storage services include:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Wasabi&lt;/td&gt; &lt;td&gt;Ultra low cost high performance object storage service - &lt;a href=&quot;https://wasabi.com/&quot;&gt;https://wasabi.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;BackBlaze B2&lt;/td&gt; &lt;td&gt;Claims to have the lowest cost high performance object storage service - &lt;a href=&quot;https://www.backblaze.com/b2/cloud-storage.html&quot;&gt;https://www.backblaze.com/b2/cloud-storage.html&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;There are also a wide range of other smaller cloud vendors that provide object storage solutions, and many of the object store technologies listed below are also available as a managed service.&lt;/p&gt; &lt;p&gt;The following analyst material covers a number of technologies in this category:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.google.co.uk/search?q=Gartner+&amp;quot;Public+Cloud+Storage+Services&amp;quot;&quot;&gt;Gartner Magic Quadrant and Critical Capabilities for Public Cloud Storage Services, Worldwide&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;open-source-object-store-technologies&quot;&gt;Open Source Object Store Technologies&lt;/h2&gt; &lt;p&gt;The following are all Open Source object store technologies:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/openstack-swift&quot;&gt;OpenStack Swift&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Supports multiple configurable storage tiers and backing storage; part of the OpenStack suite but can be installed stand-alone&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/zenko/cloudserver/&quot;&gt;Zenko CloudServer&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Open source object storage server based on the S3 compatible API from Scality RING, previously known as Scality S3 Server&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Ceph&lt;/td&gt; &lt;td&gt;Distributed object store that also supports block and file storage, with development led by RedHat - &lt;a href=&quot;http://ceph.com/&quot;&gt;http://ceph.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Redcurrent&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://www.redcurrant.io&quot;&gt;http://www.redcurrant.io&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Basho Riak S2 (formally CS)&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://basho.com/products/riak-s2/&quot;&gt;http://basho.com/products/riak-s2/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Joyent (Samsung) Triton Object Storage&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.joyent.com/triton/object-storage&quot;&gt;https://www.joyent.com/triton/object-storage&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Minio&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.minio.io/&quot;&gt;https://www.minio.io/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LinkedIn Ambry&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://github.com/linkedin/ambry&quot;&gt;https://github.com/linkedin/ambry&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Rook&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://rook.io/&quot;&gt;https://rook.io/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Open IO&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://openio.io/&quot;&gt;http://openio.io/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;Options for on premise object stores from enterprise vendors, many of which are also available as a service or as a hardware and software appliance, include:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;SwiftStack&lt;/td&gt; &lt;td&gt;Commercial on premise software solution based on &lt;a href=&quot;/technologies/openstack-swift&quot;&gt;OpenStack Swift&lt;/a&gt; with a number of added management features and synchronisation to the cloud, sold by the largest contributor to Swift - &lt;a href=&quot;https://www.swiftstack.com/&quot;&gt;https://www.swiftstack.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/scality-ring/&quot;&gt;Scality RING&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Native object store with POSIX filesystem support, and a range of object, file and OpenStack compatible APIs&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Caringo Swarm (formally CAStore)&lt;/td&gt; &lt;td&gt;Software and appliance solution - &lt;a href=&quot;https://www.caringo.com/&quot;&gt;https://www.caringo.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Cloudian HyperStore&lt;/td&gt; &lt;td&gt;Software and appliance solution - &lt;a href=&quot;https://cloudian.com/products/&quot;&gt;https://cloudian.com/products/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Dell EMC Elastic Cloud Storage (ECS)&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.emc.com/en-us/storage/ecs/&quot;&gt;https://www.emc.com/en-us/storage/ecs/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DDN WOS&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://www.ddn.com/products/object-storage-web-object-scaler-wos/&quot;&gt;http://www.ddn.com/products/object-storage-web-object-scaler-wos/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Exabloc&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.exablox.com/&quot;&gt;https://www.exablox.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RedHat Ceph&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.redhat.com/en/technologies/storage/ceph&quot;&gt;https://www.redhat.com/en/technologies/storage/ceph&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;HGST (Western Digial) ActiveScale&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.hgst.com/products/systems&quot;&gt;https://www.hgst.com/products/systems&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Hitachi (HDS) Content Platform (HCP)&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.hds.com/en-us/products-solutions/storage/content-platform.html&quot;&gt;https://www.hds.com/en-us/products-solutions/storage/content-platform.html&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;IBM Cloud Object Storage (previously Cleversafe)&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.ibm.com/cloud-computing/products/storage/object-storage/cloud/&quot;&gt;https://www.ibm.com/cloud-computing/products/storage/object-storage/cloud/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;NetApp StorageGRID Webscale&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://www.netapp.com/us/products/data-management-software/object-storage-grid-sds.aspx&quot;&gt;http://www.netapp.com/us/products/data-management-software/object-storage-grid-sds.aspx&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;The following are other object store products:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Quantum Lattus Object Storage&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://www.quantum.com/products/bigdatamanagement/lattus/index.aspx&quot;&gt;http://www.quantum.com/products/bigdatamanagement/lattus/index.aspx&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;NooBaa&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.noobaa.com&quot;&gt;https://www.noobaa.com&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;The following analyst material covers a number of technologies in this category:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.google.co.uk/search?q=Gartner+&amp;quot;Distributed+File+Systems+and+Object+Storage&amp;quot;&quot;&gt;Gartner Magic Quadrant &amp;amp; Critical Capabilities for Distributed File Systems and Object Storage&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.google.co.uk/search?q=IDC+&amp;quot;Worldwide+Object+Based+Storage&amp;quot;&quot;&gt;IDC Marketscape: Worldwide Object-Based Storage&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;parallel-distributed-filesystems&quot;&gt;Parallel Distributed Filesystems&lt;/h2&gt; &lt;p&gt;Many parallel distributed filesystems (such as Gluster) also support object store interfaces, simulating an object store using the underlying filesystem.&lt;/p&gt; &lt;h2 id=&quot;historical-technologies&quot;&gt;Historical Technologies&lt;/h2&gt; &lt;p&gt;The following technologies are no longer sold or maintained:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;CoreOS Torus&lt;/td&gt; &lt;td&gt;Development stopped Feb 2017 - &lt;a href=&quot;https://github.com/coreos/torus&quot;&gt;https://github.com/coreos/torus&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Skylable&lt;/td&gt; &lt;td&gt;Website no longer live - &lt;a href=&quot;http://www.skylable.com&quot;&gt;http://www.skylable.com&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hadoop Distributions</title><link>https://ondataengineering.net/tech-categories/hadoop-distributions/</link><description> &lt;p&gt;Our list of and information on commercial, open source and cloud based Hadoop distributions, including Cloudera, Hortonworks, MapR, Amazon EMR, Azure HDInsight, Google Cloud Dataproc and alternatives to these.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Category Definition&lt;/h2&gt; &lt;!-- Tech Vendor metadata --&gt; &lt;p&gt;Products or services built around Hadoop (or an Hadoop compatible core) combined with a number of Hadoop compatible products. Hadoop compatibility covers the use of YARN (for resource management of multiple jobs running on the same infrastructure) and HDFS (for local storage of data with support for co-locating processing with the data). &lt;!--more--&gt;&lt;/p&gt; &lt;h2 id=&quot;further-information&quot;&gt;Further Information&lt;/h2&gt; &lt;p&gt;See also our Hadoop (HDFS and YARN) &lt;a href=&quot;/tech-categories/hadoop-distributions/ecosystem/&quot;&gt;ecosystem diagrams&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also have a summary of the &lt;a href=&quot;/tech-vendors/odpi/&quot;&gt;ODPi&lt;/a&gt; organisation that’s trying to drive compatibility between Hadoop distributions.&lt;/p&gt; &lt;p&gt;Merv Adrian from Gartner maintains a tracker of the different versions of each Hadoop component in the major distributions - &lt;a href=&quot;https://blogs.gartner.com/merv-adrian/2018/01/03/january-2018-hadoop-tracker/&quot;&gt;https://blogs.gartner.com/merv-adrian/2018/01/03/january-2018-hadoop-tracker/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The following analyst material covers a number of technologies in this category:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.google.co.uk/search?q=gartner+&amp;quot;Data+Management+Solutions+for+Analytics&amp;quot;&quot;&gt;Gartner Magic Quadrant &amp;amp; Critical Capabilities for Data Management Solutions for Analytics&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;commercial-distributions&quot;&gt;Commercial Distributions&lt;/h2&gt; &lt;p&gt;The following are distributions from commercial vendors for installation on pre-provisioned infrastructure, with many also including tooling for programmatically provisioning infrastructure when installing in cloud environments.&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;Cloudera CDH&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A distribution of Hadoop based on the addition of a number of closed source products, including Cloudera Manager (for installing and managing clusters), Cloudera Director (for installing in cloud environments) and Cloudera Navigator (for managing metadata and the encryption of data). Available in free and commercial editions.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A distribution of Hadoop based on a commitment to the Apache open source ecosystem, utilising only open source products with minimal extra patching. Uses Ambari for installing and managing clusters, and Cloudbreak for installing in cloud environments. Free to use with commercial support available.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/mapr-converged-data-platform/&quot;&gt;MapR Converged Data Platform&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A data platform that provides Hadoop compatibility (via YARN and the MapR-FS HDFS compatible API), NoSQL and streaming data storage via MapR-DB and MapR-ES, and a bundle of open source Hadoop projects via the MapR Ecosystem Pack. Available in free and commercial editions.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Syncfusion Big Data Platform&lt;/td&gt; &lt;td&gt;Distribution for Windows, Linux and Azure - &lt;a href=&quot;https://www.syncfusion.com/products/big-data&quot;&gt;https://www.syncfusion.com/products/big-data&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;See also our &lt;a href=&quot;/tech-categories/hadoop-distributions/distribution-comparison/&quot;&gt;comparison&lt;/a&gt; of the major commercial Hadoop distributions.&lt;/p&gt; &lt;h2 id=&quot;hadoop-cloud-offerings&quot;&gt;Hadoop Cloud Offerings&lt;/h2&gt; &lt;p&gt;The following are cloud based Hadoop service offerings, supporting the programmatic provisioning and management of Hadoop clusters. Many also provide higher level APIs that allow for submission and management of individual Hadoop jobs, with some services allowing clusters to be automatically provisioned to execute a job and then terminated afterwards.&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/amazon-emr/&quot;&gt;Amazon EMR&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Hadoop as a service, with support for a wide range of Hadoop technologies and the ability to programmatically execute Hadoop jobs and dynamically provision clusters to execute these&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/azure-hdinsight/&quot;&gt;Azure HDInsight&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Hadoop service based on &lt;a href=&quot;/technologies/hortonworks-data-platform&quot;&gt;HDP&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/google-cloud-dataproc/&quot;&gt;Google Cloud Dataproc&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Hadoop service, with support for MapReduce, Spark, Pig and Hive, and the ability to programatically submit and manage jobs&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/qubole-data-service/&quot;&gt;Qubole Data Service&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Hadoop managed service running on AWS, Azure and Oracle Cloud&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/cloudera-altus/&quot;&gt;Cloudera Altus&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Platform for accessing individual CDH capabilities as services, with the first capabilities supported being the execution of Spark, MapReduce or Hive (over MapReduce or Spark) jobs using managed CDH clusters on AWS cloud infrastructure over data in Amazon S3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;SAP Cloud Platform Big Data Services (previously Altiscale)&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://cloudplatform.sap.com/capabilities/data-storage/big-data.html&quot;&gt;https://cloudplatform.sap.com/capabilities/data-storage/big-data.html&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Rackspace&lt;/td&gt; &lt;td&gt;Based on Hortonworks HDP - &lt;a href=&quot;https://www.rackspace.com/big-data&quot;&gt;https://www.rackspace.com/big-data&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;hadoop-hardware-appliances&quot;&gt;Hadoop Hardware Appliances&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Teradata Appliance for Hadoop&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://www.teradata.com/products-and-services/appliance-for-hadoop&quot;&gt;http://www.teradata.com/products-and-services/appliance-for-hadoop&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Oracle Big Data Appliance&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.oracle.com/engineered-systems/big-data-appliance/index.html&quot;&gt;https://www.oracle.com/engineered-systems/big-data-appliance/index.html&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;non-commercial-options&quot;&gt;Non Commercial Options&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt; &lt;td&gt;An Apache open source distribution of Hadoop. Packages up a number of Apache Hadoop components, certifies their interoperability using an automated integration test suite, and packages them up as RPMs/DEBs packages for most flavours of Linux.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;OpenStack Sahara&lt;/td&gt; &lt;td&gt;Allows provisioning of Hadoop on OpenStack - &lt;a href=&quot;https://docs.openstack.org/developer/sahara/&quot;&gt;https://docs.openstack.org/developer/sahara/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Hops&lt;/td&gt; &lt;td&gt;A distribution based on Hops HDFS and Hops YARN which use a distributed MySQL database for metadata to increase performance and scalability, available as a cloud or on premises offering - &lt;a href=&quot;http://www.hops.io&quot;&gt;http://www.hops.io&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;historical--legacy-options&quot;&gt;Historical / Legacy Options&lt;/h2&gt; &lt;p&gt;The following are either no longer available, or are now simply re-badged versions of other distributions:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Intel Distribution for Apache Hadoop&lt;/td&gt; &lt;td&gt;Focused on optimisations for Intel processors, SSD disks and networking kit; ceased when Intel invested into Cloudera - see &lt;a href=&quot;https://newsroom.intel.com/news-releases/cloudera-intel-commit-to-accelerate-and-transform-how-enterprises-use-big-data-intel-makes-significant-equity-investment-in-cloudera/&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Pivotal HD&lt;/td&gt; &lt;td&gt;Pivotal has now partnered with Hortonworks - see &lt;a href=&quot;https://hortonworks.com/press-releases/hortonworks-pivotal-expand-relationship-deliver-enterprise-ready-modern-data-platforms-data-management-analytics/&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;IBM InfoSphere BigInsights&lt;/td&gt; &lt;td&gt;IBM has now partnered with Hortonworks - see &lt;a href=&quot;https://hortonworks.com/blog/data-met-science-anything-became-possible/&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Streaming Data Stores</title><link>https://ondataengineering.net/tech-categories/streaming-data-stores/</link><description> &lt;p&gt;Our list of and information on commercial, open source and cloud based streaming data stores, including Kafka, Confluent, MapR-ES and alternatives to these.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Category Definition&lt;/h2&gt; &lt;!-- Tech Vendor metadata --&gt; &lt;p&gt;Technologies for the persistent storage of continuous streams of data, with data access based on a publish/subscribe model. Should support multiple independent publishers and subscribers, the ability to add new subscribers and replay the history of a stream, horizontal scalability and load balancing, durable writes, ordered streams (data is always read in the order it was written), high throughput and low latency characteristics, handling of updates and deletes to source records, and the ability to secure the data. &lt;!--more--&gt;&lt;/p&gt; &lt;h2 id=&quot;open-source-technologies&quot;&gt;Open Source Technologies&lt;/h2&gt; &lt;p&gt;The following are open source Streaming Data Store technologies:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Technology for buffering and storing real-time streams of data between publishers to subscribers, with a focus on high throughput at low latency.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/confluent-open-source/&quot;&gt;Confluent Open Source&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A package of open source projects built around &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt; with the addition of the Confluent Schema Registry, Kafka REST Proxy, a number of connectors for Kafka Connect and a number of Kafka clients (language SDKs).&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/pravega/&quot;&gt;Pravega&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Technology for the buffering and long term storage of streaming data, designed for low latency and high throughput, with support for exactly once semantics, durable writes, strict ordering, dynamic scaling, transactions and long term storage backed by HDFS.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Apache BookKeeper&lt;/td&gt; &lt;td&gt;Distributed log storage service from Yahoo - &lt;a href=&quot;http://bookkeeper.apache.org/&quot;&gt;http://bookkeeper.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Apache DistributedLog&lt;/td&gt; &lt;td&gt;Distributed log service from Twitter supporting durability, replication and strong consistency built over Apache BookKeeper - &lt;a href=&quot;http://bookkeeper.apache.org/distributedlog/&quot;&gt;http://bookkeeper.apache.org/distributedlog/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Apache Pulsar&lt;/td&gt; &lt;td&gt;Distributed pub-sub messaging from Yahoo, with persistent message storage based on Apache BookKeeper - &lt;a href=&quot;http://pulsar.incubator.apache.org/&quot;&gt;http://pulsar.incubator.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LogDevice&lt;/td&gt; &lt;td&gt;Open source distributed data store for sequential data from Facebook - &lt;a href=&quot;https://logdevice.io/&quot;&gt;https://logdevice.io/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;Note that &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt; is bundled with a number of &lt;a href=&quot;/tech-categories/hadoop-distributions/&quot;&gt;Hadoop distributions&lt;/a&gt;.&lt;/p&gt; &lt;h2 id=&quot;commercial-technologies&quot;&gt;Commercial Technologies&lt;/h2&gt; &lt;p&gt;The following are commercial Streaming Data Store technologies:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/confluent-enterprise/&quot;&gt;Confluent Enterprise&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A commercial version of the Confluent Open Source product, with the addition of a number of commercial closed source products including a JMS client, Control Centre (for managing Kafka clusters), Multi DC Replication (active-active replication between Kafka clusters) and Auto Data Balancing.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/mapr-es&quot;&gt;MapR-ES&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Part of the MapR Converged Data Platform - supports streaming data storage capabilities and a Kafka compatible API&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;technologies-available-as-a-service&quot;&gt;Technologies Available as a Service&lt;/h2&gt; &lt;p&gt;The following are Streaming Data Store technologies available as a managed service in the cloud:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Confluent Cloud&lt;/td&gt; &lt;td&gt;Confluent Enterprise as a service - &lt;a href=&quot;https://www.confluent.io/confluent-cloud/&quot;&gt;https://www.confluent.io/confluent-cloud/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon Kinesis Streams&lt;/td&gt; &lt;td&gt;Streaming data storage and publish service - &lt;a href=&quot;https://aws.amazon.com/kinesis/streams/&quot;&gt;https://aws.amazon.com/kinesis/streams/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Event Hubs&lt;/td&gt; &lt;td&gt;Elastic service for the buffering and publishing of streaming event data - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/event-hubs/&quot;&gt;https://azure.microsoft.com/en-us/services/event-hubs/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google Cloud Pub/Sub&lt;/td&gt; &lt;td&gt;Real time message and streaming data service with “at least once” delivery - &lt;a href=&quot;https://cloud.google.com/pubsub/&quot;&gt;https://cloud.google.com/pubsub/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Analytical Search</title><link>https://ondataengineering.net/tech-categories/analytical-search/</link><description> &lt;p&gt;Our list of and information on analytical search technologies, including Elasticsearch and Apache Solr, along with their associated technologies and deployment options.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Category Definition&lt;/h2&gt; &lt;!-- Tech Vendor metadata --&gt; &lt;p&gt;Technologies for analytics over unstructured and semi-structured data based on search. Should be distributed and horizontally scalable, support pre-defined and on-demand schemas, all standard search functionality plus analytics based on search including basic join functionality, aggregations, graph analytics and machine learning. &lt;!--more--&gt;&lt;/p&gt; &lt;h2 id=&quot;primary-technologies&quot;&gt;Primary Technologies&lt;/h2&gt; &lt;p&gt;If you’re looking for search technologies that provide a wide range of analytical functions there are two primary options:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/elasticsearch&quot;&gt;Elasticsearch&lt;/a&gt;&lt;/td&gt; &lt;td&gt;An open source distributed search server that supports a range of analytics over search results, but with some enterprise and analytics features requiring a commercial licence from Elastic who lead development&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-solr&quot;&gt;Apache Solr&lt;/a&gt;&lt;/td&gt; &lt;td&gt;An Apache open source distributed search server that supports a range of analytics over search results, and which is included in most Hadoop distributions.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;elasticsearch-options&quot;&gt;Elasticsearch options&lt;/h2&gt; &lt;p&gt;Although &lt;a href=&quot;/technologies/elasticsearch&quot;&gt;Elasticsearch&lt;/a&gt; is an open source technology, it doesn’t include some enterprise features (such as access control, monitoring, alerting or encryption) and advanced analytical capabilities (such as graph searching or machine learning based anomaly detection), which require the &lt;a href=&quot;/technologies/elastic-x-pack/&quot;&gt;X-Pack&lt;/a&gt; add on from Elastic (the creators and maintainers of Elasticsearch) that’s only available under one of their commercial packages (including &lt;a href=&quot;/technologies/elastic-cloud/&quot;&gt;Elastic Cloud&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;A range of other vendors also provide Elasticsearch as a service offering (including Amazon) based on the open source version, either focusing on traditional search use cases, or event log analysis (generally based on Elasticsearch, Kibana and Logstash - the ELK stack).&lt;/p&gt; &lt;h2 id=&quot;apache-solr-options&quot;&gt;Apache Solr options&lt;/h2&gt; &lt;p&gt;All &lt;a href=&quot;/technologies/apache-solr&quot;&gt;Apache Solr&lt;/a&gt; functionality is available in the open source product.&lt;/p&gt; &lt;p&gt;Cloudera and Lucidworks are currently two of the leading contributors to Apache Solr, distributing it as &lt;a href=&quot;/technologies/cloudera-search/&quot;&gt;Cloudera Serach&lt;/a&gt; (part of &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt;) and Lucidworks Fusion (&lt;a href=&quot;https://lucidworks.com/&quot;&gt;https://lucidworks.com/&lt;/a&gt;) respectively. Lucidworks also provide &lt;a href=&quot;/technologies/hortonworks-data-platform-search/&quot;&gt;Hortonworks Data Platform Search&lt;/a&gt; (the version of Solr within &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;HDP&lt;/a&gt;) and a Solr add on for the &lt;a href=&quot;/technologies/mapr-converged-data-platform&quot;&gt;MapR Converged Data Platform&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Pivotal also distribute Solr integrated with &lt;a href=&quot;/technologies/greenplum/&quot;&gt;Greenplum&lt;/a&gt; as part of their GPText product - &lt;a href=&quot;https://content.pivotal.io/blog/inside-the-new-solr-powered-sql-text-analytics-engine-for-greenplum&quot;&gt;intro blog post&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A range of vendors also provide Apache Solr as a service offering, however these generally focus on traditional (non analytical) search use cases.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>NoSQL Wide Column Stores</title><link>https://ondataengineering.net/tech-categories/nosql-wide-column-stores/</link><description> &lt;p&gt;Our list of and information on commercial, open source and cloud based NoSQL Wide Column Stores, including Cassandra, HBase, Accumulo, Azure Table Storage, Google Cloud Bigtable and alternatives to these.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Category Definition&lt;/h2&gt; &lt;!-- Tech Vendor metadata --&gt; &lt;p&gt;NoSQL databases based on Google Bigtable, the design of which was published in a paper in 2006. Often described as a sparse, distributed multi-dimensional sorted map (or key value store) - cells being referenced by a row and column key plus a timestamp or version (with columns arranged into column families), support for the names and formats of columns varying from row to row (within fixed column families), and architectures supporting massive horizontal scalability. Also called extensible record stores, and occasionally NoSQL column stores (although this definition is slightly inaccurate and confuses these technologies with more general purpose database columnar storage). Common functionality includes low latency high throughput reads and writes, scan/iterate operations, atomic mutations and cell level security. Common analytical use cases include the storage and serving of aggregations and metrics for real time dashboards, often as part of a wider ecosystem. &lt;!--more--&gt;&lt;/p&gt; &lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;NoSQL Wide Column stores are all based on the original design paper for Google Bigtable that can be found at &lt;a href=&quot;http://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf&quot;&gt;http://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The following analyst material covers a number of technologies in this category:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.google.co.uk/search?q=Gartner+&amp;quot;Operational+Database+Management+Systems&amp;quot;&quot;&gt;Gartner Magic Quadrant &amp;amp; Critical Capabilities for Operational Database Management Systems&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;open-source-options&quot;&gt;Open Source Options&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-cassandra/&quot;&gt;Apache Cassandra&lt;/a&gt;&lt;/td&gt; &lt;td&gt;NoSQL wide-column datastore based on Amazon Dynamo and Google BigTable, focusing on fault tolerance, linear scalability and operational simplicity with zero downtime based on a distributed masterless node and peer-to-peer design&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hbase/&quot;&gt;Apache HBase&lt;/a&gt;&lt;/td&gt; &lt;td&gt;NoSQL wide-column datastore based on Google BigTable, with deep integration to the Apache Hadoop ecosystem.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-accumulo/&quot;&gt;Apache Accumulo&lt;/a&gt;&lt;/td&gt; &lt;td&gt;NoSQL wide-column datastore based on BigTable, supporting cell based access control (based on arbitrary boolean expressions of user security labels) and atomic mutation operators.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;ScyllaDB&lt;/td&gt; &lt;td&gt;Cassandra-compatible data store re-written in C++ with the aim to provider higher throughput at lower latency, open sourced under an AGPL licence - &lt;a href=&quot;http://www.scylladb.com/&quot;&gt;http://www.scylladb.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;There are a number of associated technologies to the above:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Apache Fluo&lt;/td&gt; &lt;td&gt;Implementation of Google Percolator for maintaining aggregations in Accumulo - &lt;a href=&quot;https://fluo.apache.org/&quot;&gt;https://fluo.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Apache Omid (Incubating)&lt;/td&gt; &lt;td&gt;ACID transaction support over MVCC key/value NoSQL datastores with support for Apache HBase - &lt;a href=&quot;http://omid.apache.org/&quot;&gt;http://omid.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Apache Tephra (Incubating)&lt;/td&gt; &lt;td&gt;ACID transaction support over Apache HBase, used by Tigon and Apache Phoenix - &lt;a href=&quot;http://tephra.apache.org/&quot;&gt;http://tephra.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;commercial-options&quot;&gt;Commercial Options&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;ScyllaDB Enterprise&lt;/td&gt; &lt;td&gt;Distribution of ScyllaDB (the open source product) from ScyllaDB (the company), with added enterprise features and commercial support - &lt;a href=&quot;http://www.scylladb.com/&quot;&gt;http://www.scylladb.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;Commercial support for Apache HBase and Apache Accumulo is included in most commercial Hadoop distributions.&lt;/p&gt; &lt;h2 id=&quot;managed-service-options&quot;&gt;Managed Service Options&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Azure Table Storage&lt;/td&gt; &lt;td&gt;A NoSQL wide column store service - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/storage/tables/&quot;&gt;https://azure.microsoft.com/en-us/services/storage/tables/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google Cloud Bigtable&lt;/td&gt; &lt;td&gt;NoSQL wide column store service - &lt;a href=&quot;https://cloud.google.com/bigtable/&quot;&gt;https://cloud.google.com/bigtable/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;Apache Cassandra is also available as a managed service from a number of vendors.&lt;/p&gt; &lt;p&gt;Apache HBase and Apache Accumulo are also available as part of most Hadoop managed services.&lt;/p&gt; &lt;h2 id=&quot;multi-model-nosql-databases&quot;&gt;Multi Model NoSQL Databases&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;DataStax Enterprise&lt;/td&gt; &lt;td&gt;Commercial product built on Apache Cassandra with the addition of graph and search capabilities - &lt;a href=&quot;https://www.datastax.com/&quot;&gt;https://www.datastax.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Cosmos DB&lt;/td&gt; &lt;td&gt;Massively scalable, low latency multi-model (key-value, graph, wide column and document) NoSQL database, including support for the Cassandra API - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/cosmos-db/&quot;&gt;https://azure.microsoft.com/en-us/services/cosmos-db/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/mapr-db&quot;&gt;MapR-DB&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Commercial document and wide column NoSQL database as part of the MapR stack&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;historical--legacy-options&quot;&gt;Historical / Legacy Options&lt;/h2&gt; &lt;p&gt;The following technologies were either popular options or are referenced in on-line lists of wide column NoSQL database, but are no longer sold or maintained:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Cloudata&lt;/td&gt; &lt;td&gt;Open source Bigtable clone from Gruter, started in February 2011 before being abandoned after 4 commits in March 2011 - &lt;a href=&quot;https://github.com/gruter/cloudata&quot;&gt;https://github.com/gruter/cloudata&lt;/a&gt;; &lt;a href=&quot;http://cloudata.org/&quot;&gt;http://cloudata.org/&lt;/a&gt; (dead)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Hypertable&lt;/td&gt; &lt;td&gt;Open source C++ project based on Google Bigtable that ran over a distributed filesystem, started in 2008, with development ceasing in March 2016 - &lt;a href=&quot;http://hypertable.org/&quot;&gt;http://hypertable.org/&lt;/a&gt; (dead); &lt;a href=&quot;https://github.com/hypertable/hypertable&quot;&gt;https://github.com/hypertable/hypertable&lt;/a&gt;; &lt;a href=&quot;https://en.wikipedia.org/wiki/Hypertable&quot;&gt;https://en.wikipedia.org/wiki/Hypertable&lt;/a&gt;; &lt;a href=&quot;http://www.hypertable.com/blog/hypertable_inc._is_closing_its_doors&quot;&gt;http://www.hypertable.com/blog/hypertable_inc._is_closing_its_doors&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Compute Cluster Managers</title><link>https://ondataengineering.net/tech-categories/compute-cluster-managers/</link><description> &lt;p&gt;Our list of and information on compute cluster managers, including Apache YARN and Mesos, along with their associated technologies.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Category Definition&lt;/h2&gt; &lt;!-- Tech Vendor metadata --&gt; &lt;p&gt;Technologies that start and manage the executor processes for distributed and transient data processing jobs, and then manage the use of resources (primarily cpu and memory) across the jobs running on the cluster. Should provide a way of packaging jobs and ensuring jobs are isolated from each other during execution, for example to allow different jobs to run in different environments against different versions of libraries. When running over a combined compute / storage cluster (like Hadoop), should also support data locality ensuring that individual executors are running next to the data they require. &lt;!--more--&gt;&lt;/p&gt; &lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt; &lt;p&gt;There are two main technologies in this space:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/yarn/&quot;&gt;Apache YARN&lt;/a&gt;&lt;/td&gt; &lt;td&gt;The resource manager within the Apache Hadoop project, supporting transient jobs running on an Hadoop cluster, and with support for long running services coming.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-mesos/&quot;&gt;Apache Mesos&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Stand alone more general purpose cluster manager, with additional support for long running containerised applications ala Kerberos or Docker Swarm.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;apache-yarn&quot;&gt;Apache YARN&lt;/h2&gt; &lt;p&gt;For more information on deploying Hadoop (and YARN), see our &lt;a href=&quot;/tech-categories/hadoop-distributions/&quot;&gt;Hadoop Distributions&lt;/a&gt; page.&lt;/p&gt; &lt;p&gt;The following technologies are associated with Apache YARN:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-slider/&quot;&gt;Apache Slider (Incubating)&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Application for deploying long running cluster applications on YARN, now effectively dead following the plan to add support for long running services directly into YARN&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Apache Twill&lt;/td&gt; &lt;td&gt;Abstraction over YARN that reduces the complexity of developing distributed applications - &lt;a href=&quot;http://twill.apache.org/&quot;&gt;http://twill.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/llama/&quot;&gt;Llama&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Framework for long running low-latency distributed applications to request resources from YARN, built to support Apache Impala&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;apache-mesos&quot;&gt;Apache Mesos&lt;/h2&gt; &lt;p&gt;Apache Mesos is also available as a commercial enterprise product:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/mesosphere-dcos/&quot;&gt;Mesosphere DC/OS&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Commercial distribution of Apache Mesos with additional enterprise features&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;The following technologies are associated with Mesos:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/chronos/&quot;&gt;Chronos&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Scheduling of jobs on a Mesos cluster&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/mesosphere-marathon/&quot;&gt;Marathon&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Mesos support for long running services and applications from Mesosphere&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-aurora/&quot;&gt;Apache Aurora&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Mesos framework for long-running services and cron jobs, originally from Twitter&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;supporting-technologies&quot;&gt;Supporting Technologies&lt;/h2&gt; &lt;p&gt;Other technologies of interest in this space are:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-myriad/&quot;&gt;Apache Myriad (Incubating)&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Tool that allows YARN applications to run over Apache Mesos, allowing them to co-exist and share cluster resources.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Apache REEF&lt;/td&gt; &lt;td&gt;A library that provides an abstraction layer over YARN and Mesos, allowing applications to support both (and future technologies) via a single API - &lt;a href=&quot;http://reef.apache.org/&quot;&gt;http://reef.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-zookeeper/&quot;&gt;Apache ZooKeeper&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Service for managing coordination (e.g. configuration information and synchronisation) of distributed and clustered systems.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Apache Curator&lt;/td&gt; &lt;td&gt;A set of Java libraries that make using Apache ZooKeeper much easier - &lt;a href=&quot;http://curator.apache.org/&quot;&gt;http://curator.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;storage&quot;&gt;Storage&lt;/h2&gt; &lt;p&gt;Distributed jobs running on compute clusters will require access to storage, for which is there a number of options:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A cluster filesystem, either running over the compute cluster (e.g. Hadoop) or external to the cluster&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/tech-categories/object-stores/&quot;&gt;Object Storage&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Simple network available storage that doesn’t provide parallel data access&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Time Series Databases</title><link>https://ondataengineering.net/tech-categories/time-series-databases/</link><description> &lt;p&gt;Our list of and information on commercial, open source and cloud based time series databases and associated technologies, including OpenTSDB, Druid, InfluxDB, MachBase and alternatives to these&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Category Definition&lt;/h2&gt; &lt;!-- Tech Vendor metadata --&gt; &lt;p&gt;Databases optimised for storing very large numbers of metrics and allowing these to be aggregated and analysed. Metrics are usually organised by measurements or metric names and one or more tags (e.g. to represent a server or instance). Many implement their own data storage layers, however many also run over external databases, typically &lt;a href=&quot;/tech-categories/nosql-wide-column-stores/&quot;&gt;NoSQL Wide Column Stores&lt;/a&gt;. Use cases traditionally focus on monitoring infrastructure and applications or more recently IoT use cases, but analytics over any timestamped data is fair game. &lt;!--more--&gt;&lt;/p&gt; &lt;h2 id=&quot;further-information&quot;&gt;Further Information&lt;/h2&gt; &lt;p&gt;DB Engines maintain a list of time series databases along with their current popularity: &lt;a href=&quot;https://db-engines.com/en/ranking/time+series+dbms&quot;&gt;https://db-engines.com/en/ranking/time+series+dbms&lt;/a&gt;&lt;/p&gt; &lt;p&gt;From Steven Acreman on the Outlyer blog, a review of Time Series Databases from August 2016, some interesting thoughts, and a detailed analysis Google spreadsheet: &lt;a href=&quot;https://blog.outlyer.com/top10-open-source-time-series-databases&quot;&gt;https://blog.outlyer.com/top10-open-source-time-series-databases&lt;/a&gt;&lt;/p&gt; &lt;h2 id=&quot;open-source-over-external-data-stores&quot;&gt;Open Source over external data stores&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/opentsdb/&quot;&gt;OpenTSDB&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Time Series DB that runs over Apache HBase&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;KairosDB&lt;/td&gt; &lt;td&gt;Fork of OpenTSDB that runs over Apache Cassandra - &lt;a href=&quot;https://kairosdb.github.io/&quot;&gt;https://kairosdb.github.io/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Chronix&lt;/td&gt; &lt;td&gt;Time Series DB that runs over Apache Solr - &lt;a href=&quot;http://www.chronix.io/&quot;&gt;http://www.chronix.io/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Heroic&lt;/td&gt; &lt;td&gt;Time Series DB from Spotify based on Bigtable/Cassandra, and Elasticsearch - &lt;a href=&quot;https://github.com/spotify/heroic&quot;&gt;https://github.com/spotify/heroic&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Blueflood&lt;/td&gt; &lt;td&gt;Time Series DB over Cassandra from Rackspace - &lt;a href=&quot;http://blueflood.io/&quot;&gt;http://blueflood.io/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Warp 10&lt;/td&gt; &lt;td&gt;Geo Time Series DB that also geo co-ordinates and/or elevation alongside timestamps that runs over HBase and Kafka - &lt;a href=&quot;http://www.warp10.io/&quot;&gt;http://www.warp10.io/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Newts&lt;/td&gt; &lt;td&gt;Time Series DB over Cassandra - &lt;a href=&quot;http://opennms.github.io/newts/&quot;&gt;http://opennms.github.io/newts/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;open-source-using-custom-storage&quot;&gt;Open Source using custom storage&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;RRDtool&lt;/td&gt; &lt;td&gt;Open source data logging and graphing for time series data, originally created in 1999 - &lt;a href=&quot;https://oss.oetiker.ch/rrdtool/&quot;&gt;https://oss.oetiker.ch/rrdtool/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/druid&quot;&gt;Druid&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Runs over HDFS/S3 and supports real time aggregations of streaming data&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DalmatinerDB&lt;/td&gt; &lt;td&gt;High performance Time Series DB written in Erlang and based on Riak Core that runs over ZFS - &lt;a href=&quot;https://dalmatiner.io/&quot;&gt;https://dalmatiner.io/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Beringei&lt;/td&gt; &lt;td&gt;In memory Time Series DB from Facebook - &lt;a href=&quot;https://github.com/facebookincubator/beringei&quot;&gt;https://github.com/facebookincubator/beringei&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Atlas&lt;/td&gt; &lt;td&gt;In memory Time Series DB from Netflix - &lt;a href=&quot;https://github.com/Netflix/atlas/wiki/Overview&quot;&gt;https://github.com/Netflix/atlas/wiki/Overview&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;SiriDB&lt;/td&gt; &lt;td&gt;Open source Time Series DB - &lt;a href=&quot;http://siridb.net/&quot;&gt;http://siridb.net/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Akumuli&lt;/td&gt; &lt;td&gt;C++ Time Series DB that can be used as an embedded library - &lt;a href=&quot;http://akumuli.org/&quot;&gt;http://akumuli.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gnocchi&lt;/td&gt; &lt;td&gt;Open source Time Series DB that was spun off from the OpenStack Ceilometer project - &lt;a href=&quot;http://gnocchi.xyz/&quot;&gt;http://gnocchi.xyz/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;commercial-open-source&quot;&gt;Commercial Open Source&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/influxdb/&quot;&gt;InfluxDB&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Implemented in Go, with commercial and cloud versions also available&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Axibase&lt;/td&gt; &lt;td&gt;Time Series DB built on HDFS, with a commercial Enterprise edition available - &lt;a href=&quot;https://axibase.com/&quot;&gt;https://axibase.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Riak TS &lt;/td&gt; &lt;td&gt;Time Series DB built on the core of Riak KV, and available in a number of commercial editions - &lt;a href=&quot;http://basho.com/products/riak-ts/&quot;&gt;http://basho.com/products/riak-ts/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;TimescaleDB&lt;/td&gt; &lt;td&gt;Plugin to PostgreSQL to add time series data storage - &lt;a href=&quot;https://www.timescale.com/&quot;&gt;https://www.timescale.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;commercial-closed-source&quot;&gt;Commercial Closed Source&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;MachBase&lt;/td&gt; &lt;td&gt;Commercial Time Series DB previously known as InfiniFlux - &lt;a href=&quot;http://www.infiniflux.com/machbase/&quot;&gt;http://www.infiniflux.com/machbase/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;KDB+&lt;/td&gt; &lt;td&gt;Time Series DB from KX Systems that used heavily in financial organisations - &lt;a href=&quot;https://kx.com/discover/#time-series-database&quot;&gt;https://kx.com/discover/#time-series-database&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;cloud-services&quot;&gt;Cloud Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Azure Time Series Insights&lt;/td&gt; &lt;td&gt;Storage, analytics and visualisation service for time series data - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/time-series-insights/&quot;&gt;https://azure.microsoft.com/en-us/services/time-series-insights/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;monitoring-stacks-open-source&quot;&gt;Monitoring Stacks (open source)&lt;/h2&gt; &lt;p&gt;The following stacks support monitoring infrastructure and applications, with connectors to import metrics from a number of sources:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Graphite&lt;/td&gt; &lt;td&gt;Three tier stack consisting of Whisper (simple time series database, graphite-web (a graph/dashboard UI) and Carbon (for ingestion of data)- &lt;a href=&quot;https://graphiteapp.org/&quot;&gt;https://graphiteapp.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Prometheus&lt;/td&gt; &lt;td&gt;Open source monitoring solution that includes a time series database and alerting; part of the Cloud Native Computing Foundation - &lt;a href=&quot;https://prometheus.io/&quot;&gt;https://prometheus.io/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Hawkular&lt;/td&gt; &lt;td&gt;Open source monitoring solution over Cassandra with alerting and visualisation capabilities, backed by RedHat - &lt;a href=&quot;http://www.hawkular.org/&quot;&gt;http://www.hawkular.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;NetData&lt;/td&gt; &lt;td&gt;Local in memory metrics dashboards for monitoring servers and applications, with the ability to forward metrics to downstream time series dbs - &lt;a href=&quot;https://my-netdata.io/&quot;&gt;https://my-netdata.io/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Icinga&lt;/td&gt; &lt;td&gt;Commercial support available - &lt;a href=&quot;https://www.icinga.com/&quot;&gt;https://www.icinga.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;monitoring-stacks-as-a-service&quot;&gt;Monitoring Stacks (as a service)&lt;/h2&gt; &lt;p&gt;If you’re looking for a time series database to handle monitoring and management of infrastructure and applications, there are also a number of cloud based services you could consider:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;AWS CloudWatch&lt;/td&gt; &lt;td&gt;Collects and tracks metrics for AWS services, with support for custom metrics - &lt;a href=&quot;https://aws.amazon.com/cloudwatch/&quot;&gt;https://aws.amazon.com/cloudwatch/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Outlyer&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.outlyer.com/&quot;&gt;https://www.outlyer.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;VMWare Wavefront&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.wavefront.com/&quot;&gt;https://www.wavefront.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;VividCortex&lt;/td&gt; &lt;td&gt;Specialist database performance metrics monitoring - &lt;a href=&quot;https://www.vividcortex.com/&quot;&gt;https://www.vividcortex.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;SignalFX&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://signalfx.com&quot;&gt;https://signalfx.com&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Circonus&lt;/td&gt; &lt;td&gt;Also available for installation on premises - &lt;a href=&quot;https://www.circonus.com&quot;&gt;https://www.circonus.com&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Datadog&lt;/td&gt; &lt;td&gt;Available for free for up to 5 hosts - &lt;a href=&quot;https://www.datadoghq.com/&quot;&gt;https://www.datadoghq.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Librato&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.librato.com/&quot;&gt;https://www.librato.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;ServerDensity&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.serverdensity.com/&quot;&gt;https://www.serverdensity.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;iot-stacks&quot;&gt;IoT Stacks&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;TempoIQ&lt;/td&gt; &lt;td&gt;Commercial IoT collection, storage and analytics service - &lt;a href=&quot;https://www.tempoiq.com/&quot;&gt;https://www.tempoiq.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;SiteWhere&lt;/td&gt; &lt;td&gt;Commercial open source IoT platform built over MongoDB/HBase/InfluxDB - &lt;a href=&quot;http://www.sitewhere.org/&quot;&gt;http://www.sitewhere.org/&lt;/a&gt; / &lt;a href=&quot;http://www.sitewhere.com/&quot;&gt;http://www.sitewhere.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>In Memory Databases</title><link>https://ondataengineering.net/tech-categories/in-memory-databases/</link><description> &lt;p&gt;Our list of and information on in memory relational databases and Hadoop compatible filesystems, including MemSQL, SAP HANA, EXASOL, TimesTen, Ignite and alternatives to these.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Category Definition&lt;/h2&gt; &lt;!-- Tech Vendor metadata --&gt; &lt;p&gt;Technologies that serve data entirely or primarily from memory, with the aim of providing significant higher performance for querying and accessing data. They are generally backed by some sort of persistent storage with guarantees around durability, with some technologies able to act as in memory caches over equivalent disk based technologies. The technologies listed on this page span a number of different technology categories, including relational databases, Hadoop compatible filesystems and key value stores, and will also appear under the relevant technology category page where they exist. &lt;!--more--&gt;&lt;/p&gt; &lt;h2 id=&quot;further-information&quot;&gt;Further Information&lt;/h2&gt; &lt;p&gt;&lt;a href=&quot;https://www.forrester.com/report/The+Forrester+Wave+InMemory+Databases+Q1+2017/-/E-RES132143&quot;&gt;The Forrester Wave: In Memory Database, Q1 2017&lt;/a&gt; provides a view of “enterprise” in memory technologies, with a mix of NoSQL, enterprise RDBMS and and dedicated in memory relational technologies, however the list is limited and includes some technologies that aren’t in memory at all. It’s available from Oracle &lt;a href=&quot;http://www.oracle.com/us/corporate/analystreports/forrester-imdb-wave-2017-3616348.pdf&quot;&gt;here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There is also a list of in memory databases on Wikipedia &lt;a href=&quot;https://en.wikipedia.org/wiki/List_of_in-memory_databases&quot;&gt;here&lt;/a&gt;, however it does not clearly distinguish between the different types of databases.&lt;/p&gt; &lt;h2 id=&quot;in-memory-relational-databases&quot;&gt;In Memory Relational Databases&lt;/h2&gt; &lt;p&gt;There are a wide range of Relational Databases that operate entirely in memory, but with the ability to persist data to disk and provide durability guarantees:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;MemSQL&lt;/td&gt; &lt;td&gt;Distributed in memory relational database, with wire compatibility with MySQL and support for row and columnar storage, and a free community edition - &lt;a href=&quot;http://www.memsql.com/&quot;&gt;http://www.memsql.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;SAP HANA&lt;/td&gt; &lt;td&gt;In memory relational DBMS primarily focused on accelerating SAP applications - &lt;a href=&quot;https://www.sap.com/products/hana.html&quot;&gt;https://www.sap.com/products/hana.html&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;VoltDB&lt;/td&gt; &lt;td&gt;Scale out in memory relational from Mike Stonebraker, with open source community edition - &lt;a href=&quot;https://www.voltdb.com/&quot;&gt;https://www.voltdb.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;EXASOL&lt;/td&gt; &lt;td&gt;In memory MPP database with columnar compression and SQL support - &lt;a href=&quot;http://www.exasol.com/&quot;&gt;http://www.exasol.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Oracle TimesTen&lt;/td&gt; &lt;td&gt;In memory relational database that can also act as a cache over Oracle Database - &lt;a href=&quot;https://www.oracle.com/database/timesten-in-memory-database/index.html&quot;&gt;https://www.oracle.com/database/timesten-in-memory-database/index.html&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-ignite&quot;&gt;GridGain/Apache Ignite&lt;/a&gt;&lt;/td&gt; &lt;td&gt;In memory relational database that also supports in memory filesystem and key value stores&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;MapD&lt;/td&gt; &lt;td&gt;In memory, column store, SQL relational database that runs on GPUs - &lt;a href=&quot;https://www.mapd.com/&quot;&gt;https://www.mapd.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Kinetica&lt;/td&gt; &lt;td&gt;Distributed in memory relational database that runs on GPUs - &lt;a href=&quot;https://www.kinetica.com&quot;&gt;https://www.kinetica.com&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;All the big enterprise relational databases now have (or are starting to add) in memory capabilities, including IBM DB2, IBM PureData, SQL Server, Oracle Database and Teradata.&lt;/p&gt; &lt;p&gt;And a number of databases support in-memory only storage engines (for example MySQL), however these generally don’t provide persistent storage.&lt;/p&gt; &lt;h2 id=&quot;in-memory-hadoop-compatible-filesystems&quot;&gt;In Memory Hadoop Compatible Filesystems&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/alluxio/&quot;&gt;Alluxio&lt;/a&gt;&lt;/td&gt; &lt;td&gt;In memory (HDFS compatible) distributed filesystem, with support for tiering data onto persistent storage&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-ignite&quot;&gt;GridGain/Apache Ignite&lt;/a&gt;&lt;/td&gt; &lt;td&gt;In memory (HDFS compatible) distributed filesystem that also supports in memory relational database and key value stores&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;See out &lt;a href=&quot;/tech-categories/hadoop-compatible-filesystems/&quot;&gt;Hadoop Compatible Filesystems&lt;/a&gt; for more information on Hadoop Compatible Filesystems.&lt;/p&gt; &lt;h2 id=&quot;in-memory-nosql-key-value-stores&quot;&gt;In Memory NoSQL Key Value Stores&lt;/h2&gt; &lt;p&gt;Many NoSQL Key Value stores operated entirely in memory. These technologies aren’t a focus for us, but include Redis, Memcached, HazelCast, EhCache, Riak KV, Aerospike, Oracle Coherance, Infinispan / JBoss Data Grid and Pivotal Gemfire / Apache Geode.&lt;/p&gt; &lt;h2 id=&quot;in-memory-embeddable-databases&quot;&gt;In Memory Embeddable Databases&lt;/h2&gt; &lt;p&gt;A range of in memory databases are also available that can be embedded into custom applications. Again, these technologies aren’t a focus for us, but Baeldung has a good list of Java options from April 2017 &lt;a href=&quot;http://www.baeldung.com/java-in-memory-databases&quot;&gt;here&lt;/a&gt;. Alternatives are also available for .Net (e.g. inmemory.net) and embedded systems (e.g. FairCom or Raima)&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Data Virtualization</title><link>https://ondataengineering.net/tech-categories/data-virtualization/</link><description> &lt;p&gt;Our list of and information on data virtualization technologies, including Informatica, IBM, SAS, Cisco and Denodo technologies and alternatives to these.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Category Definition&lt;/h2&gt; &lt;!-- Tech Vendor metadata --&gt; &lt;p&gt;Tools that enable the execution of data queries across multiple data sources. Most support the ability to create a semantic layer or virtual data schema across the underlying data sources (including the caching or materialization of tables within this layer and the ability to update it and therefore the underlying data sources) alongside the ability to run arbitrary queries across the underlying data (including in some cases the ability to do this without first defining the schema of the data in these sources). Query logic is pushed down where possible so that it’s executed in the underlying data source, with joins and aggregation of data from multiple sources then performed within the data virtualisation layer. Sometimes referred to as data federation. &lt;!--more--&gt;&lt;/p&gt; &lt;h2 id=&quot;further-information&quot;&gt;Further Information&lt;/h2&gt; &lt;p&gt;The following analyst material covers a number of technologies in this category:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.google.co.uk/search?q=Gartner+&amp;quot;Data+Integration+Tools&amp;quot;&quot;&gt;Gartner Magic Quadrant &amp;amp; Critical Capabilities for Data Integration Tools&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.google.co.uk/search?q=Gartner+&amp;quot;Data+Virtualization&amp;quot;&quot;&gt;Gartner Market Guide for Data Virtualization&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.google.co.uk/search?q=Forrester+&amp;quot;Data+Virtualization&amp;quot;&quot;&gt;Forrester Wave: Enterprise Data Virtualization&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;A number of &lt;a href=&quot;/tech-categories/query-engines/&quot;&gt;Query Engines&lt;/a&gt; support joins and aggregation of data from multiple data sources&lt;/p&gt; &lt;h2 id=&quot;enterprise-vendors&quot;&gt;“Enterprise” Vendors&lt;/h2&gt; &lt;p&gt;The following are capabilities that exist as part of wider data integration and management product suites:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Informatica Platform&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.informatica.com/gb/products/data-integration/real-time-integration/data-virtualization.html&quot;&gt;https://www.informatica.com/gb/products/data-integration/real-time-integration/data-virtualization.html&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;IBM Infosphere Federation Server&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://www-03.ibm.com/software/products/da/ibminfofedeserv&quot;&gt;http://www-03.ibm.com/software/products/da/ibminfofedeserv&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;SAS Federation Server&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.sas.com/en_us/software/federation-server.html&quot;&gt;https://www.sas.com/en_us/software/federation-server.html&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;Oracle, SAP and Microsoft also claim capabilities in this space, however these generally aren’t through specific federation / virtualization products.&lt;/p&gt; &lt;h2 id=&quot;specialist-vendors&quot;&gt;Specialist Vendors&lt;/h2&gt; &lt;p&gt;The following are capabilities from specialist vendors:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Cisco Information Server&lt;/td&gt; &lt;td&gt;Originally Composite, one of the first and most established products in this space &lt;a href=&quot;http://www.compositesw.com/data-virtualization/&quot;&gt;http://www.compositesw.com/data-virtualization/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Data Virtuality&lt;/td&gt; &lt;td&gt;Marketed as a logical data warehouse tool - &lt;a href=&quot;https://datavirtuality.com/&quot;&gt;https://datavirtuality.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/denodo-platform/&quot;&gt;Denodo Platform&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Very well established and mature product, with free express edition&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Dremio&lt;/td&gt; &lt;td&gt;Supports querying over data in databases, Hadoop, NoSQL and file systems, with support for creating virtual tables and automatically physically materialising these - &lt;a href=&quot;https://www.dremio.com/&quot;&gt;https://www.dremio.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Information Builders iWay Data Hub&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://www.informationbuilders.com/products/eii&quot;&gt;http://www.informationbuilders.com/products/eii&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;K2View&lt;/td&gt; &lt;td&gt;Specialises in entity (e.g. customer) based views of data aggregated from multiple sources - &lt;a href=&quot;http://www.k2view.com&quot;&gt;http://www.k2view.com&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/openlink-virtuoso-universal-server/&quot;&gt;OpenLink Virtuoso Universal Server&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Multi-model database that supports virtualisation of local and/or remote tabular relational databases&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Querona&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.querona.com/&quot;&gt;https://www.querona.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RedHat JBoss Data Virtualization&lt;/td&gt; &lt;td&gt;Originally Metamatrix - &lt;a href=&quot;https://www.redhat.com/en/technologies/jboss-middleware/data-virtualization&quot;&gt;https://www.redhat.com/en/technologies/jboss-middleware/data-virtualization&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Rocket Data Virtualization&lt;/td&gt; &lt;td&gt;Focus on mainframe data - &lt;a href=&quot;http://www.rocketsoftware.com/products/rocket-data&quot;&gt;http://www.rocketsoftware.com/products/rocket-data&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Stone Bond Enterprise Enabler&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://stonebond.com/&quot;&gt;http://stonebond.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;data-connectors&quot;&gt;Data Connectors&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Attunity Connect&lt;/td&gt; &lt;td&gt;Connectors for relational, non relational and applications - &lt;a href=&quot;https://www.attunity.com/products/connect/&quot;&gt;https://www.attunity.com/products/connect/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;OpenLink Software Universal Data Access Suite&lt;/td&gt; &lt;td&gt;ODBC, JDBC, ADO.NET, OLE DB connectors for wide range of data sources, including IBM DB2, IBM Informix, Ingres, Microsoft SQL Server, MySQL, OpenLink Virtuoso, Oracle, PostgreSQL, Progress and other ODBC- and JDBC-accessible sources — &lt;a href=&quot;https://uda.openlinksw.com/&quot;&gt;https://uda.openlinksw.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Progress DataDirect&lt;/td&gt; &lt;td&gt;Connectors for vast range of data sources, including big data, relational, saas/cloud and text - &lt;a href=&quot;https://www.progress.com/datadirect-connectors&quot;&gt;https://www.progress.com/datadirect-connectors&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;semantic-web-technologies&quot;&gt;Semantic Web Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Capsenta Ultrawrap&lt;/td&gt; &lt;td&gt;Virtualizes relational databases as an RDF semantic web data source - &lt;a href=&quot;https://capsenta.com/&quot;&gt;https://capsenta.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Complexible Stardog&lt;/td&gt; &lt;td&gt;RDF database that also support property graphs and data virtualisation - &lt;a href=&quot;http://www.stardog.com/&quot;&gt;http://www.stardog.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;D2RQ&lt;/td&gt; &lt;td&gt;Opens source platform for accessing relational databases as an RDF graph - &lt;a href=&quot;http://d2rq.org/&quot;&gt;http://d2rq.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/openlink-virtuoso-universal-server/&quot;&gt;OpenLink Virtuoso Universal Server&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Multi-model database that supports virtualisation of local and/or remote tabular relational databases and/or other data sources as RDF semantic web data sources&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>RDF Databases</title><link>https://ondataengineering.net/tech-categories/rdf-databases/</link><description> &lt;p&gt;Our list of and information on commercial, open source and cloud based RDF databases and associated technologies, including MarkLogic, AllegroGraph, Stardog, BlazeGraph and alternatives to these.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Category Definition&lt;/h2&gt; &lt;!-- Tech Vendor metadata --&gt; &lt;p&gt;Databases designed to support the storage of RDF (or triple) data. RDF (Resource Description Framework) is a W3C data model standard that describes data as subject–predicate–object expressions (or triples). This allows the creation of graphs of knowledge, however unlike more general purpose graph databases, there is no support for properties or labels - everything is represented using triples. Data is queried using the SPARQL query language (another W3C standard). Internally, data can be considered to be stored as a single table containing three columns (the subject, predicate and object), with indexing to support the traversal and enumeration of predicates (relationships) for a given subject. A number of triple ontologies (or schemas) are also available that define standard subject/object and predicate types allowing for data interchange, including OWL and RDFS. The W3C standards were introduced to support semantic web and Linked Open Data use cases that focus on semantics and inference. &lt;!--more--&gt;&lt;/p&gt; &lt;h2 id=&quot;further-information&quot;&gt;Further Information&lt;/h2&gt; &lt;p&gt;The W3C standards for RDF and SPARQL are available online at &lt;a href=&quot;https://www.w3.org/RDF/&quot;&gt;https://www.w3.org/RDF/&lt;/a&gt; and &lt;a href=&quot;http://www.w3.org/TR/rdf-sparql-query/&quot;&gt;http://www.w3.org/TR/rdf-sparql-query/&lt;/a&gt; respectively. The relevant Wikipedia pages are also good places to start at &lt;a href=&quot;https://en.wikipedia.org/wiki/Resource_Description_Framework&quot;&gt;https://en.wikipedia.org/wiki/Resource_Description_Framework&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/SPARQL&quot;&gt;https://en.wikipedia.org/wiki/SPARQL&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;DB Engines has a list of RDF databases at &lt;a href=&quot;https://db-engines.com/en/ranking/rdf+store&quot;&gt;https://db-engines.com/en/ranking/rdf+store&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The following analyst material covers a number of technologies in this category:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.google.co.uk/search?q=Forrester+&amp;quot;Graph+Databases&amp;quot;&quot;&gt;Forrester Vendor Landscape: Graph Databases&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.bloorresearch.com/technology/graph-databases/&quot;&gt;Bloor Graph and RDF Databases&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;rdf-frameworks&quot;&gt;RDF Frameworks&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Eclipse RDF4J&lt;/td&gt; &lt;td&gt;An Eclipse open source project for working with RDF data, including provision of a standard SPARQL interface that can be integrated with backend databases. Previously known as Sesame - &lt;a href=&quot;http://rdf4j.org/&quot;&gt;http://rdf4j.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Jena&lt;/td&gt; &lt;td&gt;Framework for developing Semantic Web and Linked Data applications in Java - &lt;a href=&quot;http://jena.apache.org/&quot;&gt;http://jena.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Commons RDF&lt;/td&gt; &lt;td&gt;Commons library for working with RDF data - &lt;a href=&quot;http://commons.apache.org/proper/commons-rdf/&quot;&gt;http://commons.apache.org/proper/commons-rdf/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Redland&lt;/td&gt; &lt;td&gt;Open source C libraries for working with RDF data - &lt;a href=&quot;http://librdf.org/&quot;&gt;http://librdf.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;CubicWeb&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.cubicweb.org/&quot;&gt;https://www.cubicweb.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;commercial-rdf-databases&quot;&gt;Commercial RDF Databases&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Amazon Neptune&lt;/td&gt; &lt;td&gt;Graph and RDF database service with support for TinkerPop Gremlin and RDF SPARQL - &lt;a href=&quot;https://aws.amazon.com/neptune/&quot;&gt;https://aws.amazon.com/neptune/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;MarkLogic&lt;/td&gt; &lt;td&gt;Commercial ACID compliant XML/JSON document store with support for creation of triple indexes over documents queryable via SPARQL - &lt;a href=&quot;http://www.marklogic.com/&quot;&gt;http://www.marklogic.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;OpenLink Virtuoso Universal Server&lt;/td&gt; &lt;td&gt;Supports persistence of documents, relational, RDF and graph data - &lt;a href=&quot;https://virtuoso.openlinksw.com/&quot;&gt;https://virtuoso.openlinksw.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Franz AllegroGraph&lt;/td&gt; &lt;td&gt;Commercial ACID compliant that supports both RDF and property graphs, with a free edition available - &lt;a href=&quot;https://allegrograph.com/allegrograph/&quot;&gt;https://allegrograph.com/allegrograph/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Ontotext GraphDB&lt;/td&gt; &lt;td&gt;Commercial RDF database, previously known as OWLIM, and with a free edition available - &lt;a href=&quot;https://ontotext.com/products/graphdb/&quot;&gt;https://ontotext.com/products/graphdb/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Complexible Stardog&lt;/td&gt; &lt;td&gt;RDF database that also support property graphs and data virtualisation, with a community edition available - &lt;a href=&quot;http://www.stardog.com/&quot;&gt;http://www.stardog.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Dydra&lt;/td&gt; &lt;td&gt;Cloud based - &lt;a href=&quot;https://dydra.com/&quot;&gt;https://dydra.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;SparkleDB&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.syssurge.com/Products/SparkleDB/Home.aspx&quot;&gt;https://www.syssurge.com/Products/SparkleDB/Home.aspx&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Cray Graph Engine&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.cray.com/products/analytics/cray-graph-engine&quot;&gt;https://www.cray.com/products/analytics/cray-graph-engine&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Oracle Spatial and Graph option for Oracle Database&lt;/td&gt; &lt;td&gt;Adds RDF (and graph) capabilities to the Oracle database - &lt;a href=&quot;http://www.oracle.com/technetwork/database/options/spatialandgraph/overview/index.html&quot;&gt;http://www.oracle.com/technetwork/database/options/spatialandgraph/overview/index.html&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RDF Graph for Oracle NoSQL Database&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://www.oracle.com/technetwork/database/database-technologies/nosqldb/overview/index.html&quot;&gt;http://www.oracle.com/technetwork/database/database-technologies/nosqldb/overview/index.html&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;open-source-rdf-databases&quot;&gt;Open Source RDF Databases&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;BlazeGraph&lt;/td&gt; &lt;td&gt;Open Source RDF graph database with property graph features, queryable via SPARQL and Tinkerpop - &lt;a href=&quot;https://www.blazegraph.com/&quot;&gt;https://www.blazegraph.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;4store&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://github.com/4store/4store&quot;&gt;https://github.com/4store/4store&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RedStore&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.aelius.com/njh/redstore/&quot;&gt;https://www.aelius.com/njh/redstore/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Mulgara&lt;/td&gt; &lt;td&gt;Open source java RDF database - &lt;a href=&quot;http://mulgara.org/&quot;&gt;http://mulgara.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;BrightstarDB&lt;/td&gt; &lt;td&gt;Open Source RDF database for the .NET platform - &lt;a href=&quot;http://brightstardb.com/&quot;&gt;http://brightstardb.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Strabon&lt;/td&gt; &lt;td&gt;Spatiotemporal RDF store - &lt;a href=&quot;http://www.strabon.di.uoa.gr/&quot;&gt;http://www.strabon.di.uoa.gr/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Rya (Incubating)&lt;/td&gt; &lt;td&gt;RDF triple store built on Apache Accumulo - &lt;a href=&quot;http://rya.apache.org/&quot;&gt;http://rya.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Graph Databases</title><link>https://ondataengineering.net/tech-categories/graph-databases/</link><description> &lt;p&gt;Our list of and information on commercial, open source and cloud based graph databases, including Neo4j, OrientDB, JanusGraph and alternatives to these.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Category Definition&lt;/h2&gt; &lt;!-- Tech Vendor metadata --&gt; &lt;p&gt;Operational (OLTP) databases designed to support labelled property graph data models, where nodes and relationships have labels (types) and lists of property name-value pairs. Focus is on transactional ACID inserts/updates and traversal queries using index free adjacency (allows direct navigation between nodes without the use of indexes) over subsets of data rather than batch analytics over all data. Populate query interfaces/languages include Cypher (originated with Neo4J but now open source) and TinkerPop Gremlin (part of the TinkerPop framework that provides query capabilities and graph analytics over graph data). &lt;!--more--&gt;&lt;/p&gt; &lt;h2 id=&quot;further-information&quot;&gt;Further Information&lt;/h2&gt; &lt;p&gt;The Morning Paper review of the “Do we need specialized graph databases? Benchmarking real-time social networking applications” paper from the University of Waterloo, Ontario from May 2017 is worth a read before you go any further - this covers a number of use cases where relational databases perform better than dedicated graph databases, and also looks at the performance impact of using TinkerPop over a native API - &lt;a href=&quot;https://blog.acolyer.org/2017/07/07/do-we-need-specialized-graph-databases-benchmarking-real-time-social-networking-applications/&quot;&gt;https://blog.acolyer.org/2017/07/07/do-we-need-specialized-graph-databases-benchmarking-real-time-social-networking-applications/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;DB Engines has a list of graph databases at &lt;a href=&quot;https://db-engines.com/en/ranking/graph+dbms&quot;&gt;https://db-engines.com/en/ranking/graph+dbms&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The following analyst material covers a number of technologies in this category:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.google.co.uk/search?q=Gartner+&amp;quot;Operational+Database+Management+Systems&amp;quot;&quot;&gt;Gartner Magic Quadrant &amp;amp; Critical Capabilities for Operational Database Management Systems&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.google.co.uk/search?q=Forrester+&amp;quot;Graph+Databases&amp;quot;&quot;&gt;Forrester Vendor Landscape: Graph Databases&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.bloorresearch.com/technology/graph-databases/&quot;&gt;Bloor Graph and RDF Databases&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And Neo4j have a number of eBooks available for free download from &lt;a href=&quot;https://neo4j.com/&quot;&gt;https://neo4j.com/&lt;/a&gt; and &lt;a href=&quot;https://neo4j.com/books/&quot;&gt;https://neo4j.com/books/&lt;/a&gt;, including a copy of O’Reilly’s Graph Databases.&lt;/p&gt; &lt;h2 id=&quot;query-languages--interfaces&quot;&gt;Query Languages / Interfaces&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Cypher&lt;/td&gt; &lt;td&gt;Graph query language, originally from Neo4j, now open source and used by a wide range of graph databases, with out of the box packages for running Cypher jobs over Spark and TinkerPop Gremlin - &lt;a href=&quot;http://www.opencypher.org/&quot;&gt;http://www.opencypher.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;TinkerPop Gremlin&lt;/td&gt; &lt;td&gt;Query interface that uses low level adapters to execute graph traversals against any compatible graph databases - &lt;a href=&quot;https://tinkerpop.apache.org/gremlin.html&quot;&gt;https://tinkerpop.apache.org/gremlin.html&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;See the note above on the “Do we need specialized graph databases? Benchmarking real-time social networking applications” paper and the performance impact of using Apache TinkerPop&lt;/p&gt; &lt;h2 id=&quot;commercial-oltp-graph-databases&quot;&gt;Commercial OLTP Graph Databases&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Neo4j&lt;/td&gt; &lt;td&gt;ACID-compliant transactional database with native graph storage and processing; open source with commercial edition; utilises Cypher - &lt;a href=&quot;https://neo4j.com/&quot;&gt;https://neo4j.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;OrientDB&lt;/td&gt; &lt;td&gt;Multi-model (key-value, graph and document) NoSQL database with TinkerPop support and both community and enterprise editions - &lt;a href=&quot;http://orientdb.com/graph-database/&quot;&gt;http://orientdb.com/graph-database/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;ArrangoDB&lt;/td&gt; &lt;td&gt;Multi-model (key-value, graph and document) NoSQL database with ACID transactions, TinkerPop compatibility and it’s own AQL query language with support for cluster deployments (including running over Mesos) - &lt;a href=&quot;http://www.arangodb.com&quot;&gt;http://www.arangodb.com&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Sparksee&lt;/td&gt; &lt;td&gt;Graph database formally known as DEX, with support for a range of languages and use on mobile devices and TinkerPop support- &lt;a href=&quot;http://www.sparsity-technologies.com/&quot;&gt;http://www.sparsity-technologies.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;AgensGraph&lt;/td&gt; &lt;td&gt;Commercial multi-model (relational and graph) databases - &lt;a href=&quot;http://www.agensgraph.com/&quot;&gt;http://www.agensgraph.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Redis Graph&lt;/td&gt; &lt;td&gt;Graph database over Redis that supports a subset of neo4j’s Cypher query language - &lt;a href=&quot;http://redismodules.com/modules/redis-graph/&quot;&gt;http://redismodules.com/modules/redis-graph/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;open-source-oltp-graph-databases&quot;&gt;Open Source OLTP Graph Databases&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Dgraph&lt;/td&gt; &lt;td&gt;Open source graph database written in Go - &lt;a href=&quot;https://dgraph.io/&quot;&gt;https://dgraph.io/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;HyperGraphDB&lt;/td&gt; &lt;td&gt;Open Source Java graph database - &lt;a href=&quot;http://hypergraphdb.org&quot;&gt;http://hypergraphdb.org&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;InfoGrid&lt;/td&gt; &lt;td&gt;Open Source Java graph database with support for building REST APIs over the top - &lt;a href=&quot;http://infogrid.org/trac/&quot;&gt;http://infogrid.org/trac/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;VelocityDB&lt;/td&gt; &lt;td&gt;Open Source C# .NET embeddable/distributed graph database - &lt;a href=&quot;https://velocitydb.com/&quot;&gt;https://velocitydb.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Apache S2Graph (Incubating)&lt;/td&gt; &lt;td&gt;OLTP graph database built on Apache HBase - &lt;a href=&quot;https://s2graph.incubator.apache.org/&quot;&gt;https://s2graph.incubator.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;HGraphDB&lt;/td&gt; &lt;td&gt;Open Source implementation of TinkerPop API for Apache HBase - &lt;a href=&quot;https://github.com/rayokota/hgraphdb&quot;&gt;https://github.com/rayokota/hgraphdb&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Sqlg&lt;/td&gt; &lt;td&gt;Open Source implementation of TinkerPop API over relational databases - &lt;a href=&quot;http://www.sqlg.org/&quot;&gt;http://www.sqlg.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Unipop&lt;/td&gt; &lt;td&gt;Open source TinkerPop API over a range of backends including Elasticsearch and JDBC &lt;a href=&quot;https://github.com/unipop-graph/unipop&quot;&gt;https://github.com/unipop-graph/unipop&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;commercial-oltpanalytical-graph-databases&quot;&gt;Commercial OLTP/Analytical Graph Databases&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;DataStax Enterprise&lt;/td&gt; &lt;td&gt;Commercial product built on Apache Cassandra with the addition of graph and search capabilities - &lt;a href=&quot;https://www.datastax.com/&quot;&gt;https://www.datastax.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;TigerGraph&lt;/td&gt; &lt;td&gt;Commercial hybrid OLTP/OLAP graph database that claims order of magnitude performance and scalability improvements over it’s competitors; previously known as GraphSQL - &lt;a href=&quot;http://www.tigergraph.com&quot;&gt;http://www.tigergraph.com&lt;/a&gt;, &lt;a href=&quot;http://www.zdnet.com/article/tigergraph-a-graph-database-born-to-roar/&quot;&gt;http://www.zdnet.com/article/tigergraph-a-graph-database-born-to-roar/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GraphBase&lt;/td&gt; &lt;td&gt;Commercial graph database supporting operational and analytical use cases designed for use in AI applications - &lt;a href=&quot;https://graphbase.ai/&quot;&gt;https://graphbase.ai/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GRAKN.AI&lt;/td&gt; &lt;td&gt;Open Source graph database designed for AI use cases that also supports graph analytics - &lt;a href=&quot;https://grakn.ai&quot;&gt;https://grakn.ai&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Complexible Stardog&lt;/td&gt; &lt;td&gt;RDF database that also support property graphs and data virtualisation, with a community edition available - &lt;a href=&quot;http://www.stardog.com/&quot;&gt;http://www.stardog.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;open-source-oltpanalytical-graph-databases&quot;&gt;Open Source OLTP/Analytical Graph Databases&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;JanusGraph&lt;/td&gt; &lt;td&gt;Open source distributed graph database that runs over a number of storage backends (including Cassandra, HBase and BigTable), with TinkerPop support including support for graph analytics; previously known as Titan - &lt;a href=&quot;http://janusgraph.org/&quot;&gt;http://janusgraph.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;TinkerGraph&lt;/td&gt; &lt;td&gt;In memory graph databases that’s part of TinkerPop as a reference implementation, supporting both TinkerPop OLTP and OLAP use cases - &lt;a href=&quot;http://tinkerpop.apache.org&quot;&gt;http://tinkerpop.apache.org&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;combined-graphrdf-databases&quot;&gt;Combined Graph/RDF Databases&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;OpenLink Virtuoso Universal Server&lt;/td&gt; &lt;td&gt;Supports persistence of documents, relational, RDF and graph data - &lt;a href=&quot;https://virtuoso.openlinksw.com/&quot;&gt;https://virtuoso.openlinksw.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Franz AllegroGraph&lt;/td&gt; &lt;td&gt;Commercial ACID compliant that supports both RDF and property graphs, with a free edition available - &lt;a href=&quot;https://allegrograph.com/allegrograph/&quot;&gt;https://allegrograph.com/allegrograph/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;BlazeGraph&lt;/td&gt; &lt;td&gt;Open Source RDF graph database with property graph features, queryable via SPARQL and Tinkerpop - &lt;a href=&quot;https://www.blazegraph.com/&quot;&gt;https://www.blazegraph.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;graph-databases-as-a-service&quot;&gt;Graph Databases as a Service&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Microsoft Azure Cosmos DB&lt;/td&gt; &lt;td&gt;Massively scalable, low latency multi-model (key-value, graph, wide column and document) NoSQL database, including support for the Gremlin API - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/cosmos-db/&quot;&gt;https://azure.microsoft.com/en-us/services/cosmos-db/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;IBM Graph&lt;/td&gt; &lt;td&gt;Graph database as a service built using JanusGraph - &lt;a href=&quot;https://www.ibm.com/us-en/marketplace/graph&quot;&gt;https://www.ibm.com/us-en/marketplace/graph&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon Neptune&lt;/td&gt; &lt;td&gt;Graph and RDF database service with support for TinkerPop Gremlin and RDF SPARQL - &lt;a href=&quot;https://aws.amazon.com/neptune/&quot;&gt;https://aws.amazon.com/neptune/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;historical--deprecated-graph-databases&quot;&gt;Historical / Deprecated Graph Databases&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Objectivity InfiniteGraph&lt;/td&gt; &lt;td&gt;End of life, with functionality being migrated into Objectivity/DB and ThingSpan - &lt;a href=&quot;http://www.objectivity.com/products/infinitegraph/&quot;&gt;http://www.objectivity.com/products/infinitegraph/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;FlockDB&lt;/td&gt; &lt;td&gt;Open Source distributed graph database from Twitter, however no longer maintained - &lt;a href=&quot;https://github.com/twitter-archive/flockdb&quot;&gt;https://github.com/twitter-archive/flockdb&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GlobalsDB&lt;/td&gt; &lt;td&gt;Open Source, now dead - &lt;a href=&quot;https://github.com/Globals/GlobalsDB&quot;&gt;https://github.com/Globals/GlobalsDB&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Graph Analytics</title><link>https://ondataengineering.net/tech-categories/graph-analytics/</link><description> &lt;p&gt;Our list of and information on commercial and open source graph analytics engines and databases, including Giraph, Hama, GraphX, Flink, JanusGraph, Stardog, TinkerPop and alternatives to these.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Category Definition&lt;/h2&gt; &lt;!-- Tech Vendor metadata --&gt; &lt;p&gt;Technologies that support the execution of analytics over graph data in an external underlying store (often HDFS), generally over the entire graph database to generate aggregated results, identify data of interest, or to enrich the graph. Processing is often based on a BSP (Bulk Synchronous Processing) model made famous by Pregel, the model created by Google to run their PageRank algorithm. &lt;!--more--&gt;&lt;/p&gt; &lt;h2 id=&quot;pregel&quot;&gt;Pregel&lt;/h2&gt; &lt;p&gt;The Morning Paper blog from Adrian Colyer has a good &lt;a href=&quot;https://blog.acolyer.org/2015/05/26/pregel-a-system-for-large-scale-graph-processing/&quot;&gt;introduction to Pragel&lt;/a&gt;, and the original paper is also &lt;a href=&quot;http://www.dcs.bbk.ac.uk/~dell/teaching/cc/paper/sigmod10/p135-malewicz.pdf&quot;&gt;available online&lt;/a&gt;&lt;/p&gt; &lt;h2 id=&quot;analytics-engines&quot;&gt;Analytics Engines&lt;/h2&gt; &lt;p&gt;The following technologies all implement a graph analytics engine over external data, generally using a BSP execution model&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-giraph/&quot;&gt;Giraph&lt;/a&gt;&lt;/td&gt; &lt;td&gt;An iterative, highly scalable graph processing system based on Pregel and built over MapReduce&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hama/&quot;&gt;Hama&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A general purpose BSP (Bulk Synchronous Parallel) processing engine inspired by Pregel and DistBelief that runs over Mesos or YARN.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-spark/graphx/&quot;&gt;Spark/GraphX&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Spark library for processing graphs and running graph algorithms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-flink/&quot;&gt;Flink&lt;/a&gt; Gelly&lt;/td&gt; &lt;td&gt;Graph processing API and library on top of Apache Flink&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gaffer&lt;/td&gt; &lt;td&gt;Open source project for running analytics over very large graphs in HDFS, Accumulo or HBase - &lt;a href=&quot;https://github.com/gchq/Gaffer&quot;&gt;https://github.com/gchq/Gaffer&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;apache-tinkerpop&quot;&gt;Apache TinkerPop&lt;/h2&gt; &lt;p&gt;Apache TinkerPop provides support for running analytics over graphs from Gremlin using an external query engine (or GraphComputer) - see &lt;a href=&quot;http://tinkerpop.apache.org/docs/3.3.0/reference/#graphcomputer&quot;&gt;http://tinkerpop.apache.org/docs/3.3.0/reference/#graphcomputer&lt;/a&gt; for further information. TinkerPop includes GraphComputer adapters for Spark and Giraph out of the box, with the analytics generally running on an external cluster reading the data on job startup from the source graph database via TinkerPop.&lt;/p&gt; &lt;p&gt;Not all graph databases that support TinkerPop support the execution of graph analytics - those that do are listed as OLAP databases at &lt;a href=&quot;http://tinkerpop.apache.org/#graph-systems&quot;&gt;http://tinkerpop.apache.org/#graph-systems&lt;/a&gt;.&lt;/p&gt; &lt;h2 id=&quot;analytical-databases&quot;&gt;Analytical Databases&lt;/h2&gt; &lt;p&gt;See also our &lt;a href=&quot;/tech-categories/analytical-databases/&quot;&gt;Analytical Databases&lt;/a&gt; page for databases that support graph analytics, including specialist graph analytical databases&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Data Storage Formats</title><link>https://ondataengineering.net/tech-categories/data-storage-formats/</link><description> &lt;p&gt;Our list of and information on data storage formats, including Avro, Parquet, ORCCFile, Carbondata and alternatives to these.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Category Definition&lt;/h2&gt; &lt;!-- Tech Vendor metadata --&gt; &lt;p&gt;Libraries that support the storage of data on disk for data storage, real-time or batch analytics. Popularised by the use of distributed file systems in analytical platforms, common features include support for schema evolution (the ability to make changes to the schema but still read all historical data), support for both row and columnar data layouts (supporting efficient batch processing and analytical workloads respectively), complex record formats including nested objects and arrays, indexing to support random data access, and support for efficient inserts, updates and deletes as well as ACID transactions. &lt;!--more--&gt;&lt;/p&gt; &lt;h2 id=&quot;further-information&quot;&gt;Further Information&lt;/h2&gt; &lt;p&gt;A good introduction from Silicon Valley Data Science to text, SequenceFile, Avro, Parquet and ORC, along with some benchmarks - &lt;a href=&quot;http://www.svds.com/dataformats/&quot;&gt;http://www.svds.com/dataformats/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Comparison of JSON, Avro, ORC and Parquet, including benchmarks, from August 2016 - &lt;a href=&quot;https://www.slideshare.net/HadoopSummit/file-format-benchmark-avro-json-orc-parquet-65740483&quot;&gt;https://www.slideshare.net/HadoopSummit/file-format-benchmark-avro-json-orc-parquet-65740483&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Comparison of Parquet, Avro, HBase and Kudu, from January 2017 - &lt;a href=&quot;https://db-blog.web.cern.ch/blog/zbigniew-baranowski/2017-01-performance-comparison-different-file-formats-and-storage-engines&quot;&gt;https://db-blog.web.cern.ch/blog/zbigniew-baranowski/2017-01-performance-comparison-different-file-formats-and-storage-engines&lt;/a&gt;&lt;/p&gt; &lt;h2 id=&quot;text-based-formats&quot;&gt;Text Based Formats&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Plain Text&lt;/td&gt; &lt;td&gt;Often formatted as either delimited (e.g. comma separated values (CSV) or pipe separated values (PSV)) or fixed width fields&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;XML&lt;/td&gt; &lt;td&gt;Supports the use of external schema definitions, but format can be verbose and serialisation/deserialisation performance often poor&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;JSON (Javascript Object Notation)&lt;/td&gt; &lt;td&gt;More efficient and terse than XML, but still suffers poor serialisation/deserialise performance&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;row-based-formats&quot;&gt;Row Based Formats&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;SequenceFile&lt;/td&gt; &lt;td&gt;Part of the Hadoop project, a container of binary key/value pairs used to store multiple smaller files inside a single large file - &lt;a href=&quot;https://wiki.apache.org/hadoop/SequenceFile&quot;&gt;https://wiki.apache.org/hadoop/SequenceFile&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-avro/&quot;&gt;Avro&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Binary serialisation format supporting schema evolution and complex record types&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;column-based-formats&quot;&gt;Column Based Formats&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;RCFile&lt;/td&gt; &lt;td&gt;Early implementation of a columnar record format as part of the Apache Hive project - &lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/RCFile&quot;&gt;https://cwiki.apache.org/confluence/display/Hive/RCFile&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-orc/&quot;&gt;ORCFile&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Evolution of RCFile, spun out into it’s own Apache project&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-parquet/&quot;&gt;Parquet&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Columnar format spun out from the Avro Trevni format&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-carbondata/&quot;&gt;CarbonData&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Columnar format created by Huawei to address a number of perceived shortcomings in existing formats&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;map-based-formats&quot;&gt;Map Based Formats&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;MapFile, SetFile, ArrayFile, BloomMapFile&lt;/td&gt; &lt;td&gt;Part of the Hadoop project and built on SequenceFile, with MapFile being the original file format used by HBase - &lt;a href=&quot;http://blog.cloudera.com/blog/2011/01/hadoop-io-sequence-map-set-array-bloommap-files/&quot;&gt;http://blog.cloudera.com/blog/2011/01/hadoop-io-sequence-map-set-array-bloommap-files/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;TFile&lt;/td&gt; &lt;td&gt;Container of key-value pairs, part of the Hadoop project - &lt;a href=&quot;http://hadoop.apache.org/docs/r2.6.1/api/org/apache/hadoop/io/file/tfile/TFile.html&quot;&gt;http://hadoop.apache.org/docs/r2.6.1/api/org/apache/hadoop/io/file/tfile/TFile.html&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;HFile&lt;/td&gt; &lt;td&gt;HBase storage format based on TFile and the SSTable format from the Google BigTable paper - &lt;a href=&quot;http://hbase.apache.org/book.html#_hfile_format_2&quot;&gt;http://hbase.apache.org/book.html#_hfile_format_2&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;And of course, any &lt;a href=&quot;/tech-categories/object-stores/&quot;&gt;object store&lt;/a&gt;, NoSQL key value store or &lt;a href=&quot;/tech-categories/nosql-wide-column-stores/&quot;&gt;NoSQL wide column store&lt;/a&gt; can be used to store data by key.&lt;/p&gt; &lt;h2 id=&quot;in-memory-formats&quot;&gt;In Memory Formats&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-arrow/&quot;&gt;Arrow&lt;/a&gt;&lt;/td&gt; &lt;td&gt;In memory columnar data format supporting high performance data exchange and fast analytical access&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Mnemonic&lt;/td&gt; &lt;td&gt;Hybrid memory / storage object model framework with Spark and MapReduce support - &lt;a href=&quot;http://mnemonic.apache.org/&quot;&gt;http://mnemonic.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;data-storage-services&quot;&gt;Data Storage Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-kudu/&quot;&gt;Kudu&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Cluster based columnar storage engine that supports updates and deletes by primary key&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Query Engines</title><link>https://ondataengineering.net/tech-categories/query-engines/</link><description> &lt;p&gt;Our list of and information on commercial, open source and cloud based query engines, including Hive, Impala, Drill, Pig, Kognitio, Jethro, Amazon Athena, Azure Data Lake Analytics and alternatives to these.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Category Definition&lt;/h2&gt; &lt;!-- Tech Vendor metadata --&gt; &lt;p&gt;Engines that allow analytical queries expressed in a high level language (often SQL) to be run over one or more underlying data stores or databases, often including HDFS (and often using table definitions from the Hive Metastore), but with support for other Hadoop, relational and NoSQL databases commonly supported. Will support exploitation of raw data (focusing on schema on read and the ability to query across sources) and/or exploitation of data prepared for analytics (focusing on competing with analytical databases). Many technologies started as batch query engines (with high query startup costs and limited support for concurrent queries), but most can now be considered interactive with support for multiple concurrent low latency queries. Given the propensity for querying over Hadoop data using SQL, many of these technologies are often referred to as SQL-on-Hadoop technologies. &lt;!--more--&gt;&lt;/p&gt; &lt;h2 id=&quot;further-information&quot;&gt;Further Information&lt;/h2&gt; &lt;p&gt;See our &lt;a href=&quot;/tech-categories/data-storage-formats/&quot;&gt;Data Storage Formats&lt;/a&gt; page for information on file formats to use with these query engines.&lt;/p&gt; &lt;p&gt;See our &lt;a href=&quot;/tech-categories/analytical-databases/&quot;&gt;Analytical Databases&lt;/a&gt; page for information on technologies that provide analytical capabilities as a self contained storage and query engine stack.&lt;/p&gt; &lt;p&gt;The following analyst material covers a number of technologies in this category:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.bloorresearch.com/research/sql-engines-hadoop/&quot;&gt;Bloor SQL Engines on Hadoop&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;open-source-technologies&quot;&gt;Open Source Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Apache Hive&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Supports the execution of SQL queries over data in HDFS using MapReduce, Spark or Tez based on tables defined in the Hive Metastore&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-impala/&quot;&gt;Apache Impala&lt;/a&gt;&lt;/td&gt; &lt;td&gt;An MPP query engine that supports the execution of SQL queries over in HDFS, HBase, Kudu and S3 based on tables defined in the Hive Metastore&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Presto&lt;/td&gt; &lt;td&gt;Distributed SQL query engine over data in HDFS, NoSQL and relational databases and Kafka, originally created and open sourced by Facebook - &lt;a href=&quot;https://prestodb.io/&quot;&gt;https://prestodb.io/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-drill/&quot;&gt;Apache Drill&lt;/a&gt;&lt;/td&gt; &lt;td&gt;An MPP query engine that supports queries over one or more underlying databases or datasets without first defining a schema and with the ability to join data from multiple data stores together.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Apache Lens&lt;/td&gt; &lt;td&gt;Provides a cube based federated view over a range of data stores including HDFS, HBase, relational databases, S3 and Redshift - &lt;a href=&quot;http://lens.apache.org/&quot;&gt;http://lens.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-spark/spark-sql/&quot;&gt;Apache Spark SQL&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Hive compatible SQL query engine that use Spark to execute queries over any Spark supported data source&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-pig/&quot;&gt;Apache Pig&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Technology for running analytical and data processing jobs written in Pig Latin against data in Hadoop using MapReduce, Tez and Spark&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Apache MRQL (Incubating)&lt;/td&gt; &lt;td&gt;Supports the execution of MRQL queries over data in Hadoop using MapReduce, Hama, Spark or Flink - &lt;a href=&quot;http://mrql.apache.org/&quot;&gt;http://mrql.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-kylin/&quot;&gt;Kylin&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Supports the querying of Hive tables as OLAP cubes&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;commercial-technologies&quot;&gt;Commercial Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Kognitio&lt;/td&gt; &lt;td&gt;In memory database engine that can run as a YARN application on Hadoop over data in HDFS (as a free offering) or as a standalone cluster over data in HDFS, the cloud and other databases (as a commercial offering with a free trial) - &lt;a href=&quot;https://kognitio.com/&quot;&gt;https://kognitio.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Jethro&lt;/td&gt; &lt;td&gt;SQL query engine over HDFS and S3 that supports indexing, auto generation of cubes and results caching - &lt;a href=&quot;https://jethro.io/&quot;&gt;https://jethro.io/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;AtScale&lt;/td&gt; &lt;td&gt;Cube based semantic layer with query optimisation, virtual cube caching and row level security over Hadoop, RedShift and SQL data sources - &lt;a href=&quot;https://atscale.com/&quot;&gt;https://atscale.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;IBM Big SQL&lt;/td&gt; &lt;td&gt;SQL engine that runs on Hadoop over Hive tables, but that can also federate into RDMS and NoSQL databases and object stores - &lt;a href=&quot;https://www.ibm.com/us-en/marketplace/big-sql&quot;&gt;https://www.ibm.com/us-en/marketplace/big-sql&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Oracle Big Data SQL&lt;/td&gt; &lt;td&gt;Allows federated queries from an Oracle databases over Hadoop and NoSQL databases, with push down of logic and support for Oracle security - &lt;a href=&quot;https://www.oracle.com/database/big-data-sql/index.html&quot;&gt;https://www.oracle.com/database/big-data-sql/index.html&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Arcadia Data&lt;/td&gt; &lt;td&gt;Analytics engine that runs over Hadoop, with integrated drag and drop visual analytics and dashboards and a free tier (Arcadia Instant) - &lt;a href=&quot;https://www.arcadiadata.com/product/&quot;&gt;https://www.arcadiadata.com/product/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Kyvos Insights&lt;/td&gt; &lt;td&gt;OLAP cubes on Hadoop - &lt;a href=&quot;http://www.kyvosinsights.com/olap-cubes-on-hadoop/&quot;&gt;http://www.kyvosinsights.com/olap-cubes-on-hadoop/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;cloud-technologies&quot;&gt;Cloud Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Amazon Athena&lt;/td&gt; &lt;td&gt;SQL query service over data in Amazon S3 - &lt;a href=&quot;https://aws.amazon.com/athena/&quot;&gt;https://aws.amazon.com/athena/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Data Lake Analytics&lt;/td&gt; &lt;td&gt;Massively parallel analytics job service, with support for U-SQL, R, Python, and .NET - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/data-lake-analytics/&quot;&gt;https://azure.microsoft.com/en-us/services/data-lake-analytics/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Analytical Databases</title><link>https://ondataengineering.net/tech-categories/analytical-databases/</link><description> &lt;p&gt;Our list of and information on commercial, open source and cloud based analytical databases, including Teradata, Exadata, Redhift and alternatives to these.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Category Definition&lt;/h2&gt; &lt;!-- Tech Vendor metadata --&gt; &lt;p&gt;Full stack databases (supporting both storage and query of data) that focus on analytical or OLAP use cases that generally involve large scanning or aggregation operations. Typically support distributed parallel execution of queries (and are therefore commonly referred to as MPP databases) with columnar compression, and often support a range of analytics beyond SQL queries, for example cube based MDX queries, machine learning, graph or geographical analytics. Some technologies also support a level of query federation using external tables (for example over data in Hadoop). Some technologies run over Hadoop (exploiting HDFS and YARN), but will either use their own proprietary data format or will be positioned as a self contained analytical database. &lt;!--more--&gt;&lt;/p&gt; &lt;h2 id=&quot;further-information&quot;&gt;Further Information&lt;/h2&gt; &lt;p&gt;The following analyst material covers a number of technologies in this category::&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.google.co.uk/search?q=gartner+&amp;quot;Data+Management+Solutions+for+Analytics&amp;quot;&quot;&gt;Gartner Magic Quadrant &amp;amp; Critical Capabilities for Data Management Solutions for Analytics&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.google.co.uk/search?q=Forrester+&amp;quot;Big+Data+Warehouse&amp;quot;&quot;&gt;Forrester Wave: Big Data Warehouse&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;See our &lt;a href=&quot;/tech-categories/query-engines/&quot;&gt;Query Engines&lt;/a&gt; page for details of technologies that support similar capabilities but over external data (e.g. data in HDFS or source databases).&lt;/p&gt; &lt;h2 id=&quot;commercial-technologies&quot;&gt;Commercial Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;IBM Db2 Warehouse (formerly dashDB for Analytics)&lt;/td&gt; &lt;td&gt;IBM Db2 and BLU (in memory) for Docker container supported infrastructure (also available as an appliance and cloud service - see below) - &lt;a href=&quot;https://www.ibm.com/aw-en/marketplace/db2-warehouse&quot;&gt;https://www.ibm.com/aw-en/marketplace/db2-warehouse&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Teradata Database&lt;/td&gt; &lt;td&gt;MPP database with support for a range of data warehouse and analytics functions; deployable on private or public cloud (also available as an appliance and a cloud service - see below) - &lt;a href=&quot;https://www.teradata.com/Products/Database&quot;&gt;https://www.teradata.com/Products/Database&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Teradata Analytics Platform&lt;/td&gt; &lt;td&gt;Analytics platform that supports graph, text and IoT analysis plus machine learning (also available as an appliance and a cloud service - see below) - &lt;a href=&quot;https://www.teradata.co.uk/Products/Analytics-Platform&quot;&gt;https://www.teradata.co.uk/Products/Analytics-Platform&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Vertica&lt;/td&gt; &lt;td&gt;MPP columnar database with support for a range of analytical functions including machine learning; deployable on commodity infrastructure or public/private cloud - &lt;a href=&quot;https://www.vertica.com/overview/&quot;&gt;https://www.vertica.com/overview/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Actian Vector&lt;/td&gt; &lt;td&gt;MPP columnar database with support for vectorized execution, small incremental inserts and min-max indices, with a free community edition for databases under 1 Tb - &lt;a href=&quot;https://www.actian.com/analytic-database/vector-smp-analytic-database/&quot;&gt;https://www.actian.com/analytic-database/vector-smp-analytic-database/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;InfoBrightDB&lt;/td&gt; &lt;td&gt;Columnar database, now sold by Ignite Technologies - &lt;a href=&quot;http://www.ignitetech.com/solutions/information-technology/infobrightdb&quot;&gt;http://www.ignitetech.com/solutions/information-technology/infobrightdb&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;SQream&lt;/td&gt; &lt;td&gt;Columnar GPU accelerated analytical database, available on premise or in the cloud - &lt;a href=&quot;https://sqream.com/&quot;&gt;https://sqream.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;open-source-technologies&quot;&gt;Open Source Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/greenplum/&quot;&gt;Greenplum&lt;/a&gt;&lt;/td&gt; &lt;td&gt;MPP database based on PostgreSQL, with support for multiple storage models and analytical capabilities (including graph); open sourced in October 2015&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;MariaDB ColumnStore&lt;/td&gt; &lt;td&gt;Columnar storage for MariaDB (the open source fork of MySQL) based on a fork of InfiniDB - &lt;a href=&quot;https://mariadb.com/products/technology/columnstore&quot;&gt;https://mariadb.com/products/technology/columnstore&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;MariaDB AX&lt;/td&gt; &lt;td&gt;Data warehousing solution based on MariaDB ColumnStore, with commercial support available from MariaDB - &lt;a href=&quot;https://mariadb.com/products/solutions/olap-database-ax&quot;&gt;https://mariadb.com/products/solutions/olap-database-ax&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;MonetDB&lt;/td&gt; &lt;td&gt;Open source columnar database - &lt;a href=&quot;https://www.monetdb.org/&quot;&gt;https://www.monetdb.org/&lt;/a&gt; ; &lt;a href=&quot;https://www.monetdbsolutions.com/&quot;&gt;https://www.monetdbsolutions.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;InifiDB&lt;/td&gt; &lt;td&gt;Open source columnar database, inactive since March 2015 - &lt;a href=&quot;https://github.com/infinidb/infinidb&quot;&gt;https://github.com/infinidb/infinidb&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Pinot&lt;/td&gt; &lt;td&gt;Open source realtime distributed OLAP datastore from LinkedIn - &lt;a href=&quot;https://github.com/linkedin/pinot&quot;&gt;https://github.com/linkedin/pinot&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;hadoop-based-open-source-technologies&quot;&gt;Hadoop Based Open Source Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Apache Hive&lt;/a&gt;&lt;/td&gt; &lt;td&gt;An analytical database when used with LLAP, ORCFile and Tez; runs over Hadoop&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-impala/&quot;&gt;Apache Impala&lt;/a&gt;&lt;/td&gt; &lt;td&gt;An analytical database when used with Kudu or Parquet over HDFS; runs over Hadoop&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hawq/&quot;&gt;Apache HAWQ&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A port of the Greenplum MPP database (which itself is based on PostgreSQL) to run over Hadoop&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-tajo/&quot;&gt;Apache Tajo&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Distributed analytical database engine that runs over Hadoop&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Presto&lt;/td&gt; &lt;td&gt;An analytical database when used over Hive/Hadoop , originally created and open sourced by Facebook - &lt;a href=&quot;https://prestodb.io/&quot;&gt;https://prestodb.io/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/druid&quot;&gt;Druid&lt;/a&gt;&lt;/td&gt; &lt;td&gt;OLAP database supporting real time aggregations of streaming data using HDFS/S3 as backing storage&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;hadoop-based-commercial-technologies&quot;&gt;Hadoop Based Commercial Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Vertica on Hadoop&lt;/td&gt; &lt;td&gt;Vertica running on Hadoop - &lt;a href=&quot;https://www.vertica.com/product/vertica-for-sql-on-hadoop/&quot;&gt;https://www.vertica.com/product/vertica-for-sql-on-hadoop/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Actian Vector H&lt;/td&gt; &lt;td&gt;Version of Action Vector that runs as a native YARN app but requires data to be loaded into its proprietary data format - &lt;a href=&quot;https://www.actian.com/analytic-database/vectorh-sql-hadoop/&quot;&gt;https://www.actian.com/analytic-database/vectorh-sql-hadoop/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;appliances&quot;&gt;Appliances&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Oracle Exadata&lt;/td&gt; &lt;td&gt;An appliance consisting of an Oracle RAC cluster combined with a set of storage nodes via high bandwidth interconnect, with support for hybrid columnar compression - &lt;a href=&quot;https://www.oracle.com/engineered-systems/exadata/index.html&quot;&gt;https://www.oracle.com/engineered-systems/exadata/index.html&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Microsoft Analytics Platform System&lt;/td&gt; &lt;td&gt;An appliance built around SQL Server Parallel Data Warehouse and PolyBase - &lt;a href=&quot;https://www.microsoft.com/en-us/sql-server/analytics-platform-system&quot;&gt;https://www.microsoft.com/en-us/sql-server/analytics-platform-system&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;IBM Integrated Analytics System&lt;/td&gt; &lt;td&gt;Appliance built around Db2 Warehouse and BLU (in memory) acceleration with support for Spark - &lt;a href=&quot;https://www.ibm.com/us-en/marketplace/integrated-analytics-system&quot;&gt;https://www.ibm.com/us-en/marketplace/integrated-analytics-system&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;IBM PureData System for Analytics (formally Netezza)&lt;/td&gt; &lt;td&gt;Appliance utilising FPGA chips to run elements of queries in hardware, with support for a range of languages including R - &lt;a href=&quot;https://www.ibm.com/us-en/marketplace/puredata-system-for-analytics#product-header-top&quot;&gt;https://www.ibm.com/us-en/marketplace/puredata-system-for-analytics#product-header-top&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Teradata IntelliFlex&lt;/td&gt; &lt;td&gt;Teradata capabilities as an appliance - &lt;a href=&quot;https://www.teradata.com/Products/IntelliFlex&quot;&gt;https://www.teradata.com/Products/IntelliFlex&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Teradata IntelliBase&lt;/td&gt; &lt;td&gt;A mixture of Teradata and Hadoop nodes in a single appliance - &lt;a href=&quot;https://www.teradata.com/Products/IntelliBase&quot;&gt;https://www.teradata.com/Products/IntelliBase&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Pivotal EMC Data Computing Appliance (DCA)&lt;/td&gt; &lt;td&gt;Greenplum appliance - &lt;a href=&quot;https://pivotal.io/emc-dca&quot;&gt;https://pivotal.io/emc-dca&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;cloud-services&quot;&gt;Cloud Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Amazon Redshift&lt;/td&gt; &lt;td&gt;A MPP analytical database, with support for columnar storage and the ability to query data in Amazon S3 as external tables (Redshift Spectrum) - &lt;a href=&quot;https://aws.amazon.com/redshift/&quot;&gt;https://aws.amazon.com/redshift/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google BigQuery&lt;/td&gt; &lt;td&gt;Analytical SQL database service, with cost based on storage and query execution - &lt;a href=&quot;https://cloud.google.com/bigquery/&quot;&gt;https://cloud.google.com/bigquery/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure SQL Data Warehouse&lt;/td&gt; &lt;td&gt;Scalable analytical database, with support for Azure Data Lake Store external tables - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/sql-data-warehouse/&quot;&gt;https://azure.microsoft.com/en-us/services/sql-data-warehouse/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/cloudera-altus/data-warehouse/&quot;&gt;Cloudera Altus Data Warehouse&lt;/a&gt; - &lt;a href=&quot;/technologies/apache-impala/&quot;&gt;Apache Impala&lt;/a&gt; as a cloud managed service over AWS or Azure&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Oracle Exadata Cloud&lt;/td&gt; &lt;td&gt;Oracle Exadata as a managed cloud service (including as an Oracle managed on premises offering) - &lt;a href=&quot;https://cloud.oracle.com/database&quot;&gt;https://cloud.oracle.com/database&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;IBM Db2 Warehouse (formerly dashDB for Analytics)&lt;/td&gt; &lt;td&gt;IBM Db2 and BLU (in memory) acceleration as a cloud service - &lt;a href=&quot;https://www.ibm.com/us-en/marketplace/db2-warehouse-on-cloud&quot;&gt;https://www.ibm.com/us-en/marketplace/db2-warehouse-on-cloud&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Teradata IntilliCloud&lt;/td&gt; &lt;td&gt;Teradata Database, Hadoop and Aster as a service - &lt;a href=&quot;https://www.teradata.com/Products/Cloud/IntelliCloud&quot;&gt;https://www.teradata.com/Products/Cloud/IntelliCloud&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Snowflake&lt;/td&gt; &lt;td&gt;Data Warehouse for the cloud, with separated compute and storage, columnar storage, vectorized execution, adaptive optimisation (no indexes, keys or tuning required) and support for semi-structured (JSON, Avro and XML) data - &lt;a href=&quot;https://www.snowflake.net/&quot;&gt;https://www.snowflake.net/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;analytical-in-memory-technologies&quot;&gt;Analytical In Memory Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;MemSQL&lt;/td&gt; &lt;td&gt;Distributed in memory relational database, with wire compatibility with MySQL and support for row and columnar storage, and a free community edition - &lt;a href=&quot;http://www.memsql.com/&quot;&gt;http://www.memsql.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;SAP HANA&lt;/td&gt; &lt;td&gt;In memory relational DBMS primarily focused on accelerating SAP applications - &lt;a href=&quot;https://www.sap.com/products/hana.html&quot;&gt;https://www.sap.com/products/hana.html&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;EXASOL&lt;/td&gt; &lt;td&gt;In memory MPP database with columnar compression and SQL support - &lt;a href=&quot;http://www.exasol.com/&quot;&gt;http://www.exasol.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;MapD&lt;/td&gt; &lt;td&gt;In memory, column store, SQL relational database that runs on GPUs - &lt;a href=&quot;https://www.mapd.com/&quot;&gt;https://www.mapd.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Kinetica&lt;/td&gt; &lt;td&gt;Distributed in memory relational database that runs on GPUs - &lt;a href=&quot;https://www.kinetica.com&quot;&gt;https://www.kinetica.com&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;analytical-graph-databases&quot;&gt;Analytical Graph Databases&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;DataStax Enterprise&lt;/td&gt; &lt;td&gt;Commercial product built on Apache Cassandra with the addition of graph and search capabilities - &lt;a href=&quot;https://www.datastax.com/&quot;&gt;https://www.datastax.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;TigerGraph&lt;/td&gt; &lt;td&gt;Commercial hybrid OLTP/OLAP graph database that claims order of magnitude performance and scalability improvements over it’s competitors; previously known as GraphSQL - &lt;a href=&quot;http://www.tigergraph.com&quot;&gt;http://www.tigergraph.com&lt;/a&gt;, &lt;a href=&quot;http://www.zdnet.com/article/tigergraph-a-graph-database-born-to-roar/&quot;&gt;http://www.zdnet.com/article/tigergraph-a-graph-database-born-to-roar/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;AnzoGraph&lt;/td&gt; &lt;td&gt;Massively parallel distributed graph database built for analytics - &lt;a href=&quot;https://www.cambridgesemantics.com/product/anzograph/&quot;&gt;https://www.cambridgesemantics.com/product/anzograph/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GraphBase&lt;/td&gt; &lt;td&gt;Commercial graph database designed for use in AI applications - &lt;a href=&quot;https://graphbase.ai/&quot;&gt;https://graphbase.ai/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;JanusGraph&lt;/td&gt; &lt;td&gt;Open source distributed graph database that runs over a number of storage backends (including Cassandra, HBase and BigTable), with TinkerPop support including support for graph analytics; previously known as Titan - &lt;a href=&quot;http://janusgraph.org/&quot;&gt;http://janusgraph.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;TinkerGraph&lt;/td&gt; &lt;td&gt;In memory graph databases that’s part of TinkerPop as a reference implementation - &lt;a href=&quot;http://tinkerpop.apache.org&quot;&gt;http://tinkerpop.apache.org&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GRAKN.AI&lt;/td&gt; &lt;td&gt;Open Source graph database designed for AI use cases that also supports graph analytics - &lt;a href=&quot;https://grakn.ai&quot;&gt;https://grakn.ai&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Complexible Stardog&lt;/td&gt; &lt;td&gt;RDF database that also support property graphs and data virtualisation, with a community edition available - &lt;a href=&quot;http://www.stardog.com/&quot;&gt;http://www.stardog.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Data Ingestion</title><link>https://ondataengineering.net/tech-categories/data-ingestion/</link><description> &lt;p&gt;Our list of and information on commercial, open source and cloud based data ingestion tools, including NiFi, StreamSets, Gobblin, Logstash, Flume, FluentD, Sqoop, GoldenGate and alternatives to these.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Category Definition&lt;/h2&gt; &lt;!-- Tech Vendor metadata --&gt; &lt;p&gt;Specialist tools designed to acquire and ingest data into an analytical platform ready for analysis or for further transformation to support analysis. And although more general purpose data integration/transformation tools can fulfil this function, specialist data ingestion tools provide capabilities designed to make this faster and more reliable. Key features include support for remote agents to acquire and forward data, GUIs for configuring ingestion pipelines, support for data quality checks to monitor and/or reject incoming data, and basic file and record level transformations on top of the standard functionality to acquire data from a wide range of sources out of the box. &lt;!--more--&gt;&lt;/p&gt; &lt;h2 id=&quot;further-information&quot;&gt;Further Information&lt;/h2&gt; &lt;p&gt;The following analyst material covers a number of technologies in this category:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.google.co.uk/search?q=Gartner+&amp;quot;Data+Integration+Tools&amp;quot;&quot;&gt;Gartner Magic Quadrant &amp;amp; Critical Capabilities for Data Integration Tools&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;general-purpose-ingestion-tools&quot;&gt;General Purpose Ingestion Tools&lt;/h2&gt; &lt;p&gt;These tools support both batch and streaming ingestion from a wide range of data sources:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-nifi/&quot;&gt;Apache Nifi&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Open source, with commercial support available from Hortonworks through &lt;a href=&quot;/technologies/hortonworks-data-flow/&quot;&gt;Hortonworks Data Flow&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/streamsets-data-collector/&quot;&gt;StreamSets Data Collector&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Open source, with commercial support available from StreamSets&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-gobblin/&quot;&gt;Apache Gobblin (Incubating)&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Open Source Java framework for managing big data ingestion, including replication, organisation and lifecycle management&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Skool&lt;/td&gt; &lt;td&gt;Open source tool from BT for bring database and file data into Hadoop through generation of Sqoop, Hive, Pig and Oozie code from configuration; open sourced in September 2016 but has seen limited development since - &lt;a href=&quot;https://github.com/BT-OpenSource/Skool&quot;&gt;https://github.com/BT-OpenSource/Skool&lt;/a&gt;; &lt;a href=&quot;https://blog.cloudera.com/blog/2016/09/skool-an-open-source-data-integration-tool-for-apache-hadoop-from-british-telecom/&quot;&gt;https://blog.cloudera.com/blog/2016/09/skool-an-open-source-data-integration-tool-for-apache-hadoop-from-british-telecom/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;streaming-ingestion-tools&quot;&gt;Streaming Ingestion Tools&lt;/h2&gt; &lt;p&gt;Tools specialising in the ingestion of log files or events, with support for distributed collection and forwarding of data, sometimes called log shipping tools. There’s a write up of some of the tools available from Sematext: &lt;a href=&quot;https://sematext.com/blog/logstash-alternatives/&quot;&gt;https://sematext.com/blog/logstash-alternatives/&lt;/a&gt;&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Logstash&lt;/td&gt; &lt;td&gt;Heavily integrated with ElasticSearch but also supports a number of other targets; open source with commercial support from Elastic as part of their ELK stack - &lt;a href=&quot;https://www.elastic.co/products/logstash&quot;&gt;https://www.elastic.co/products/logstash&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Beats&lt;/td&gt; &lt;td&gt;Lightweight technology written in Go to forward events to Logstash; open source with commercial support from Elastic as part of their ELK stack - &lt;a href=&quot;https://www.elastic.co/products/beats&quot;&gt;https://www.elastic.co/products/beats&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-flume/&quot;&gt;Apache Flume&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Runs on Hadoop and supports the continuous ingestion of data using a set of independent agents connected together into pipelines&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Fluentd&lt;/td&gt; &lt;td&gt;Ruby based tool, part of the Cloud Native Computing Foundation; open source, with commercial support available from TreasureData - &lt;a href=&quot;http://www.fluentd.org/&quot;&gt;http://www.fluentd.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Logagent-js&lt;/td&gt; &lt;td&gt;JavaScript based tool; open source, with commercial support available from Sematext - &lt;a href=&quot;https://github.com/sematext/logagent-js&quot;&gt;https://github.com/sematext/logagent-js&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;rsyslog&lt;/td&gt; &lt;td&gt;Focused on log processing, with lineage back to UNIX syslogd; written in C; open source, with commercial support available from Adiscon - &lt;a href=&quot;http://www.rsyslog.com/&quot;&gt;http://www.rsyslog.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Syslog-ng&lt;/td&gt; &lt;td&gt;Focused on log processing, with lineage back to UNIX syslogd; written in C; open source, with commercial support available from BalaBit - &lt;a href=&quot;https://syslog-ng.org/&quot;&gt;https://syslog-ng.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gollum&lt;/td&gt; &lt;td&gt;Open source project from Trivago; written in Go, quiet, but with new releases still being produced - &lt;a href=&quot;https://github.com/trivago/gollum/&quot;&gt;https://github.com/trivago/gollum/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LogZoom&lt;/td&gt; &lt;td&gt;Open source tool from PacketZoom for processing data from processing data from Beats, written in Go, however inactive since November 2016 - &lt;a href=&quot;https://github.com/packetzoom/logzoom&quot;&gt;https://github.com/packetzoom/logzoom&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Heka&lt;/td&gt; &lt;td&gt;Open source tool from Mozilla, however inactive since August 2016 - &lt;a href=&quot;https://github.com/mozilla-services/heka&quot;&gt;https://github.com/mozilla-services/heka&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Suro&lt;/td&gt; &lt;td&gt;Open source tool from Netflix, however inactive since December 2015 - &lt;a href=&quot;https://github.com/Netflix/suro&quot;&gt;https://github.com/Netflix/suro&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Scribe&lt;/td&gt; &lt;td&gt;Open source tool from Facebook, however inactive since May 2014 - &lt;a href=&quot;https://github.com/facebookarchive/scribe&quot;&gt;https://github.com/facebookarchive/scribe&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;database-unload-tools&quot;&gt;Database Unload Tools&lt;/h2&gt; &lt;p&gt;The following are specialist tools for unloading data form databases. Most data transformation tools and processing tools will also be able to unload data from databases, and are therefore an alternative to using a specialist tool:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-sqoop/&quot;&gt;Apache Sqoop&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Specialist technology for moving bulk data between Hadoop and structured (relational) databases.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;database-change-capture-tools&quot;&gt;Database Change Capture Tools&lt;/h2&gt; &lt;p&gt;The following technologies support the continuous capture and ingestion of record change events from databases, and are sometimes known as change data capture tools:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Oracle GoldenGate for Big Data 12c&lt;/td&gt; &lt;td&gt;Commercial product for the continuous replication of data from a wide range of relational databases into a wide range of “Big Data” targets - &lt;a href=&quot;https://www.oracle.com/middleware/data-integration/goldengate/big-data/index.html&quot;&gt;https://www.oracle.com/middleware/data-integration/goldengate/big-data/index.html&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;IBM Infosphere Data Replication&lt;/td&gt; &lt;td&gt;Commercial product for the continuous replication from relational databases, including IBM systems on mainframes to a range of systems including kafka and Hadoop - &lt;a href=&quot;https://www.ibm.com/us-en/marketplace/infosphere-data-replication&quot;&gt;https://www.ibm.com/us-en/marketplace/infosphere-data-replication&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;SyncSort DMX Change Data Capture&lt;/td&gt; &lt;td&gt;Commercial tool for continually capturing data from mainframe databases - &lt;a href=&quot;https://www.syncsort.com/en/Products/BigData/DMX-Change-Data-Capture&quot;&gt;https://www.syncsort.com/en/Products/BigData/DMX-Change-Data-Capture&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Quest Shareplex&lt;/td&gt; &lt;td&gt;Commercial product for the continuous replication of data from Oracle or SQL Server to a range of targets including Kafka, Hadoop and flat files; previously known as Dell Shareplex, SharePlex for Oracle and Quest Data Connector for Oracle and Hadoop - &lt;a href=&quot;https://www.quest.com/products/shareplex/&quot;&gt;https://www.quest.com/products/shareplex/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Attunity Replicate&lt;/td&gt; &lt;td&gt;Commercial technology for the continuous replication of data between a wide variety of sources including Kafka, relational and analytical databases, mainframes, Hadoop and the cloud; with a free limited Express edition - &lt;a href=&quot;https://www.attunity.com/products/replicate/&quot;&gt;https://www.attunity.com/products/replicate/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Continuent Tungsten Replicator&lt;/td&gt; &lt;td&gt;Continuous replication of Oracle, MySQL and Amazon RDS databases to Hadoop, Vertica, RedShift and others, with an open source version available - &lt;a href=&quot;https://www.continuent.com/solutions/#bigdata&quot;&gt;https://www.continuent.com/solutions/#bigdata&lt;/a&gt;; &lt;a href=&quot;https://github.com/continuent/tungsten-replicator&quot;&gt;https://github.com/continuent/tungsten-replicator&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Dbvisit Replicate&lt;/td&gt; &lt;td&gt;Commercial product for the continuous replication of data from Oracle to a number of targets including Hadoop and Kafka - &lt;a href=&quot;http://www.dbvisit.com/products/dbvisit_replicate_real_time_oracle_database_replication/&quot;&gt;http://www.dbvisit.com/products/dbvisit_replicate_real_time_oracle_database_replication/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;SQData CDC&lt;/td&gt; &lt;td&gt;Commercial tool for continuous replication with a wide range of sources and targets - &lt;a href=&quot;https://www.sqdata.com/changed-data-capture/&quot;&gt;https://www.sqdata.com/changed-data-capture/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Spinal Tap&lt;/td&gt; &lt;td&gt;Open source Change Data Capture service from AirBnB (&lt;a href=&quot;https://medium.com/airbnb-engineering/capturing-data-evolution-in-a-service-oriented-architecture-72f7c643ee6f&quot;&gt;blog post&lt;/a&gt;) - &lt;a href=&quot;https://github.com/airbnb/SpinalTap&quot;&gt;https://github.com/airbnb/SpinalTap&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;streaming-data-store-ingestion&quot;&gt;Streaming Data Store Ingestion&lt;/h2&gt; &lt;p&gt;A number of &lt;a href=&quot;/tech-categories/streaming-data-stores/&quot;&gt;streaming data stores&lt;/a&gt; have integrated tools for the aquisition of data:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-kafka/kafka-connect/&quot;&gt;Kafka Connect&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Framework for building scalable and reliable integrations between Kafka and other technologies, including the ingestion of data, that’s part of the core Apache Kafka technology&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Debezium&lt;/td&gt; &lt;td&gt;Open Source tool for continuous replication from a number of databases based on Kafka and Kafka Connect - &lt;a href=&quot;http://debezium.io/&quot;&gt;http://debezium.io/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon Kinesis Streams&lt;/td&gt; &lt;td&gt;Includes an Amazon Kinesis Agent for capture and ingestion of data - &lt;a href=&quot;https://aws.amazon.com/kinesis/streams/&quot;&gt;https://aws.amazon.com/kinesis/streams/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;cloud-based-ingestion-tools&quot;&gt;Cloud Based Ingestion Tools&lt;/h2&gt; &lt;p&gt;The following are cloud based ingestion as a service tools, primarily for ingesting data into cloud based analytical platforms:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Azure Data Factory&lt;/td&gt; &lt;td&gt;Data ingestion as a service - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/data-factory/&quot;&gt;https://azure.microsoft.com/en-us/services/data-factory/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;AWS Data Pipelines&lt;/td&gt; &lt;td&gt;Data ingestion as a service - &lt;a href=&quot;https://aws.amazon.com/datapipeline/&quot;&gt;https://aws.amazon.com/datapipeline/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon Kinesis Firehose&lt;/td&gt; &lt;td&gt;Streaming data movement, with support for basic transformation including routing, splitting and batching - &lt;a href=&quot;https://aws.amazon.com/kinesis/firehose/&quot;&gt;https://aws.amazon.com/kinesis/firehose/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;other-tools&quot;&gt;Other Tools&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Apache Chukwa&lt;/td&gt; &lt;td&gt;Specialist technology for the ingestion of continuous data flows into an Hadoop cluster, and the subsequent management and analysis of the data; donated by Yahoo in 2010 but now largely abandoned - &lt;a href=&quot;https://chukwa.apache.org/&quot;&gt;https://chukwa.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Apache ManifoldCF&lt;/td&gt; &lt;td&gt;Framework for replicating data from content repositories to analytical search technologies - &lt;a href=&quot;http://manifoldcf.apache.org/&quot;&gt;http://manifoldcf.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hortonworks</title><link>https://ondataengineering.net/tech-vendors/hortonworks/</link><description> &lt;p&gt;Hortonworks is a commercial company focusing on products that support the exploitation of data both at rest and in motion. Their business model is to provide support and professional services for a range of Apache open source technologies which they package and distribute for free. They are therefore extreemly active in the Apache open source space, with committers on all the technologies they distribute, and with a history of donating projects to the Apache Foundation that they have either initiated or acquired. Hortonworks was formed in June 2011 by ex-Yahoo employees.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;!-- Technology metadata --&gt; &lt;h2 id=&quot;vendor-technologies&quot;&gt;Vendor Technologies&lt;/h2&gt; &lt;p&gt;Hortonworks have two primary offerings - &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt; (HDP), a fully open source distribution of Hadoop, and &lt;a href=&quot;/technologies/hortonworks-data-flow/&quot;&gt;Hortonworks Data Flow&lt;/a&gt; (HDF), a distribution of (primarily) Apache NiFi, Kafka and Storm for processing data in motion. The linked technology pages include further information of the options for deploying and managing these products. They also provide &lt;a href=&quot;/technologies/hortonworks-dataplane-service/&quot;&gt;Hortonworks DataPlane Service&lt;/a&gt;, a commercial product for managing and governing data, and Hortonworks Operational Services, a subscription service whereby Hortonworks engineers and support personnel manage your HDP or HDF instances on site or in the cloud.&lt;/p&gt; &lt;p&gt;Hortonworks also offer the Hortonworks Cybersecurity Package (HCP), a cyber security platform built on HDP and HDF that’s based on the Apache Metron open source project, and which is installable and manageable via Apache Ambari.&lt;/p&gt; &lt;p&gt;Hortonworks have strong partnerships with both Pivotal and IBM, with those companies reselling Hortonworks products (primarily HDP), and Hortonworks reselling their products - specifically Apache HAWQ / Pivotal HDB (as Hortonworks HDB, an addon to HDP), IBM Data Science Experience (DSX) (a public or private cloud based web based notebook for interactive data analytics), and IBM BigSQL (a SQL engine that runs on Hadoop over Hive tables).&lt;/p&gt; &lt;p&gt;Hortonworks have donated a number of projects to the Apache Foundation including &lt;a href=&quot;/technologies/apache-ambari&quot;&gt;Ambari&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-tez&quot;&gt;Tez&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-knox&quot;&gt;Knox&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-ranger&quot;&gt;Ranger&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-slider&quot;&gt;Slider&lt;/a&gt;. They also have a number of non-Apache open source tools including &lt;a href=&quot;/technologies/cloudbreak&quot;&gt;Cloudbreak&lt;/a&gt;, &lt;a href=&quot;/technologies/schema-registry/&quot;&gt;Schema Registry&lt;/a&gt; and &lt;a href=&quot;/technologies/streaming-analytics-manager&quot;&gt;Streaming Analytics Manager&lt;/a&gt;, all of which Hortonworks have suggested they will donate it to the Apache Foundation.&lt;/p&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/&quot;&gt;https://hortonworks.com/&lt;/a&gt; - homepage&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/&quot;&gt;https://hortonworks.com/blog/&lt;/a&gt; - Hortonworks blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Cloudera</title><link>https://ondataengineering.net/tech-vendors/cloudera/</link><description> &lt;p&gt;Cloudera is a commercial company focusing on offerings based around an Apache Hadoop distribution that's supplemented with a number of commercial components, distributed as a free express version (with cut down versions of some of the commercial components), and as an enterprise version with an annual subscription fee. They are extreemly active in the Apache open source space, with committers on all the technologies they distribute, and with a history of donating projects to the Apache Foundation that they have either initiated or acquired. Formed in 2008 by ex-employees from Google, Yahoo, Facebook and Oracle, with Doug Cutting, the original author of Hadoop, joining in 2009 as Chief Architect.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;!-- Technology metadata --&gt; &lt;h2 id=&quot;vendor-technologies&quot;&gt;Vendor Technologies&lt;/h2&gt; &lt;p&gt;Cloudera’s offerings are based around &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt;, their distribution of Apache Hadoop, which includes a number of commercial components including &lt;a href=&quot;/technologies/cloudera-manager/&quot;&gt;Cloudera Manager&lt;/a&gt; (for creating and managing clusters), &lt;a href=&quot;/technologies/cloudera-navigator/&quot;&gt;Cloudera Navigator&lt;/a&gt; (a data management and encryption solution) and &lt;a href=&quot;/technologies/cloudera-director/&quot;&gt;Cloudera Director&lt;/a&gt; (for installing CDH on cloud based platforms).&lt;/p&gt; &lt;p&gt;Cloudera also offer &lt;a href=&quot;/technologies/cloudera-data-science-workbench&quot;&gt;Cloudera Data Science Workbench&lt;/a&gt;, a docker powered analytics notebook, and &lt;a href=&quot;/technologies/cloudera-altus&quot;&gt;Cloudera Altus&lt;/a&gt;, a managed service for running a number of Hadoop workloads in the cloud.&lt;/p&gt; &lt;p&gt;The Cloudera technologies for managing metadata, data governance and security are collectively referred to as the Cloudera Shared Data Experience (SDX), as they enable multiple workloads to run over a shared set of data on a Cloudera Hadoop cluster. Cloudera also provide an SDX Cloud Reference Architecture (which describes how to implement a shared metadata, data governance and security layer when using transient Hadoop clusters in the cloud), and have baked this into &lt;a href=&quot;/technologies/cloudera-altus&quot;&gt;Cloudera Altus&lt;/a&gt; as Altus SDX.&lt;/p&gt; &lt;p&gt;Cloudera have donated a number of projects to the Apache Foundation, including &lt;a href=&quot;/technologies/apache-flume&quot;&gt;Flume&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-sqoop&quot;&gt;Sqoop&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-avro&quot;&gt;Avro&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-parquet&quot;&gt;Parquet&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-crunch&quot;&gt;Crunch&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-sentry&quot;&gt;Sentry&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-impala&quot;&gt;Impala&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-kudu&quot;&gt;Kudu&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-livy&quot;&gt;Livy&lt;/a&gt;, and have a number of non-Apache open source projects including &lt;a href=&quot;/technologies/recordservice&quot;&gt;RecordService&lt;/a&gt;, &lt;a href=&quot;/technologies/hue&quot;&gt;Hue&lt;/a&gt;, &lt;a href=&quot;/technologies/kite&quot;&gt;Kite&lt;/a&gt; and &lt;a href=&quot;/technologies/llama&quot;&gt;Llama&lt;/a&gt;.&lt;/p&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/&quot;&gt;https://www.cloudera.com/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/documentation.html&quot;&gt;https://www.cloudera.com/documentation.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/downloads.html&quot;&gt;https://www.cloudera.com/downloads.html&lt;/a&gt; - downloads&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://vision.cloudera.com/&quot;&gt;https://vision.cloudera.com/&lt;/a&gt; - Leadership blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/&quot;&gt;http://blog.cloudera.com/&lt;/a&gt; - Engineering blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://community.cloudera.com/t5/Community-News-Release/bd-p/RelAnnounce&quot;&gt;http://community.cloudera.com/t5/Community-News-Release/bd-p/RelAnnounce&lt;/a&gt; - Release Announcements&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>MapR</title><link>https://ondataengineering.net/tech-vendors/mapr/</link><description> &lt;p&gt;MapR is a commercial company focusing on products built around it's Converged Data Platform, which provides Hadoop compatibility plus NoSQL and streaming data storage capabilities, and which is bundled with a number of Hadoop open source products. They have started and are active in a number of open source components, including Apache Drill and Apache Myriad, both of which they founded. MapR was founded in 2009.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;!-- Technology metadata --&gt; &lt;h2 id=&quot;vendor-technologies&quot;&gt;Vendor Technologies&lt;/h2&gt; &lt;p&gt;MapR’s primary offering is their &lt;a href=&quot;/technologies/mapr-converged-data-platform/&quot;&gt;Converged Data Platform&lt;/a&gt;, with is available in a number of editions and with a number of add ons.&lt;/p&gt; &lt;p&gt;MapR were the founders of the &lt;a href=&quot;/technologies/apache-drill/&quot;&gt;Apache Drill&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-myriad/&quot;&gt;Apache Myriad&lt;/a&gt; projects.&lt;/p&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/&quot;&gt;https://mapr.com/&lt;/a&gt; - company homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/products/&quot;&gt;https://mapr.com/products/&lt;/a&gt; - products homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://maprdocs.mapr.com/home/&quot;&gt;http://maprdocs.mapr.com/home/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/products/hadoop-as-a-service/&quot;&gt;https://mapr.com/products/hadoop-as-a-service/&lt;/a&gt; - MapR cloud vendor support&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/blog/&quot;&gt;https://mapr.com/blog/&lt;/a&gt; - MapR blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://community.mapr.com/docs/DOC-1426-how-to-get-notified-of-release-product-and-patch-announcements&quot;&gt;https://community.mapr.com/docs/DOC-1426-how-to-get-notified-of-release-product-and-patch-announcements&lt;/a&gt; - product and patch announcements&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>ODPi</title><link>https://ondataengineering.net/tech-vendors/odpi/</link><description> &lt;p&gt;ODPi is a non profit organisation and member of the Linux Foundation that distributes reference specifications for key Hadoop components and APIs to help drive compatibility between Hadoop distributions, sponsoring Apache Bigtop as a reference implementation. Compliance against the spec for platform vendors (to ensure any certified app will run on their platform) and software vendors (to ensure their app will run on any certified platform) is achieved through self-certification against a test suite that's bundled with Apache Bigtop. Current technologies covered by the specifications are HDFS, YARN, MapReduce, HCFS and Hive. Current certified distributions include Altiscale, ArenaData, Hortonworks, IBM and Infosys but notably does not include either Cloudera or MapR who have both publicly stated their objections to the organisation. Currently certified applications are limited to DataTorrent, Apache Hawq, SAS, Syncsort, WANDisco and a range of IBM technologies. Originally founded in February 2005 as the Open Data Platform with language that suggested it was looking to build a standard Hadoop core (the ODP core) based on HDFS, Ambari, YARN and MapReduce. Moved under the Linux Foundation in September 2015.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Vendor Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Open Data Platform Initiative&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;!-- Technology metadata --&gt; &lt;h2 id=&quot;specifications&quot;&gt;Specifications&lt;/h2&gt; &lt;p&gt;ODPi manages and issues two specifications:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The &lt;a href=&quot;https://github.com/odpi/specs/blob/master/ODPi-Runtime.md&quot;&gt;Runtime Specification&lt;/a&gt;, which provides compatibility and interoperability for a range of key Hadoop components and APIs, including HDFS, YARN, MapReduce, HCFS (Hadoop Compatible Filesystems) and Hive (specifically SQL, JDBC and beeline). Requires the use of Hadoop 2.7, Java 7 or 8, a standard set of environment variables, no modification of the public API and any addition features functions to be committed to the ASF. Uses Apache Bigtop as a reference implementation, with Bigtop also providing the certification test suite.&lt;/li&gt; &lt;li&gt;The &lt;a href=&quot;https://github.com/odpi/specs/blob/master/ODPi-Operations.md&quot;&gt;Operations Specification&lt;/a&gt;, which provides the same for managing and monitoring Apache Hadoop clusters with Ambari as reference implementation, including a contrib management pack that allows any product based on Ambari to deploy the ODPi reference implementation.&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;td&gt;March 2016&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt;MVP for ODPi runtime spec, validation test suite and reference implementation covering HDFS, YARN and MapReduce&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.0&lt;/td&gt; &lt;td&gt;November 2016&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://www.theregister.co.uk/2016/11/14/odpi_20/&quot;&gt;viewpoint from TheRegister&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Added Operations Specification, and support for HCFS and Hive to the Runtime Specification&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.1&lt;/td&gt; &lt;td&gt;April 2017&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.odpi.org/blog/2017/04/05/odpi-2-1-a-tick-for-the-future-tock&quot;&gt;summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Moved to using Apache Bigtop for validation testsuite and reference implementation - ODPi release now only includes specifications&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;criticisms&quot;&gt;Criticisms&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://vision.cloudera.com/the-open-data-platform-alliance/&quot;&gt;Cloudera’s statement&lt;/a&gt; was that there was no demand for a compliance standard from any of their partners, that the organisation was a pay-to-join vendor driven consortium that was contrary to the open source nature of Hadoop, and that open source trumps vendor driven standardisation with reference to Linux’s success and the subsequent Linux ecosystem over the attend to standardise *NIX distributions.&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/blog/our-view-open-data-platform/&quot;&gt;MapR’s statement&lt;/a&gt; largely echoed Cloudera’s with the primary argument being that there wasn’t a requirement, citing a Gartner survey in which less than 1% of companies thought vendor lock in was an issue, and that there was a significant lack of breadth in the compliance technologies&lt;/li&gt; &lt;li&gt;TheRegister has a good summary of Cloudera and MapR’s positions &lt;a href=&quot;http://www.theregister.co.uk/2015/04/24/mapr_odp_cloudera/&quot;&gt;here&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A &lt;a href=&quot;http://www.theregister.co.uk/2016/11/14/odpi_20/&quot;&gt;subsequent article from TheRegister&lt;/a&gt; to coincide with the release of ODPi 2.0 includes an interview with Charaka Goonatilake, the CTO of Panaseer, that covers their two main issues as a software vendor, specifically that it doesn’t cover enough technologies or go deep enough, meaning it doesn’t actually help them test their product once and have confidence it’s going to work across all Hadoop distributions.&lt;/li&gt; &lt;li&gt;ZDNet have also a &lt;a href=&quot;http://www.zdnet.com/article/odpi-runtime-spec-aims-to-defrag-hadoop/&quot;&gt;useful write-up&lt;/a&gt; of the challenges ODPi is experiencing&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.odpi.org/&quot;&gt;https://www.odpi.org/&lt;/a&gt; - ODPi homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/odpi&quot;&gt;https://github.com/odpi&lt;/a&gt; - ODPi repository&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/odpi/specs/wiki&quot;&gt;https://github.com/odpi/specs/wiki&lt;/a&gt; - ODPi specifications Wiki&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://pivotal.io/big-data/press-release/technology-leaders-unite-around-open-data-platform-to-increase-enterprise-adoption-of-hadoop-and-big-data&quot;&gt;Pivotal launch press release&lt;/a&gt;, including links to the press releases from Altiscale, SAS and Verizon&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://content.pivotal.io/blog/open-data-platform-initiative-putting-an-end-to-faux-pen-source-apache-hadoop-distributions&quot;&gt;Summary of the original ODP plans&lt;/a&gt; from Roman Shaposhnik&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.odpi.org/blog&quot;&gt;https://www.odpi.org/blog&lt;/a&gt; - ODPi blog&lt;/li&gt; &lt;li&gt;ODPi also have a subscription newsletter&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Amazon Web Services</title><link>https://ondataengineering.net/tech-vendors/amazon-web-services/</link><description> &lt;p&gt;A subsidiary of Amazon.com that provides infrastructure and platform cloud services, including virtual machines and storage infrastructure services and plus database, analytics, real time data processing and data pipeline platform services, with services available in 16 geographical regions. Launched to support internal Amazon.com services in July 2002, with the first launch of a public service in November 2004, and now comfortably the largest cloud services provider.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Vendor Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Amazon&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;!-- Technology metadata --&gt; &lt;h2 id=&quot;infrastructure-services&quot;&gt;Infrastructure Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Amazon EC2&lt;/td&gt; &lt;td&gt;Virtual servers - &lt;a href=&quot;https://aws.amazon.com/ec2/&quot;&gt;https://aws.amazon.com/ec2/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon Elastic Container Service (Amazon ECS)&lt;/td&gt; &lt;td&gt;Docker based container service - &lt;a href=&quot;https://aws.amazon.com/ecs/&quot;&gt;https://aws.amazon.com/ecs/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon Elastic Container Service for Kubernetes (Amazon EKS)&lt;/td&gt; &lt;td&gt;Kubernetes as a service - &lt;a href=&quot;https://aws.amazon.com/eks/&quot;&gt;https://aws.amazon.com/eks/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;AWS Fargate&lt;/td&gt; &lt;td&gt;Provisioning of containers on ECS or EKS without managing servers or clusters - &lt;a href=&quot;https://aws.amazon.com/fargate/&quot;&gt;https://aws.amazon.com/fargate/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon EBS&lt;/td&gt; &lt;td&gt;Block storage, used to provide local storage for virtual machine - &lt;a href=&quot;https://aws.amazon.com/ebs/&quot;&gt;https://aws.amazon.com/ebs/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;storage-services&quot;&gt;Storage Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Amazon Elastic File System&lt;/td&gt; &lt;td&gt;Network storage, mountable on multiple machines over NFS - &lt;a href=&quot;https://aws.amazon.com/efs/&quot;&gt;https://aws.amazon.com/efs/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/amazon-s3&quot;&gt;Amazon S3&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Highly scalable and resilient object storage, including local cost archival storage (Amazon Glacier)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;compute-services&quot;&gt;Compute Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;AWS Lambda&lt;/td&gt; &lt;td&gt;Service for executing arbitrary code in response to a triggers - &lt;a href=&quot;https://aws.amazon.com/lambda/&quot;&gt;https://aws.amazon.com/lambda/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;AWS Batch&lt;/td&gt; &lt;td&gt;Service for executing arbitrary batch jobs, including support for &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt; - &lt;a href=&quot;https://aws.amazon.com/batch/&quot;&gt;https://aws.amazon.com/batch/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;database-services&quot;&gt;Database Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/amazon-emr/&quot;&gt;Amazon EMR&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Hadoop service, with support for a range of Hadoop technologies including HBase, Hive, Hue, Livy, Mahout, Oozie, Phoenix, Presto, Pig, Spark, Sqoop and Zeppelin&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon Relational Database Service&lt;/td&gt; &lt;td&gt;Relational database service, with support for Amazon Aurora (a MySQL/PostgreSQL compatible database), PostgreSQL, MySQL, MariaDB, Oracle, and Microsoft SQL Server - &lt;a href=&quot;https://aws.amazon.com/rds/&quot;&gt;https://aws.amazon.com/rds/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon Redshift&lt;/td&gt; &lt;td&gt;A MPP analytical database, with support for columnar storage and the ability to query data in Amazon S3 as external tables (Redshift Spectrum) - &lt;a href=&quot;https://aws.amazon.com/redshift/&quot;&gt;https://aws.amazon.com/redshift/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon Elasticache&lt;/td&gt; &lt;td&gt;Redis or Memcached service - &lt;a href=&quot;https://aws.amazon.com/elasticache/&quot;&gt;https://aws.amazon.com/elasticache/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon DynamoDB&lt;/td&gt; &lt;td&gt;A NoSQL document store service - &lt;a href=&quot;https://aws.amazon.com/dynamodb/&quot;&gt;https://aws.amazon.com/dynamodb/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon Cloudsearch&lt;/td&gt; &lt;td&gt;Amazon search service - &lt;a href=&quot;https://aws.amazon.com/cloudsearch/&quot;&gt;https://aws.amazon.com/cloudsearch/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon ElasticSearch Service&lt;/td&gt; &lt;td&gt;Elasticsearch service - &lt;a href=&quot;https://aws.amazon.com/elasticsearch-service/&quot;&gt;https://aws.amazon.com/elasticsearch-service/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon Neptune (Preview)&lt;/td&gt; &lt;td&gt;Graph and RDF database service with support for TinkerPop Gremlin and RDF SPARQL - &lt;a href=&quot;https://aws.amazon.com/neptune/&quot;&gt;https://aws.amazon.com/neptune/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;analytics-services&quot;&gt;Analytics Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Amazon Athena&lt;/td&gt; &lt;td&gt;SQL query service over data in Amazon S3 - &lt;a href=&quot;https://aws.amazon.com/athena/&quot;&gt;https://aws.amazon.com/athena/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon Quicksight&lt;/td&gt; &lt;td&gt;Web based analytics and visualisation tool - &lt;a href=&quot;https://quicksight.aws/&quot;&gt;https://quicksight.aws/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;streaming-data-services&quot;&gt;Streaming Data Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Amazon Kinesis&lt;/td&gt; &lt;td&gt;Suite of services relating to the processing and analytics of streaming data - &lt;a href=&quot;https://aws.amazon.com/kinesis/&quot;&gt;https://aws.amazon.com/kinesis/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon Kinesis Streams&lt;/td&gt; &lt;td&gt;Streaming data storage and publish service - &lt;a href=&quot;https://aws.amazon.com/kinesis/streams/&quot;&gt;https://aws.amazon.com/kinesis/streams/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon Kinesis Firehose&lt;/td&gt; &lt;td&gt;Streaming data movement, with support for basic transformation including routing, splitting and batching - &lt;a href=&quot;https://aws.amazon.com/kinesis/firehose/&quot;&gt;https://aws.amazon.com/kinesis/firehose/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon Kinesis Analytics&lt;/td&gt; &lt;td&gt;SQL over data streams for transformation or analytics - &lt;a href=&quot;https://aws.amazon.com/kinesis/analytics/&quot;&gt;https://aws.amazon.com/kinesis/analytics/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;data-integration-services&quot;&gt;Data Integration Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;AWS Data Pipelines&lt;/td&gt; &lt;td&gt;Data ingestion as a service - &lt;a href=&quot;https://aws.amazon.com/datapipeline/&quot;&gt;https://aws.amazon.com/datapipeline/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;AWS Glue (Preview)&lt;/td&gt; &lt;td&gt;Service for integrating data, with support for automating data discovery, conversion, mapping, and job scheduling - &lt;a href=&quot;https://aws.amazon.com/glue/&quot;&gt;https://aws.amazon.com/glue/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;machine-learning-services&quot;&gt;Machine Learning Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Amazon Rekognition&lt;/td&gt; &lt;td&gt;Service for searching and analysing images - &lt;a href=&quot;https://aws.amazon.com/rekognition/&quot;&gt;https://aws.amazon.com/rekognition/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon Machine Learning&lt;/td&gt; &lt;td&gt;Machine learning service - &lt;a href=&quot;https://aws.amazon.com/machine-learning/&quot;&gt;https://aws.amazon.com/machine-learning/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon SageMaker&lt;/td&gt; &lt;td&gt;Service for building, training and deploying machine learning at scale - &lt;a href=&quot;https://aws.amazon.com/sagemaker/&quot;&gt;https://aws.amazon.com/sagemaker/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/&quot;&gt;https://aws.amazon.com/&lt;/a&gt; - Amazon Web Services homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/products/&quot;&gt;https://aws.amazon.com/products/&lt;/a&gt; - products homepage&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/blogs/big-data/&quot;&gt;https://aws.amazon.com/blogs/big-data/&lt;/a&gt; - AWS Big Data blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/blogs/database/&quot;&gt;https://aws.amazon.com/blogs/database/&lt;/a&gt; - AWS Databases blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/new/&quot;&gt;https://aws.amazon.com/new/&lt;/a&gt; - AWS Announcements&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Microsoft Azure</title><link>https://ondataengineering.net/tech-vendors/microsoft-azure/</link><description> &lt;p&gt;A cloud computing service operated by Microsoft, with support for infrastructure, storage, databases and analytics services, available in 34 geographical regions. Announced in Otober 2008, with first services available in February 2010. Previously known as Windows Azure.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Vendor Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Azure&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;!-- Technology metadata --&gt; &lt;h2 id=&quot;infrastructure-services&quot;&gt;Infrastructure Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Azure Virtual Machines&lt;/td&gt; &lt;td&gt;Virtual servers with support for a range of operating systems and pre-build images, and for management at scale (Virtual Machine Scale Sets) - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/virtual-machines/&quot;&gt;https://azure.microsoft.com/en-us/services/virtual-machines/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Kubernetes Service&lt;/td&gt; &lt;td&gt;Kubernetes container service - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/kubernetes-service/&quot;&gt;https://azure.microsoft.com/en-us/services/kubernetes-service/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Disk Storage&lt;/td&gt; &lt;td&gt;Persistent disk storage to support virtual machines - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/storage/unmanaged-disks/&quot;&gt;https://azure.microsoft.com/en-us/services/storage/unmanaged-disks/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;storage-services&quot;&gt;Storage Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Azure Files&lt;/td&gt; &lt;td&gt;Network storage, mountable on multiple machines over REST and SMB - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/storage/files/&quot;&gt;https://azure.microsoft.com/en-us/services/storage/files/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/microsoft-azure-blob-storage/&quot;&gt;Azure Blob Storage&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Highly scalable and resilient object storage&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/microsoft-azure-data-lake-store/&quot;&gt;Azure Data Lake Store&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Massively scalable HDFS compatible filesystem as a service, based on Microsoft’s Cosmos technology&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;compute-services&quot;&gt;Compute Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Azure Functions&lt;/td&gt; &lt;td&gt;Service for executing arbitrary code in response to a trigger or timer - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/functions/&quot;&gt;https://azure.microsoft.com/en-us/services/functions/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Batch&lt;/td&gt; &lt;td&gt;Service for executing batch jobs across a pool of compute servers - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/batch/&quot;&gt;https://azure.microsoft.com/en-us/services/batch/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;database-services&quot;&gt;Database Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/azure-hdinsight/&quot;&gt;Azure HDInsight&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Hadoop service based on &lt;a href=&quot;/technologies/hortonworks-data-platform&quot;&gt;HDP&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure SQL Database&lt;/td&gt; &lt;td&gt;Scalable relational database - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/sql-database/&quot;&gt;https://azure.microsoft.com/en-us/services/sql-database/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Analysis Services&lt;/td&gt; &lt;td&gt;SQL Server Analysis Services as a service - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/analysis-services/&quot;&gt;https://azure.microsoft.com/en-us/services/analysis-services/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Database for MySQL&lt;/td&gt; &lt;td&gt;MySQL as a service - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/mysql/&quot;&gt;https://azure.microsoft.com/en-us/services/mysql/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Database for PostgreSQL&lt;/td&gt; &lt;td&gt;PostgreSQL as a service - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/postgresql/&quot;&gt;https://azure.microsoft.com/en-us/services/postgresql/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Database for MariaDB&lt;/td&gt; &lt;td&gt;MariaDB as a service, currently in preview - &lt;a href=&quot;https://azure.microsoft.com/en-gb/blog/mariadb-postgresql-and-mysql-more-choices-on-microsoft-azure/&quot;&gt;https://azure.microsoft.com/en-gb/blog/mariadb-postgresql-and-mysql-more-choices-on-microsoft-azure/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure SQL Data Warehouse&lt;/td&gt; &lt;td&gt;Scalable analytical database - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/sql-data-warehouse/&quot;&gt;https://azure.microsoft.com/en-us/services/sql-data-warehouse/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Redis Cache&lt;/td&gt; &lt;td&gt;Redis as a service - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/cache/&quot;&gt;https://azure.microsoft.com/en-us/services/cache/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Table Storage&lt;/td&gt; &lt;td&gt;A NoSQL wide column store service - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/storage/tables/&quot;&gt;https://azure.microsoft.com/en-us/services/storage/tables/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Cosmos DB&lt;/td&gt; &lt;td&gt;Massively scalable, low latency multi-model (key-value, graph, wide column and document) NoSQL database, previously known as Azure DocumentDB - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/cosmos-db/&quot;&gt;https://azure.microsoft.com/en-us/services/cosmos-db/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Search&lt;/td&gt; &lt;td&gt;Search service - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/search/&quot;&gt;https://azure.microsoft.com/en-us/services/search/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;analytics-services&quot;&gt;Analytics Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Azure Data Lake Analytics&lt;/td&gt; &lt;td&gt;Massively parallel analytics job service, with support for U-SQL, R, Python, and .NET - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/data-lake-analytics/&quot;&gt;https://azure.microsoft.com/en-us/services/data-lake-analytics/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Data Catalog&lt;/td&gt; &lt;td&gt;A metadata catalog service - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/data-catalog/&quot;&gt;https://azure.microsoft.com/en-us/services/data-catalog/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Time Series Insights&lt;/td&gt; &lt;td&gt;Storage, analytics and visualisation service for time series data - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/time-series-insights/&quot;&gt;https://azure.microsoft.com/en-us/services/time-series-insights/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Log Analytics&lt;/td&gt; &lt;td&gt;Storage, analytics and analytics service for log data - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/log-analytics/&quot;&gt;https://azure.microsoft.com/en-us/services/log-analytics/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Databricks&lt;/td&gt; &lt;td&gt;Spark as a service jointly developed with Databricks, currently in preview - &lt;a href=&quot;https://azure.microsoft.com/en-us/campaigns/databricks/&quot;&gt;https://azure.microsoft.com/en-us/campaigns/databricks/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;streaming-data-services&quot;&gt;Streaming Data Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Azure Event Hubs&lt;/td&gt; &lt;td&gt;Elastic service for the buffering and publishing of streaming event data - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/event-hubs/&quot;&gt;https://azure.microsoft.com/en-us/services/event-hubs/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Stream Analytics&lt;/td&gt; &lt;td&gt;Service for querying streams of data using a SQL like language - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/stream-analytics/&quot;&gt;https://azure.microsoft.com/en-us/services/stream-analytics/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;data-integration-services&quot;&gt;Data Integration Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Azure Data Factory&lt;/td&gt; &lt;td&gt;Data ingestion as a service - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/data-factory/&quot;&gt;https://azure.microsoft.com/en-us/services/data-factory/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Scheduler&lt;/td&gt; &lt;td&gt;Scheduling as a service - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/scheduler/&quot;&gt;https://azure.microsoft.com/en-us/services/scheduler/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;machine-learning-services&quot;&gt;Machine Learning Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Azure Machine Learning&lt;/td&gt; &lt;td&gt;Machine learning service - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/machine-learning/&quot;&gt;https://azure.microsoft.com/en-us/services/machine-learning/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Cognitive Services&lt;/td&gt; &lt;td&gt;Suite of services including vision, speech and text analysis - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/cognitive-services/&quot;&gt;https://azure.microsoft.com/en-us/services/cognitive-services/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://azure.microsoft.com/&quot;&gt;https://azure.microsoft.com/&lt;/a&gt; - Amazon Web Services homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://azure.microsoft.com/en-us/services/&quot;&gt;https://azure.microsoft.com/en-us/services/&lt;/a&gt; - products homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/azure/architecture/aws-professional/services&quot;&gt;https://docs.microsoft.com/en-us/azure/architecture/aws-professional/services&lt;/a&gt; - comparison of AWS to Azure offerings&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://azure.microsoft.com/en-us/blog/&quot;&gt;https://azure.microsoft.com/en-us/blog/&lt;/a&gt; - Azure Blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://azure.microsoft.com/en-us/blog/topics/announcements/&quot;&gt;https://azure.microsoft.com/en-us/blog/topics/announcements/&lt;/a&gt; - Azure Blog (announcements)&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Google Cloud Platform</title><link>https://ondataengineering.net/tech-vendors/google-cloud-platform/</link><description> &lt;p&gt;A cloud computing service operated by Google, with support for infrastructure, storage, databases and analytics services. First services were available in preview in April 2008.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Vendor Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Google&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;!-- Technology metadata --&gt; &lt;h2 id=&quot;infrastructure-services&quot;&gt;Infrastructure Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Google Compute Engine&lt;/td&gt; &lt;td&gt;Virtual servers - &lt;a href=&quot;https://cloud.google.com/compute/&quot;&gt;https://cloud.google.com/compute/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google Kubernetes Engine&lt;/td&gt; &lt;td&gt;Kubernetes cluster service - &lt;a href=&quot;https://cloud.google.com/kubernetes-engine/&quot;&gt;https://cloud.google.com/kubernetes-engine/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google Persistent Disk&lt;/td&gt; &lt;td&gt;Block storage for virtual machines - &lt;a href=&quot;https://cloud.google.com/persistent-disk/&quot;&gt;https://cloud.google.com/persistent-disk/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;storage-services&quot;&gt;Storage Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Google Filestore&lt;/td&gt; &lt;td&gt;Network storage, mountable on multiple machines over NFS, available in beta - &lt;a href=&quot;https://cloud.google.com/filestore/&quot;&gt;https://cloud.google.com/filestore/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/google-cloud-storage/&quot;&gt;Google Cloud Storage&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Object store service with strong consistency, multiple storage tiers and deep integration to the Google Cloud ecosystem.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;compute-services&quot;&gt;Compute Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Google Cloud Functions&lt;/td&gt; &lt;td&gt;Serverless code execution service - &lt;a href=&quot;https://cloud.google.com/functions/&quot;&gt;https://cloud.google.com/functions/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;database-services&quot;&gt;Database Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/google-cloud-dataproc/&quot;&gt;Google Cloud Dataproc&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Hadoop service, with support for MapReduce, Spark, Pig and Hive&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google Cloud Spanner&lt;/td&gt; &lt;td&gt;Horizontally scalable relational database for transaction processing - &lt;a href=&quot;https://cloud.google.com/spanner/&quot;&gt;https://cloud.google.com/spanner/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google Cloud SQL&lt;/td&gt; &lt;td&gt;Managed database service, with support for MySQL and PostgreSQL - &lt;a href=&quot;https://cloud.google.com/sql/&quot;&gt;https://cloud.google.com/sql/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google Big Query&lt;/td&gt; &lt;td&gt;Analytical SQL database service - &lt;a href=&quot;https://cloud.google.com/bigquery/&quot;&gt;https://cloud.google.com/bigquery/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google Cloud Bigtable&lt;/td&gt; &lt;td&gt;NoSQL wide column store service - &lt;a href=&quot;https://cloud.google.com/bigtable/&quot;&gt;https://cloud.google.com/bigtable/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google Cloud Datastore&lt;/td&gt; &lt;td&gt;NoSQL document store service - &lt;a href=&quot;https://cloud.google.com/datastore/&quot;&gt;https://cloud.google.com/datastore/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google Cloud Memorystore&lt;/td&gt; &lt;td&gt;NoSQL key value store service, compatible with the Redis protocol - &lt;a href=&quot;https://cloud.google.com/memorystore/&quot;&gt;https://cloud.google.com/memorystore/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;analytics-services&quot;&gt;Analytics Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Google Datalab&lt;/td&gt; &lt;td&gt;Web based data exploration and analysis tool based on Jupyter - &lt;a href=&quot;https://cloud.google.com/datalab/&quot;&gt;https://cloud.google.com/datalab/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google Data Studio&lt;/td&gt; &lt;td&gt;Drag and drop reporting and dashboarding tool - &lt;a href=&quot;https://cloud.google.com/data-studio/&quot;&gt;https://cloud.google.com/data-studio/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;streaming-data-services&quot;&gt;Streaming Data Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Google Cloud Pub/Sub&lt;/td&gt; &lt;td&gt;Real time message and streaming data service with “at least once” delivery - &lt;a href=&quot;https://cloud.google.com/pubsub/&quot;&gt;https://cloud.google.com/pubsub/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google Cloud Dataflow&lt;/td&gt; &lt;td&gt;Batch and streaming data flow service based on Apache Beam - &lt;a href=&quot;https://cloud.google.com/dataflow/&quot;&gt;https://cloud.google.com/dataflow/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;data-integration-services&quot;&gt;Data Integration Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Google Cloud Dataprep&lt;/td&gt; &lt;td&gt;Data preparation service for “visually exploring, cleaning, and preparing structured and unstructured data for analysis” based on Trifacta - &lt;a href=&quot;https://cloud.google.com/dataprep/&quot;&gt;https://cloud.google.com/dataprep/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google Cloud Composer&lt;/td&gt; &lt;td&gt;Orchestration service based on Apache Airflow - &lt;a href=&quot;https://cloud.google.com/composer&quot;&gt;https://cloud.google.com/composer&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;machine-learning-services&quot;&gt;Machine Learning Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Google Cloud Machine Learning Engine&lt;/td&gt; &lt;td&gt;Machine learning service based on TensorFlow - &lt;a href=&quot;https://cloud.google.com/ml-engine/&quot;&gt;https://cloud.google.com/ml-engine/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google Machine Learning Services&lt;/td&gt; &lt;td&gt;Suite of machine learning services including video, image, speech and text analysis - &lt;a href=&quot;https://cloud.google.com/products/machine-learning/&quot;&gt;https://cloud.google.com/products/machine-learning/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/&quot;&gt;https://cloud.google.com/&lt;/a&gt; - Google Cloud Platform homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/products/&quot;&gt;https://cloud.google.com/products/&lt;/a&gt; - products homepage&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/blog/&quot;&gt;https://cloud.google.com/blog/&lt;/a&gt; - Google Cloud Platform blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/blog/big-data/&quot;&gt;https://cloud.google.com/blog/big-data/&lt;/a&gt; - Google Cloud Platform Big Data and Machine Learning blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Mesosphere</title><link>https://ondataengineering.net/tech-vendors/mesosphere/</link><description> &lt;p&gt;Mesosphere is a commercial company developing the Mesosphere Datacenter Operating System (DC/OS). DC/OS is built around Apache Mesos and is itself an open source project. They are therefore extremely active in the open source space. Their business model is to sell subscription licenses based around an Enterprise version of DC/OS, provide training and support for DC/OS and partner-supported technologies. Mesosphere was founded in May 2013 by ex-engineers from Twitter and Airbnb.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Vendor Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-mesos/&quot;&gt;Apache Mesos&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Open source cluster manager for providing efficient resource utilization across a cluster of servers through resource sharing and isolation. Allows a cluster of servers to be shared across diverse cluster computing frameworks so that different distributed workloads such as container orchestration, machine learning, analytics and stateful big data technologies can be run without interfering with each other. Has the ability to dynamically allocate resources across the servers as needed and delegates control over scheduling to the frameworks through an abstraction layer called a resource offer to support a wide array of computing frameworks. Resource isolation is implemented using a universal containeriser, supporting numerous containers including native Mesos containers and Docker containers. Fault tolerance of the Mesos instance in control of the cluster is implemented using ZooKeeper. Started as a research project in the UC Berkeley RAD Lab, open sourced in 2011, with a v1.0 release in July 2016, which, included the 'unified containeriser' and GPU-based scheduling. Written in C++, uses Google Protocol Buffers for messaging and serialization to allow frameworks to be written in a variety of languages including C++, Java, Python, Go, Haskell, and Scala. Under active development, open sourced under the Apache 2.0 license, hosted on the Apache git repository and mirrored on GitHub. Software startup Mesosphere sells the Datacenter Operating System, a distributed operating system, based on Apache Mesos.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;!-- Technology metadata --&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://mesosphere.com/&quot;&gt;https://mesosphere.com/&lt;/a&gt; - homepage&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://mesosphere.com/blog/&quot;&gt;https://mesosphere.com/blog/&lt;/a&gt; - Mesosphere blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Apache Software Foundation</title><link>https://ondataengineering.net/tech-vendors/apache/</link><description> &lt;p&gt;The Apache Software Foundation is a non-profit organisation that supports a wide range of open source projects, including providing and mandating a standard governance model (including the use of the Apache license), holding all trademarks for project names and logos, and providing legal protection to developers. It was founded in 1999 and now oversees nearly 200 projects.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Vendor Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Apache&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;!-- Technology metadata --&gt; &lt;h2 id=&quot;analytical-query-capabilities&quot;&gt;Analytical Query Capabilities&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hawq/&quot;&gt;HAWQ&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A port of the Greenplum MPP database (which itself is based on PostgreSQL) to run over YARN and HDFS.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-tajo/&quot;&gt;Tajo&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Distributed analytical database engine supporting queries over data in HDFS, Amazon S3, Google Cloud Storage, OpenStack Swift and local storage, and querying over Postgres, HBase and Hive tables.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-kudu/&quot;&gt;Kudu&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Columnar storage technology for tables of structured data, supporting low latency reads, updates and deletes by primary key, as well as analytical column/table scans.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Quickstep (Incubating)&lt;/td&gt; &lt;td&gt;High performance database engine supporting SQL queries based on a University of Wisconsin-Madison project - &lt;a href=&quot;http://quickstep.apache.org/&quot;&gt;http://quickstep.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Hive&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Supports the execution of SQL queries over data in HDFS using MapReduce, Spark or Tez based on tables defined in the Hive Metastore&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-pig/&quot;&gt;Pig&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Technology for running analytical and data processing jobs written in Pig Latin against data in Hadoop using MapReduce, Tez and Spark&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;MRQL (Incubating)&lt;/td&gt; &lt;td&gt;Supports the execution of MRQL queries over data in Hadoop using MapReduce, Hama, Spark or Flink - &lt;a href=&quot;http://mrql.apache.org/&quot;&gt;http://mrql.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-impala/&quot;&gt;Impala&lt;/a&gt;&lt;/td&gt; &lt;td&gt;An MPP query engine that supports the execution of SQL queries over in HDFS, HBase, Kudu and S3 based on tables defined in the Hive Metastore&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-drill/&quot;&gt;Drill&lt;/a&gt;&lt;/td&gt; &lt;td&gt;An MPP query engine that supports queries over one or more underlying databases or datasets without first defining a schema and with the ability to join data from multiple datastores together.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Lens&lt;/td&gt; &lt;td&gt;Provides a federated view over multiple data stores using a single shared schema server based on the Hive Metastore - &lt;a href=&quot;http://lens.apache.org/&quot;&gt;http://lens.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-kylin/&quot;&gt;Kylin&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Supports the creation and querying of OLAP cubes on Hadoop, building cubes from star schema data in Hive into HBase, and then providing a SQL interface that queries across Hive and HBase as required - &lt;a href=&quot;http://kylin.apache.org/ &quot;&gt;http://kylin.apache.org/ &lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;analytical-search-capabilities&quot;&gt;Analytical Search Capabilities&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-solr/&quot;&gt;Solr&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A search server built on Apache Lucene with a REST-like API for loading and searching data.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;compute-cluster-management&quot;&gt;Compute Cluster Management&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/yarn/&quot;&gt;Hadoop/YARN&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Resource management and job scheduling &amp;amp; monitoring for the Hadoop ecosystem.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-slider/&quot;&gt;Slider (Incubating)&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Application for deploying long running cluster applications on YARN, now effectively dead following the plan to add support for long running services directly into YARN&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Twill&lt;/td&gt; &lt;td&gt;Abstraction over YARN that reduces the complexity of developing distributed applications - &lt;a href=&quot;http://twill.apache.org/&quot;&gt;http://twill.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-mesos/&quot;&gt;Mesos&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Resource management over large clusters of machines&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-aurora/&quot;&gt;Aurora&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Mesos framework for long-running services and cron jobs&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-zookeeper/&quot;&gt;ZooKeeper&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Service for managing coordination (e.g. configuration information and synchronisation) of distributed and clustered systems.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Curator&lt;/td&gt; &lt;td&gt;A set of Java libraries that make using Apache ZooKeeper much easier - &lt;a href=&quot;http://curator.apache.org/&quot;&gt;http://curator.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-myriad/&quot;&gt;Myriad (Incubating)&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Tool that allows YARN applications to run over Apache Mesos, allowing them to co-exist and share cluster resources.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;REEF&lt;/td&gt; &lt;td&gt;A framework for developing distributed apps on top of cluster frameworks such as YARN or Mesos - &lt;a href=&quot;http://reef.apache.org/&quot;&gt;http://reef.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;data-formats&quot;&gt;Data Formats&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-avro/&quot;&gt;Avro&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Data serialisation framework that supports both messaging and data storage, primarily using a compact binary format but also supports a JSON format.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-parquet/&quot;&gt;Parquet&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Data serialisation framework that supports a columnar storage format to enable efficient querying of data.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-arrow/&quot;&gt;Arrow&lt;/a&gt;&lt;/td&gt; &lt;td&gt;In memory columnar data format supporting high performance data exchange and fast analytical access&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-orc/&quot;&gt;ORCFile&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Evolution of RCFile, spun out into it’s own Apache project&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-carbondata/&quot;&gt;CarbonData&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Columnar format created by Huawei to address a number of perceived shortcomings in existing formats&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;data-ingestion&quot;&gt;Data Ingestion&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-nifi/&quot;&gt;Nifi&lt;/a&gt;&lt;/td&gt; &lt;td&gt;General purpose technology for the movement of data between systems, including the ingestion of data into an analytical platform.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-gobblin/&quot;&gt;Gobblin (Incubating)&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Framework for managing big data ingestion, including replication, organization and lifecycle management&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-flume/&quot;&gt;Flume&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Specialist technology for the continuous movement of data using a set of independent agents connected together into pipelines.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-sqoop/&quot;&gt;Sqoop&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Specialist technology for moving bulk data between Hadoop and structured (relational) databases.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;ManifoldCF&lt;/td&gt; &lt;td&gt;Framework for replicating data from content repositories to analytical search technologies - &lt;a href=&quot;http://manifoldcf.apache.org/&quot;&gt;http://manifoldcf.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;data-processing&quot;&gt;Data Processing&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/map-reduce/&quot;&gt;Hadoop/MapReduce&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A data transformation and aggregation technology proven at extreme scale that works on key value pairs&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Spark&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A high performance general purpose distributed data processing engine based on directed acyclic graphs that primarily runs in memory, but can spill to disk if required&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-tez/&quot;&gt;Tez&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Data processing framework based on Directed Acyclic Graphs (DAGs), that runs natively on YARN and was designed to be a replacement for the use of MapReduce within Hadoop analytical tools&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-crunch/&quot;&gt;Crunch&lt;/a&gt;&lt;/td&gt; &lt;td&gt;An abstraction layer over MapReduce (and now Spark) that provides a high level Java API for creating data transformation pipelines&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Nemo (Incubating)&lt;/td&gt; &lt;td&gt;A runtime for data processing languages that dynamically adjusts to the runtime environment - &lt;a href=&quot;https://nemo.incubator.apache.org/&quot;&gt;https://nemo.incubator.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Crail (Incubating)&lt;/td&gt; &lt;td&gt;High performance distributed and tiered (in memory, flash and disk) storage layer for temporary data that provides memory, storage and network access that bypasses the JVM and OS, and support for Spark and Hadoop - &lt;a href=&quot;http://crail.incubator.apache.org/&quot;&gt;http://crail.incubator.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;graph-technologies&quot;&gt;Graph Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-giraph/&quot;&gt;Giraph&lt;/a&gt;&lt;/td&gt; &lt;td&gt;An iterative, highly scalable graph processing system built on top of MapReduce and based on Pregel&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hama/&quot;&gt;Hama&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A general purpose BSP (Bulk Synchronous Parallel) processing engine inspired by Pregel and DistBelief that runs over Mesos or YARN.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Commons RDF (0)&lt;/td&gt; &lt;td&gt;Commons library for working with RDF data - &amp;lt;commons.apache.org/proper/commons-rdf/&amp;gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Jena&lt;/td&gt; &lt;td&gt;Framework for developing Semantic Web and Linked Data applications in Java - &lt;a href=&quot;http://jena.apache.org/&quot;&gt;http://jena.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Rya (Incubating)&lt;/td&gt; &lt;td&gt;RDF triple store built on Apache Accumulo - &lt;a href=&quot;http://rya.apache.org/&quot;&gt;http://rya.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;S2Graph (Incubating)&lt;/td&gt; &lt;td&gt;OLTP graph database built on Apache HBase - &lt;a href=&quot;https://s2graph.incubator.apache.org/&quot;&gt;https://s2graph.incubator.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;TinkerPop&lt;/td&gt; &lt;td&gt;Graph compute framework for transactional and analytical use cases that’s integrated with a number of graph database technologies - &lt;a href=&quot;http://tinkerpop.apache.org&quot;&gt;http://tinkerpop.apache.org&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-spark/graphx/&quot;&gt;Spark/GraphX&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Spark library for processing graphs and running graph algorithms&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;hadoop-and-related-technologies&quot;&gt;Hadoop and Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Hadoop&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A distributed storage and compute platform consisting of a distributed filesystem (HDFS), a cluster resource management layer (YARN), and MapReduce, a solution built on HDFS and YARN for massive scale parallel processing of data&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Bigtop&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Apache open source distribution of Hadoop&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-ambari/&quot;&gt;Ambari&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Platform for installing, managing and monitoring Apache Hadoop clusters&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-atlas/&quot;&gt;Atlas&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A metadata and data governance solution for Hadoop.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-knox/&quot;&gt;Knox&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A stateless gateway for the Apache Hadoop ecosystem that provides perimeter security&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-ranger/&quot;&gt;Ranger&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A centralised security framework for managing access to data in Hadoop&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-sentry/&quot;&gt;Sentry&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A centralised security framework for managing access to data in Hadoop&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Eagle&lt;/td&gt; &lt;td&gt;Security and performance monitoring solution for Hadoop, donated by eBay - &lt;a href=&quot;http://eagle.apache.org/&quot;&gt;http://eagle.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-falcon/&quot;&gt;Falcon&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Data feed management system for Hadoop, although no longer appears under development and is deprecated from HDP.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;in-memory-technologies&quot;&gt;In Memory Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-ignite/&quot;&gt;Ignite&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A distributed in-memory data fabric/grid, supporting a range of different use cases and capabilities&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Geode&lt;/td&gt; &lt;td&gt;In memory data management platform, born of Pivotal Gemfire - &lt;a href=&quot;http://geode.apache.org/&quot;&gt;http://geode.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Mnemonic&lt;/td&gt; &lt;td&gt;Hybrid memory / storage object model framework - &lt;a href=&quot;http://mnemonic.apache.org/&quot;&gt;http://mnemonic.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;machine-learning-technologies&quot;&gt;Machine Learning Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-spark/mllib/&quot;&gt;Spark/MLLib&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Spark library for running Machine Learning algorithms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-mahout/&quot;&gt;Mahout&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Machine learning technology comprising of a Scala based linear algebra engine (codenamed Samsara) with an R-like DSL/API that runs over Spark (with experimental support for H2O and Flink)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;MADlib&lt;/td&gt; &lt;td&gt;Machine learning in SQL for PostgreSQL, Greenplum and Apache HAWQ - &lt;a href=&quot;http://madlib.apache.org/&quot;&gt;http://madlib.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;OpenNLP&lt;/td&gt; &lt;td&gt;Machine learning based toolkit for the processing of natural language text - &lt;a href=&quot;http://opennlp.apache.org/&quot;&gt;http://opennlp.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;SAMOA (Incubating)&lt;/td&gt; &lt;td&gt;Machine learning framework that runs over multiple stream processing engines including Storm, Flink and Samza - &lt;a href=&quot;http://samoa.apache.org/&quot;&gt;http://samoa.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;SINGA (Incubating)&lt;/td&gt; &lt;td&gt;Framework for developing machine learning libraries over a range of hardware - &lt;a href=&quot;https://singa.apache.org/&quot;&gt;https://singa.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;SystemML&lt;/td&gt; &lt;td&gt;Delarative machine learning over local, Spark or MapReduce execution engines - &lt;a href=&quot;http://systemml.apache.org/&quot;&gt;http://systemml.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Hivemall (Incubating)&lt;/td&gt; &lt;td&gt;Scalable machine learning library implemented as Hive UDFs/UDAFs/UDTFs - &lt;a href=&quot;http://hivemall.incubator.apache.org/&quot;&gt;http://hivemall.incubator.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;nosql-wide-column-stores&quot;&gt;NoSQL Wide Column Stores&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-accumulo/&quot;&gt;Accumulo&lt;/a&gt;&lt;/td&gt; &lt;td&gt;NoSQL wide-column datastore based on Google BigTable that runs on Hadoop and HDFS&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-cassandra/&quot;&gt;Cassandra&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Distributed wide-column datastore based on Amazon Dynamo and Google BigTable&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hbase/&quot;&gt;HBase&lt;/a&gt;&lt;/td&gt; &lt;td&gt;NoSQL wide-column datastore based on Google BigTable that runs on Hadoop and HDFS&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Fluo&lt;/td&gt; &lt;td&gt;Implementation of Google Percolator for maintaining aggregations in Accumulo - &lt;a href=&quot;https://fluo.apache.org/&quot;&gt;https://fluo.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Omid (Incubating)&lt;/td&gt; &lt;td&gt;ACID transaction support over MVCC key/value NoSQL datastores with support for Apache Hbase - &lt;a href=&quot;http://omid.apache.org/&quot;&gt;http://omid.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Tephra (Incubating)&lt;/td&gt; &lt;td&gt;ACID transaction support over Apache Hbase, used by Tigon and Apache Phoenix - &lt;a href=&quot;http://tephra.apache.org/&quot;&gt;http://tephra.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;oltp-databases&quot;&gt;OLTP Databases&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-phoenix/&quot;&gt;Phoenix&lt;/a&gt;&lt;/td&gt; &lt;td&gt;An OLTP SQL query engine over Apache HBase tables that supports a subset of SQL 92 (including joins), and comes with a JDBC driver.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Trafodion&lt;/td&gt; &lt;td&gt;OLTP on Hadoop solution based on Tandom NoStop database IP with commercial support from Esgyn - &lt;a href=&quot;https://trafodion.incubator.apache.org/&quot;&gt;https://trafodion.incubator.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;streaming-analytics&quot;&gt;Streaming Analytics&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-storm/&quot;&gt;Storm&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Specialised distributed stream processing technology based on a single record (not micro batch) model with at least once processing semantics.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-flink/&quot;&gt;Flink&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Specialised stream processing technology inspired by the Google Data Flow model based on a single record (not micro batch) model, with exactly once processing semantics (for supported sources and sinks) via light weight checkpointing and support for batch processing.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-spark/spark-streaming/&quot;&gt;Spark/Streaming&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Spark library for continuous stream processing, that allows stream and batch processing (including Spark SQL and MLlib operations) to be combined&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-kafka/kafka-streams/&quot;&gt;Apache Kafka Streams&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Stream processing framework built over Apache Kafka, with support for stateful tables&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-beam&quot;&gt;Beam&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Model and SDKs for running batch and streaming workflows over Apex, Flink, Spark and Google Dataflow - &lt;a href=&quot;https://beam.apache.org/&quot;&gt;https://beam.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-apex/&quot;&gt;Apex&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Data transformation engine based on Directed Acyclic Graph (DAG) flows configured through a Java API or via JSON that runs over YARN and HDFS with native support for both micro-batch streaming and batch uses cases&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Heron (Incubating)&lt;/td&gt; &lt;td&gt;The stream processing framework that Twitter built after Storm, with a Storm compatible API - &lt;a href=&quot;http://heron.incubator.apache.org/&quot;&gt;http://heron.incubator.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Samza&lt;/td&gt; &lt;td&gt;Stream processing framework built on Kafka and YARN - &lt;a href=&quot;http://samza.apache.org/&quot;&gt;http://samza.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Bahir&lt;/td&gt; &lt;td&gt;A suite of streaming connectors for Spark and Flink, including support for Akka, MQTT, Twitter and ZeroMQ - &lt;a href=&quot;http://bahir.apache.org/&quot;&gt;http://bahir.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gearpump (Incubating)&lt;/td&gt; &lt;td&gt;Real-time streaming engine based on the micro-service Actor model - &lt;a href=&quot;http://gearpump.apache.org/&quot;&gt;http://gearpump.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;streaming-data-stores&quot;&gt;Streaming Data Stores&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Kafka&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Technology for buffering and storing real-time streams of data between producers and consumers, with a focus on high throughput at low latency.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;BookKeeper&lt;/td&gt; &lt;td&gt;Distributed log storage service from Yahoo - &lt;a href=&quot;http://bookkeeper.apache.org/&quot;&gt;http://bookkeeper.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DistributedLog&lt;/td&gt; &lt;td&gt;Distributed log service from Twitter supporting durability, replication and strong consistency built over Apache BookKeeper - &lt;a href=&quot;http://bookkeeper.apache.org/distributedlog/&quot;&gt;http://bookkeeper.apache.org/distributedlog/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Pulsar&lt;/td&gt; &lt;td&gt;Distributed pub-sub messaging from Yahoo, with persistent message storage based on Apache BookKeeper - &lt;a href=&quot;http://pulsar.incubator.apache.org/&quot;&gt;http://pulsar.incubator.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;workflow-management&quot;&gt;Workflow Management&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-oozie/&quot;&gt;Oozie&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Technology for managing workflows of jobs on Hadoop clusters.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Airflow (Incubating)&lt;/td&gt; &lt;td&gt;Workflow automation and scheduling system that can be used to author and manage data pipelines - &lt;a href=&quot;http://airflow.apache.org/&quot;&gt;http://airflow.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;other-technologies&quot;&gt;Other Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-datafu/&quot;&gt;DataFu&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A set of libraries for working with data in Hadoop, consisting of two sub-projects - DataFu Pig (a set of Pig User Defined Functions) and DataFu Hourglass (a framework for incremental processing using MapReduce).&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;AsterixDB&lt;/td&gt; &lt;td&gt;Scalable “Big Data Management System” - &lt;a href=&quot;https://asterixdb.apache.org/&quot;&gt;https://asterixdb.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Chukwa&lt;/td&gt; &lt;td&gt;Specialist technology for the ingestion of continuous data flows into an Hadoop cluster, and the subsequent management and analysis of the data - &lt;a href=&quot;https://chukwa.apache.org/ &quot;&gt;https://chukwa.apache.org/ &lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Edgent (Incubating)&lt;/td&gt; &lt;td&gt;Stream processing programming model and lightweight runtime to execute analytics at devices on the edge or at the gateway, previously known as Quarks - &lt;a href=&quot;http://edgent.apache.org/&quot;&gt;http://edgent.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gora&lt;/td&gt; &lt;td&gt;ORM with support for a range of NoSQL, Search and Hadoop data formats - &lt;a href=&quot;http://gora.apache.org/&quot;&gt;http://gora.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Helix&lt;/td&gt; &lt;td&gt;A framework for building long lived persistent distributed systems - &lt;a href=&quot;http://helix.apache.org/&quot;&gt;http://helix.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;MetaModel&lt;/td&gt; &lt;td&gt;Technology for reading and writing database metadata with connectors for a wide range of databases - &lt;a href=&quot;http://metamodel.apache.org/&quot;&gt;http://metamodel.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Toree (Incubating)&lt;/td&gt; &lt;td&gt;Framework to allow interactive applications to communicate with a remote Spark cluster - &lt;a href=&quot;http://toree.apache.org/&quot;&gt;http://toree.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-calcite/&quot;&gt;Calcite&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A framework for building SQL based data access capabilities, supporting a SQL parser and validator and tools for the transformation and (cost based) optimisation of SQL expression trees.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-livy/&quot;&gt;Livy (Incubating)&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A service that allows Spark jobs (pre-compiled JARs) or code snippets (Scala or Python) to be executed by remote systems over a REST API or via clients for Java, Scala and Python.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-superset/&quot;&gt;Superset (Incubating)&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Web based tool for interactive exploration for OLAP style data, supporting interactive drag and drop querying, composable dashboards and a SQL workspace (SQL Lab).&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-zeppelin/&quot;&gt;Zeppelin&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A web based notebook for interactive data analytics.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Commons Compress&lt;/td&gt; &lt;td&gt;Suite of Java libraries for working with a range of compression and packaging formats - &lt;a href=&quot;https://commons.apache.org/proper/commons-compress/&quot;&gt;https://commons.apache.org/proper/commons-compress/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Commons CSV&lt;/td&gt; &lt;td&gt;Suite of Java libraries for workng with CSV files - &lt;a href=&quot;https://commons.apache.org/proper/commons-csv/&quot;&gt;https://commons.apache.org/proper/commons-csv/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Griffin (Incubating)&lt;/td&gt; &lt;td&gt;Data Quality Service platform built on Apache Hadoop and Apache Spark - &lt;a href=&quot;http://griffin.incubator.apache.org/&quot;&gt;http://griffin.incubator.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Tika&lt;/td&gt; &lt;td&gt;Toolkit for extracting text from a wide range of document formats - &lt;a href=&quot;http://tika.apache.org/&quot;&gt;http://tika.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;UIMA&lt;/td&gt; &lt;td&gt;Framework for unstructured data analysis - &lt;a href=&quot;http://uima.apache.org/&quot;&gt;http://uima.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.apache.org/&quot;&gt;https://www.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.apache.org/foundation/how-it-works.html&quot;&gt;https://www.apache.org/foundation/how-it-works.html&lt;/a&gt; - information on the foundation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://apache.org/foundation/mailinglists.html#foundation-announce&quot;&gt;http://apache.org/foundation/mailinglists.html#foundation-announce&lt;/a&gt; - the Apache Foundation announcements mailing list&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/&quot;&gt;https://blogs.apache.org/&lt;/a&gt;; &lt;a href=&quot;https://blogs.apache.org/planet/feed/entries/rss&quot;&gt;https://blogs.apache.org/planet/feed/entries/rss&lt;/a&gt; - The set of Apache Foundation blogs&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Hadoop</title><link>https://ondataengineering.net/technologies/apache-hadoop/</link><description> &lt;p&gt;A distributed storage and compute platform consisting of a distributed filesystem (HDFS) and a cluster workload and resource management layer (YARN), along with MapReduce, a solution built on HDFS and YARN for massive scale parallel processing of data. Has an extensive ecosystem of compatible technologies. An Apache Open Source project, started in January 2006 as a Lucene sub-project, becoming a top level project in January 2008, with a 1.0 release in December 2011 (containing HDFS and MapReduce), and a 2.2 release (the first 2.x GA release) in October 2013 (adding YARN). Work is currently underway to split out the data storage layer of HDFS (the HDDS sub-project) and to implement an object store on top of this that can co-exist with HDFS (the Ozone sub-project). Very active, with a deep and broad range of contributors, and backing from multiple commercial vendors.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Hadoop&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;September 2018 - v3.1&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Sub-projects&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; Apache Hadoop&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-hadoop/hdfs/&quot;&gt;HDFS&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A highly resilient distributed cluster file system proven at extreme scale. Consists of a single NameNode service (that's responsible for all metadata management, including the filesystem namespace and block management) plus DataNode services that run on all storage nodes (that manage block IO). Supports NameNode high availability, metadata resilience (via a transaction log), data resilience (via block replication or erasure coding), user authentication, extended ACLs, snapshots, quotas, central caching, a REST API, an NFS gateway, rolling upgrades, rack awareness, transparent encryption, NameNode federation (support for multiple independant NameNodes on the same cluster serving different namespaces) and support for heterogeneous storage. Part of the original Hadoop code base, becoming an Apache Hadoop sub-project in July 2009. Currently being updated to run over the new HDDS (Hadoop Distributed Data Storage) layer, moving block management from the NameNode to a new Storage Container Manager to increase scalability.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Hadoop&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-hadoop/map-reduce/&quot;&gt;MapReduce&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A data transformation and aggregation technology proven at extreme scale that works on key value pairs and consists of three transformation stages - map (a general transformation of the input key value pairs), shuffle (brings all pairs with the same key together) and reduce (an aggregation of all pairs with the same key). Part of the original Hadoop code base, becoming an Apache Hadoop sub-project in July 2009.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Hadoop&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-hadoop/yarn/&quot;&gt;YARN&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Resource management and job scheduling &amp; monitoring for the Hadoop ecosystem. Includes support for capacity guarantees amongst other scheduling options, long running services, GPU and FPGA scheduling and isolation and experimental support for launching applications within docker containers. Added as an Apache Hadoop sub-project as part of Hadoop 2.x (with a GA release as part of 2.2 in October 2013) having been started in January 2008.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Hadoop&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-hadoop/hdds/&quot;&gt;HDDS&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A common distributed and resilient block storage layer that will eventually underpin HDFS and Ozone, delivering increased scalability. Implemented as a Storage Container Manager (SCM) service (that performs block management) and DataNode services (inherited from HDFS that run on storage nodes and manage block IO). Blocks are arranged into containers (with the replication strategy defined at the container level). Currently under active development as part of the development of Ozone. Previously known as HDSL (Hadoop Distributed Storage Layer)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Hadoop&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-hadoop/ozone/&quot;&gt;Ozone&lt;/a&gt; &lt;/td&gt; &lt;td&gt;An object store built on top of the new Hadoop HDDS block storage layer that can co-exist with HDFS. Implemented as an Ozone Manager (OM) service that manages the object store namespace, utilising the HDDS Storage Container Manager for block management. Objects are arranged into buckets, which themselves are arranged into volumes. Supports consistent writes, an RPC API, an Amazon S3 compatible REST API, a CLI, a load generation tool (Freon, previously Corona), and an Hadoop Compatible File System (OzoneFS), with a stated plan for mountable LUN storage (Quadra). Originally announced in October 2014, re-invigorated under the Hortonwworks Open Hybrid Architecture Initiative in September 2018, and currently under active development with a suggested release as part of HDP 3.2.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;, &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;Cloudera CDH&lt;/a&gt;, &lt;a href=&quot;/technologies/amazon-emr/&quot;&gt;Amazon EMR&lt;/a&gt;, &lt;a href=&quot;/technologies/google-cloud-dataproc/&quot;&gt;Google Cloud DataProc&lt;/a&gt;, &lt;a href=&quot;/technologies/qubole-data-service/&quot;&gt;Qubole Data Service&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.8&lt;/td&gt; &lt;td&gt;2017-03-22&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://hadoop.apache.org/docs/r2.8.0/index.html&quot;&gt;summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Note that 2.8.2 is the first GA version for production use&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.9&lt;/td&gt; &lt;td&gt;2017-11-17&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://hadoop.apache.org/docs/r2.9.0/index.html&quot;&gt;summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;td&gt;2017-12-14&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://hadoop.apache.org/docs/r3.0.0/index.html&quot;&gt;summary&lt;/a&gt;; &lt;a href=&quot;https://blogs.apache.org/foundation/entry/the-apache-software-foundation-announces26&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;3.1&lt;/td&gt; &lt;td&gt;2018-04-06&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://hadoop.apache.org/docs/r3.1.0/index.html&quot;&gt;summary&lt;/a&gt;; &lt;a href=&quot;https://blogs.apache.org/hadoop/entry/announce-apache-hadoop-3-1&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;https://hortonworks.com/blog/apache-hadoop-3-1-giant-leap-big-data/&quot;&gt;Hortonworks post&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Support for containerised workloads, GPU/FPGA support&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://hadoop.apache.org/&quot;&gt;http://hadoop.apache.org/&lt;/a&gt; - Project Homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://hadoop.apache.org/releases.html&quot;&gt;http://hadoop.apache.org/releases.html&lt;/a&gt; - full release history&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://medium.com/@markobonaci/the-history-of-hadoop-68984a11704&quot;&gt;https://medium.com/@markobonaci/the-history-of-hadoop-68984a11704&lt;/a&gt; - history of Hadoop&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/apache/hadoop/&quot;&gt;https://hortonworks.com/apache/hadoop/&lt;/a&gt; - Hortonworks information&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/products/open-source/apache-hadoop.html&quot;&gt;https://www.cloudera.com/products/open-source/apache-hadoop.html&lt;/a&gt; - Cloudera information&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://hadoop.apache.org/&quot;&gt;http://hadoop.apache.org/&lt;/a&gt; - project updates and releases&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>HDFS</title><link>https://ondataengineering.net/technologies/apache-hadoop/hdfs/</link><description> &lt;p&gt;A highly resilient distributed cluster file system proven at extreme scale. Consists of a single NameNode service (that's responsible for all metadata management, including the filesystem namespace and block management) plus DataNode services that run on all storage nodes (that manage block IO). Supports NameNode high availability, metadata resilience (via a transaction log), data resilience (via block replication or erasure coding), user authentication, extended ACLs, snapshots, quotas, central caching, a REST API, an NFS gateway, rolling upgrades, rack awareness, transparent encryption, NameNode federation (support for multiple independant NameNodes on the same cluster serving different namespaces) and support for heterogeneous storage. Part of the original Hadoop code base, becoming an Apache Hadoop sub-project in July 2009. Currently being updated to run over the new HDDS (Hadoop Distributed Data Storage) layer, moving block management from the NameNode to a new Storage Container Manager to increase scalability.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Hadoop Distributed File System&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Apache Hadoop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;September 2018&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;hdfs-ecosystem&quot;&gt;HDFS Ecosystem&lt;/h2&gt; &lt;p&gt;&lt;img src=&quot;/images/hdfs-ecosystem.png&quot; alt=&quot;HDFS Ecosystem&quot; /&gt;&lt;/p&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html&quot;&gt;http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html&lt;/a&gt; - HDFS documentation home&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://bradhedlund.com/2011/09/10/understanding-hadoop-clusters-and-the-network/&quot;&gt;http://bradhedlund.com/2011/09/10/understanding-hadoop-clusters-and-the-network/&lt;/a&gt; - good intro the the architecture of HDFS&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-10419&quot;&gt;https://issues.apache.org/jira/browse/HDFS-10419&lt;/a&gt; - JIRA ticket for moving to HDFS to run over HDDS, including design doc&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/apache/hdfs/&quot;&gt;https://hortonworks.com/apache/hdfs/&lt;/a&gt; - Hortonworks information&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/products/open-source/apache-hadoop/hdfs-mapreduce-yarn.html&quot;&gt;https://www.cloudera.com/products/open-source/apache-hadoop/hdfs-mapreduce-yarn.html&lt;/a&gt; - Cloudera information&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>MapReduce</title><link>https://ondataengineering.net/technologies/apache-hadoop/map-reduce/</link><description> &lt;p&gt;A data transformation and aggregation technology proven at extreme scale that works on key value pairs and consists of three transformation stages - map (a general transformation of the input key value pairs), shuffle (brings all pairs with the same key together) and reduce (an aggregation of all pairs with the same key). Part of the original Hadoop code base, becoming an Apache Hadoop sub-project in July 2009.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Apache Hadoop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;December 2016&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/mapr-converged-data-platform/&quot;&gt;MapR Converged Data Platform&lt;/a&gt;, &lt;a href=&quot;/technologies/cloudera-altus/data-engineering/&quot;&gt;Cloudera Altus Data Engineering&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html&quot;&gt;http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html&lt;/a&gt; - MapReduce tutorial and documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/apache/mapreduce/&quot;&gt;https://hortonworks.com/apache/mapreduce/&lt;/a&gt; - Hortonworks information&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/products/open-source/apache-hadoop/hdfs-mapreduce-yarn.html&quot;&gt;https://www.cloudera.com/products/open-source/apache-hadoop/hdfs-mapreduce-yarn.html&lt;/a&gt; - Cloudera information&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>YARN</title><link>https://ondataengineering.net/technologies/apache-hadoop/yarn/</link><description> &lt;p&gt;Resource management and job scheduling &amp; monitoring for the Hadoop ecosystem. Includes support for capacity guarantees amongst other scheduling options, long running services, GPU and FPGA scheduling and isolation and experimental support for launching applications within docker containers. Added as an Apache Hadoop sub-project as part of Hadoop 2.x (with a GA release as part of 2.2 in October 2013) having been started in January 2008.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Yet Another Resource Negotiator&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Apache Hadoop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;December 2016&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Add ons&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-slider/&quot;&gt;Apache Slider (retired)&lt;/a&gt;, &lt;a href=&quot;/technologies/llama/&quot;&gt;Llama&lt;/a&gt;, Apache REEF, Apache Twill&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Compatible technologies&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-myriad/&quot;&gt;Apache Myriad&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/mapr-converged-data-platform/&quot;&gt;MapR Converged Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;yarn-ecosystem&quot;&gt;YARN Ecosystem&lt;/h2&gt; &lt;p&gt;&lt;img src=&quot;/images/yarn-ecosystem.png&quot; alt=&quot;YARN Ecosystem&quot; /&gt;&lt;/p&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html&quot;&gt;http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html&lt;/a&gt; - YARN architecture overview and documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/apache/yarn/&quot;&gt;https://hortonworks.com/apache/yarn/&lt;/a&gt; - Hortonworks information&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/products/open-source/apache-hadoop/hdfs-mapreduce-yarn.html&quot;&gt;https://www.cloudera.com/products/open-source/apache-hadoop/hdfs-mapreduce-yarn.html&lt;/a&gt; - Cloudera information&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Flume</title><link>https://ondataengineering.net/technologies/apache-flume/</link><description> &lt;p&gt;Specialist technology for the continuous movement of data using a set of independent agents connected together into pipelines. Supports a wide range of sources, targets and buffers (channels), along with the ability to chain agents together and to modify and drop events in-flight. Designed to be highly reliable, and to support reconfiguration without the need for a restart. Heavily integrated with the Hadoop ecosystem. An Apache project, donated by Cloudera in June 2011, graduating in June 2012, with a v1.2 release (the first considered ready for production use) in July 2012. Java based, with commercial support available as part of most Hadoop distributions.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Flume&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;October 2017 - v1.8&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;Cloudera CDH&lt;/a&gt;, &lt;a href=&quot;/technologies/mapr-expansion-pack/&quot;&gt;MapR Expansion Pack&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.8&lt;/td&gt; &lt;td&gt;2017-10-04&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://flume.apache.org/&quot;&gt;announcement&lt;/a&gt; &lt;a href=&quot;http://flume.apache.org/releases/1.8.0.html&quot;&gt;release page&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://flume.apache.org/&quot;&gt;http://flume.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://flume.apache.org/FlumeUserGuide.html&quot;&gt;http://flume.apache.org/FlumeUserGuide.html&lt;/a&gt; - user guide&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/apache/flume/&quot;&gt;https://hortonworks.com/apache/flume/&lt;/a&gt; - Hortonworks information&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.4/bk_flume-component-guide/content/index.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.4/bk_flume-component-guide/content/index.html&lt;/a&gt; - Hortonworks 2.6.4 documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/products/open-source/apache-hadoop/apache-flume.html&quot;&gt;https://www.cloudera.com/products/open-source/apache-hadoop/apache-flume.html&lt;/a&gt; - Cloudera information&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://flume.apache.org/&quot;&gt;http://flume.apache.org/&lt;/a&gt; - project updates and releases&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache HBase</title><link>https://ondataengineering.net/technologies/apache-hbase/</link><description> &lt;p&gt;NoSQL wide-column datastore based on Google BigTable. Data for an HBase table is distributed across regions, with each region made up of a store per column family (with stores either hosted in memory or on disk), with regions served and managed by region servers, which in turn are monitored and managed by master servers (which are also responsible for metadata changes and can run in a multi-master configuration), with the architecture supporting horizontal scalability and high availability. Supports strongly consistent reads and writes (with all reads and writes going through a single region server), with the option to perform non consistent reads from data replicated between multiple region servers given more consistent performance during region server failure. Supports get, put (insert/update), scan (iterating over a set of rows) and delete operations, the option to bulk load via Map Reduce and Spark, and the option to execute custom code within the HBase cluster via co-processors (observer co-processors execute either before or after specific events, endpoint co-processors allow execution of batch analytics). Also supports medium sized binary objects (up to 10Mb), versioning and fine grained RBAC security controls, including visibility expressions at the cell level for authorising end user access. Runs on Hadoop and HDFS, and is heavily integrated with the Hadoop ecosystem. Supports a CLI plus Java, Thrift and REST API, along with MapReduce and Spark integration as both a source and sink. An Apache project, first released as part of Hadoop 0.15 in October 2007 before graduating as a top level project in May 2010. Java based, with commercial support available as part of most Hadoop distributions.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;HBase&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;July 2018 - v2.1&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Add ons&lt;/td&gt;&lt;td&gt;Apache Omid, Apache Tephra&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;, &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;Cloudera CDH&lt;/a&gt;, &lt;a href=&quot;/technologies/amazon-emr/&quot;&gt;Amazon EMR&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.3&lt;/td&gt; &lt;td&gt;2017-01-17&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/www-announce/201701.mbox/%3CCAHxLZBWn6eLPTjLG7NxpVNQzf-M1T984N90W9bswSUVDk5vYPA@mail.gmail.com%3E&quot;&gt;announcement&lt;/a&gt; &lt;a href=&quot;https://www.infoq.com/news/2017/01/apache-hbase-1.3&quot;&gt;infoq summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.4&lt;/td&gt; &lt;td&gt;2017-12-19&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://mail-archives.us.apache.org/mod_mbox/www-announce/201712.mbox/%3CCA+RK=_AU+tB=7SU1HRbeKVEd-sKA5WcJo3oa43vQ6PMB3L9pgQ@mail.gmail.com%3E&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.0&lt;/td&gt; &lt;td&gt;2018-05-01&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://mail-archives.us.apache.org/mod_mbox/www-announce/201804.mbox/%3CCADcMMgHzTOesqmqEKV+CrGwh6R4XsCaT2DmZt5vG1jRW6XxpaA@mail.gmail.com%3E&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;https://www.slideshare.net/enissoz/meet-hbase-20&quot;&gt;2.0 details presentation&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.1&lt;/td&gt; &lt;td&gt;2018-07-22&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/www-announce/201807.mbox/%3CCAAAYAnOObYM9b-6Etqpuxti-FTkC_dhiTD8Du+WZ2vC=L5E4Yg@mail.gmail.com%3E&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://hbase.apache.org/&quot;&gt;http://hbase.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://hbase.apache.org/book.html&quot;&gt;http://hbase.apache.org/book.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://jimbojw.com/wiki/index.php?title=Understanding_Hbase_and_BigTable&quot;&gt;http://jimbojw.com/wiki/index.php?title=Understanding_Hbase_and_BigTable&lt;/a&gt; - HBase primer&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/hbase/entry/start_of_a_new_era&quot;&gt;https://blogs.apache.org/hbase/entry/start_of_a_new_era&lt;/a&gt; - introduction to HBase versioning scheme&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/apache/hbase/&quot;&gt;https://hortonworks.com/apache/hbase/&lt;/a&gt; - Hortonworks information&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.4/bk_data-access/content/ch05.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.4/bk_data-access/content/ch05.html&lt;/a&gt; - Hortonworks 2.6.4 documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/products/open-source/apache-hadoop/apache-hbase.html&quot;&gt;https://www.cloudera.com/products/open-source/apache-hadoop/apache-hbase.html&lt;/a&gt; - Cloudera information&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/documentation/enterprise/latest/topics/hbase.html&quot;&gt;https://www.cloudera.com/documentation/enterprise/latest/topics/hbase.html&lt;/a&gt; - Cloudera documentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/hbase/&quot;&gt;https://blogs.apache.org/hbase/&lt;/a&gt; - Apache HBase Blog&lt;/li&gt; &lt;li&gt;HBase release announcements only appear to be available via the Apache announcements mailing list&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Hive</title><link>https://ondataengineering.net/technologies/apache-hive/</link><description> &lt;p&gt;Technology that supports the exposure of data in Hadoop as structured tables and the execution of analytical SQL queries over these. Consists of a number of distinct components (that we treat as sub-projects) including Hive Metastore (stores the definitions of the structured tables), Hive Server (supports the execution of analytical SQL queries as MapReduce, Spark or Tez jobs) and HCatalog (allows MapReduce and Pig jobs to read and write Hive tables). First released by Facebook as an Hadoop contrib module in September 2008, becoming an Hadoop sub-project in November 2008, and a top level Apache project in September 2010, following a first official stable release (0.3) in April 2009. Java based, under active development from a number of large commercial sponsors, with commercial support available as part of most Hadoop distributions.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Hive&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;August 2018 - v3.1&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Sub-projects&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; Apache Hive&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-hive/hcatalog/&quot;&gt;HCatalog&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Libraries for MapReduce and Pig to read and write data to and from Hive tables, albeit with some limitations. Also supports a CLI for querying and updating the Hive Metastore, however this doesn't support the full range of Hive DDL commands. Includes WebHCat, a REST API over the HCatalog CLI that also supports the execution of MapReduce, Pig, Hive and Sqoop jobs. Donated to the Apache foundation by Yahoo in March 2011, had WebHCat folded in in July 2012, graduating as a top level project in February 2013, but then almost immediately was folded into Hive in March 2013 as part of the Hive 0.11 release. Has seem limited development since this time.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Hive&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-hive/hive-metastore/&quot;&gt;Hive Metastore&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A metadata service that allows structured tables to be defined over files in HDFS (and also HBase or Accumulo), providing an API that allows the metadata to be queried and updated by other tools including Impala, Spark SQL or RecordService. Supports partitioned and clustered tables, as well as complex field types such as arrays, maps and structs. Backed by a relational database (either MySQL, Postgres and Oracle). Part of the original Hive code base.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Hive&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-hive/hive-server/&quot;&gt;Hive Server&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Supports the execution of SQL queries over data in HDFS based on tables defined in the Hive Metastore, as well as DDL to query and update the Hive Metastore. Focus is on analytical (OLAP) use cases, with some support for batch updates to data. Originally executed queries as MapReduce jobs, but significant investment from has seen support for executing queries as Spark and as Tez jobs, with work underway to support sub second query times using Tez (Hive LLAP). Recent changes have also seen it achieve significant SQL compliance, with support for SQL:2011 analytical functions on-going. Accepts queries over an API with JDBC and ODBC drivers available, and includes Beeline, a command line JDBC client. Technically referred to as Hive Server 2, and was introduced in Hive 0.11 as a replacement for the original Hive Server to address a number of concurrency and security issues.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;, &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;Cloudera CDH&lt;/a&gt;, &lt;a href=&quot;/technologies/mapr-expansion-pack/&quot;&gt;MapR Expansion Pack&lt;/a&gt;, &lt;a href=&quot;/technologies/cloudera-altus/data-engineering/&quot;&gt;Cloudera Altus Data Engineering&lt;/a&gt;, &lt;a href=&quot;/technologies/amazon-emr/&quot;&gt;Amazon EMR&lt;/a&gt;, &lt;a href=&quot;/technologies/google-cloud-dataproc/&quot;&gt;Google Cloud DataProc&lt;/a&gt;, &lt;a href=&quot;/technologies/qubole-data-service/&quot;&gt;Qubole Data Service&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.2&lt;/td&gt; &lt;td&gt;2017-07-25&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://hive.apache.org/downloads.html#25-july-2017-release-220-available&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.3&lt;/td&gt; &lt;td&gt;2017-07-17&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://hive.apache.org/downloads.html#17-july-2017-release-230-available&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;td&gt;2018-05-21&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://hive.apache.org/downloads.html#21-may-2018-release-300-available&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Support for Hadoop 3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;3.1&lt;/td&gt; &lt;td&gt;2018-07-30&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://hive.apache.org/downloads.html#30-july-2018-release-310-available&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://hive.apache.org/&quot;&gt;http://hive.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/Home&quot;&gt;https://cwiki.apache.org/confluence/display/Hive/Home&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/apache/hive/&quot;&gt;https://hortonworks.com/apache/hive/&lt;/a&gt; - Hortonworks information&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.4/bk_data-access/content/ch_using-hive.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.4/bk_data-access/content/ch_using-hive.html&lt;/a&gt; - Hortonworks 2.6.4 documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/products/open-source/apache-hadoop/apache-hive.html&quot;&gt;https://www.cloudera.com/products/open-source/apache-hadoop/apache-hive.html&lt;/a&gt; - Cloudera information&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/documentation/enterprise/latest/topics/hive.html&quot;&gt;https://www.cloudera.com/documentation/enterprise/latest/topics/hive.html&lt;/a&gt; - Cloudera documentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://hive.apache.org/downloads.html&quot;&gt;http://hive.apache.org/downloads.html&lt;/a&gt; - details of new releases&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/category/hive/&quot;&gt;http://blog.cloudera.com/blog/category/hive/&lt;/a&gt; - Cloudera Hive News&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://hortonworks.com/blog/category/hive/&quot;&gt;http://hortonworks.com/blog/category/hive/&lt;/a&gt; - Hortonworks Hive News&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>HCatalog</title><link>https://ondataengineering.net/technologies/apache-hive/hcatalog/</link><description> &lt;p&gt;Libraries for MapReduce and Pig to read and write data to and from Hive tables, albeit with some limitations. Also supports a CLI for querying and updating the Hive Metastore, however this doesn't support the full range of Hive DDL commands. Includes WebHCat, a REST API over the HCatalog CLI that also supports the execution of MapReduce, Pig, Hive and Sqoop jobs. Donated to the Apache foundation by Yahoo in March 2011, had WebHCat folded in in July 2012, graduating as a top level project in February 2013, but then almost immediately was folded into Hive in March 2013 as part of the Hive 0.11 release. Has seem limited development since this time.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;WebHCat&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Apache Hive&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/amazon-emr/&quot;&gt;Amazon EMR&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/HCatalog&quot;&gt;https://cwiki.apache.org/confluence/display/Hive/HCatalog&lt;/a&gt; - HCatalog documentation home&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/WebHCat&quot;&gt;https://cwiki.apache.org/confluence/display/Hive/WebHCat&lt;/a&gt; - WebHCat documentation home&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.4/bk_data-access/content/ch_using-hcatalog.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.4/bk_data-access/content/ch_using-hcatalog.html&lt;/a&gt; - Hortonworks 2.6.4 documentation&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hive Metastore</title><link>https://ondataengineering.net/technologies/apache-hive/hive-metastore/</link><description> &lt;p&gt;A metadata service that allows structured tables to be defined over files in HDFS (and also HBase or Accumulo), providing an API that allows the metadata to be queried and updated by other tools including Impala, Spark SQL or RecordService. Supports partitioned and clustered tables, as well as complex field types such as arrays, maps and structs. Backed by a relational database (either MySQL, Postgres and Oracle). Part of the original Hive code base.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Apache Hive&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin&quot;&gt;https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin&lt;/a&gt; - documentation home&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hive Server</title><link>https://ondataengineering.net/technologies/apache-hive/hive-server/</link><description> &lt;p&gt;Supports the execution of SQL queries over data in HDFS based on tables defined in the Hive Metastore, as well as DDL to query and update the Hive Metastore. Focus is on analytical (OLAP) use cases, with some support for batch updates to data. Originally executed queries as MapReduce jobs, but significant investment from has seen support for executing queries as Spark and as Tez jobs, with work underway to support sub second query times using Tez (Hive LLAP). Recent changes have also seen it achieve significant SQL compliance, with support for SQL:2011 analytical functions on-going. Accepts queries over an API with JDBC and ODBC drivers available, and includes Beeline, a command line JDBC client. Technically referred to as Hive Server 2, and was introduced in Hive 0.11 as a replacement for the original Hive Server to address a number of concurrency and security issues.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Apache Hive&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2013/07/how-hiveserver2-brings-security-and-concurrency-to-apache-hive/&quot;&gt;http://blog.cloudera.com/blog/2013/07/how-hiveserver2-brings-security-and-concurrency-to-apache-hive/&lt;/a&gt; - introduction to Hive Server 2&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Solr</title><link>https://ondataengineering.net/technologies/apache-solr/</link><description> &lt;p&gt;A search server built on Apache Lucene with a REST-like API for loading and searching data. Supports a distributed deployment (SolrCloud) that can run over HDFS on an Hadoop cluster. Includes an administration web interface, an extensible plugin architecture, support for schemaless indexing, faceted, grouped and clustered results, hit highlighting, geo-spacial and graph searches, near real time indexing and searching, (experimental) streaming expressions for parallel compute (including support for MapReduce and SQL) and broad authentication and security capabilities. A sub-project of the Apache Lucene project, originally donated to the Apache foundation by CNET Networks in January 2006, graduating as a top level project in January 2007, before merging with the Lucene project in March 2010. Java based, with commercial support available as part of most Hadoop distributions (although this is bundled as Cloudera Search with CDH and HDP Search with HDP), as well as from Lucidworks.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Solr&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;, Lucidworks&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;September 2018 - v7.5&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform-search/&quot;&gt;Hortonworks Data Platform Search&lt;/a&gt;, &lt;a href=&quot;/technologies/cloudera-search/&quot;&gt;Cloudera Search&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6.4&lt;/td&gt; &lt;td&gt;2017-01-23&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/www-announce/201701.mbox/%3CCAKUpjcSRFuD_hD%2B-Zj4fbjr-dL1tA8AZO8q6An6kgJGkTB7UbQ@mail.gmail.com%3E&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;http://yonik.com/solr-6-4/&quot;&gt;Solr ‘n Stuff summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6.5&lt;/td&gt; &lt;td&gt;2017-03-27&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/www-announce/201703.mbox/%3CCAKUpjcQijk5pwbAVAW6Zfc1fz-cARMDUJyW5L67RGov%2BTcd%2B5w%40mail.gmail.com%3E&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;http://yonik.com/solr-6-5/&quot;&gt;Solr ‘n Stuff summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6.6&lt;/td&gt; &lt;td&gt;2017-06-06&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/www-announce/201706.mbox/%3CCAHPRk5GFOA=gZOzU6PVJ+N4Mmj64V3UKthP+45xQmp08mCN8nw@mail.gmail.com%3E&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;http://yonik.com/solr-6-6/&quot;&gt;Solr ‘n Stuff summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;7.0&lt;/td&gt; &lt;td&gt;2017-09-20&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/www-announce/201709.mbox/%3CCAKiERN4YkSkh0BSRn6Y1N8C1gweAWCLjoLTg8a2gJACmh94bVg@mail.gmail.com%3E&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;http://yonik.com/solr-7/&quot;&gt;Solr ‘n Stuff summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;7.1&lt;/td&gt; &lt;td&gt;2017-10-17&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/www-announce/201710.mbox/%3CCAOOKt52VryT_dp-6+GWFp521hpRhghemOVoG8FzqCV8eZhJF-Q@mail.gmail.com%3E&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;http://yonik.com/solr-7-1/&quot;&gt;Solr ‘n Stuff summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;7.2&lt;/td&gt; &lt;td&gt;2017-12-21&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/www-announce/201712.mbox/%3CCAPsWd+NrEtEuegF4UhuDPeu-mJWo9FyREBaY0eHUkxJExEd=7w@mail.gmail.com%3E&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;http://yonik.com/solr-7-2/&quot;&gt;Solr ‘n Stuff summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;7.3&lt;/td&gt; &lt;td&gt;2018-04-04&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://lucene.apache.org/solr/news.html#4-april-2018-apache-solrtm-730-available&quot;&gt;news&lt;/a&gt;; &lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/lucene-solr-user/201804.mbox/%3CCAFv9U3ndd9KYef7KoLtTX96YHqF=YFMW1POxuCRbLnH=SAPYUA@mail.gmail.com%3E&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;7.4&lt;/td&gt; &lt;td&gt;2018-06-27&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://lucene.apache.org/solr/news.html#27-june-2018-apache-solrtm-740-available&quot;&gt;news&lt;/a&gt;; &lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/www-announce/201806.mbox/%3CCAPsWd+ONJ7eBi0EONjdjWRTuVgMbNDuN2uPU2uXs-31L_z70UQ@mail.gmail.com%3E&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;7.5&lt;/td&gt; &lt;td&gt;2018-09-24&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://lucene.apache.org/solr/news.html#24-september-2018-apache-solrtm-750-available&quot;&gt;news&lt;/a&gt;; &lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/lucene-dev/201809.mbox/%3CCAKUpjcRky7yz_fy0+Fr76KDiuz-ykmTpkv7D62Xq7iWwNMmnow@mail.gmail.com%3E&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://lucene.apache.org/solr&quot;&gt;http://lucene.apache.org/solr&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://lucene.apache.org/solr/features.html&quot;&gt;http://lucene.apache.org/solr/features.html&lt;/a&gt; - good summary of Solr features&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.apache.org/dyn/closer.lua/lucene/solr/ref-guide/&quot;&gt;https://www.apache.org/dyn/closer.lua/lucene/solr/ref-guide/&lt;/a&gt; - PDF download of documentation for latest release&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/solr/&quot;&gt;https://cwiki.apache.org/confluence/display/solr/&lt;/a&gt; - online working version of documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Apache_Solr&quot;&gt;https://en.wikipedia.org/wiki/Apache_Solr&lt;/a&gt; - history of Solr&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/apache/solr/&quot;&gt;https://hortonworks.com/apache/solr/&lt;/a&gt; - Hortonworks information&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/products/open-source/apache-hadoop/apache-solr.html&quot;&gt;https://www.cloudera.com/products/open-source/apache-hadoop/apache-solr.html&lt;/a&gt; - Cloudera information&lt;/li&gt; &lt;li&gt;For Hortonworks and Cloudera documentation, see &lt;a href=&quot;/technologies/cloudera-search/&quot;&gt;Cloudera Search&lt;/a&gt; and &lt;a href=&quot;/technologies/hortonworks-data-platform-search/&quot;&gt;HDP Search&lt;/a&gt; respectively.&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://lucene.apache.org/solr/news.html&quot;&gt;http://lucene.apache.org/solr/news.html&lt;/a&gt; - project news&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://yonik.com/&quot;&gt;http://yonik.com/&lt;/a&gt; - details of new features from the creator of Solr&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Spark</title><link>https://ondataengineering.net/technologies/apache-spark/</link><description> &lt;p&gt;A high performance general purpose distributed data processing engine based on directed acyclic graphs that primarily runs in memory, but can spill to disk if required, and which supports processing applications written in Java, Scala, Python and R (SparkR). Includes a number of sub-projects that support more specialised analytics including Spark SQL (batch and streaming analytics using declarative logic over structured data), Spark Streaming (micro-batch stream processing), MLlib (machine learning) and GraphX (graph analytics). Requires a cluster manager (YARN, EC2, Kubernetes and Mesos are supported as well as standalone clusters) and can access data in a wide range of technologies (including HDFS, other Hadoop data sources, relational databases and NoSQL databases). An Apache project, originally started at UC Berkley in 2009, open sourced in 2010, and donated to the Apache foundation in June 2013, graduating in February 2014. v1.0 was released in May 2014, with a v2.0 release in July 2016. Java based, with development led by Databricks (who sell a Spark hosted service), and with commercial support available as part of most Hadoop distributions.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Spark&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;March 2018 - v2.3&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Sub-projects&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; Apache Spark&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-spark/graphx/&quot;&gt;GraphX&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Spark library for processing graphs and running graph algorithms, based on graph model that supports directional edges with properties on both vertices and edges. Graphs are constructed from a pair of collections representing the edges and vertex, either directly from data on disk using builders, or prepared using other Spark functionality, with the ability to also view the graph as a set of triples. Supports a range of graph operations, as well as an optimised variant of the Pregel API, and a set of out of the box algorithms (including PageRank, connected components and triangle count). First introduced in Spark 0.9, with a production release as part of Spark 1.2, however has seen almost no new functionality since then.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Spark&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-spark/mllib/&quot;&gt;MLlib&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Spark library for running Machine Learning algorithms. Supports a range of algorithms (including classifications, regressions, decision trees, recommendations, clustering and topic modelling), including iterative algorithms. As of Spark 2.0 utilises a DataFrame (Spark SQL) based API, with the original RDD based API now in maintenance only. First introduced in Spark 0.8 after being collaboratively developed with the UC Berkeley MLbase project, and still under active development.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Spark&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-spark/spark-sql/&quot;&gt;Spark SQL&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Spark library for processing structured data, using either SQL statements or a DataFrame API. Supports querying and writing to local datasets (including JSON, Parquet, Avro, Orc and CSV) as well as external data sources (including Hive and JDBC), including the ability to query across data sources. Includes Catalyst, a cost based optimiser that turns high level operations into low level Spark DAGs for execution. Also includes a Hive compatible Thrift JDBC/ODBC server that's compatible with Beeline and the Hive JDBC and ODBC drivers, and a REPL CLI for interactive queries. Introduced in Spark 1.0 with a production release in Spark 1.3, with substantially improved SQL functionalities in Spark 2.0.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Spark&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-spark/spark-streaming/&quot;&gt;Spark Streaming&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Spark library for continuous stream processing, using a DStream (discretized stream) API. Uses a micro-batch execution model leveraging core Spark to execute the specified logic against each micro-batch (a DStream is a sequence of Spark RDDs), with the ability to also use other Spark batch operations (including Spark SQL and MLlib) against each micro-batch. This model also provides fault tolerance through exactly-once processing semantics. Supports a number of data sources (including HDFS, sockets, Flume, Kafka, Kinesis and messaging buses), as well as functions to maintain state and to execute windowed operations. First introduced in Spark 0.7, with a production release as part of Spark 0.9, however development appears to be largely stopped following the introduction of Structured Streaming in Spark 2.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Spark&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-spark/structured-streaming/&quot;&gt;Structured Streaming&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Extension to the Spark SQL DataFrame API to allow Spark SQL queries to be executed over streams of data, with the engine continuously updating and maintaining the result as new data arrives. Uses the full Spark SQL engine (including the Catalyst optimiser), and supports end-to-end exactly-once semantics via checkpointing when sources have sequential offsets. Supports aggregations over sliding event-time windows, including support for late data and watermarking. Introduced in Spark 2.0 with a production release in Spark 2.2.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;, &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;Cloudera CDH&lt;/a&gt;, &lt;a href=&quot;/technologies/mapr-expansion-pack/&quot;&gt;MapR Expansion Pack&lt;/a&gt;, &lt;a href=&quot;/technologies/cloudera-altus/data-engineering/&quot;&gt;Cloudera Altus Data Engineering&lt;/a&gt;, &lt;a href=&quot;/technologies/amazon-emr/&quot;&gt;Amazon EMR&lt;/a&gt;, &lt;a href=&quot;/technologies/google-cloud-dataproc/&quot;&gt;Google Cloud DataProc&lt;/a&gt;, &lt;a href=&quot;/technologies/qubole-data-service/&quot;&gt;Qubole Data Service&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.2&lt;/td&gt; &lt;td&gt;2017-07-11&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://spark.apache.org/releases/spark-release-2-2-0.html&quot;&gt;release notes&lt;/a&gt;; &lt;a href=&quot;https://databricks.com/blog/2017/07/11/introducing-apache-spark-2-2.html&quot;&gt;databricks view&lt;/a&gt; &lt;a href=&quot;https://databricks.com/blog/2017/08/31/cost-based-optimizer-in-apache-spark-2-2.html&quot;&gt;cost based optimiser&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.3&lt;/td&gt; &lt;td&gt;2018-02-28&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://spark.apache.org/releases/spark-release-2-3-0.html&quot;&gt;release notes&lt;/a&gt;; &lt;a href=&quot;https://databricks.com/blog/2018/02/28/introducing-apache-spark-2-3.html&quot;&gt;databricks view&lt;/a&gt;; &lt;a href=&quot;https://databricks.com/blog/2018/03/06/apache-spark-2-3-with-native-kubernetes-support.html&quot;&gt;Kubernetes support&lt;/a&gt;; &lt;a href=&quot;https://databricks.com/blog/2018/03/13/introducing-stream-stream-joins-in-apache-spark-2-3.html&quot;&gt;stream to stream joins&lt;/a&gt;; &lt;a href=&quot;https://databricks.com/blog/2018/03/20/low-latency-continuous-processing-mode-in-structured-streaming-in-apache-spark-2-3-0.html&quot;&gt;continuous streaming&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/&quot;&gt;http://spark.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/&quot;&gt;http://spark.apache.org/docs/latest/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/apache/spark/&quot;&gt;https://hortonworks.com/apache/spark/&lt;/a&gt; - Hortonworks information&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.4/bk_spark-component-guide/content/index.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.4/bk_spark-component-guide/content/index.html&lt;/a&gt; - Hortonworks 2.6.4 documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/products/open-source/apache-hadoop/apache-spark.html&quot;&gt;https://www.cloudera.com/products/open-source/apache-hadoop/apache-spark.html&lt;/a&gt; - Cloudera information&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/documentation/enterprise/latest/topics/spark.html&quot;&gt;https://www.cloudera.com/documentation/enterprise/latest/topics/spark.html&lt;/a&gt; - Cloudera documentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/news/index.html&quot;&gt;http://spark.apache.org/news/index.html&lt;/a&gt; - project news&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://databricks.com/blog/category/engineering&quot;&gt;https://databricks.com/blog/category/engineering&lt;/a&gt; - Databricks engineering blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>GraphX</title><link>https://ondataengineering.net/technologies/apache-spark/graphx/</link><description> &lt;p&gt;Spark library for processing graphs and running graph algorithms, based on graph model that supports directional edges with properties on both vertices and edges. Graphs are constructed from a pair of collections representing the edges and vertex, either directly from data on disk using builders, or prepared using other Spark functionality, with the ability to also view the graph as a set of triples. Supports a range of graph operations, as well as an optimised variant of the Pregel API, and a set of out of the box algorithms (including PageRank, connected components and triangle count). First introduced in Spark 0.9, with a production release as part of Spark 1.2, however has seen almost no new functionality since then.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;August 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/graphx/&quot;&gt;http://spark.apache.org/graphx/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/graphx-programming-guide.html&quot;&gt;http://spark.apache.org/docs/latest/graphx-programming-guide.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>MLlib</title><link>https://ondataengineering.net/technologies/apache-spark/mllib/</link><description> &lt;p&gt;Spark library for running Machine Learning algorithms. Supports a range of algorithms (including classifications, regressions, decision trees, recommendations, clustering and topic modelling), including iterative algorithms. As of Spark 2.0 utilises a DataFrame (Spark SQL) based API, with the original RDD based API now in maintenance only. First introduced in Spark 0.8 after being collaboratively developed with the UC Berkeley MLbase project, and still under active development.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;August 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/mllib//&quot;&gt;http://spark.apache.org/mllib//&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/ml-guide.html&quot;&gt;http://spark.apache.org/docs/latest/ml-guide.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.mlbase.org/&quot;&gt;http://www.mlbase.org/&lt;/a&gt; - the UC Berkeley MLbase project&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Spark SQL</title><link>https://ondataengineering.net/technologies/apache-spark/spark-sql/</link><description> &lt;p&gt;Spark library for processing structured data, using either SQL statements or a DataFrame API. Supports querying and writing to local datasets (including JSON, Parquet, Avro, Orc and CSV) as well as external data sources (including Hive and JDBC), including the ability to query across data sources. Includes Catalyst, a cost based optimiser that turns high level operations into low level Spark DAGs for execution. Also includes a Hive compatible Thrift JDBC/ODBC server that's compatible with Beeline and the Hive JDBC and ODBC drivers, and a REPL CLI for interactive queries. Introduced in Spark 1.0 with a production release in Spark 1.3, with substantially improved SQL functionalities in Spark 2.0.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;August 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/sql/&quot;&gt;http://spark.apache.org/sql/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/sql-programming-guide.html&quot;&gt;http://spark.apache.org/docs/latest/sql-programming-guide.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Spark Streaming</title><link>https://ondataengineering.net/technologies/apache-spark/spark-streaming/</link><description> &lt;p&gt;Spark library for continuous stream processing, using a DStream (discretized stream) API. Uses a micro-batch execution model leveraging core Spark to execute the specified logic against each micro-batch (a DStream is a sequence of Spark RDDs), with the ability to also use other Spark batch operations (including Spark SQL and MLlib) against each micro-batch. This model also provides fault tolerance through exactly-once processing semantics. Supports a number of data sources (including HDFS, sockets, Flume, Kafka, Kinesis and messaging buses), as well as functions to maintain state and to execute windowed operations. First introduced in Spark 0.7, with a production release as part of Spark 0.9, however development appears to be largely stopped following the introduction of Structured Streaming in Spark 2.0&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;August 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/sql/&quot;&gt;http://spark.apache.org/sql/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/streaming-programming-guide.html&quot;&gt;http://spark.apache.org/docs/latest/streaming-programming-guide.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Sqoop</title><link>https://ondataengineering.net/technologies/apache-sqoop/</link><description> &lt;p&gt;Specialist technology for moving bulk data between Hadoop and structured (relational) databases. Command line based, with the ability to import and export data between a range of databases (including mainframe partitioned datasets) and HDFS, Hive, HBase and Accumulo. Executes as MapReduce jobs, supports parallel partitioned unloads, writing to Avro, Sequence File, Parquet and text files, incremental imports and saved jobs that can be shared via a simple metadata store. An Apache project, started in May 2009 as an Hadoop contrib module, migrating to a Cloudera GitHub project in April 2010 (with a v1.0 release shortly after), before being donated to the Apache foundation in June 2011, graduating in March 2012. The last major release (v1.4) was in November 2011, with only minor releases since then. However in January 2012 a significant re-write was announced as part of a proposed v2.0 release to address a number of usability, security and architectural issues. This will introduce a new Sqoop Server and Metadata Repository, supporting both a CLI and web UI, centralising job definitions, database connections and credentials, as well as enabling support for a wider range of connectors including NoSQL databases, Kafka and (S)FTP folders. Java based, with commercial support available as part of most Hadoop distributions.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Sqoop&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v1.4&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;, &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;Cloudera CDH&lt;/a&gt;, &lt;a href=&quot;/technologies/mapr-expansion-pack/&quot;&gt;MapR Expansion Pack&lt;/a&gt;, &lt;a href=&quot;/technologies/amazon-emr/&quot;&gt;Amazon EMR&lt;/a&gt;, &lt;a href=&quot;/technologies/qubole-data-service/&quot;&gt;Qubole Data Service&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://sqoop.apache.org&quot;&gt;http://sqoop.apache.org&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://sqoop.apache.org/docs/&quot;&gt;http://sqoop.apache.org/docs/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/sqoop/entry/apache_sqoop_highlights_of_sqoop&quot;&gt;https://blogs.apache.org/sqoop/entry/apache_sqoop_highlights_of_sqoop&lt;/a&gt; - introduction to Sqoop 2&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/sqoop/entry/apache_sqoop_graduates_from_incubator&quot;&gt;https://blogs.apache.org/sqoop/entry/apache_sqoop_graduates_from_incubator&lt;/a&gt; - early history of Sqoop&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/apache/sqoop/&quot;&gt;https://hortonworks.com/apache/sqoop/&lt;/a&gt; - Hortonworks information&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/products/open-source/apache-hadoop/apache-sqoop.html&quot;&gt;https://www.cloudera.com/products/open-source/apache-hadoop/apache-sqoop.html&lt;/a&gt; - Cloudera information&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://sqoop.apache.org/&quot;&gt;http://sqoop.apache.org/&lt;/a&gt; - details latest release, and hosts release notes for v1.4.0 onwards&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/sqoop/&quot;&gt;https://blogs.apache.org/sqoop/&lt;/a&gt; - project blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Avro</title><link>https://ondataengineering.net/technologies/apache-avro/</link><description> &lt;p&gt;Data serialisation framework that supports both messaging and data storage. Primarily uses a compact binary format but also supports a JSON format. Supports a range of data structures (including records, enumerations, arrays and maps) with APIs for a wide range of both static and dynamically typed languages. Schema based, with schemas primarily specified in JSON, and support for both code generation from schema definitions as well as dynamic runtime usage. Schemas are serialised alongside data, with support for automatic schema resolution if the schema used to read the data differs from that used to write it. Started as an Hadoop sub-project by Cloudera in April 2009, with an initial v1.0 release in July 2009, before becoming a top level Apache project in May 2010. Has seen significant adoption in the Hadoop ecosystem.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Avro&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v1.8&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;Cloudera CDH&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://avro.apache.org/&quot;&gt;http://avro.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://avro.apache.org/docs/current/&quot;&gt;https://avro.apache.org/docs/current/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2009/11/avro-a-new-format-for-data-interchange/&quot;&gt;http://blog.cloudera.com/blog/2009/11/avro-a-new-format-for-data-interchange/&lt;/a&gt; - original introduction to Avro&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces4&quot;&gt;https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces4&lt;/a&gt; - Avro top level project announcement&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://avro.apache.org/releases.html&quot;&gt;http://avro.apache.org/releases.html&lt;/a&gt; - project releases&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Kafka</title><link>https://ondataengineering.net/technologies/apache-kafka/</link><description> &lt;p&gt;Technology for buffering and storing real-time streams of data between producers and consumers, with a focus on high throughput at low latency. Based on a distributed, horizontally scalable architecture, with messages organised into topics which are partitioned and replicated across nodes (called brokers by Kafka) to provide resilience and written to disk to provide persistence. Topics may have multiple producers and consumers, with ability to do fault tolerant reads and to load balance across consumers (consumer groups). Records consist of a key, value and timestamp, with the ability to compact topics to remove updates and deletes by key. Supports rolling upgrades, a full security model (including secure and authenticated connections and ACLs for controlling access to topics), the ability to set quotas (for data produced or consumed), Yammer metrics for both servers and clients, and tools to mirror data to a second cluster (mirror maker) and re-distribute partitions across nodes (for example when adding new nodes). Comes with a Java client, but clients for a wide range of languages are also available. Has two sub-projects (Kafka Connect and Kafka Streams) that are bundled with the main product. Originally developed at LinkedIn, being open sourced in January 2011, before being donated to the Apache Foundation in July 2011. Graduated in October 2012, and although it has not had a v1.0 release is considered production quality and stable. Development is primarily led by Confluent (which was founded by the team that built Kafka at LinkedIn), who have a number of open source and commercial offerings based around Kafka. Commercial support is also available from most Hadoop vendors.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Kafka&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;August 2018 - v2.0&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Sub-projects&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; Apache Kafka&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-kafka/kafka-connect/&quot;&gt;Kafka Connect&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Framework for building scalable and reliable integrations between Kafka and other technologies, either for importing or exporting data. Part of the core Apache Kafka open source technology, connectors are available for a wide range of systems, including Hadoop, relational, NoSQL and analytical databases, search technologies and message queues amongst others, with an API for developing custom connectors. Supports lightweight transformations, and runs separately to Kafka, in either a stand-alone or distributed cluster mode, with a REST API for managing connectors. Introduced in Kafka 0.9, previously known as Copycat&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Kafka&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-kafka/kafka-streams/&quot;&gt;Kafka Streams&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A stream processing technologies that's tightly integrated to Apache Kafka, consuming and publishing events from and to Kafka topics (and potentially writing output to external systems). Based on an event-at-a-time model (i.e. not micro batch), with support for stateful processing, windowing, aggregations, joining and re-processing data. Supports a low level DSL API, as well as a high level API that provides both stream and table abstractions (where tables present the latest record for each key). Executes as a stand-alone process, with support for parallel processing across threads within a single instance and across multiple instances, with the ability to dynamically scale the number of instances. Introduced in Kafka 0.10.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Manageable via&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-dataplane/streams-messaging-manager/&quot;&gt;Streams Messaging Manager&lt;/a&gt;, Burrow, Confluent Control Centre, LinkedIn Kafka Monitor, Nastel AutoPilot&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-flow/&quot;&gt;Hortonworks DataFlow&lt;/a&gt;, &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;Cloudera CDH&lt;/a&gt;, &lt;a href=&quot;/technologies/confluent-open-source/&quot;&gt;Confluent Open Source&lt;/a&gt;, &lt;a href=&quot;/technologies/confluent-enterprise/&quot;&gt;Confluent Enterprise&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.11&lt;/td&gt; &lt;td&gt;2017-06-28&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/www-announce/201706.mbox/%3CCAD5tkZZx3uGrLEYyjZte8aCTq=OYVLAiFz1uwMaxdO3yRoraBg@mail.gmail.com%3E&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Includes support for &lt;a href=&quot;https://www.confluent.io/blog/exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it/&quot;&gt;exactly once semantics&lt;/a&gt; and easier &lt;a href=&quot;https://www.confluent.io/blog/upgrading-apache-kafka-clients-just-got-easier/&quot;&gt;client upgrades&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;td&gt;2017-11-01&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://kafka.apache.org/downloads#1.0.0&quot;&gt;news&lt;/a&gt;; &lt;a href=&quot;https://www.confluent.io/blog/apache-kafka-goes-1-0/&quot;&gt;blog post&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.1&lt;/td&gt; &lt;td&gt;2018-03-29&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://kafka.apache.org/downloads#1.1.0&quot;&gt;news&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.0&lt;/td&gt; &lt;td&gt;2018-07-30&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://kafka.apache.org/downloads#2.0.0&quot;&gt;news&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://kafka.apache.org&quot;&gt;http://kafka.apache.org&lt;/a&gt; - project home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://kafka.apache.org/intro&quot;&gt;http://kafka.apache.org/intro&lt;/a&gt; - great introduction to Kafka&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://docs.confluent.io/current/&quot;&gt;http://docs.confluent.io/current/&lt;/a&gt; - Confluent documentation (covering Apache Kafka as well)&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blog.linkedin.com/2011/01/11/open-source-linkedin-kafka&quot;&gt;https://blog.linkedin.com/2011/01/11/open-source-linkedin-kafka&lt;/a&gt; - open source announcement&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/Clients&quot;&gt;https://cwiki.apache.org/confluence/display/KAFKA/Clients&lt;/a&gt; - list of clients&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/clients/&quot;&gt;https://www.confluent.io/clients/&lt;/a&gt; - Confluent’s list of available clients&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem&quot;&gt;https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem&lt;/a&gt; - associated technologies&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/apache/kafka/&quot;&gt;https://hortonworks.com/apache/kafka/&lt;/a&gt; - Hortonworks information&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.4/bk_kafka-component-guide/content/index.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.4/bk_kafka-component-guide/content/index.html&lt;/a&gt; - Hortonworks 2.6.4 documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/products/open-source/apache-hadoop/apache-kafka.html&quot;&gt;https://www.cloudera.com/products/open-source/apache-hadoop/apache-kafka.html&lt;/a&gt; - Cloudera information&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/documentation/kafka/latest.html&quot;&gt;https://www.cloudera.com/documentation/kafka/latest.html&lt;/a&gt; - Cloudera documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://dzone.com/refcardz/apache-kafka&quot;&gt;https://dzone.com/refcardz/apache-kafka&lt;/a&gt; - DZone Refcard covering useful getting started information&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/resources/kafka-the-definitive-guide/&quot;&gt;https://www.confluent.io/resources/kafka-the-definitive-guide/&lt;/a&gt; - Free copy of Kafka: The Definitive Guide eBook&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://kafka.apache.org/downloads&quot;&gt;http://kafka.apache.org/downloads&lt;/a&gt; - release history&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Kafka Connect</title><link>https://ondataengineering.net/technologies/apache-kafka/kafka-connect/</link><description> &lt;p&gt;Framework for building scalable and reliable integrations between Kafka and other technologies, either for importing or exporting data. Part of the core Apache Kafka open source technology, connectors are available for a wide range of systems, including Hadoop, relational, NoSQL and analytical databases, search technologies and message queues amongst others, with an API for developing custom connectors. Supports lightweight transformations, and runs separately to Kafka, in either a stand-alone or distributed cluster mode, with a REST API for managing connectors. Introduced in Kafka 0.9, previously known as Copycat&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/mapr-expansion-pack/&quot;&gt;MapR Expansion Pack&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://kafka.apache.org/documentation/#connect&quot;&gt;http://kafka.apache.org/documentation/#connect&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://docs.confluent.io/current/connect/index.html&quot;&gt;http://docs.confluent.io/current/connect/index.html&lt;/a&gt; - Confluent documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/product/connectors/&quot;&gt;https://www.confluent.io/product/connectors/&lt;/a&gt; - Confluent information, including list of available connectors&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/blog/how-to-build-a-scalable-etl-pipeline-with-kafka-connect/&quot;&gt;https://www.confluent.io/blog/how-to-build-a-scalable-etl-pipeline-with-kafka-connect/&lt;/a&gt; - introduction blog post&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Kafka Streams</title><link>https://ondataengineering.net/technologies/apache-kafka/kafka-streams/</link><description> &lt;p&gt;A stream processing technologies that's tightly integrated to Apache Kafka, consuming and publishing events from and to Kafka topics (and potentially writing output to external systems). Based on an event-at-a-time model (i.e. not micro batch), with support for stateful processing, windowing, aggregations, joining and re-processing data. Supports a low level DSL API, as well as a high level API that provides both stream and table abstractions (where tables present the latest record for each key). Executes as a stand-alone process, with support for parallel processing across threads within a single instance and across multiple instances, with the ability to dynamically scale the number of instances. Introduced in Kafka 0.10.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/mapr-expansion-pack/&quot;&gt;MapR Expansion Pack&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://kafka.apache.org/documentation/streams&quot;&gt;http://kafka.apache.org/documentation/streams&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://docs.confluent.io/current/streams/index.html&quot;&gt;http://docs.confluent.io/current/streams/index.html&lt;/a&gt; - Confluent documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/product/kafka-streams/&quot;&gt;https://www.confluent.io/product/kafka-streams/&lt;/a&gt; - Confluent information page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/&quot;&gt;https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/&lt;/a&gt; - introduction blog post&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Oozie</title><link>https://ondataengineering.net/technologies/apache-oozie/</link><description> &lt;p&gt;Technology for managing workflows of jobs on Hadoop clusters. Primary concepts include workflows (a sequence of jobs modelled as a directed acyclic graph), coordinators (schedule the execution of workflows based on the time or the presence of data) and bundles (collections of coordinators), with all configuration specified in XML. Supports a range of technologies, including MapReduce, Pig, Hive, Sqoop, Spark, Java executables and shell scripts. Includes a server component, a metadata database for holding definitions and state (with support for a range of database technologies), a command line interface and a read only web interface for viewing the status of jobs. Also supports the parameterisation of workflows, the modelling of datasets (and the use of these to manage dependencies between workflows within coordinators), automatic retry and failure handling, and the ability to send job status notifications via HTTP or JMS. Open sourced by Yahoo in June 2010. Donated to the Apache Foundation in July 2011, graduating in August 2012. Commercial support available as part of most Hadoop distributions&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Oozie&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;April 2018 - 5.0&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;, &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;Cloudera CDH&lt;/a&gt;, &lt;a href=&quot;/technologies/mapr-expansion-pack/&quot;&gt;MapR Expansion Pack&lt;/a&gt;, &lt;a href=&quot;/technologies/amazon-emr/&quot;&gt;Amazon EMR&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.0&lt;/td&gt; &lt;td&gt;2018-04-18&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://blogs.apache.org/foundation/entry/the-apache-software-foundation-announces32&quot;&gt;summary&lt;/a&gt;; &lt;a href=&quot;https://oozie.apache.org/docs/5.0.0/release-log.txt&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://oozie.apache.org/&quot;&gt;http://oozie.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://oozie.apache.org/docs/4.3.0/index.html&quot;&gt;http://oozie.apache.org/docs/4.3.0/index.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://jaxenter.com/yahoos-hadoop-based-project-proposed-for-apache-incubator-103651.html&quot;&gt;https://jaxenter.com/yahoos-hadoop-based-project-proposed-for-apache-incubator-103651.html&lt;/a&gt; - intro interview&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/apache/oozie/&quot;&gt;https://hortonworks.com/apache/oozie/&lt;/a&gt; - Hortonworks information&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Oozie release announcements only appear to be available via the Apache announcements mailing list&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Parquet</title><link>https://ondataengineering.net/technologies/apache-parquet/</link><description> &lt;p&gt;Data serialisation framework that supports a columnar storage format to enable efficient querying of data. Built using Apache Thrift, and supports complex nested data structures, using techniques from the Google Dremel paper. Consists of three sub-projects, the specification and Thrift definitions (Parquet Format), the Java and Hadoop libraries (Parquet MR) and the C++ implementation (Parquet CPP). Created as a joint effort between Twitter and Cloudera based on work started as part of Avro Trevni, with an initial v1.0 release in July 2013. Donated to the Apache Foundation in May 2014, graduating in April 2015. Has seen significant adoption in the Hadoop ecosystem.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Parquet&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;September 2018 - v1.10 (Parquet MR), v2.3 (Parquet Format), v1.5 (Parquet C++)&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;Cloudera CDH&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.0 (Parquet C++)&lt;/td&gt; &lt;td&gt;2017-03-17&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/www-announce/201703.mbox/%3C1489763208.180625.914669528.4C3F126B%40webmail.messagingengine.com%3E&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.1 (Parquet C++)&lt;/td&gt; &lt;td&gt;2017-05-22&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.2 (Parquet C++)&lt;/td&gt; &lt;td&gt;2017-08-04&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/www-announce/201708.mbox/%3C1501853659.3701438.1063168192.750F6F08@webmail.messagingengine.com%3E&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.3 (Parquet C++)&lt;/td&gt; &lt;td&gt;2017-09-25&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/www-announce/201709.mbox/%3C1506372726.2995644.1117929408.5D40AC49@webmail.messagingengine.com%3E&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.4 (Parquet C++)&lt;/td&gt; &lt;td&gt;2018-03-07&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/www-announce/201803.mbox/%3C1520429566.2875966.1294691512.05A524C2%40webmail.messagingengine.com%3E&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.10 (Parquet MR)&lt;/td&gt; &lt;td&gt;2018-03-31&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/parquet-mr/master/CHANGES.md&quot;&gt;changelog&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.5 (Parquet C++)&lt;/td&gt; &lt;td&gt;2018-09-22&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/www-announce/201809.mbox/%3C1537629109.1960853.1517045392.2E984E92%40webmail.messagingengine.com%3E&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://parquet.apache.org/&quot;&gt;http://parquet.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://parquet.apache.org/documentation/latest/&quot;&gt;http://parquet.apache.org/documentation/latest/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2013/03/introducing-parquet-columnar-storage-for-apache-hadoop/&quot;&gt;http://blog.cloudera.com/blog/2013/03/introducing-parquet-columnar-storage-for-apache-hadoop/&lt;/a&gt; - initial announcement&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blog.twitter.com/2013/dremel-made-simple-with-parquet&quot;&gt;https://blog.twitter.com/2013/dremel-made-simple-with-parquet&lt;/a&gt; - good introduction to Parquet&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blog.twitter.com/2013/announcing-parquet-10-columnar-storage-for-hadoop&quot;&gt;https://blog.twitter.com/2013/announcing-parquet-10-columnar-storage-for-hadoop&lt;/a&gt; - 1.0 release announcement&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces75&quot;&gt;https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces75&lt;/a&gt; - top level project announcement, including summary of technology that support it&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/apache/parquet-mr/blob/master/CHANGES.md&quot;&gt;https://github.com/apache/parquet-mr/blob/master/CHANGES.md&lt;/a&gt; - Parquet MR release and change summary&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/apache/parquet-format/blob/master/CHANGES.md&quot;&gt;https://github.com/apache/parquet-format/blob/master/CHANGES.md&lt;/a&gt; - Parquet Format release and change summary&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/apache/parquet-cpp/blob/master/CHANGELOG&quot;&gt;https://github.com/apache/parquet-cpp/blob/master/CHANGELOG&lt;/a&gt; - Parquet C++ release and change summary&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Pig</title><link>https://ondataengineering.net/technologies/apache-pig/</link><description> &lt;p&gt;Technology for running analytical and data processing jobs against data in Hadoop. Jobs are written in Pig Latin (a custom procedural language that can be extended using user defined functions in a range of languages), which is then translated into Map Reduce or Tez (with Spark in preview) for execution. Supports both a batch mode for running pre-defined scripts and an interactive mode, and connectors for reading and writing to HBase and Accumulo as well as HDFS. Originally developed at Yahoo in 2006 before being donated to the Apache Foundation in October 2007. Graduated as an Hadoop sub-project in October 2008, before becoming a top level project in September 2010. Although has not had a v1.0 release, has been production quality for many years. Commercial support available as part of most Hadoop distributions&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Pig&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;June 2017 - v0.17&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;, &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;Cloudera CDH&lt;/a&gt;, &lt;a href=&quot;/technologies/mapr-expansion-pack/&quot;&gt;MapR Expansion Pack&lt;/a&gt;, &lt;a href=&quot;/technologies/amazon-emr/&quot;&gt;Amazon EMR&lt;/a&gt;, &lt;a href=&quot;/technologies/google-cloud-dataproc/&quot;&gt;Google Cloud DataProc&lt;/a&gt;, &lt;a href=&quot;/technologies/qubole-data-service/&quot;&gt;Qubole Data Service&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Is used by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-datafu/datafu-pig/&quot;&gt;DataFu Pig&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.17&lt;/td&gt; &lt;td&gt;2017-06-19&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://pig.apache.org/releases.html#19+June%2C+2017%3A+release+0.17.0+available&quot;&gt;summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Introduction of Pig on Spark&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.16&lt;/td&gt; &lt;td&gt;2016-06-08&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://pig.apache.org/releases.html#8+June%2C+2016%3A+release+0.16.0+available&quot;&gt;summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Stabilisation of Pig on Tez&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://pig.apache.org/&quot;&gt;https://pig.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://pig.apache.org/releases.html&quot;&gt;https://pig.apache.org/releases.html&lt;/a&gt; - release history&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://hortonworks.com/blog/announcing-apache-pig-0-14-0/&quot;&gt;http://hortonworks.com/blog/announcing-apache-pig-0-14-0/&lt;/a&gt; - Pig on Tez release announcement&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2014/09/pig-is-flying-apache-pig-on-apache-spark/&quot;&gt;http://blog.cloudera.com/blog/2014/09/pig-is-flying-apache-pig-on-apache-spark/&lt;/a&gt; - first details of Pig on Spark&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/apache/pig/&quot;&gt;https://hortonworks.com/apache/pig/&lt;/a&gt; - Hortonworks information&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/products/open-source/apache-hadoop/apache-pig.html&quot;&gt;https://www.cloudera.com/products/open-source/apache-hadoop/apache-pig.html&lt;/a&gt; - Cloudera information&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://pig.apache.org/&quot;&gt;https://pig.apache.org/&lt;/a&gt; - news and updates&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Bigtop</title><link>https://ondataengineering.net/technologies/apache-bigtop/</link><description> &lt;p&gt;An Apache open source distribution of Hadoop. Packages up a number of Apache Hadoop components, certifies their interoperability using an automated integration test suite, and packages them up as RPMs/DEBs packages for most flavours of Linux. Also includes virtual machine images and vagrant, docker and puppet recipes for deploying and working with Hadoop. Does not patch projects for distribution, but requires any fixes to be made upstream. An Apache Open Source project, started by Cloudera, donated to the Apache foundation in June 2011, graduating in September 2012, with a 1.0 release in August 2015 based on Hadoop 2.6. Since donating the project, Cloudera have backed away from it, with the project lead moving to Pivotal in December 2013. Now has a broad range of contributors, however usage by the major distributors is not clear.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Bigtop&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Categories&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-categories/hadoop-distributions/&quot;&gt;Hadoop Distributions&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Open Source - Active&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;April 2017 - v1.2&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Packages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/alluxio/&quot;&gt;Alluxio&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-apex/&quot;&gt;Apache Apex&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-crunch/&quot;&gt;Apache Crunch&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-datafu/&quot;&gt;Apache DataFu&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-flink/&quot;&gt;Apache Flink&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-flume/&quot;&gt;Apache Flume&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-giraph/&quot;&gt;Apache Giraph&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Apache Hadoop&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hama/&quot;&gt;Apache Hama&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hbase/&quot;&gt;Apache HBase&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Apache Hive&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-ignite/&quot;&gt;Apache Ignite&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-mahout/&quot;&gt;Apache Mahout&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-oozie/&quot;&gt;Apache Oozie&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-phoenix/&quot;&gt;Apache Phoenix&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-pig/&quot;&gt;Apache Pig&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-solr/&quot;&gt;Apache Solr&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-sqoop/&quot;&gt;Apache Sqoop&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-tajo/&quot;&gt;Apache Tajo&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-tez/&quot;&gt;Apache Tez&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-zeppelin/&quot;&gt;Apache Zeppelin&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-zookeeper/&quot;&gt;Apache ZooKeeper&lt;/a&gt;, &lt;a href=&quot;/technologies/greenplum/&quot;&gt;Greenplum&lt;/a&gt;, &lt;a href=&quot;/technologies/hue/&quot;&gt;Hue&lt;/a&gt;, &lt;a href=&quot;/technologies/kite/&quot;&gt;Kite&lt;/a&gt;, &lt;a href=&quot;/technologies/quantcast-file-system/&quot;&gt;Quantcast File System&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.5.0&lt;/td&gt; &lt;td&gt;27th Dec 2012&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://blogs.apache.org/bigtop/entry/apache_bigtop_0_5_0&quot;&gt;https://blogs.apache.org/bigtop/entry/apache_bigtop_0_5_0&lt;/a&gt;&lt;/td&gt; &lt;td&gt;first release as TLP&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.6.0&lt;/td&gt; &lt;td&gt;22nd June 2013&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://blogs.apache.org/bigtop/entry/apache_bigtop_0_6_0&quot;&gt;https://blogs.apache.org/bigtop/entry/apache_bigtop_0_6_0&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.7.0&lt;/td&gt; &lt;td&gt;6th Nov 2013&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://blogs.apache.org/bigtop/entry/release_of_apache_bigtop_0&quot;&gt;https://blogs.apache.org/bigtop/entry/release_of_apache_bigtop_0&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.8.0&lt;/td&gt; &lt;td&gt;6th Oct 2014&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://blogs.apache.org/bigtop/entry/release_of_apache_bigtop_01&quot;&gt;https://blogs.apache.org/bigtop/entry/release_of_apache_bigtop_01&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.0.0&lt;/td&gt; &lt;td&gt;17th Aug 2015&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://blogs.apache.org/bigtop/entry/not_just_yet_another_release&quot;&gt;https://blogs.apache.org/bigtop/entry/not_just_yet_another_release&lt;/a&gt;&lt;/td&gt; &lt;td&gt;based on Hadoop 2.6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.1.0&lt;/td&gt; &lt;td&gt;17th Feb 2016&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://blogs.apache.org/bigtop/entry/release_1_1_0_is&quot;&gt;https://blogs.apache.org/bigtop/entry/release_1_1_0_is&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.2.0&lt;/td&gt; &lt;td&gt;2017-03-30&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt;added Flink, Tajo, Apex, QFS and GPDB; 1.2.1 patch released&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://bigtop.apache.org&quot;&gt;http://bigtop.apache.org&lt;/a&gt; - Homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/BIGTOP/Index&quot;&gt;https://cwiki.apache.org/confluence/display/BIGTOP/Index&lt;/a&gt; - The Apache Bigtop Wiki&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/apache/bigtop/blob/master/bigtop.bom&quot;&gt;https://github.com/apache/bigtop/blob/master/bigtop.bom&lt;/a&gt; - details of all included packages and their versions (as of current development snapshot)&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2012/07/update-on-apache-bigtop-incubating/&quot;&gt;http://blog.cloudera.com/blog/2012/07/update-on-apache-bigtop-incubating/&lt;/a&gt; - Cloudera introduction to Bigtop&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/bigtop/entry/bigtop_and_why_should_you&quot;&gt;https://blogs.apache.org/bigtop/entry/bigtop_and_why_should_you&lt;/a&gt; - Early introduction to Bigtop&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blog.pivotal.io/pivotal/pivotal-people/pivotal-people-roman-shaposhnik-founder-of-apache-bigtop-joins-pivotal&quot;&gt;https://blog.pivotal.io/pivotal/pivotal-people/pivotal-people-roman-shaposhnik-founder-of-apache-bigtop-joins-pivotal&lt;/a&gt; - Interview with Roman Shoposhnik on the aims of Bigtop&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/BIGTOP/?selectedTab=com.atlassian.jira.jira-projects-plugin:versions-panel&quot;&gt;https://issues.apache.org/jira/browse/BIGTOP/?selectedTab=com.atlassian.jira.jira-projects-plugin:versions-panel&lt;/a&gt; - Release list and link to release notes&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/bigtop/&quot;&gt;https://blogs.apache.org/bigtop/&lt;/a&gt; - The Apache Bigtop Blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://bigtop.apache.org/&quot;&gt;http://bigtop.apache.org/&lt;/a&gt; - details of latest release&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Ignite</title><link>https://ondataengineering.net/technologies/apache-ignite/</link><description> &lt;p&gt;A distributed in-memory data fabric/grid with the ability to persist data to disk, supporting a number of use cases including a key value store (with SQL support), real time stream/event processing engine, arbitrary compute, long running service management, an in-memory HDFS compatible file system for acceleration of Hadoop jobs, an in-memory machine learning grid and in-memory shared Spark RDDs and Data Frames. An Apache project, graduating in September 2015, having been originally donated by GridGain from their In-Memory Data Fabric product launched in 2007. Java based, with development lead by GridGain who also supply commercial support (as GridGain Professional with ongoing Q&amp;A and bug fixes before they're included in Ignite) along with GridGain Enterprise and Ultimate (which includes extra features such as a management GUI, enterprise security, rolling upgrades, backup and recovery) and GridGain Cloud (beta).&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Ignite, GridGain&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;, GridGain&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;July 2018 - v2.6&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.9&lt;/td&gt; &lt;td&gt;2017-03-07&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://blogs.apache.org/ignite/entry/apache-ignite-1-9-released&quot;&gt;Ignite announcement&lt;/a&gt; &lt;a href=&quot;https://www.gridgain.com/resources/blog/gridgain-professional-edition-19-improves-performance-adds-kubernetesr-support-and&quot;&gt;GridGain Professional announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.0&lt;/td&gt; &lt;td&gt;2017-05-05&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://blogs.apache.org/ignite/entry/apache-ignite-2-0-redesigned&quot;&gt;Ignite announcement&lt;/a&gt; &lt;a href=&quot;https://www.gridgain.com/resources/blog/gridgain-professional-edition-20-released-today&quot;&gt;GridGain Professional announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.gridgain.com/resources/blog/what-you-need-know-about-apacher-ignitetm-20-21&quot;&gt;post&lt;/a&gt; on upgrading to 2.0 or 2.1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.1&lt;/td&gt; &lt;td&gt;2017-07-27&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://blogs.apache.org/ignite/entry/apache-ignite-2-1-a&quot;&gt;Ignite announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Adds persistent storage of data held in memory - see &lt;a href=&quot;https://www.gridgain.com/resources/blog/apacher-ignitetm-native-persistence-what-about-data-recovery-solved&quot;&gt;post&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.2&lt;/td&gt; &lt;td&gt;2017-09-21&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/www-announce/201709.mbox/%3C68C2E42E-117B-46CF-A854-2C0D89172E9B@apache.org%3E&quot;&gt;Ignite announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.3&lt;/td&gt; &lt;td&gt;2017-11-01&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://blogs.apache.org/ignite/entry/apache-ignite-2-3-more&quot;&gt;Ignite announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.4&lt;/td&gt; &lt;td&gt;2018-03-15&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://blogs.apache.org/ignite/entry/apache-ignite-2-4-brings&quot;&gt;Ignite announcement&lt;/a&gt;; &lt;a href=&quot;https://www.gridgain.com/resources/blog/gridgain-professional-edition-24-now-available&quot;&gt;GridGain Professional announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Machine Learning GA; Spark DataFrames&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.5&lt;/td&gt; &lt;td&gt;2018-05-31&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://blogs.apache.org/ignite/entry/apache-ignite-2-5-scaling&quot;&gt;Ignite announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.6&lt;/td&gt; &lt;td&gt;2018-07-18&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/www-announce/201807.mbox/%3CCAK1mX7HdatVZ0mvn==tOJx7wXNVsMZ16JBKEXge43F351Wt8XQ@mail.gmail.com%3E&quot;&gt;Ignite announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://ignite.apache.org/&quot;&gt;https://ignite.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.gridgain.com/&quot;&gt;http://www.gridgain.com/&lt;/a&gt; - Grid Gain home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.gridgain.com/products/pricing/&quot;&gt;http://www.gridgain.com/products/pricing/&lt;/a&gt; - details of GridGain editions&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.gridgain.com/products/gridgain-cloud&quot;&gt;https://www.gridgain.com/products/gridgain-cloud&lt;/a&gt; - GridGain cloud homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://apacheignite.readme.io/docs&quot;&gt;https://apacheignite.readme.io/docs&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.infoq.com/presentations/apache-ignite&quot;&gt;https://www.infoq.com/presentations/apache-ignite&lt;/a&gt; - intro presentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://ignite.apache.org/blogs.html&quot;&gt;https://ignite.apache.org/blogs.html&lt;/a&gt; - Ingite blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.gridgain.com/resources/blog&quot;&gt;https://www.gridgain.com/resources/blog&lt;/a&gt; - GridGain blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.gridgain.com/resources/download&quot;&gt;https://www.gridgain.com/resources/download&lt;/a&gt; - details of current versions&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Giraph</title><link>https://ondataengineering.net/technologies/apache-giraph/</link><description> &lt;p&gt;An iterative, highly scalable graph processing system built on top of MapReduce and based on Pregel, with a number of features added including a framework for creating re-usable code (called blocks). An Apache project, graduating in May 2012, having been originally donated by Yahoo in August 2011. Java based, no commercial support available, but is mature and has been adopted by a number of companies (including LinkedIn and most famously Facebook who scaled it to process a trillion edges), and has a number of active developers.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Giraph&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Open Source - Active&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v1.2&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://giraph.apache.org/&quot;&gt;http://giraph.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://giraph.apache.org/intro.html&quot;&gt;http://giraph.apache.org/intro.html&lt;/a&gt; - introduction&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://engineering.linkedin.com/open-source/apache-giraph-framework-large-scale-graph-processing-hadoop-reaches-01-milestone&quot;&gt;https://engineering.linkedin.com/open-source/apache-giraph-framework-large-scale-graph-processing-hadoop-reaches-01-milestone&lt;/a&gt; - v0.1 release announcement from LinkedIn&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.facebook.com/notes/facebook-engineering/scaling-apache-giraph-to-a-trillion-edges/10151617006153920/&quot;&gt;https://www.facebook.com/notes/facebook-engineering/scaling-apache-giraph-to-a-trillion-edges/10151617006153920/&lt;/a&gt; - Facebook’s story on scaling Giraph to a trillion edges&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://giraph.apache.org/&quot;&gt;http://giraph.apache.org/&lt;/a&gt; - news via homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/giraph&quot;&gt;https://blogs.apache.org/giraph&lt;/a&gt; - blog, with first entry being v1.2 announcement&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Hama</title><link>https://ondataengineering.net/technologies/apache-hama/</link><description> &lt;p&gt;A general purpose BSP (Bulk Synchronous Parallel) processing engine inspired by Pregel and DistBelief that runs over Mesos or YARN. Supports BSP, graph computing and machine learning programming models, as well as Apache MRQL. An Apache project, donated in 2008, and graduated in 2012. Java based, with no commercial support available, limited case studies for it's use and limited active developers, with the last release being in June 2015.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Hama&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Open Source - Quiet&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v0.7&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://hama.apache.org/&quot;&gt;http://hama.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://wiki.apache.org/hama/&quot;&gt;https://wiki.apache.org/hama/&lt;/a&gt; - Wiki / documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://wiki.apache.org/incubator/HamaProposal&quot;&gt;https://wiki.apache.org/incubator/HamaProposal&lt;/a&gt; - details of Apache incubation proposal&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/hama/&quot;&gt;https://blogs.apache.org/hama/&lt;/a&gt; - blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Alluxio</title><link>https://ondataengineering.net/technologies/alluxio/</link><description> &lt;p&gt;A distributed virtual storage layer, supporting key-value and filesystem interfaces (including HDFS compatibility and a FUSE driver) with support for a range of computation and storage frameworks (including Spark, MapReduce, HBase and Hive) over multiple storage layers (including in-memory, local, network, cloud and cluster file systems) with the ability to create unified and tiered storage, for example to create an in memory filesystem backed by disk to accelerate analytics jobs. Supports a POSIX like access control model, and a CLI and web interface for browsing the storage layer and an S3 compatible API. Java based, Open Source under the Apache 2.0 licence, hosted on GitHub, with development led by Alluxio (with significant external contributions), although they don't appear to yet provide commercial support (but do provide training). Started in December 2012, open sourced in April 2013, with a v1.0 release in February 2016. Formally known as Tachyon.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Alluxio, Tachyon&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;Alluxio&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;July 2018 - v1.8&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.5&lt;/td&gt; &lt;td&gt;2017-06-12&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://www.alluxio.org/download/releases/alluxio-150-release&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.6&lt;/td&gt; &lt;td&gt;2017-09-22&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://www.alluxio.org/download/releases/alluxio-160-release&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.7&lt;/td&gt; &lt;td&gt;2018-01-16&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://www.alluxio.org/download/releases/alluxio-170-release&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.8&lt;/td&gt; &lt;td&gt;2018-07-09&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://www.alluxio.org/download/releases/alluxio-180-release&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://alluxio.org/&quot;&gt;http://alluxio.org/&lt;/a&gt; - product home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/Alluxio/alluxio&quot;&gt;https://github.com/Alluxio/alluxio&lt;/a&gt; - source code&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.alluxio.org/docs/master/en/&quot;&gt;http://www.alluxio.org/docs/master/en/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.alluxio.com/2016/02/alluxio-formerly-tachyon-is-entering-a-new-era-with-1-0-release/&quot;&gt;http://www.alluxio.com/2016/02/alluxio-formerly-tachyon-is-entering-a-new-era-with-1-0-release/&lt;/a&gt; - history&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.alluxio.com/blog&quot;&gt;https://www.alluxio.com/blog&lt;/a&gt; - blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.alluxio.org/download&quot;&gt;http://www.alluxio.org/download&lt;/a&gt; - details of releases&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Kite</title><link>https://ondataengineering.net/technologies/kite/</link><description> &lt;p&gt;A set of libraries, tools, examples, and documentation focused on making it easier to build systems on top of the Hadoop ecosystem. Consists of three sub-projects - Kite Data (a logical dataset abstraction over Hadoop), Morphlines (embeddable configuration driven transformation pipelines) and Kite Maven Plugin (a Maven plugin for deploying Hadoop applications). Java based, Open Source under the Apache 2.0 licence and hosted on GitHub. First released in May 2013 by Cloudera as the Cloudera Development Kit (CDK), renamed to Kite in December 2013, and reached a v1.0 release in February 2015 with a number of external contributors. Last release was v1.1 in June 2015, with very little development activity since this time.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Cloudera Development Kit, CDK&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/cloudera/&quot;&gt;Cloudera&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v1.1&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Sub-projects&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; Kite&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/kite/kite-data/&quot;&gt;Kite Data&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Library that provides a logical dataset and record abstraction over HDFS, S3, local filesystems and HBase, including support for partitioning and views (which allow datasets to be filtered and supports automatic partition pruning). Provides a command line interface and Maven plugin for managing and viewing datasets. Supports Crunch, Flume, Spark and MapReduce, and can integrate with a Hive Metastore to make datasets available through Hive and Impala. Stores data using Avro (utilising Avro schema evolution / resolution) or Parquet.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Kite&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/kite/kite-maven-plugin/&quot;&gt;Kite Maven Plugin&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A Maven plugin that supports the packaging, deployment and execution of applications onto Hadoop.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Kite&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/kite/morphlines/&quot;&gt;Morphlines&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A configuration driven in-memory transformation pipeline that can be embedded into any Java code base, with specific support for Flume, MapReduce, HBase, Spark and Solr. Supports multiple different file types including CSV, Avro, JSON, Parquet, RCFile, SequenceFile, ProtoBuf and XML plus gzip, bzip2, tar zip and jar files. Also supports a number of transformation steps out of the box, including integration with Apache Tika for reading common file formats.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://kitesdk.org/docs/current/&quot;&gt;http://kitesdk.org/docs/current/&lt;/a&gt; - homepage and documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/kite-sdk/kite&quot;&gt;https://github.com/kite-sdk/kite&lt;/a&gt; - source code&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Any updates to Kite are likely to be published on the Cloudera blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Kite Data</title><link>https://ondataengineering.net/technologies/kite/kite-data/</link><description> &lt;p&gt;Library that provides a logical dataset and record abstraction over HDFS, S3, local filesystems and HBase, including support for partitioning and views (which allow datasets to be filtered and supports automatic partition pruning). Provides a command line interface and Maven plugin for managing and viewing datasets. Supports Crunch, Flume, Spark and MapReduce, and can integrate with a Hive Metastore to make datasets available through Hive and Impala. Stores data using Avro (utilising Avro schema evolution / resolution) or Parquet.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/kite/&quot;&gt;Kite&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by (but deprecated)&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;Cloudera CDH&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://kitesdk.org/docs/current/Kite-SDK-Guide.html&quot;&gt;http://kitesdk.org/docs/current/Kite-SDK-Guide.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Kite Maven Plugin</title><link>https://ondataengineering.net/technologies/kite/kite-maven-plugin/</link><description> &lt;p&gt;A Maven plugin that supports the packaging, deployment and execution of applications onto Hadoop.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/kite/&quot;&gt;Kite&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://kitesdk.org/docs/current/maven/plugin-info.html&quot;&gt;http://kitesdk.org/docs/current/maven/plugin-info.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Morphlines</title><link>https://ondataengineering.net/technologies/kite/morphlines/</link><description> &lt;p&gt;A configuration driven in-memory transformation pipeline that can be embedded into any Java code base, with specific support for Flume, MapReduce, HBase, Spark and Solr. Supports multiple different file types including CSV, Avro, JSON, Parquet, RCFile, SequenceFile, ProtoBuf and XML plus gzip, bzip2, tar zip and jar files. Also supports a number of transformation steps out of the box, including integration with Apache Tika for reading common file formats.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/kite/&quot;&gt;Kite&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;Cloudera CDH&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://kitesdk.org/docs/current/morphlines/&quot;&gt;http://kitesdk.org/docs/current/morphlines/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://kitesdk.org/docs/current/morphlines/morphlines-reference-guide.html&quot;&gt;http://kitesdk.org/docs/current/morphlines/morphlines-reference-guide.html&lt;/a&gt; - reference guide&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2013/07/morphlines-the-easy-way-to-build-and-integrate-etl-apps-for-apache-hadoop/&quot;&gt;http://blog.cloudera.com/blog/2013/07/morphlines-the-easy-way-to-build-and-integrate-etl-apps-for-apache-hadoop/&lt;/a&gt; - intro&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache DataFu</title><link>https://ondataengineering.net/technologies/apache-datafu/</link><description> &lt;p&gt;A set of libraries for working with data in Hadoop. Consists of two sub-projects - DataFu Pig (a set of Pig User Defined Functions) and DataFu Hourglass (a framework for incremental processing using MapReduce). Originally created at LinkedIn, with the Pig UDFs being open sourced in January 2012 as DataFu, with a v1.0 release in September 2013. Split into sub-projects in October 2013 when LinkedIn open sourced DataFu Hourglass and added it to the project. Donated to the Apache Foundation in January 2014, graduating in February 2018. Last major release was v1.3 in November 2015, with a handful of bug fix releases but little development activity since then.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;DataFu&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;March 2018 - v1.4&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Sub-projects&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; Apache DataFu&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-datafu/datafu-hourglass/&quot;&gt;DataFu Hourglass&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A framework over MapReduce that supports the efficient generation of statistics of dated data by incrementally updating the previous days output. Supports both fixed length and fixed start point windows, and the generation of statistics by input partition or as a total over all input data.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache DataFu&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-datafu/datafu-pig/&quot;&gt;DataFu Pig&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A set of user defined functions for Apache Pig, including support for statistical calculations, bag and set operations, sessionisation of streams of data, cardinality estimation, sampling, hashing, PageRank and others.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;td&gt;2013-09-04&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://datafu.apache.org/blog/2013/09/04/datafu-1-0.html&quot;&gt;summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.3&lt;/td&gt; &lt;td&gt;2015-11-18&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://datafu.apache.org/blog/2015/11/17/datafu-1-3-0-released.html&quot;&gt;summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt;First Apache (Incubating) release&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.4&lt;/td&gt; &lt;td&gt;2018-03-25&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://datafu.apache.org/blog/2018/03/22/datafu-1-4-0-released.html&quot;&gt;summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Release to mark Apache graduation; includes 1.3.x patches&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://datafu.apache.org&quot;&gt;https://datafu.apache.org&lt;/a&gt; - homepage&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://datafu.apache.org/blog/&quot;&gt;https://datafu.apache.org/blog/&lt;/a&gt; - blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>DataFu Hourglass</title><link>https://ondataengineering.net/technologies/apache-datafu/datafu-hourglass/</link><description> &lt;p&gt;A framework over MapReduce that supports the efficient generation of statistics of dated data by incrementally updating the previous days output. Supports both fixed length and fixed start point windows, and the generation of statistics by input partition or as a total over all input data.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-datafu/&quot;&gt;Apache DataFu&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://datafu.apache.org/docs/hourglass/getting-started.html&quot;&gt;https://datafu.apache.org/docs/hourglass/getting-started.html&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://datafu.apache.org/blog/2013/10/03/datafus-hourglass-incremental-data-processing-in-hadoop.html&quot;&gt;https://datafu.apache.org/blog/2013/10/03/datafus-hourglass-incremental-data-processing-in-hadoop.html&lt;/a&gt; - introduction (contains examples and information not available in the documentation)&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>DataFu Pig</title><link>https://ondataengineering.net/technologies/apache-datafu/datafu-pig/</link><description> &lt;p&gt;A set of user defined functions for Apache Pig, including support for statistical calculations, bag and set operations, sessionisation of streams of data, cardinality estimation, sampling, hashing, PageRank and others.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-datafu/&quot;&gt;Apache DataFu&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Uses&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-pig/&quot;&gt;Apache Pig&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://datafu.apache.org/docs/datafu/getting-started.html&quot;&gt;https://datafu.apache.org/docs/datafu/getting-started.html&lt;/a&gt; - homepage (see linked blog posts for further information and examples)&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://datafu.apache.org/docs/datafu/guide.html&quot;&gt;https://datafu.apache.org/docs/datafu/guide.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Phoenix</title><link>https://ondataengineering.net/technologies/apache-phoenix/</link><description> &lt;p&gt;A SQL query engine over Apache HBase tables that supports a subset of SQL 92 (including joins), and comes with a JDBC driver. Supports a range of features including ACID transactions (via Apache Tephra), user defined functions, secondary indexes, atomic upserts, views, multi tenancy tables (where each user or tenant can only see their data) and dynamic columns (which are only specified at query time). Supports a range of SQL DDL commands, creating and modifying underlying HBase tables as required, or can run over existing HBase tables in a read only mode. Comes with connectors to allow Spark, Hive, Pig, Flume and MapReduce to read and write Phoenix tables, and a number of utilities, including a bulk loader and a command line SQL tool. Open sourced by SalesForce in January 2013 at v1.0, donated to the Apache foundation in December 2013, before graduating in May 2014. Commercial support available through Hortonworks as part of HDP, with Cloudera making it available via Cloudera Labs without support. Active project with a range of contributors, including many from SalesForce and Hortonworks.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Phoenix&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;July 2018 - v5.0&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;, &lt;a href=&quot;/technologies/amazon-emr/&quot;&gt;Amazon EMR&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;4.10&lt;/td&gt; &lt;td&gt;2017-03-23&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://blogs.apache.org/phoenix/entry/announcing-phoenix-4-10-released&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;4.11&lt;/td&gt; &lt;td&gt;2017-07-07&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://blogs.apache.org/phoenix/entry/announcing-phoenix-4-11-released&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;4.12&lt;/td&gt; &lt;td&gt;2017-10-11&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://blogs.apache.org/phoenix/entry/announcing-phoenix-4-12-released&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;4.13&lt;/td&gt; &lt;td&gt;2017-11-12&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://blogs.apache.org/phoenix/entry/announcing-phoenix-4-13-released&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;4.14&lt;/td&gt; &lt;td&gt;2018-06-12&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://blogs.apache.org/phoenix/entry/announcing-phoenix-4-14-released&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.0&lt;/td&gt; &lt;td&gt;2018-07-14&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://blogs.apache.org/phoenix/entry/apache-phoenix-releases-next-major&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Hadoop 3.0 and HBase 2.0 compatibility&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://phoenix.apache.org/&quot;&gt;http://phoenix.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/phoenix/entry/apache_phoenix_released_next_major&quot;&gt;https://blogs.apache.org/phoenix/entry/apache_phoenix_released_next_major&lt;/a&gt; - 3.0/4.0 announcement&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blog.cloudera.com/blog/2015/05/apache-phoenix-joins-cloudera-labs&quot;&gt;https://blog.cloudera.com/blog/2015/05/apache-phoenix-joins-cloudera-labs&lt;/a&gt; - Cloudera labs announcement&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/apache/phoenix/&quot;&gt;https://hortonworks.com/apache/phoenix/&lt;/a&gt; - Hortonworks information&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.4/bk_data-access/content/ch_using-phoenix.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.4/bk_data-access/content/ch_using-phoenix.html&lt;/a&gt; - Hortonworks 2.6.4 documentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://phoenix-hbase.blogspot.co.uk/&quot;&gt;http://phoenix-hbase.blogspot.co.uk/&lt;/a&gt; - original blog, now superseded by the Apache blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/phoenix/&quot;&gt;https://blogs.apache.org/phoenix/&lt;/a&gt; - project blog including release announcements&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Tajo</title><link>https://ondataengineering.net/technologies/apache-tajo/</link><description> &lt;p&gt;Distributed analytical database engine. Supports HDFS, Amazon S3, Google Cloud Storage, OpenStack Swift and local storage, and querying over Postgres, HBase and Hive tables. Provides a standard SQL interface, JDBC driver, and supports partitioning, compression and indexing (currently experimental). An Apache project, donated by Gruter in March 2013, and graduated in April 2014. Java based, with development lead by Gruter who also supply commercial support, a Tajo managed service, a data analytics hub (Qrytica) built on Tajo, and a Tajo Data Warehouse appliance.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Tajo&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;, Gruter&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;February 2017 - v0.11&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://tajo.apache.org/&quot;&gt;http://tajo.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://tajo.apache.org/docs/current/&quot;&gt;http://tajo.apache.org/docs/current/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://gruter.com/technology/tajo/&quot;&gt;http://gruter.com/technology/tajo/&lt;/a&gt; - Gruter product page&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://tajo.apache.org/&quot;&gt;http://tajo.apache.org/&lt;/a&gt; - news and releases&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.gruter.com/blog/&quot;&gt;http://www.gruter.com/blog/&lt;/a&gt; - Gruter blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Mahout</title><link>https://ondataengineering.net/technologies/apache-mahout/</link><description> &lt;p&gt;Machine learning technology comprising of a Scala based linear algebra engine (codenamed Samsara) with an R-like DSL/API that runs over Spark (with experimental support for H2O and Flink), an optimiser, a wide variety of pre-made algorithms, and a Scala REPL (based on Spark Shell) for interactive execution. Can be embedded and integrated within larger applications, for example MLlib when running over Spark. Also includes some original, now deprecated, algorithms implemented over MapReduce. Created in January 2008 as a Lucene sub-project, becoming a top level Apache project in April 2010. The original MapReduce algorithms were deprecated and Samsara introduced as part of v0.10 in April 2015.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Mahout&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;April 2017 - 0.13&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/mapr-expansion-pack/&quot;&gt;MapR Expansion Pack&lt;/a&gt;, &lt;a href=&quot;/technologies/amazon-emr/&quot;&gt;Amazon EMR&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.13&lt;/td&gt; &lt;td&gt;2017-04-17&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/www-announce/201704.mbox/%3CCANg8BGBe%2BWwdZC6z6BAm3hqTOMjA2ma76y0dig0Jf5LHtgF56g%40mail.gmail.com%3E&quot;&gt;announcement&lt;/a&gt; &lt;a href=&quot;https://blogs.apache.org/foundation/entry/the-apache-software-foundation-announces9&quot;&gt;Apache announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://mahout.apache.org/&quot;&gt;https://mahout.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.weatheringthroughtechdays.com/2015/04/mahout-010x-first-mahout-release-as.html&quot;&gt;http://www.weatheringthroughtechdays.com/2015/04/mahout-010x-first-mahout-release-as.html&lt;/a&gt; - introduction to new architecture introduced in v0.10&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://mahout.apache.org/general/release-notes.html&quot;&gt;https://mahout.apache.org/general/release-notes.html&lt;/a&gt; - new releases&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Apex</title><link>https://ondataengineering.net/technologies/apache-apex/</link><description> &lt;p&gt;Data transformation engine based on Directed Acyclic Graph (DAG) flows configured through a Java API or via JSON, with a stated focus on performance, code re-use, testability and ease of operations. Runs over YARN and HDFS with native support for both micro-batch streaming and batch uses cases, and includes a range of standard operators and connectors (called Apex Malhar). An Apache project, graduating in April 2016, having been originally donated in August 2015 by DataTorrent from their DataTorrent RTS product which launched in June 2014. Java based, with development lead by DataTorrent who distribute it as DataTorrent RTS in two editions - a Community Edition (which also includes a basic management GUI and a tool for configuring Apex for data ingestion), and an Enterprise Edition (which further includes a graphical transformation editor, a self service dashboard, security integration and commercial support, and is also available as a cloud offering).&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Apex, DataTorrent RTS&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;, DataTorrent&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;May 2018 - v3.7 (Apex Core), v3.8 (Apex Malhar), v3.8 (DataTorrent RTS)&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;3.7 (Apex Malhar)&lt;/td&gt; &lt;td&gt;2017-03-31&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://blogs.apache.org/apex/entry/apache_apex_malhar_3_6&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;https://github.com/apache/apex-malhar/blob/v3.7.0/CHANGELOG.md&quot;&gt;changelist&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;3.8 (DataTorrent RTS)&lt;/td&gt; &lt;td&gt;2017-04-18&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.datatorrent.com/blog/big-data-real-time-analytics-insights/&quot;&gt;blog post&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;3.6 (Apex Core)&lt;/td&gt; &lt;td&gt;2017-05-04&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://github.com/apache/apex-core/blob/v3.6.0/CHANGELOG.md&quot;&gt;changelist&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;3.8 (Apex Malhar)&lt;/td&gt; &lt;td&gt;2017-11-12&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://github.com/apache/apex-malhar/blob/v3.8.0/CHANGELOG.md&quot;&gt;changelist&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;3.7 (Apex Core)&lt;/td&gt; &lt;td&gt;2018-04-28&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://github.com/apache/apex-core/blob/v3.7.0/CHANGELOG.md&quot;&gt;changelist&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://apex.apache.org/&quot;&gt;https://apex.apache.org/&lt;/a&gt; - Apex homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://apex.apache.org/docs.html&quot;&gt;https://apex.apache.org/docs.html&lt;/a&gt; -Apex documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.datatorrent.com/products-services/datatorrent-rts/&quot;&gt;https://www.datatorrent.com/products-services/datatorrent-rts/&lt;/a&gt; - DataTorrent RTS home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://docs.datatorrent.com/&quot;&gt;http://docs.datatorrent.com/&lt;/a&gt; - DataTorrent documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.datatorrent.com/blog/introducing-apache-apex-incubating/&quot;&gt;https://www.datatorrent.com/blog/introducing-apache-apex-incubating/&lt;/a&gt; - introductory blog post&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.datatorrent.com/products-services/edition-comparison/&quot;&gt;https://www.datatorrent.com/products-services/edition-comparison/&lt;/a&gt; - DataTorrent editions comparison&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://apex.apache.org/&quot;&gt;https://apex.apache.org/&lt;/a&gt; - release announcements&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.datatorrent.com/blog/&quot;&gt;https://www.datatorrent.com/blog/&lt;/a&gt; - DataTorrent blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Crunch</title><link>https://ondataengineering.net/technologies/apache-crunch/</link><description> &lt;p&gt;An abstraction layer over MapReduce (and now Spark) that provides a high level Java API for creating data transformation pipelines, originally designed to make working with MapReduce easier based on the Google FlumeJava paper. Also includes connectors for HBase, Hive and Kafka, Java 8 lambda support, an experimental Scala wrapper for the API (Scrunch), and support for in memory pipelines and helper classes to support testing. Open sourced by Cloudera in October 2011, donated to the Apache Foundation in May 2012, before graduating in February 2013. Support for Spark was added as part of v0.10 in June 2014. Still being maintained, and appears to have had been adopted at a number of large companies, but with limited new development.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Crunch&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;April 2017 - 0.15&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Is packaged by (but deprecated)&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;Cloudera CDH&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.15&lt;/td&gt; &lt;td&gt;2017-02-26&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://github.com/apache/crunch/releases/tag/apache-crunch-0.15.0&quot;&gt;GitHub release page&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://crunch.apache.org/&quot;&gt;https://crunch.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://crunch.apache.org/user-guide.html&quot;&gt;https://crunch.apache.org/user-guide.html&lt;/a&gt; - user guide&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2011/10/introducing-crunch/&quot;&gt;http://blog.cloudera.com/blog/2011/10/introducing-crunch/&lt;/a&gt; - initial intro blog post&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://crunch.apache.org/user-guide.html#motivation&quot;&gt;https://crunch.apache.org/user-guide.html#motivation&lt;/a&gt; - the motivation behind Crunch&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://crunch.apache.org/scrunch.html&quot;&gt;http://crunch.apache.org/scrunch.html&lt;/a&gt; - Scrunch page&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://crunch.apache.org/download.html&quot;&gt;https://crunch.apache.org/download.html&lt;/a&gt; - new releases only appear to be announced on download page&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Flink</title><link>https://ondataengineering.net/technologies/apache-flink/</link><description> &lt;p&gt;Specialised stream processing technology inspired by the Google Data Flow model. Based on a single record (not micro batch) model, with exactly once processing semantics (for supported sources and sinks) via light weight checkpointing, and focusing on high throughput, low latency use cases. Supports both a Java and Scala API, with a fluent DataStream API for working with continuous data flows (including a flexible windowing API that supports both event time and processing time windows and support for out of order or late data), and a DataSet API for working with batch data sets (that uses the same streaming execution engine). Also supports a number of connectors and extra libraries, including experimental support for SQL expressions, a CEP library (FlinkCEP) that can be used to detect complex event patterns, a beta package for running Storm apps on Flink, a graph processing library (Gelly) and a machine learning library (FlinkML). Clustered, with support for YARN and Mesos as well as standalone clusters. Open sourced by Data Artisans in April 2013, donated to the Apache Foundation in April 2014 before graduating in August 2014. Under active development with a large number of contributors and a range of user case studies. Sold as a hosted managed service (dA Platform) by Data Artisans who also supply training.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Flink&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;, Data Artisans&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;August 2018 - 1.6&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/amazon-emr/&quot;&gt;Amazon EMR&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.3&lt;/td&gt; &lt;td&gt;2017-06-01&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://flink.apache.org/news/2017/06/01/release-1.3.0.html&quot;&gt;summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.4&lt;/td&gt; &lt;td&gt;2017-12-12&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://flink.apache.org/news/2017/12/12/release-1.4.0.html&quot;&gt;summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.5&lt;/td&gt; &lt;td&gt;2018-06-05&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://flink.apache.org/news/2018/05/25/release-1.5.0.html&quot;&gt;summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.datanami.com/2018/05/29/apache-flink-gets-an-sql-client/&quot;&gt;Datanami view&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.6&lt;/td&gt; &lt;td&gt;2018-08-09&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://flink.apache.org/news/2018/08/09/release-1.6.0.html&quot;&gt;summary&lt;/a&gt;; &lt;a href=&quot;https://data-artisans.com/blog/apache-flink-1-6-0-whats-new-in-the-latest-apache-flink-release&quot;&gt;Data Artisans view&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://flink.apache.org/&quot;&gt;http://flink.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://ci.apache.org/projects/flink/flink-docs-release-1.2&quot;&gt;http://ci.apache.org/projects/flink/flink-docs-release-1.2&lt;/a&gt; - 1.2 release documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.slideshare.net/KostasTzoumas/apache-flink-at-strata-san-jose-2016&quot;&gt;http://www.slideshare.net/KostasTzoumas/apache-flink-at-strata-san-jose-2016&lt;/a&gt; - good intro slides, including comparison to other technologies&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://data-artisans.com/&quot;&gt;http://data-artisans.com/&lt;/a&gt; - Data Artisans homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://data-artisans.com/blog/apache-flink-1-3-0-evolution-stream-processing&quot;&gt;https://data-artisans.com/blog/apache-flink-1-3-0-evolution-stream-processing&lt;/a&gt; - history of Flink&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://flink.apache.org/blog/&quot;&gt;https://flink.apache.org/blog/&lt;/a&gt; - flink blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://data-artisans.com/blog/&quot;&gt;http://data-artisans.com/blog/&lt;/a&gt; - Data Artisans blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Tez</title><link>https://ondataengineering.net/technologies/apache-tez/</link><description> &lt;p&gt;Data processing framework based on Directed Acyclic Graphs (DAGs), that runs natively on YARN and was designed to be a replacement for the use of MapReduce within Hadoop analytical tools (primarily Hive and Pig), and therefore offer better performance with similar scalability. Targeted more at application developers rather than data engineers, includes a number of performance optimisations (including dynamic DAG re-configuration during execution and re-use of sessions and containers), and comes with a UI for viewing live and historic Tez job executions based on information in the YARN Application Timeline Server. Created by Hortonworks and donated to the Apache Foundation in February 2013 before graduating in July 2014. Still under active development, and now used by Cascading and Flink in addition to Hive and Pig.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Tez&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;August 2017 - 0.9&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;, &lt;a href=&quot;/technologies/mapr-expansion-pack/&quot;&gt;MapR Expansion Pack&lt;/a&gt;, &lt;a href=&quot;/technologies/amazon-emr/&quot;&gt;Amazon EMR&lt;/a&gt;, &lt;a href=&quot;/technologies/google-cloud-dataproc/&quot;&gt;Google Cloud DataProc&lt;/a&gt;, &lt;a href=&quot;/technologies/qubole-data-service/&quot;&gt;Qubole Data Service&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.9&lt;/td&gt; &lt;td&gt;2017-07-27&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://tez.apache.org/releases/apache-tez-0-9-0.html&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://tez.apache.org/&quot;&gt;https://tez.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://hortonworks.com/blog/apache-tez-a-new-chapter-in-hadoop-data-processing/&quot;&gt;http://hortonworks.com/blog/apache-tez-a-new-chapter-in-hadoop-data-processing/&lt;/a&gt; - introduction to Tez, and links to further documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://hortonworks.com/blog/introducing-apache-tez-0-5/&quot;&gt;http://hortonworks.com/blog/introducing-apache-tez-0-5/&lt;/a&gt; - developer documentation introduction&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.slideshare.net/HadoopSummit/yahoos-experience-running-pig-on-tez-at-scale&quot;&gt;http://www.slideshare.net/HadoopSummit/yahoos-experience-running-pig-on-tez-at-scale&lt;/a&gt; - case study of Tez at scale&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/apache/tez/&quot;&gt;https://hortonworks.com/apache/tez/&lt;/a&gt; - Hortonworks information&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://tez.apache.org/releases/index.html&quot;&gt;https://tez.apache.org/releases/index.html&lt;/a&gt; - release history&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache ZooKeeper</title><link>https://ondataengineering.net/technologies/apache-zookeeper/</link><description> &lt;p&gt;Service for managing coordination (e.g. configuration information and synchronisation) of distributed and clustered systems. Based on a hierarchical key-value store, with support for things such as sequential nodes (whose names are automatically assigned a sequence number suffix), ephemeral nodes (which only exist whilst their owners session exists) and the ability to watch nodes. Guarantees that all writes are serial and ordered (i.e. all clients will see them in the same order), meaning it's more appropriate for low write high read scenarios. Can run in a high available cluster called an ensemble. Originally an Hadoop sub-project, but graduated to a top level Apache project in January 2011. Java based, still under active development, and used by a range of technologies including Hadoop, Mesos, HBase, Kafka and Solr.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;ZooKeeper&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Open Source - Active&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;February 2017 - 3.4&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Add ons&lt;/td&gt;&lt;td&gt;Apache Curator&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-flow/&quot;&gt;Hortonworks DataFlow&lt;/a&gt;, &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;Cloudera CDH&lt;/a&gt;, &lt;a href=&quot;/technologies/mapr-converged-data-platform/&quot;&gt;MapR Converged Data Platform&lt;/a&gt;, &lt;a href=&quot;/technologies/amazon-emr/&quot;&gt;Amazon EMR&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Is used by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-mesos/&quot;&gt;Apache Mesos&lt;/a&gt;, &lt;a href=&quot;/technologies/chronos/&quot;&gt;Chronos&lt;/a&gt;, &lt;a href=&quot;/technologies/mesosphere-marathon/&quot;&gt;Mesosphere Marathon&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://zookeeper.apache.org/&quot;&gt;https://zookeeper.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://zookeeper.apache.org/doc/current/&quot;&gt;https://zookeeper.apache.org/doc/current/&lt;/a&gt; - current documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://zookeeper.apache.org/doc/current/zookeeperOver.html&quot;&gt;https://zookeeper.apache.org/doc/current/zookeeperOver.html&lt;/a&gt; - excellent introduction&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/apache/zookeeper/&quot;&gt;https://hortonworks.com/apache/zookeeper/&lt;/a&gt; - Hortonworks information&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://zookeeper.apache.org/releases.html&quot;&gt;https://zookeeper.apache.org/releases.html&lt;/a&gt; - release history&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hue</title><link>https://ondataengineering.net/technologies/hue/</link><description> &lt;p&gt;Web application to allow users and administrators to work with a Hadoop cluster. Features include a SQL query tool (with auto-complete, a SQL expression builder, plotting results as a graph or on a map, and the ability to refine results) over any JDBC compatible database, a Pig query tool (with auto-complete and parameterised queries), a Solr search tool (drag a drop creation of Solr dashboards with grid, timeline, graph, map and filter widgets, a tool for indexing data into Solr and a Solr index browser), a query notebook (Spark, PySpark, Scala, Hive, Impala, Pig and R queries along with visualisation of results as graphs and maps), an Oozie management tool (graphical Oozie workflow, coordinator and bundle editors and an Oozie monitoring and management dashboard), an Apache Sentry configuration tool (for managing permissions to Hive tables and Solr collections), an HDFS and S3 file browser and manager (including the ability to upload and edit data), a YARN job browser (viewing logs and statistics), a Hive Metastore manager (browse, view sample data, create and manage databases and tables), an HBase table manager (browse, view, edit, create and manage tables), a Sqoop2 manager (create, manage and execute Sqoop2 jobs), a ZooKeeper manager (list, view and edit) and a user workspace for saving work done in Hue, organising this in folders and sharing it with other users. Originally released by Cloudera as Cloudera Desktop in October 2009, before being open sourced as Hue in June 2010. Python/Django based, under active development with a wide range of contributors, and available for all major Hadoop distributions.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Hue&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/cloudera/&quot;&gt;Cloudera&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;April 2018 - 4.2&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;Cloudera CDH&lt;/a&gt;, &lt;a href=&quot;/technologies/mapr-expansion-pack/&quot;&gt;MapR Expansion Pack&lt;/a&gt;, &lt;a href=&quot;/technologies/amazon-emr/&quot;&gt;Amazon EMR&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;2017-07-31&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://gethue.com/hue-4-and-its-new-interface-is-out/&quot;&gt;announcement&lt;/a&gt; &lt;a href=&quot;http://gethue.com/the-hue-4-user-interface-in-detail/&quot;&gt;UI details&lt;/a&gt;&lt;/td&gt; &lt;td&gt;New UI&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;4.1&lt;/td&gt; &lt;td&gt;2017-10-04&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://gethue.com/hue-4-1-is-out/&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;4.2&lt;/td&gt; &lt;td&gt;2018-04-04&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://gethue.com/hue-4-2-and-its-self-service-bi-improvements-are-out/&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;http://gethue.com/category/hue-4-2/&quot;&gt;blog posts&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;4.3&lt;/td&gt; &lt;td&gt;tbc&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://gethue.com/category/hue-4-3/&quot;&gt;blog posts&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://gethue.com/&quot;&gt;http://gethue.com/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/cloudera/hue&quot;&gt;https://github.com/cloudera/hue&lt;/a&gt; - code repository&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/products/open-source/apache-hadoop/hue.html&quot;&gt;https://www.cloudera.com/products/open-source/apache-hadoop/hue.html&lt;/a&gt; - Cloudera information&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/documentation/enterprise/latest/topics/hue.html&quot;&gt;https://www.cloudera.com/documentation/enterprise/latest/topics/hue.html&lt;/a&gt; - Cloudera documentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://gethue.com/blog/&quot;&gt;http://gethue.com/blog/&lt;/a&gt; - Hue blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Zeppelin</title><link>https://ondataengineering.net/technologies/apache-zeppelin/</link><description> &lt;p&gt;A web based notebook for interactive data analytics. Supports a wide range of interpreters (including Spark, JDBC SQL, Pig, Elasticsearch, Beam, Flink, Shell, Python amongst many others), a range of output formats (plain text, HTML, mathematical expressions using MathJax and tabular data), a range of visualisations for tabular data (including the ability to add more via a JavaScript NPM based plugin system called Helium), forms for user entry of parameters, and an Angular API to enable dynamic and interactive functionality within notebooks. Has a plugable storage for notebooks (with out of the box support for git, S3, Azure and ZeppelinHub), support for multi-user environments and a security model. Open sourced by NFLabs (now called ZEPL) in 2013 before being donated to the Apache Foundation in December 2014, graduating in May 2016. Under active development with a wide range of contributors, led by ZEPL, who sell Zeppelin as a managed service (previously called ZeppelinHub, now just called Zepl).&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Zeppelin&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;, ZEPL&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;July 2018 - 0.8&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;, &lt;a href=&quot;/technologies/amazon-emr/&quot;&gt;Amazon EMR&lt;/a&gt;, &lt;a href=&quot;/technologies/qubole-data-service/&quot;&gt;Qubole Data Service&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.7&lt;/td&gt; &lt;td&gt;2017-02-05&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://zeppelin.apache.org/releases/zeppelin-release-0.7.0.html&quot;&gt;release note&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.8&lt;/td&gt; &lt;td&gt;2018-06-28&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://zeppelin.apache.org/releases/zeppelin-release-0.8.0.html&quot;&gt;release note&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://zeppelin.apache.org/&quot;&gt;http://zeppelin.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://zeppelin.apache.org/docs/&quot;&gt;https://zeppelin.apache.org/docs/&lt;/a&gt; - documentation by version&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.zepl.com/&quot;&gt;https://www.zepl.com/&lt;/a&gt; - Zepl homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/apache/zeppelin/&quot;&gt;https://hortonworks.com/apache/zeppelin/&lt;/a&gt; - Hortonworks information&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.4/bk_zeppelin-component-guide/content/index.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.4/bk_zeppelin-component-guide/content/index.html&lt;/a&gt; - Hortonworks 2.6.4 documentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://zeppelin.apache.org/&quot;&gt;https://zeppelin.apache.org/&lt;/a&gt; - release announcements via the homepage&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Quantcast File System</title><link>https://ondataengineering.net/technologies/quantcast-file-system/</link><description> &lt;p&gt;Open source HDFS compatible distributed file system, which focuses on improving performance and scalability over HDFS. Uses erase coding (specifically Reed-Solomon error correction) allowing each data block to be stored with a 50% overhead over 9 nodes with data able to be read from any 6 (half the space required by HDFS with 3 way replication). Also supports online addition of new data (chunk) nodes, automatic re-balancing and re-replication of data, Unix style permissions support and C++ and Java client libraries. Published benchmarks suggest a 50/75% read/write performance increase over HDFS, and significantly faster metadata operations. Now also runs over Amazon S3. Built and maintained by Quantcast, who open sourced it in August 2012. An evolution of the Kosmos File System (KFS), an open source project started by Kosmix in 2005, which Quantcast first adopted in 2007. Built in C++ and released under the Apache 2.0 licence.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;QFS&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;Quantcast&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Open Source - Quiet&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;February 2017 - 1.2&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;p&gt;&lt;a href=&quot;https://quantcast.github.io/qfs/&quot;&gt;https://quantcast.github.io/qfs/&lt;/a&gt; - homepage &lt;a href=&quot;https://github.com/quantcast/qfs/&quot;&gt;https://github.com/quantcast/qfs/&lt;/a&gt; - code &lt;a href=&quot;https://github.com/quantcast/qfs/wiki/&quot;&gt;https://github.com/quantcast/qfs/wiki/&lt;/a&gt; - documentation &lt;a href=&quot;https://github.com/quantcast/qfs/wiki/Introduction-To-QFS&quot;&gt;https://github.com/quantcast/qfs/wiki/Introduction-To-QFS&lt;/a&gt; - introduction and summary of benefits &lt;a href=&quot;https://github.com/quantcast/qfs/wiki/Performance-Comparison-to-HDFS&quot;&gt;https://github.com/quantcast/qfs/wiki/Performance-Comparison-to-HDFS&lt;/a&gt; - performance comparison to HDFS &lt;a href=&quot;https://www.quantcast.com/blog/quantcast-file-system-on-amazon-s3/&quot;&gt;https://www.quantcast.com/blog/quantcast-file-system-on-amazon-s3/&lt;/a&gt; - information on running over S3 &lt;a href=&quot;https://gigaom.com/2012/09/27/quantcast-releases-bigger-faster-stronger-hadoop-file-system/&quot;&gt;https://gigaom.com/2012/09/27/quantcast-releases-bigger-faster-stronger-hadoop-file-system/&lt;/a&gt; - background information &lt;a href=&quot;http://www.odbms.org/blog/2013/03/big-data-improving-hadoop-for-petascale-processing-at-quantcast/&quot;&gt;http://www.odbms.org/blog/2013/03/big-data-improving-hadoop-for-petascale-processing-at-quantcast/&lt;/a&gt; - interview with creators&lt;/p&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;p&gt;&lt;a href=&quot;http://www.quantcast.com/feed/&quot;&gt;http://www.quantcast.com/feed/&lt;/a&gt; - occasional updates on the Quantcast blog&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Greenplum</title><link>https://ondataengineering.net/technologies/greenplum/</link><description> &lt;p&gt;A shared nothing, massively parallel processing (MPP) database optimised for analytical / OLAP workloads. Based on a fork PostgreSQL, it is essentially multiple PostgreSQL databases working together as a single logical database. Supports a cost-based query optimiser optimised for large analytical workloads, multiple storage models (including append only, columnar and heap), full ACID compliance and concurrent transactions, multiple index types, broad SQL support, a range of client connectors (including ODBC and JDBC), high capacity bulk load and unload tools, in database query language support (including Python, R, Perl, Java and C), and in database analytics support (including machine learning via Apache MADLib, search using Solr via GPText, geographic analytics via PostGIS and encryption via PGCrypto). Originally created by Greenplum (the company) which was founded in September 2003 before being brought by EMC in 2010, with Greenplum (the database) then spun out as part of Pivotal Software in 2013 before being open sourced in in October 2015 under the Apache 2.0 licence with the source code hosted on GitHub. Development is still led by Pivotal (with little evidence of outside contributions), who also distribute binaries as Pivotal Greenplum and provide training, consultancy and support.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Pivotal Greenplum, GPDB&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;Pivotal&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;September 2018 - 5.11&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.0&lt;/td&gt; &lt;td&gt;2017-09-14&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://gpdb.docs.pivotal.io/500/relnotes/GPDB_500_README.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.1&lt;/td&gt; &lt;td&gt;2017-10-21&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://gpdb.docs.pivotal.io/510/relnotes/GPDB_510_README.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.2&lt;/td&gt; &lt;td&gt;2017-11-22&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://gpdb.docs.pivotal.io/520/relnotes/GPDB_520_README.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.3&lt;/td&gt; &lt;td&gt;2017-12-07&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://gpdb.docs.pivotal.io/530/relnotes/GPDB_530_README.html&quot;&gt;release notes&lt;/a&gt; &lt;a href=&quot;https://content.pivotal.io/blog/introducing-greenplum-5-3&quot;&gt;blog&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Initial containerisation support&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.4&lt;/td&gt; &lt;td&gt;2018-01-17&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://gpdb.docs.pivotal.io/540/relnotes/GPDB_540_README.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.5&lt;/td&gt; &lt;td&gt;2018-02-21&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://gpdb.docs.pivotal.io/550/relnotes/GPDB_550_README.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.6&lt;/td&gt; &lt;td&gt;2018-03-23&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://gpdb.docs.pivotal.io/560/relnotes/GPDB_560_README.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.7&lt;/td&gt; &lt;td&gt;2018-04-03&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://content.pivotal.io/pivotal-greenplum/pivotal-greenplum-5-8-released&quot;&gt;blog post&lt;/a&gt;; &lt;a href=&quot;http://gpdb.docs.pivotal.io/570/relnotes/GPDB_570_README.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.8&lt;/td&gt; &lt;td&gt;2018-05-10&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://gpdb.docs.pivotal.io/580/relnotes/GPDB_580_README.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.9&lt;/td&gt; &lt;td&gt;2018-06-20&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://greenplum.org/greenplum-5-9-0-a-minor-but-powerful-release/&quot;&gt;blog post&lt;/a&gt;; &lt;a href=&quot;http://gpdb.docs.pivotal.io/590/relnotes/GPDB_590_README.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.10&lt;/td&gt; &lt;td&gt;2018-07-26&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://gpdb.docs.pivotal.io/5100/relnotes/GPDB_5100_README.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.11&lt;/td&gt; &lt;td&gt;2018-09-15&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://gpdb.docs.pivotal.io/5110/relnotes/GPDB_5110_README.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;p&gt;&lt;a href=&quot;http://greenplum.org/&quot;&gt;http://greenplum.org/&lt;/a&gt; - open source project homepage &lt;a href=&quot;https://github.com/greenplum-db/gpdb&quot;&gt;https://github.com/greenplum-db/gpdb&lt;/a&gt; - code repository &lt;a href=&quot;https://pivotal.io/pivotal-greenplum&quot;&gt;https://pivotal.io/pivotal-greenplum&lt;/a&gt; - Pivotal Greenplum homepage &lt;a href=&quot;https://github.com/greenplum-db/greenplum-db.github.io/wiki/Greenplum-Architecture&quot;&gt;https://github.com/greenplum-db/greenplum-db.github.io/wiki/Greenplum-Architecture&lt;/a&gt; - architecture overview &lt;a href=&quot;https://content.pivotal.io/datasheets/pivotal-greenplum&quot;&gt;https://content.pivotal.io/datasheets/pivotal-greenplum&lt;/a&gt; - Pivotal Greenplum datasheet &lt;a href=&quot;http://gpdb.docs.pivotal.io/&quot;&gt;http://gpdb.docs.pivotal.io/&lt;/a&gt; - documentation &lt;a href=&quot;https://network.pivotal.io/products/pivotal-gpdb&quot;&gt;https://network.pivotal.io/products/pivotal-gpdb&lt;/a&gt; - download site&lt;/p&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;p&gt;&lt;a href=&quot;http://greenplum.org/&quot;&gt;http://greenplum.org/&lt;/a&gt; - link to Greenplum announcement mailing list&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hortonworks Data Platform</title><link>https://ondataengineering.net/technologies/hortonworks-data-platform/</link><description> &lt;p&gt;A distribution of Hadoop based on a commitment to the Apache open source ecosystem. All bundled projects are Apache open source projects based on official Apache project releases, with any patches for bug fixes or new features being official Apache project patches pulled from later releases of the relevant project. Available as RPMs, through Apache Ambari (for local installs) or Cloudbreak (for installation on cloud platforms), and as an on-site or in the cloud managed service (as Hortonworks Operational Services). Comes with a number of add-ons that aren't part of the core product, including HDP Search, Hortonworks HDB and ODBC and JDBC drivers for Hive, Spark SQL and Apache Phoenix. The HDP software is available free of charge, with training, consultancy and support available from Hortonworks, including a flex support subscription, a consumption based model for the use of HDP on-premise or in the cloud. Also available for IBM Power Systems. The Hortonworks Data Platform was first released in June 2012.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;HDP&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/hortonworks/&quot;&gt;Hortonworks&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Categories&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-categories/hadoop-distributions/&quot;&gt;Hadoop Distributions&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;July 2017 - 3.0&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Packages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-accumulo/&quot;&gt;Apache Accumulo&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-atlas/&quot;&gt;Apache Atlas&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-calcite/&quot;&gt;Apache Calcite&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-datafu/&quot;&gt;Apache DataFu&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-druid/&quot;&gt;Apache Druid (Incubating)&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Apache Hadoop&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hbase/&quot;&gt;Apache HBase&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Apache Hive&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-knox/&quot;&gt;Apache Knox&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-livy/&quot;&gt;Apache Livy&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-oozie/&quot;&gt;Apache Oozie&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-phoenix/&quot;&gt;Apache Phoenix&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-pig/&quot;&gt;Apache Pig&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-ranger/&quot;&gt;Apache Ranger&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-solr/&quot;&gt;Apache Solr&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-sqoop/&quot;&gt;Apache Sqoop&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-storm/&quot;&gt;Apache Storm&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-tez/&quot;&gt;Apache Tez&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-zeppelin/&quot;&gt;Apache Zeppelin&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-zookeeper/&quot;&gt;Apache ZooKeeper&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Add ons&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-hawq/&quot;&gt;Apache HAWQ&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform-search/&quot;&gt;Hortonworks Data Platform Search&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-smartsense/&quot;&gt;Hortonworks SmartSense&lt;/a&gt;, Hortonworks Cybersecurity Package&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Manageable via&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-ambari/&quot;&gt;Apache Ambari&lt;/a&gt;, &lt;a href=&quot;/technologies/cloudbreak/&quot;&gt;Cloudbreak&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/azure-hdinsight/&quot;&gt;Azure HDInsight&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;bundled-technologies&quot;&gt;Bundled Technologies&lt;/h2&gt; &lt;p&gt;The base Apache project versions bundled with each version of HDP are shown on the HDP home page, as well as on the first page of the release notes (see &lt;a href=&quot;#links&quot;&gt;Links&lt;/a&gt; section below for link to latest release notes). Details of the features in these releases that Hortonworks don’t support, and the patches that have been applied to these releases are also available in the release notes, along with known vulnerabilities, fixes from previous versions and known issues. Note that DataFu and Livy are referenced in the release notes but not on the HDP home page.&lt;/p&gt; &lt;p&gt;Notable packaged software changes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/druid&quot;&gt;Druid&lt;/a&gt; was added in tech preview in HDP 2.6&lt;/li&gt; &lt;li&gt;The following components were deprecated in HDP 2.6 and removed in HDP 3.0: &lt;a href=&quot;/technologies/apache-falcon&quot;&gt;Apache Falcon&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-flume&quot;&gt;Apache Flume&lt;/a&gt; (advice is to consider &lt;a href=&quot;/technologies/hortonworks-data-flow&quot;&gt;HDF&lt;/a&gt; instead), &lt;a href=&quot;/technologies/apache-mahout&quot;&gt;Apache Mahout&lt;/a&gt; (advice is to consider &lt;a href=&quot;/technologies/apache-spark/mllib&quot;&gt;Spark MLLib&lt;/a&gt; instead), &lt;a href=&quot;/technologies/apache-slider&quot;&gt;Apache Slider&lt;/a&gt; (being folded into YARN), &lt;a href=&quot;/technologies/hue&quot;&gt;Hue&lt;/a&gt; (advice is to consider &lt;a href=&quot;/technologies/apache-ambari/ambari-views&quot;&gt;Ambari Views&lt;/a&gt; instead) and Cascading.&lt;/li&gt; &lt;li&gt;The following components were deprecated in HDP 2.6 as they were being moved into other Hortonworks products/offerings as of HDP 3.0, however this deprecation appeared to be reversed in HDP 2.6.4: &lt;a href=&quot;/technologies/apache-accumulo&quot;&gt;Apache Accumulo&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-kafka&quot;&gt;Apache Kafka&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-storm&quot;&gt;Apache Storm&lt;/a&gt; and &lt;a href=&quot;/technologies/cloudbreak/&quot;&gt;Cloudbreak&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Solr 7 was added in HDP 3.0, with HDP Search upgraded to 3.0 and still available, however only supporting Solr 6&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.6&lt;/td&gt; &lt;td&gt;2017-02-28&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://hortonworks.com/blog/announcing-the-general-availability-of-hortonworks-data-platform-2-6/&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.0/bk_release-notes/content/new_features.html&quot;&gt;new features&lt;/a&gt;&lt;/td&gt; &lt;td&gt;See notes above for tech changes&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;td&gt;2018-07-13&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://hortonworks.com/blog/announcing-hdp-3-0-faster-smarter-hybrid-data/&quot;&gt;early access announcement&lt;/a&gt;; &lt;a href=&quot;https://hortonworks.com/blog/announcing-general-availability-hortonworks-data-platform-3-0-0-ambari-2-7-0-smartsense-1-5-0/&quot;&gt;GA announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;A version of the Hortonworks Data Platform for Windows (&lt;a href=&quot;/technologies/hortonworks-data-platform-for-windows/&quot;&gt;HDP for Windows&lt;/a&gt; was available, however was discontinued in August 2016 with the last release being HDP 2.4. A specific product for running HDP on AWS (&lt;a href=&quot;/technologies/hortonworks-data-cloud-for-aws/&quot;&gt;Hortonworks Data Cloud for AWS&lt;/a&gt;) was also available, but was discontinued with the release of HDP 3.0 and the move to a multi cloud strategy.&lt;/p&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/products/data-platforms/hdp/&quot;&gt;https://hortonworks.com/products/data-platforms/hdp/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.0/index.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.0/index.html&lt;/a&gt; - HDP 3.0.0 documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.0/release-notes/index.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.0/release-notes/index.html&lt;/a&gt; - HDP 3.0.0 release notes&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://docs.hortonworks.com/index.html&quot;&gt;http://docs.hortonworks.com/index.html&lt;/a&gt; - shows latest Hortonworks release version&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/category/hdp/&quot;&gt;https://hortonworks.com/blog/category/hdp/&lt;/a&gt; - Hortonworks blog posts&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Atlas</title><link>https://ondataengineering.net/technologies/apache-atlas/</link><description> &lt;p&gt;A metadata and data governance solution for Hadoop. Supports an extensible metadata model with out of the box support for Hive datasets and data lineage from Hive queries and Sqoop imports, with limited support for Falcon, Storm and Kafka. Allows datasets and data items to be tagged (and for these tags to be used for access control by Apache Ranger), and includes support for business taxonomies as a technical preview. Implemented as a graph based database using Titan (which by default uses HBase and Solr), with a web based user interface and a REST API for searching and visualising/retrieving metadata, and Kafka topics for the ingest of metadata (primarily from hooks in metadata sources such as Hive or Sqoop) and the publishing of metadata change events. Donated to the Apache Foundation in May 2015 by the Hortonworks Data Governance Initiative in partnership with Aetna, Merck, Target, Schlumberger and SAS, graduating in June 2017. Has not yet reached a v1.0 milestone, but is still under active development.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Atlas&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;September 2018 - v1.1&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.8&lt;/td&gt; &lt;td&gt;2017-03-31&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/www-announce/201703.mbox/%3C8634D8C3-56D3-4E13-B292-B6C51F6AD5CC%40apache.org%3E&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;td&gt;2018-06-03&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/www-announce/201806.mbox/%3C05E989B0-4F8E-4BCE-88B0-2CC06DE2241F%40apache.org%3E&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;http://atlas.apache.org/1.0.0/WhatsNew-1.0.html&quot;&gt;What’s New&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.1&lt;/td&gt; &lt;td&gt;2018-09-19&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/www-announce/201809.mbox/%3CCAF5p1-MtRfvSWy4zBGmPZDAaisKCmMuxsWvEE3A115XJnDj1Nw@mail.gmail.com%3E&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;http://atlas.apache.org/WhatsNew-1.1.html&quot;&gt;What’s New&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://atlas.apache.org/&quot;&gt;http://atlas.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://atlas.apache.org/Architecture.html&quot;&gt;http://atlas.apache.org/Architecture.html&lt;/a&gt; - Architecture overview&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://hortonworks.com/apache/atlas&quot;&gt;http://hortonworks.com/apache/atlas&lt;/a&gt; - Hortonworks background information, including links to relevant blog posts&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.4/bk_data-governance/content/index.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.4/bk_data-governance/content/index.html&lt;/a&gt; - Hortonworks 2.6.4 documentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Blog updates via the Hortonworks and Apache blogs&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Ranger</title><link>https://ondataengineering.net/technologies/apache-ranger/</link><description> &lt;p&gt;A centralised security framework for managing access to data in Hadoop. Supports integration with LDAP and Active Directory for user authentication, a central policy server/store (with a web based administration interface and REST API), and plugins for Hadoop components (including HDFS, Hive, HBase, Storm, Knox, Solr, Kafka, YARN, Atlas and NiFi) to manage authorisation of user access to data. Supports data masking and row level access policies (currently only supported by Hive), the ability to define policies against tags as well as directly against resources (with tags assigned to resources externally, e.g. in Apache Atlas), and the ability to use more complex conditions (e.g. denying access after an expiration date or based on a users location). Extendable with the ability to add support for new services (Ranger Stacks) and to add custom decision rules (via content enrichers and condition evaluators). Also supports a full audit capability of access requests and decisions, and a key management service for HDFS encryption keys. An Apache project, donated in July 2014 as Argus by the Hortonworks following their acquisition of XA Secure, graaduating in February 2017. Reached v1.0 in March 2018, and is still under active development with a range of contributors.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Ranger&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;October 2018 - v1.2&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-flow/&quot;&gt;Hortonworks DataFlow&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.7&lt;/td&gt; &lt;td&gt;2017-02-28&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/RANGER/0.7.0+Release+-+Apache+Ranger&quot;&gt;wiki page&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;td&gt;2018-03-19&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/RANGER/1.0.0+Release+-+Apache+Ranger&quot;&gt;wiki page&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.1&lt;/td&gt; &lt;td&gt;2018-07-09&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/RANGER/1.1.0+Release+-+Apache+Ranger&quot;&gt;wiki page&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.2&lt;/td&gt; &lt;td&gt;2018-10-04&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/RANGER/1.2.0+Release+-+Apache+Ranger&quot;&gt;wiki page&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://ranger.incubator.apache.org/&quot;&gt;http://ranger.incubator.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://ranger.apache.org/faq.html&quot;&gt;http://ranger.apache.org/faq.html&lt;/a&gt; - FAQs&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/RANGER&quot;&gt;https://cwiki.apache.org/confluence/display/RANGER&lt;/a&gt; - Apache Ranger Wiki, with most information detailed by release under the Release Folders page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://hortonworks.com/apache/ranger/&quot;&gt;http://hortonworks.com/apache/ranger/&lt;/a&gt; - Hortonworks information on Ranger, including tutorials and blog posts&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.4/bk_security/content/ch_hdp-security-guide-authorization.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.4/bk_security/content/ch_hdp-security-guide-authorization.html&lt;/a&gt; - Hortonworks 2.6.4 documentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Blog updates via the Hortonworks and Apache blogs&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Knox</title><link>https://ondataengineering.net/technologies/apache-knox/</link><description> &lt;p&gt;A stateless gateway for the Apache Hadoop ecosystem that provides perimeter security. Includes support for user authentication (via LDAP, Active Directory and a number of single sign on solutions), access authorisation on a per service basis, transitions to Kerberos authentication, reverse proxying and auditing, extension points for supporting new services, audit capabilities, and out of the box support for a number of Hadoop technology end points. An Apache project, started by Hortonworks in February 2013, donated to the Apache Foundation two months later in April, before graduating in February 2014. Hit v1.0 in February 2018, and still under active development.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Knox&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;August 2018 - v1.1&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.12&lt;/td&gt; &lt;td&gt;2017-03-20&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/www-announce/201703.mbox/%3CCA%2BTBRctuHBLB%3DC4gHggQJaGjzPaMUMprcXx-P_mmSnLvf-55OQ%40mail.gmail.com%3E&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.13&lt;/td&gt; &lt;td&gt;2017-08-19&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/www-announce/201708.mbox/%3CCACRbFyhaO6vfUn66toWKVuOhoPKeab9G0AOZPWcHRjjN+zaWQg@mail.gmail.com%3E&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KNOX/News#News-2017-08-19ApacheKnoxGateway0.13.0Released!&quot;&gt;post&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.14&lt;/td&gt; &lt;td&gt;2017-12-14&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://mail-archives.us.apache.org/mod_mbox/www-announce/201712.mbox/%3CCACRbFygcSMzkP9yNNvm2jCn2Sibz02OWTjJw3_2S5TWw1UTM=g@mail.gmail.com%3E&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KNOX/News#News-2017-12-14ApacheKnoxGateway0.14.0Released!&quot;&gt;post&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;td&gt;2018-02-07&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://mail-archives.us.apache.org/mod_mbox/www-announce/201802.mbox/%3CCACRbFyheDO-jqjYKvdQ_Rht2O=OHCfDjAktsmEF2+sEkcg7Zjg@mail.gmail.com%3E&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KNOX/News#News-2018-02-07ApacheKnoxGateway1.0.0Released!&quot;&gt;post&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.1&lt;/td&gt; &lt;td&gt;2018-07-30&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/www-announce/201807.mbox/%3CCACRbFyg-F_17QiohoJB+69Hv_K6BamUCmMyZ0M8fghibwi7b5g@mail.gmail.com%3E&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KNOX/News#News-2018-07-30ApacheKnoxGateway1.1.0Released!&quot;&gt;post&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://knox.apache.org/&quot;&gt;http://knox.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://knox.apache.org/books/knox-0-11-0/user-guide.html&quot;&gt;http://knox.apache.org/books/knox-0-11-0/user-guide.html&lt;/a&gt; - extreemly comprehensive documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://hortonworks.com/apache/knox-gateway/&quot;&gt;http://hortonworks.com/apache/knox-gateway/&lt;/a&gt; - Hortonworks information&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.4/bk_security/content/perimeter_security_with_apache_knox.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.4/bk_security/content/perimeter_security_with_apache_knox.html&lt;/a&gt; - Hortonworks 2.6.4 documentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Blog updates via the Hortonworks and Apache blogs&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/KNOX/News&quot;&gt;https://cwiki.apache.org/confluence/display/KNOX/News&lt;/a&gt; - news updates&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Falcon</title><link>https://ondataengineering.net/technologies/apache-falcon/</link><description> &lt;p&gt;Data feed management system for Hadoop. Supports the definition, scheduling and orchestration (including support for late data and retry policies) of data processing pipelines (referred to as processes, with support for Ozzie, Spark, Hive and Pig jobs), the management of the data produced and consumed by these pipelines (referred to as feeds, with support for data in HDFS and Hive) and the generation and visualisation of pipeline lineage information, all across multiple Hadoop clusters. Also includes the ability to mirror or replicate HDFS and Hive data between clusters, to failover processing between clusters and to import and export data using Sqoop. Supports both a web and command line interface and a REST API. An Apache project, graduating in December 2014, having been originally donated by inMobi in April 2013. Hasn't yet reached a v1.0 milestone, is seeing very little development activity, and as of HDP 3.0 will no longer be distributed by Hortonworks.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Falcon&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Open Source - Quiet&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;March 2018 - v0.11&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.11&lt;/td&gt; &lt;td&gt;2018-03-12&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/download/attachments/61318307/RelaseNotes-ApacheFalcon-0.11.pdf?api=v2&quot;&gt;release note&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://falcon.apache.org/&quot;&gt;http://falcon.apache.org/&lt;/a&gt; - homepage and documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/FALCON/Release+Notes&quot;&gt;https://cwiki.apache.org/confluence/display/FALCON/Release+Notes&lt;/a&gt; - release notes&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://hortonworks.com/apache/falcon/&quot;&gt;http://hortonworks.com/apache/falcon/&lt;/a&gt; - Hortonworks information on Falcon, including tutorials and blog posts&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.4/bk_data-movement-and-integration/content/index.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.4/bk_data-movement-and-integration/content/index.html&lt;/a&gt; - Hortonworks 2.6.4 documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://community.hortonworks.com/questions/97570/apache-falcon-in-hdp-30.html&quot;&gt;https://community.hortonworks.com/questions/97570/apache-falcon-in-hdp-30.html&lt;/a&gt; - Hortonworks community update on deprecation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Blog updates via the Hortonworks and Apache blogs&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Calcite</title><link>https://ondataengineering.net/technologies/apache-calcite/</link><description> &lt;p&gt;A framework for building SQL based data access capabilities. Supports a SQL parser and validator, tools for the transformation and (cost based) optimisation of SQL expression trees, and an adapter framework for accessing metadata and executing queries (including out of the box adapters for a number of database technologies as well as CSV files and POJO objects), along with specific support for streaming SQL queries and optimising data cube queries to use materialised views. Also includes (as a sub-project named Avatica), a framework for building database drivers with support for a standard JDBC driver, server and wire protocols, plus a local embeddable JDBC driver. Used in a range of other projects including Drill, Flink, Hive, Kylin, Phoenix, Samza, Storm and Cascading. An Apache project, originally created by Julian Hyde in May 2012 as Optiq, donated to the Apache Foundation in May 2014, graduating in October 2015 following a v1.0 release in January 2015. Under active development with a range of contributors.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Calcite, Avatica&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Open Source - Active&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;July 2018 - v1.17&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.12&lt;/td&gt; &lt;td&gt;2017-03-24&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://calcite.apache.org/news/2017/03/24/release-1.12.0/&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.13&lt;/td&gt; &lt;td&gt;2017-06-26&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://calcite.apache.org/news/2017/06/26/release-1.13.0/&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.14&lt;/td&gt; &lt;td&gt;2017-10-04&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://calcite.apache.org/news/2017/10/02/release-1.14.0/&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.15&lt;/td&gt; &lt;td&gt;2017-12-11&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://calcite.apache.org/news/2017/12/11/release-1.15.0/&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.16&lt;/td&gt; &lt;td&gt;2018-03-19&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://calcite.apache.org/news/2018/03/19/release-1.16.0/&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.17&lt;/td&gt; &lt;td&gt;2018-07-20&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://calcite.apache.org/news/2018/07/20/release-1.17.0/&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://calcite.apache.org/&quot;&gt;https://calcite.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://calcite.apache.org/docs/&quot;&gt;https://calcite.apache.org/docs/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://calcite.apache.org/docs/avatica_overview.html&quot;&gt;https://calcite.apache.org/docs/avatica_overview.html&lt;/a&gt; - Avatica overview&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://calcite.apache.org/news/&quot;&gt;https://calcite.apache.org/news/&lt;/a&gt; - Calcite news&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://calcite.apache.org/avatica/news/&quot;&gt;https://calcite.apache.org/avatica/news/&lt;/a&gt; - Avatica news&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Slider (retired)</title><link>https://ondataengineering.net/technologies/apache-slider/</link><description> &lt;p&gt;Framework for hosting long running distributed applications on YARN, allowing YARN to manage the resources these applications use. Can handle any application that supports a base set of requirements (including being able to install and run from a tarball), with experimental support for docker packaged applications. Operates as a YARN application master (the Slider AM), an associated command line interface and lightweight agents to manage running components. Supports manual scaling, automatic recovery, rolling upgrades and component placement controls, and includes out of the box configuration for a number of applications including Accumulo, HBase, Kafka, Memcached, Solr, Storm and Tomcat. Originally donated to the Apache Foundation in April 2014 based on the Hortonworks Hoya (HBase on YARN) project, and subsequently consumed the DataTorrent Koya (Kafka on YARN) project. Retired before graduating in May 2018 following the plan to add support for long running services directly into YARN under YARN-4692.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Slider&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Open Source - Inactive&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;June 2018 - v0.92&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Add on to&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/yarn/&quot;&gt;YARN&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://slider.incubator.apache.org&quot;&gt;http://slider.incubator.apache.org&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/deploying-long-running-services-on-apache-hadoop-yarn-cluster/&quot;&gt;https://hortonworks.com/blog/deploying-long-running-services-on-apache-hadoop-yarn-cluster/&lt;/a&gt; - introduction blog post&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/apache/slider/&quot;&gt;https://hortonworks.com/apache/slider/&lt;/a&gt; - Hortonworks information on slider, including links to blog posts&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.4/bk_yarn-resource-management/content/ch_slider.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.4/bk_yarn-resource-management/content/ch_slider.html&lt;/a&gt; - Hortonworks 2.6.4 documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://slider.incubator.apache.org/docs/&quot;&gt;http://slider.incubator.apache.org/docs/&lt;/a&gt; - Slider documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/YARN-4692&quot;&gt;https://issues.apache.org/jira/browse/YARN-4692&lt;/a&gt; - YARN change that adds support for long running services to YARN&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Blog updates via the Hortonworks and Apache blogs&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Storm</title><link>https://ondataengineering.net/technologies/apache-storm/</link><description> &lt;p&gt;Specialised distributed stream processing technology based on a single record (not micro batch) model with at least once processing semantics. Processing flows are called topologies based on a directed acyclic graph of spouts (which produce unbounded streams of tuples) and bolts (which process streams and optionally produce output streams). Supports high throughput and low latency use cases, horizontal scalability, fault tolerance (failed workers are automatically restarted and failed over to new nodes if required), back pressure, windowing (with support for sliding and tumbling windows based on time or event counts), stateful bolts and a shared bolt storage cache (that's updatable from the command line). Also includes a higher level micro batch API (Trident) that supports exactly-once processing semantics, fault-tolerant state management and higher level operations including joins, aggregations and groupings, support for SQL (StormSQL) and frameworks and utilities to make defining and deploying topologies easier (Flux). Has both a graphical web based and command line interface, plus a REST API. Primarily written in Clojure, JVM based, but supports multiple languages through the use of Thrift for defining and submitting topologies, and the use of spouts that can interface to other languages using JSON over stdin/stdout. Originally created at BackType, before being open sourced in September 2011 after the acquisition of BackType by Twitter. Donated to the Apache Foundation in September 2013, graduating in September 2014, with a 1.0 release in April 2016. Has multiple reference cases for being deployed at scale, including Twitter, and is still under active development.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Storm&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;February 2018 - v1.2&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-flow/&quot;&gt;Hortonworks DataFlow&lt;/a&gt;, &lt;a href=&quot;/technologies/mapr-expansion-pack/&quot;&gt;MapR Expansion Pack&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.1&lt;/td&gt; &lt;td&gt;2017-03-29&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://storm.apache.org/2017/03/29/storm110-released.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.2&lt;/td&gt; &lt;td&gt;2018-02-16&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://storm.apache.org/2018/02/15/storm120-released.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://storm.apache.org/&quot;&gt;http://storm.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/apache/storm/&quot;&gt;https://hortonworks.com/apache/storm/&lt;/a&gt; - Hortonworks information, including tutorials and blog posts&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://storm.apache.org/releases/1.0.3/index.html&quot;&gt;http://storm.apache.org/releases/1.0.3/index.html&lt;/a&gt; - documentation for current release&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.4/bk_storm-component-guide/content/index.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.4/bk_storm-component-guide/content/index.html&lt;/a&gt; - Hortonworks 2.6.4 documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://nathanmarz.com/blog/history-of-apache-storm-and-lessons-learned.html&quot;&gt;http://nathanmarz.com/blog/history-of-apache-storm-and-lessons-learned.html&lt;/a&gt; - history of storm&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://storm.apache.org/index.html&quot;&gt;http://storm.apache.org/index.html&lt;/a&gt; - Storm new announced on Apache homepage&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Accumulo</title><link>https://ondataengineering.net/technologies/apache-accumulo/</link><description> &lt;p&gt;NoSQL wide-column datastore based on BigTable. Supports horizontal scalability, cell based access control (based on arbitrary boolean expressions of user security labels), high availability, atomic read-modify-write operations, map reduce support (both as a source and sink), table constraints, LDAP and Kerberos integration, the use of HDFS for underlying storage, and replication between instances. Comes with a web based monitoring interface (Accumulo Monitor) and a CLI. Written in Java, with thrift based API allowing access from other languages including C++, Python, Ruby. Originally developed at the NSA, donated to the Apache Foundation in September 2011, before graduating in March 2012, and is still under active development.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Accumulo&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;May 2018 - v1.9&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Add ons&lt;/td&gt;&lt;td&gt;Apache Fluo&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Manageable via&lt;/td&gt;&lt;td&gt;Muchos, Uno&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Add on to&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;Cloudera CDH&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.8&lt;/td&gt; &lt;td&gt;2016-09-06&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://accumulo.apache.org/release/accumulo-1.8.0/&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.9&lt;/td&gt; &lt;td&gt;2018-04-23&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://accumulo.apache.org/release/accumulo-1.9.0/&quot;&gt;release notes&lt;/a&gt;; &lt;a href=&quot;http://mail-archives.us.apache.org/mod_mbox/www-announce/201805.mbox/%3CCAL5zq9bpV1wU1Eo=8d0N46Sdp8stbPGJFQryQ7agKbXH6XSAKg@mail.gmail.com%3E&quot;&gt;critical bug fix release&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://accumulo.apache.org/&quot;&gt;http://accumulo.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://accumulo.apache.org/docs-archive/&quot;&gt;http://accumulo.apache.org/docs-archive/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/apache/accumulo/&quot;&gt;https://hortonworks.com/apache/accumulo/&lt;/a&gt; - Hortonworks information&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/products/open-source/apache-hadoop/apache-accumulo.html&quot;&gt;https://www.cloudera.com/products/open-source/apache-hadoop/apache-accumulo.html&lt;/a&gt; - Cloudera information&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/documentation/other/accumulo/latest.html&quot;&gt;https://www.cloudera.com/documentation/other/accumulo/latest.html&lt;/a&gt; - Cloudera documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://accumulo.apache.org/blog/2017/04/21/introducing-uno-and-muchos.html&quot;&gt;http://accumulo.apache.org/blog/2017/04/21/introducing-uno-and-muchos.html&lt;/a&gt; - blog post on Uno and Muchos&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://accumulo.apache.org/news/&quot;&gt;http://accumulo.apache.org/news/&lt;/a&gt; - news page&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Livy</title><link>https://ondataengineering.net/technologies/apache-livy/</link><description> &lt;p&gt;A service that allows Spark jobs (pre-compiled JARs) or code snippets (Scala or Python) to be executed by remote systems over a REST API or via clients for Java, Scala and Python. Supports re-use of Spark Contexts (and caching and sharing of RDDs across jobs and clients), multiple concurrent clients, secure authenticated communications and batch job submissions. Started in November 2015 based on code from Hue, with a formal announcement and first release in June 2016 based on development led by Cloudera, Hortonworks and Microsoft, before being donated to the Apache Foundation in June 2017. Hasn't yet graduated, but under active development, and used by tools such as Hue and Zeppelin.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Livy&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Open Source - Active&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;June 2017 - v0.3&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;, &lt;a href=&quot;/technologies/amazon-emr/&quot;&gt;Amazon EMR&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://livy.incubator.apache.org/&quot;&gt;https://livy.incubator.apache.org/&lt;/a&gt; - Apache homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/more/news-and-events/press-releases/2016-06-06-cloudera-microsoft-lead-development-open-source-project-livy-for-easy-use-spark-end-user-applications.html&quot;&gt;https://www.cloudera.com/more/news-and-events/press-releases/2016-06-06-cloudera-microsoft-lead-development-open-source-project-livy-for-easy-use-spark-end-user-applications.html&lt;/a&gt; - original announcement&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/livy-a-rest-interface-for-apache-spark/&quot;&gt;https://hortonworks.com/blog/livy-a-rest-interface-for-apache-spark/&lt;/a&gt; - Hortonworks view&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/apache/incubator-livy/releases&quot;&gt;https://github.com/apache/incubator-livy/releases&lt;/a&gt; - details of releases&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hortonworks Data Platform Search</title><link>https://ondataengineering.net/technologies/hortonworks-data-platform-search/</link><description> &lt;p&gt;An add on package to HDP that bundles up Solr, Banana, and a suite of libraries and tools for integrating with Solr from Hadoop (utilities for loading data from HDFS), Hive (a SerDe to allow Solr data to be read and written as a Hive table), Pig (store and load functions), HBase (replication of HBase events to Solr based on the Lily HBase indexer), Storm and Spark (both SDKs for integrating with Solr). Available as an add on Ambari management pack or as a set of RPMs. Built, maintained and supported by Lucidworks on behalf of Hortonworks, first announced in April 2014 as part of the introduction of Solr with HDP 2.1.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;HDP Search&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/hortonworks/&quot;&gt;Hortonworks&lt;/a&gt;, Lucidworks&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;July 2017 - 3.0&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Packages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-solr/&quot;&gt;Apache Solr&lt;/a&gt;, Banana&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Add on to&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.x&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt;Aligned to HDP 2.x releases, bundles Solr 5.x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;td&gt;2018-07-13&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDPS/HDPS-3.0.0/bk_solr-search-installation/content/hdp-search-30-relnotes.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Aligned to HDP 3.0 release, bundles Solr 6, however as of HDP 3.0 HDP bundles Solr 7 directly&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDPS/HDPS-3.0.0/index.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDPS/HDPS-3.0.0/index.html&lt;/a&gt; - 3.0 documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://doc.lucidworks.com/lucidworks-hdpsearch/2.6/index.html&quot;&gt;https://doc.lucidworks.com/lucidworks-hdpsearch/2.6/index.html&lt;/a&gt; - 2.6 Lucidworks documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.0/bk_solr-search-installation/content/ch_hdp-search.html&quot;&gt;http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.0/bk_solr-search-installation/content/ch_hdp-search.html&lt;/a&gt; - 2.6 Hortonworks installation documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/bringing-enterprise-search-enterprise-hadoop/&quot;&gt;https://hortonworks.com/blog/bringing-enterprise-search-enterprise-hadoop/&lt;/a&gt; - Partnership announcement blog post&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;See HDP updates - HDP Search tracks HDP releases&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache HAWQ</title><link>https://ondataengineering.net/technologies/apache-hawq/</link><description> &lt;p&gt;A port of the Greenplum MPP database (which itself is based on PostgreSQL) to run over YARN and HDFS. Supports all the features of Greenplum (ACID transactions, broad SQL support and in database language and analytics support, including support for Apache MADLib), integration with Apache Ambari, an Input Format for MapReduce to read HAWQ tables, and both row and Parquet (column) based storage of data managed by HAWQ. Also supports queries over data not managed by HAWQ via external tables, with a Java based framework (PXF) for accessing external data, and out of the box support for accessing data in HDFS (text, Avro, JSON), Hive and HBase, with a number of open source connectors also available. Fault tolerant and horizontally scalable, with the ability to scale up or down on the fly. Originally created as Pivotal HAWQ based on a fork of Greenplum in 2011, with an initial 1.0 release as part of Pivotal HD in July 2013. Open sourced and donated to the Apache Foundation in September 2015, becoming Apache HAWQ, with the first open source release (2.0) in October 2016, and graduating in August 2018. Development led by Pivotal, who also distribute binaries as Pivotal HDB and provide training, consultancy and support. Pivotal HDB is also available as Hortonworks HDB, with support and consultancy provided by Hortonworks.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Pivotal HDB, Hortonworks HDB&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;, Pivotal, &lt;a href=&quot;/tech-vendors/hortonworks/&quot;&gt;Hortonworks&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;September 2018 - v2.4&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Add on to&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.2&lt;/td&gt; &lt;td&gt;2017-07-21&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/incubator-general/201707.mbox/%3C3f6be308.9642.15d35aa787e.Coremail.huor@apache.org%3E&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.3&lt;/td&gt; &lt;td&gt;2018-03-18&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://mail-archives.us.apache.org/mod_mbox/www-announce/201803.mbox/%3CCAEUT=QtiQ=9XVDtyZjY8aocjy6J972+zBTuVD_30XX1GHq6pWA@mail.gmail.com%3E&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.4&lt;/td&gt; &lt;td&gt;2018-09-25&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://mail-archives.us.apache.org/mod_mbox/www-announce/201809.mbox/%3CCAH=zMbozqwnXLb4HCJs8-6rn6AFdezneS845Nz-nPL7PB8bSsw@mail.gmail.com%3E&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://hawq.incubator.apache.org/&quot;&gt;http://hawq.incubator.apache.org/&lt;/a&gt; - Apache Hawq homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://hawq.incubator.apache.org/docs/userguide/latest/index.html&quot;&gt;http://hawq.incubator.apache.org/docs/userguide/latest/index.html&lt;/a&gt; - Apache Hawq documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/HAWQ/&quot;&gt;https://cwiki.apache.org/confluence/display/HAWQ/&lt;/a&gt; - Apache Hawq Wiki&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://hortonworks.com/apache/hawk/&quot;&gt;http://hortonworks.com/apache/hawk/&lt;/a&gt; - Hortonworks information on Apache Hawq&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://pivotal.io/pivotal-hdb&quot;&gt;https://pivotal.io/pivotal-hdb&lt;/a&gt; - Pivotal HDB homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://hdb.docs.pivotal.io/&quot;&gt;http://hdb.docs.pivotal.io/&lt;/a&gt; - Pivotal HDB documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://content.pivotal.io/blog/the-way-to-hadoop-native-sql&quot;&gt;https://content.pivotal.io/blog/the-way-to-hadoop-native-sql&lt;/a&gt; - Hawq open source announcement&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/press-releases/hortonworks-pivotal-expand-relationship-deliver-enterprise-ready-modern-data-platforms-data-management-analytics/&quot;&gt;https://hortonworks.com/press-releases/hortonworks-pivotal-expand-relationship-deliver-enterprise-ready-modern-data-platforms-data-management-analytics/&lt;/a&gt; - Hortonworks and Pivotal HDB announcement&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;News via Pivotal blog&lt;/li&gt; &lt;li&gt;Latest release detailed on Pivotal HDB documentation page&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Ambari</title><link>https://ondataengineering.net/technologies/apache-ambari/</link><description> &lt;p&gt;Platform for installing, managing and monitoring Apache Hadoop clusters. Supports the installation of different versions of different distributions of Hadoop through Stack definitions (with support for HDP out of the box, and further stacks and add ons available through management packs), and the specification of Blueprints (cluster layouts and configuration for a given Stack) that can be used to programmatically create multiple clusters (e.g. dev, test and production). Also supports both rolling (no downtime) and express (faster but with downtime) upgrades; cluster administration (including adding and removing nodes/services, viewing the status of nodes/services, and configuring services with the versioning of configuration and the ability to rollback changes); the automated Kerberization of clusters; the collection, storage (in HBase) and visualisation (via Grafana or through dashboards in Ambari) of system and Hadoop component metrics via the Ambari Metrics System (AMS); alerting on statuses and metrics; the collection, storage (in Solr) and searching/viewing of log entries from across the Hadoop cluster (currently in technical preview); and a framework for UI components within Ambari (Ambari Views, treated here as a sub-project). Web based, with a REST API, and backed by a backend database (Oracle, MySQL or Postgres). Donated to the Apache Foundation by Hortonworks, IBM and Yahoo in August 2011 as the Hadoop Management System (HMS), graduating in December 2013 after changing it's name to Ambari. Still under active development with a large number of contributors.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Ambari&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;July 2018 - v2.7&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Sub-projects&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; Apache Ambari&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-ambari/ambari-views/&quot;&gt;Ambari Views&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Framework within Ambari that allows new applications or views to be added to Ambari, based on new client side code (HTML, JavaScript and CSS) supported by new backend code (Java) that exposes REST API end points for the UI to consume. Comes with support for a number of views out of the box, including YARN Queue Manager (supports the creation and configuration of YARN capacity schedule queues), Files (supports copying and moving, uploading and setting permissions on files in HDFS), Falcon (supports defining, scheduling and monitoring data management pipelines), Hive (supports browsing databases, executing queries and viewing explain plans, saving queries, viewing query history and uploading data to Hive tables), Pig (supports executing Pig scripts and viewing execution history), SmartSense (supports capture and download of bundles), Storm (supports viewing cluster status, monitoring topologies, perform topology management and access metrics and logs) and Tez (supports viewing and debugging Tez jobs), along with technical previews of Workflow Designer, Zeppelin and Hue migration views. Views can be deployed into a standalone Ambari instance to separate these from the primary Ambari management instance and to support scaling out.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Manages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-flow/&quot;&gt;Hortonworks DataFlow&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.5&lt;/td&gt; &lt;td&gt;2017-03-24&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/Ambari-2.5.0.3/bk_ambari-release-notes/content/ch_relnotes-ambari-2.5.0.3.html&quot;&gt;release notes&lt;/a&gt; &lt;a href=&quot;http://docs.hortonworks.com/posts/2017/04/13/ambari-25-operations-views.html&quot;&gt;changes&lt;/a&gt; &lt;a href=&quot;https://hortonworks.com/blog/ambari-2-5/&quot;&gt;Hortonworks post&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.6&lt;/td&gt; &lt;td&gt;2017-10-31&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/Ambari-2.6.0.0/bk_ambari-release-notes/content/ch_relnotes-ambari-2.6.0.0.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.7&lt;/td&gt; &lt;td&gt;2018-07-13&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://hortonworks.com/blog/announcing-general-availability-hortonworks-data-platform-3-0-0-ambari-2-7-0-smartsense-1-5-0/&quot;&gt;blog post&lt;/a&gt;; &lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/Ambari-2.7.0.0/bk_ambari-release-notes/content/ch_relnotes-ambari-2.7.0.0.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://ambari.incubator.apache.org/&quot;&gt;http://ambari.incubator.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://hortonworks.com/apache/ambari/&quot;&gt;http://hortonworks.com/apache/ambari/&lt;/a&gt; - Hortonworks information, including tutorials and blog posts&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/Ambari/Ambari-2.6.1.3/index.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/Ambari/Ambari-2.6.1.3/index.html&lt;/a&gt; - Hortonworks documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/AMBARI/Ambari&quot;&gt;https://cwiki.apache.org/confluence/display/AMBARI/Ambari&lt;/a&gt; - Apache developer level documentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://ambari.apache.org/&quot;&gt;https://ambari.apache.org/&lt;/a&gt; - shows latest release version&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://docs.hortonworks.com/index.html&quot;&gt;http://docs.hortonworks.com/index.html&lt;/a&gt; - shows latest Hortonworks release version&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Ambari Views</title><link>https://ondataengineering.net/technologies/apache-ambari/ambari-views/</link><description> &lt;p&gt;Framework within Ambari that allows new applications or views to be added to Ambari, based on new client side code (HTML, JavaScript and CSS) supported by new backend code (Java) that exposes REST API end points for the UI to consume. Comes with support for a number of views out of the box, including YARN Queue Manager (supports the creation and configuration of YARN capacity schedule queues), Files (supports copying and moving, uploading and setting permissions on files in HDFS), Falcon (supports defining, scheduling and monitoring data management pipelines), Hive (supports browsing databases, executing queries and viewing explain plans, saving queries, viewing query history and uploading data to Hive tables), Pig (supports executing Pig scripts and viewing execution history), SmartSense (supports capture and download of bundles), Storm (supports viewing cluster status, monitoring topologies, perform topology management and access metrics and logs) and Tez (supports viewing and debugging Tez jobs), along with technical previews of Workflow Designer, Zeppelin and Hue migration views. Views can be deployed into a standalone Ambari instance to separate these from the primary Ambari management instance and to support scaling out.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-ambari/&quot;&gt;Apache Ambari&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;March 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Cloudbreak</title><link>https://ondataengineering.net/technologies/cloudbreak/</link><description> &lt;p&gt;Solution for deploying and managing Hadoop clusters on cloud infrastructure based on automatically provisioned infrastructure running base docker images with Hadoop provisioned on top via Apache Ambari using Blueprints. Includes out of the box support for Amazon Web Services, Microsoft Azure, Google Cloud Platform and OpenStack, plus a Service Provider Interface (SPI) for adding support for new providers. Supports automated scaling of clusters based on Ambari Metrics and Alerts (Periscope), custom scripts that can be run on hosts before or after deployment (Recipes), a number of out of the box Blueprints, the use of custom docker images, data locality specifiers, Kerberized clusters and support for external AD/LDAP servers. Manageable through a web UI, a REST API, a CLI and an interactive shell. Originally created by SequenceIQ, with an initial beta release in July 2014, with SequenceIQ then acquired by Hortonworks in April 2015, and a 1.0 release of Cloudbreak included in HDP 2.3 in July 2015. Open sourced under the Apache 2.0 licence, with a stated plan for the code to be donated to the Apache Foundation.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/hortonworks/&quot;&gt;Hortonworks&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;September 2018 - v2.8 (TP)&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Manages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.14&lt;/td&gt; &lt;td&gt;2017-04-05&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://sequenceiq.com/cloudbreak-docs/release-1.14.0/releasenotes/&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.16&lt;/td&gt; &lt;td&gt;2017-06-16&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://sequenceiq.com/cloudbreak-docs/release-1.16.1/releasenotes/&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Adds support for Hortonworks Flex Support Subscription&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.1 TP&lt;/td&gt; &lt;td&gt;2017-11-20&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://docs.hortonworks.com/posts/2017/11/20/cloudbreak2.1.0.html&quot;&gt;docs blog&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Tech preview; new documentation&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.2 TP&lt;/td&gt; &lt;td&gt;2017-12-20&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://docs.hortonworks.com/posts/2017/12/20/cloudbreak2.2.0.html&quot;&gt;docs blog&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Second TP release; doc updates&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.4&lt;/td&gt; &lt;td&gt;2018-02-24&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://hortonworks.com/blog/announcing-cloudbreak-2-4/&quot;&gt;blog post&lt;/a&gt;; &lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/Cloudbreak/Cloudbreak-2.4.0/content/releasenotes/index.html&quot;&gt;release notes&lt;/a&gt;; &lt;a href=&quot;https://docs.hortonworks.com/posts/2018/02/27/cloudbreak2.4.0.html&quot;&gt;docs blog&lt;/a&gt;&lt;/td&gt; &lt;td&gt;New UI and CLI&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.5 TP&lt;/td&gt; &lt;td&gt;2018-04-04&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/Cloudbreak/Cloudbreak-2.5.0/content/releasenotes/index.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt;HDF support&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.7&lt;/td&gt; &lt;td&gt;2018-06-16&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://hortonworks.com/blog/announcing-cloudbreak-2-7-ga/&quot;&gt;blog post&lt;/a&gt;; &lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/Cloudbreak/Cloudbreak-2.7.0/content/releasenotes/index.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.8 TP&lt;/td&gt; &lt;td&gt;2018-09-19&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/Cloudbreak/Cloudbreak-2.8.0/release-notes/index.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/open-source/cloudbreak/&quot;&gt;https://hortonworks.com/open-source/cloudbreak/&lt;/a&gt; - Hortonworks information, including blog posts&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/Cloudbreak/Cloudbreak-2.7.0/index.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/Cloudbreak/Cloudbreak-2.7.0/index.html&lt;/a&gt; - Documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/hortonworks/cloudbreak&quot;&gt;https://github.com/hortonworks/cloudbreak&lt;/a&gt; - Code&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/hortonworks-acquires-sequenceiq-to-provide-automated-deployment-of-hadoop-everywhere/&quot;&gt;https://hortonworks.com/blog/hortonworks-acquires-sequenceiq-to-provide-automated-deployment-of-hadoop-everywhere/&lt;/a&gt; - SequenceIQ acquisition announcement&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://docs.hortonworks.com/index.html&quot;&gt;http://docs.hortonworks.com/index.html&lt;/a&gt; - shows latest Hortonworks release version&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hortonworks SmartSense</title><link>https://ondataengineering.net/technologies/hortonworks-smartsense/</link><description> &lt;p&gt;Supports the capture of diagnostic information from HDP and HDF clusters (including configuration, metrics and logs from both Hadoop and the Operating System) into a bundle for upload (either manually or automatically) to the Hortonworks support portal to assist in the resolution of support issues and the delivery of cluster optimisation and preventative action recommendations, with support for anonymisation (including IP addresses and host names, with support for further custom rules) and encryption of information in bundles and a SmartSense gateway to proxy uploads if direct internet access isn't available. Also includes functionality to help understand and analyse cluster activity include the Activity Analyser (aggregates data from YARN, Tez, MapReduce and HDFS into Ambari Metrics) and Activity Explorer (an embedded instance of Apache Zeppelin with pre-built notebooks for exploring and visualising cluster activity). Installable and manageable through Apache Ambari. Part of the Hortonworks support offering, introduced in June 2015 as part of HDP 2.3.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;SmartSense&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/hortonworks/&quot;&gt;Hortonworks&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;July 2017 - v1.5&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Add on to&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.4&lt;/td&gt; &lt;td&gt;2017-04-13&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/SS1/SmartSense-1.4.0/bk_release-notes/content/ch_relnotes_smartsense.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.5&lt;/td&gt; &lt;td&gt;2017-07-13&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/SS1/SmartSense-1.5.0/release-notes/index.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/services/support/smartsense/&quot;&gt;https://hortonworks.com/services/support/smartsense/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://docs.hortonworks.com/HDPDocuments/SS1/SmartSense-1.3.1/index.html&quot;&gt;http://docs.hortonworks.com/HDPDocuments/SS1/SmartSense-1.3.1/index.html&lt;/a&gt; - current documentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://docs.hortonworks.com/index.html&quot;&gt;http://docs.hortonworks.com/index.html&lt;/a&gt; - shows latest Hortonworks release version&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hortonworks DataFlow</title><link>https://ondataengineering.net/technologies/hortonworks-data-flow/</link><description> &lt;p&gt;A distribution of a set of Apache and Hortonworks open source technologies for processing and running analytics on data 'in motion', with all products integrated with Apache Ranger for security, Apache Ambari for management and Schema Registery for central schema management. All bundled Apache open source projects are based on official Apache project releases, with any patches for bug fixes or new features being official Apache project patches from later releases of the relevant project. Available as RPMs or through Apache Ambari (via a management pack), and as an on-site or in the cloud managed service (as Hortonworks Operational Services), but is not currently available via Cloudbreak or as a cloud service. The HDF softare is provided free of charge, with training, consultancy and support available from Hortonworks. First released in September 2015 as a distribution of just NiFi following the acquisition by Hortonworks of Onyara (a company founded by some of the original creators of NiFi).&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;HDF&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/hortonworks/&quot;&gt;Hortonworks&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;August 2018 - 3.2&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Packages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-nifi/&quot;&gt;Apache NiFi&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-nifi/minifi/&quot;&gt;MiNiFi&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-nifi/registry/&quot;&gt;NiFi Registry&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-ranger/&quot;&gt;Apache Ranger&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-storm/&quot;&gt;Apache Storm&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-zookeeper/&quot;&gt;Apache ZooKeeper&lt;/a&gt;, &lt;a href=&quot;/technologies/schema-registry/&quot;&gt;Schema Registry&lt;/a&gt;, &lt;a href=&quot;/technologies/streaming-analytics-manager/&quot;&gt;Streaming Analytics Manager&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Manageable via&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-ambari/&quot;&gt;Apache Ambari&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;bundled-technologies&quot;&gt;Bundled Technologies&lt;/h2&gt; &lt;p&gt;The details of the Apache projects distributed as part of Hortonworks DataFlow are detailed in the release notes, along with the specific versions included, the unsupported features, the patches pulled forward from future project releases, and the known vulnerabilities and issues.&lt;/p&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;td&gt;2015-09&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://hortonworks.com/press-releases/hortonworks-to-acquire-onyara-to-turn-internet-of-anything-data-into-actionable-insights/&quot;&gt;press release&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Initial version consisting of just Apache NiFi&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.1&lt;/td&gt; &lt;td&gt;2015-12&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://hortonworks.com/blog/hortonworks-dataflow-1-1-released/&quot;&gt;blog post&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.2&lt;/td&gt; &lt;td&gt;2016-03&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://hortonworks.com/blog/hortonworks-dataflow-1-2-released/&quot;&gt;blog post&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Storm and Kafka added&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.0&lt;/td&gt; &lt;td&gt;2016-09&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://hortonworks.com/blog/hortonworks-dataflow-2-0-ga/&quot;&gt;blog post&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Ranger and Ambari support added&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.1&lt;/td&gt; &lt;td&gt;2016-12&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://hortonworks.com/blog/announcing-availability-hortonworks-dataflow-hdf-2-1/&quot;&gt;blog post&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;td&gt;2017-06-12&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://hortonworks.com/blog/hortonworks-dataflow-3-0/&quot;&gt;blog post&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Streaming Analytics Manager and Schema Registry added; technical preview of SAM Stream Insights which bundles &lt;a href=&quot;/technologies/druid&quot;&gt;Druid&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-superset&quot;&gt;Apache Superset&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;3.1&lt;/td&gt; &lt;td&gt;2018-02-01&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://hortonworks.com/blog/announcing-general-availability-hortonworks-dataflow-3-1/&quot;&gt;blog post&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Upgrade to NiFi 1.5 and Kafka 1.0; adds MiNiFi C++ agent and NiFi Registry&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;3.2&lt;/td&gt; &lt;td&gt;2018-08-14&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://hortonworks.com/blog/whats-new-hortonworks-dataflow-hdf-3-2/&quot;&gt;blog post&lt;/a&gt;; &lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDF3/HDF-3.2.0/release-notes/content/whats-new.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/products/data-center/hdf/&quot;&gt;https://hortonworks.com/products/data-center/hdf/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDF3/HDF-3.1.0/index.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDF3/HDF-3.1.0/index.html&lt;/a&gt; - HDF 3.1 documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDF3/HDF-3.1.0/bk_release-notes/content/ch_hdf_relnotes.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDF3/HDF-3.1.0/bk_release-notes/content/ch_hdf_relnotes.html&lt;/a&gt; - HDF 3.1 release notes&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://docs.hortonworks.com/index.html&quot;&gt;http://docs.hortonworks.com/index.html&lt;/a&gt; - shows latest Hortonworks release version&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/category/hdf/&quot;&gt;https://hortonworks.com/blog/category/hdf/&lt;/a&gt; - Hortonworks blog posts&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache NiFi</title><link>https://ondataengineering.net/technologies/apache-nifi/</link><description> &lt;p&gt;General purpose technology for the movement of data between systems, including the ingestion of data into an analytical platform. Based on directed acyclic graph of Processors and Connections, with the unit of work being a FlowFile (a blob of data plus a set of key/value pair attributes). Supports guaranteed delivery of FlowFiles, with NiFi resiliently storing state (by default to a local write ahead log) and data blobs (by default a set of local partitions on disk), with all FlowFile transformations executed via a thread pool within the NiFi instance (with the option to deploy multiple NiFi instances as a cluster). All flows are configured in a graphical user interface, which is also used for management and operations (starting/stopping individual Processors and viewing real time statuses, statistics and other information). Also supports some record level operations (via RecordReaders and RecordSetWriters), data provenance (reporting on the processing events and lineage of individual FlowFiles), scheduling of Processor execution (based on periodic execution timers or cron specifications), multi-threaded Processor execution, configuration of Processor batch sizes (to enable low latency or high throughput), prioritised queues within Connections (allowing FlowFiles to be processed based on their age or a priority attribute as an alternative to FIFO), back pressure (based on counts or data volume against individual Connections) and pressure release (automatic discarding of FlowFiles based on their age), the ability to stream data to and from other NiFi instances and other streaming technologies, the ability to import and export flows as XML (flow templates), an expression language for setting Processor configuration and populating FlowFile attributes, Controller Services to provide shared services to processors (e.g. access to credentials, shared state), Reporting Tasks to output status and statistics information and a user security model. Extensible through the addition of custom Processors, Controller Services, Reporting Tasks and Prioritizers, and integrates with Apache Ranger and Apache Ambari. Originally developed at the NSA as Niagara Files, before being donated to the Apache Foundation in November 2014, graduating in July 2015. Java based, with development lead by Hortonworks after their aquisition of Onyara (which was set up by original NiFi developers to provide commercial support and services).&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;NiFi&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;June 2018 - v1.7&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Sub-projects&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; Apache NiFi&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-nifi/minifi/&quot;&gt;MiNiFi&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Lightweight headless version of NiFi used to collect and process data at it's source, before forwarding it on for centralised processing. Supports all key NiFi functionality including all NiFi processors, guaranteed delivery, data buffering (including back pressure and pressure release) and prioritised queuing, however flows are specified in configuration files, status information and statistics are only available via Reporting Tasks or via a CLI, and provenance can only be viewed by exporting events via Reporting Tasks to log files or a full NiFi instance. Supports warm re-deployments, automatically restarting to load a new configuration written to disk or pushed or pulled over HTTP. Available as a Java or Native C++ executable. Started in March 2016, with a 0.1 release in December 2016.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache NiFi&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/apache-nifi/registry/&quot;&gt;NiFi Registry&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A solution for the configuration management of NiFi flows. Integrates with NiFi to allow users to store, retrieve and upgrade flows, keeping a full history of all changes to a flow committed to the registry, with flows stored and organised by buckets. Supports local users and groups, or authentication via certificates, LDAP or Kerberos, with access control policies allowing read, write and delete permissions to be specified for buckets, users and groups. Has a Web based UI and a REST interface for managing buckets, local users and groups, viewing flow history and for managing access control. First released in January 2018.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-flow/&quot;&gt;Hortonworks DataFlow&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.2&lt;/td&gt; &lt;td&gt;2017-05-08&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/NIFI/Release+Notes#ReleaseNotes-Version1.2.0&quot;&gt;summary&lt;/a&gt;; &lt;a href=&quot;https://blogs.apache.org/nifi/entry/record-oriented-data-with-nifi&quot;&gt;record level processing&lt;/a&gt;; &lt;a href=&quot;https://blogs.apache.org/nifi/entry/real-time-sql-on-event&quot;&gt;running SQL on event streams&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.3&lt;/td&gt; &lt;td&gt;2017-06-08&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/NIFI/Release+Notes#ReleaseNotes-Version1.3.0&quot;&gt;summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.4&lt;/td&gt; &lt;td&gt;2017-10-02&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/NIFI/Release+Notes#ReleaseNotes-Version1.4.0&quot;&gt;summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.5&lt;/td&gt; &lt;td&gt;2018-01-12&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/NIFI/Release+Notes#ReleaseNotes-Version1.5.0&quot;&gt;summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://dzone.com/articles/new-features-in-apache-nifi-15-apache-nifi-registr&quot;&gt;registry / version control blog post from DZone&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.6&lt;/td&gt; &lt;td&gt;2018-04-08&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/NIFI/Release+Notes#ReleaseNotes-Version1.6.0&quot;&gt;summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.7&lt;/td&gt; &lt;td&gt;2018-06-25&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/NIFI/Release+Notes#ReleaseNotes-Version1.7.0&quot;&gt;summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://nifi.apache.org/&quot;&gt;http://nifi.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://nifi.apache.org/docs.html&quot;&gt;http://nifi.apache.org/docs.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/apache/nifi/&quot;&gt;https://hortonworks.com/apache/nifi/&lt;/a&gt; - Hortonworks information, including tutorials and blog posts&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDF3/HDF-3.1.0/index.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDF3/HDF-3.1.0/index.html&lt;/a&gt; - Hortonworks documentation (as part of HDF 3.1)&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/NIFI/Release+Notes&quot;&gt;https://cwiki.apache.org/confluence/display/NIFI/Release+Notes&lt;/a&gt; - release notes&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>MiNiFi</title><link>https://ondataengineering.net/technologies/apache-nifi/minifi/</link><description> &lt;p&gt;Lightweight headless version of NiFi used to collect and process data at it's source, before forwarding it on for centralised processing. Supports all key NiFi functionality including all NiFi processors, guaranteed delivery, data buffering (including back pressure and pressure release) and prioritised queuing, however flows are specified in configuration files, status information and statistics are only available via Reporting Tasks or via a CLI, and provenance can only be viewed by exporting events via Reporting Tasks to log files or a full NiFi instance. Supports warm re-deployments, automatically restarting to load a new configuration written to disk or pushed or pulled over HTTP. Available as a Java or Native C++ executable. Started in March 2016, with a 0.1 release in December 2016.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-nifi/&quot;&gt;Apache NiFi&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;July 2018 - v0.5 (Java) v0.5 (C++)&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-flow/&quot;&gt;Hortonworks DataFlow&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.2 (C++)&lt;/td&gt; &lt;td&gt;2017-05-11&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/MINIFI/Release+Notes#ReleaseNotes-Versioncpp-0.2.0&quot;&gt;summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.2 (Java)&lt;/td&gt; &lt;td&gt;2017-05-18&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/MINIFI/Release+Notes#ReleaseNotes-Version0.2.0&quot;&gt;summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.3 (C++)&lt;/td&gt; &lt;td&gt;2017-11-30&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/MINIFI/Release+Notes#ReleaseNotes-Versioncpp-0.3.0&quot;&gt;summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.3 (Java)&lt;/td&gt; &lt;td&gt;2017-12-22&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/MINIFI/Release+Notes#ReleaseNotes-Version0.3.0&quot;&gt;summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.4 (C++)&lt;/td&gt; &lt;td&gt;2018-01-27&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/MINIFI/Release+Notes#ReleaseNotes-Versioncpp-0.4.0&quot;&gt;summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.4 (Java)&lt;/td&gt; &lt;td&gt;2018-01-22&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/MINIFI/Release+Notes#ReleaseNotes-Version0.4.0&quot;&gt;summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.5 (C++)&lt;/td&gt; &lt;td&gt;2018-06-06&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/MINIFI/Release+Notes#ReleaseNotes-Versioncpp-0.5.0&quot;&gt;summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.5 (Java)&lt;/td&gt; &lt;td&gt;2018-07-07&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/MINIFI/Release+Notes#ReleaseNotes-Version0.5.0&quot;&gt;summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://nifi.apache.org/minifi/index.html&quot;&gt;http://nifi.apache.org/minifi/index.html&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://nifi.apache.org/minifi/system-admin-guide.html&quot;&gt;http://nifi.apache.org/minifi/system-admin-guide.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/apache/nifi/#section_4&quot;&gt;https://hortonworks.com/apache/nifi/#section_4&lt;/a&gt; - Hortonworks information&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDF3/HDF-3.1.0/index.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDF3/HDF-3.1.0/index.html&lt;/a&gt; - Hortonworks documentation (as part of HDF 3.1)&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/MINIFI/Release+Notes&quot;&gt;https://cwiki.apache.org/confluence/display/MINIFI/Release+Notes&lt;/a&gt; - release notes&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hortonworks Data Cloud for AWS</title><link>https://ondataengineering.net/technologies/hortonworks-data-cloud-for-aws/</link><description> &lt;p&gt;Service that supports the creation and management of HDP clusters on Amazon Web Services (AWS). Management is done through a Cloud Controller AWS Product that provides a web interface and CLI for orchestrating the creation of AWS resources and the deployment of clusters using Ambari, and the subsequent scaling or cloning of the cluster. Supports a number of standard cluster types, including Data Science (Spark, Zeppelin), EDW-ETL (Hive, Spark) and EDW-Analytics (Hive, Zeppelin), with clusters also including Tez, Pig and Scoop, along with a number of standard node types, including worker nodes (that support HDFS and YARN) and computer nodes (that only support YARN). Clusters are designed to be ephemeral, however Amazon RDS can be used to provide persistent storage of Cloud Controller and Hive metadata, and Amazon S3 can be used to provide persistent cluster storage. Also supports Hortonworks SmartSense, cluster templates, the use of Spot Instances for compute nodes, and node recipes for executing custom scripts pre/post the Ambari cluster setup. Comes with free community support from Hortonworks. First launched in November 2016, but appears to be discontinued as of HDP 3.0 with Hortonworks move to a multi cloud strategy via Cloudbreak&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;HDCloud for AWS&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/hortonworks/&quot;&gt;Hortonworks&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Discontinued&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;July 2018 - v1.16&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.14&lt;/td&gt; &lt;td&gt;2017-04-05&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://hortonworks.com/blog/announcing-hortonworks-data-cloud-for-aws-1-14-1/&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDCloudAWS/HDCloudAWS-1.14.1/bk_hdcloud-aws/content/releasenotes/index.html&quot;&gt;summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt;HDP 2.6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.16&lt;/td&gt; &lt;td&gt;2017-06-15&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://hortonworks.com/blog/plenty-hortonworks-data-cloud/&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDCloudAWS/HDCloudAWS-1.16.0/bk_hdcloud-aws/content/releasenotes/index.html&quot;&gt;summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/products/cloud/aws/&quot;&gt;https://hortonworks.com/products/cloud/aws/&lt;/a&gt; - home page (now redirects to general Hortonworks cloud page)&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDCloudAWS/HDCloudAWS-1.16.5/index.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDCloudAWS/HDCloudAWS-1.16.5/index.html&lt;/a&gt; - v1.16.5 documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/press-releases/availability-hortonworks-data-cloud-aws/&quot;&gt;https://hortonworks.com/press-releases/availability-hortonworks-data-cloud-aws/&lt;/a&gt; - original press release&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/marketplace/pp/B01LXOQBOU&quot;&gt;https://aws.amazon.com/marketplace/pp/B01LXOQBOU&lt;/a&gt; - AWS Cloud Controller product page&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://docs.hortonworks.com/index.html&quot;&gt;http://docs.hortonworks.com/index.html&lt;/a&gt; - shows latest Hortonworks release version&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hortonworks Data Platform for Windows</title><link>https://ondataengineering.net/technologies/hortonworks-data-platform-for-windows/</link><description> &lt;p&gt;A version of the Hortonworks Data Platform natively compiled for Windows that was discontinued as of HDP 2.5 in August 2016. First announced in March 2013, with a GA release in May 2013. Didn't use Apache Ambari for installation and management (instead being installed via a standard Windows installer), didn't support SmartSense, and didn't include some technologies (such as Accumulo, Atlas, Kafka, Solr, Spark and Hue).&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;HDP for Windows&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/hortonworks/&quot;&gt;Hortonworks&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Discontinued&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;May 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://community.hortonworks.com/questions/58663/hdp-25-support-for-windows.html&quot;&gt;https://community.hortonworks.com/questions/58663/hdp-25-support-for-windows.html&lt;/a&gt; - reference to HDP for Windows discontinuation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.2-Win/index.html&quot;&gt;http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.2-Win/index.html&lt;/a&gt; - HDP 2.4.2 for Windows documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.2-Win/bk_HDP_RelNotes_Win/content/ch_relnotes_v242.html&quot;&gt;http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.2-Win/bk_HDP_RelNotes_Win/content/ch_relnotes_v242.html&lt;/a&gt; - HDP 2.4.2 for Windows release notes&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Cloudera CDH</title><link>https://ondataengineering.net/technologies/cloudera-cdh/</link><description> &lt;p&gt;A distribution of Hadoop based on the addition of a number of closed source products, including Cloudera Manager (for installing and managing clusters) and Cloudera Navigator (for managing metadata and the encryption of data). Bundled projects tend to lag the open source versions and pull forward more patches than other distributions. Also comes with a number of add-ons, including ODBC and JDBC drivers for Hive and Impala, a number of Apache projects that aren't (yet) part of the core CDH distribution, and Workload XM (a cloud based service for analysing job logs). Available via RPMs, or can be installed using Cloudera Manager (for local installs) or Cloudera Director (for installation on cloud platforms). Comes in a number of editions including Cloudera Enterprise (under an annual per node or elastic cloud licence model with commercial support) and Cloudera Express (a free version without some enterprise features), with Cloudera Enterprise coming in a range of licence options (listed on the Cloudera website under products) with each including support for different Apache products. First released in March 2009.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;CDH, Cloudera Express, Cloudera Enterprise&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/cloudera/&quot;&gt;Cloudera&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Categories&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-categories/hadoop-distributions/&quot;&gt;Hadoop Distributions&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;September 2018 - v6.0&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Packages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-avro/&quot;&gt;Apache Avro&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-flume/&quot;&gt;Apache Flume&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Apache Hadoop&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hbase/&quot;&gt;Apache HBase&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Apache Hive&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-impala/&quot;&gt;Apache Impala&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-kudu/&quot;&gt;Apache Kudu&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-oozie/&quot;&gt;Apache Oozie&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-parquet/&quot;&gt;Apache Parquet&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-pig/&quot;&gt;Apache Pig&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-sentry/&quot;&gt;Apache Sentry&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-sqoop/&quot;&gt;Apache Sqoop&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-zookeeper/&quot;&gt;Apache ZooKeeper&lt;/a&gt;, &lt;a href=&quot;/technologies/cloudera-search/&quot;&gt;Cloudera Search&lt;/a&gt;, &lt;a href=&quot;/technologies/hue/&quot;&gt;Hue&lt;/a&gt;, &lt;a href=&quot;/technologies/kite/morphlines/&quot;&gt;Morphlines&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Packages (but deprecated)&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-crunch/&quot;&gt;Apache Crunch&lt;/a&gt;, &lt;a href=&quot;/technologies/kite/kite-data/&quot;&gt;Kite Data&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Add ons&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-accumulo/&quot;&gt;Apache Accumulo&lt;/a&gt;, &lt;a href=&quot;/technologies/cloudera-navigator/&quot;&gt;Cloudera Navigator&lt;/a&gt;, Workload XM&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Manageable via&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/cloudera-altus/director/&quot;&gt;Cloudera Altus Director&lt;/a&gt;, &lt;a href=&quot;/technologies/cloudera-manager/&quot;&gt;Cloudera Manager&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Is used by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/cloudera-altus/&quot;&gt;Cloudera Altus&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;bundled-technologies&quot;&gt;Bundled Technologies&lt;/h2&gt; &lt;p&gt;Details of the Apache project versions bundled with each version of CDH are available under &lt;a href=&quot;https://www.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_version_packaging_download.html&quot;&gt;this page&lt;/a&gt; of the CDH release notes. Deprecated items and projects are detailed on &lt;a href=&quot;https://www.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_deprecated_items.html&quot;&gt;this page&lt;/a&gt;. New features, known issues and fixed issues are available under &lt;a href=&quot;https://www.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_cdh_6_release_notes.html&quot;&gt;this page&lt;/a&gt;. See some of the links below for details on the different Cloudera versions and options.&lt;/p&gt; &lt;p&gt;Project marked as add ons above are not bundled with CDH, but are available separately, either via direct download through Cloudera Manager or via separate download from the Cloudera website, and are versioned as per the open source project and not as per a CDH version.&lt;/p&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.11&lt;/td&gt; &lt;td&gt;2017-04-18&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://community.cloudera.com/t5/Community-News-Release/Announce-Cloudera-Enterprise-5-11-is-Now-Available/m-p/53808#M170&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.12&lt;/td&gt; &lt;td&gt;2017-07-13&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://community.cloudera.com/t5/Community-News-Release/Cloudera-Enterprise-5-12-is-Now-Available/m-p/57359#M184&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;http://blog.cloudera.com/blog/2017/08/new-in-cloudera-enterprise-5-12-hue-4-interface-and-query-assistant/&quot;&gt;New Hue 4 functionality&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.13&lt;/td&gt; &lt;td&gt;2017-10-12&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://community.cloudera.com/t5/Community-News-Release/ANNOUNCE-Cloudera-Enterprise-5-13-is-Now-Available/m-p/60879#M200&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;https://www.cloudera.com/documentation/enterprise/release-notes/topics/cdh_rn_new_in_cdh_513.html&quot;&gt;Release Notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Kudu fully bundled; Spark 1.x deprecated&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.14&lt;/td&gt; &lt;td&gt;2018-01-26&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://community.cloudera.com/t5/Community-News-Release/ANNOUNCE-Cloudera-Enterprise-5-14-is-Now-Available/td-p/64064&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;https://www.cloudera.com/documentation/enterprise/release-notes/topics/cdh_rn_new_in_cdh_514.html&quot;&gt;Release Notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.15&lt;/td&gt; &lt;td&gt;2018-06-15&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://community.cloudera.com/t5/Community-News-Release/ANNOUNCE-Cloudera-Enterprise-5-15-is-Now-Available/td-p/69154&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;https://www.cloudera.com/documentation/enterprise/release-notes/topics/cdh_rn_new_in_cdh_515.html&quot;&gt;Release Notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6.0&lt;/td&gt; &lt;td&gt;2018-08-30&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://community.cloudera.com/t5/Community-News-Release/ANNOUNCE-Cloudera-Enterprise-6-0-Released/td-p/79235&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;http://vision.cloudera.com/building-the-modern-platform-with-cloudera-enterprise-6-x-and-altus/&quot;&gt;blog post&lt;/a&gt;; &lt;a href=&quot;http://vision.cloudera.com/introducing-cloudera-enterprise-6-0/&quot;&gt;blog post&lt;/a&gt;; &lt;a href=&quot;http://blog.cloudera.com/blog/2018/05/new-in-cloudera-enterprise-6-0-analytic-search/&quot;&gt;Solr 7&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Hadoop 3.0, Hive 2.1, HBase 2.0, Solr 7, Spark 2.2, Hue 4.2, Kafka 1.0, Oozie 5.0, Avro 1.8, Parquet 1.9; Spark 2.x and Kafka fully bundled; Crunch &amp;amp; Kite Data deprecated; DataFu, Mahout, Whirr, Sqoop2 and Llama removed&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/products/cloudera-enterprise-6.html&quot;&gt;https://www.cloudera.com/products/cloudera-enterprise-6.html&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/documentation/enterprise/latest.html&quot;&gt;https://www.cloudera.com/documentation/enterprise/latest.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_release_notes.html&quot;&gt;https://www.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_release_notes.html&lt;/a&gt; - release notes&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/content/dam/www/marketing/resources/datasheets/cloudera-cdh-projects-and-specifications-datasheet.pdf&quot;&gt;https://www.cloudera.com/content/dam/www/marketing/resources/datasheets/cloudera-cdh-projects-and-specifications-datasheet.pdf&lt;/a&gt; - CDH bundled projects datasheet&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/content/dam/www/marketing/resources/datasheets/cloudera-enterprise-datasheet.pdf&quot;&gt;https://www.cloudera.com/content/dam/www/marketing/resources/datasheets/cloudera-enterprise-datasheet.pdf&lt;/a&gt; - Cloudera Enterprise datasheet (including details of products supported under each licence option)&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/documentation/director/cloud.html&quot;&gt;https://www.cloudera.com/documentation/director/cloud.html&lt;/a&gt; - best practice for running CDH in the cloud&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/&quot;&gt;http://blog.cloudera.com/&lt;/a&gt; - Cloudera engineering blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://community.cloudera.com/t5/Community-News-Release/bd-p/RelAnnounce&quot;&gt;http://community.cloudera.com/t5/Community-News-Release/bd-p/RelAnnounce&lt;/a&gt; - Release Announcements&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Llama</title><link>https://ondataengineering.net/technologies/llama/</link><description> &lt;p&gt;Framework for long running low-latency distributed applications to request resources from YARN, built to support Apache Impala. Operates as an un-managed YARN application master (that handles resource requests over a Thrift API and delivers resource notifications) and a node manager plugin (that delivers resource availability information to co-located services). Created by Cloudera in August 2013 and hosted on GitHub under an Apache 2.0 licence. Maintained by Cloudera to support new Impala and CDH releases, but now deprecated and will no longer be included in CDH from v6.0 onwards.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/cloudera/&quot;&gt;Cloudera&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Open Source - InActive&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;March 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Add on to&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/yarn/&quot;&gt;YARN&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://cloudera.github.io/llama/&quot;&gt;http://cloudera.github.io/llama/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/cloudera/llama&quot;&gt;https://github.com/cloudera/llama&lt;/a&gt; - code repository&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Whirr</title><link>https://ondataengineering.net/technologies/apache-whirr/</link><description> &lt;p&gt;A set of libraries (now moved to the Apache Attic and no longer maintained) for deploying and managing a supported set of services in a cloud environment. Written in Java, with explicit support for a set of standard services (including Hadoop, Cassandra, HBase, Elasticsearch and Solr) configured through property files. Uses jclouds to provision and manage cloud infrastructure, and provides both a CLI and Java API. Originally a set of python scripts maintained as an Hadoop contrib project. Donated to the Apache Foundation in May 2010, graduating in August 2011. Development ceased in September 2012, with the project being moved to the Apache Attic in March 2015.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Whirr&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Open Source - InActive&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;March 2017 - v5.10&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://whirr.apache.org/&quot;&gt;https://whirr.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/WHIRR&quot;&gt;https://cwiki.apache.org/confluence/display/WHIRR&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://attic.apache.org/projects/whirr.html&quot;&gt;https://attic.apache.org/projects/whirr.html&lt;/a&gt; - Apache Attic page&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Impala</title><link>https://ondataengineering.net/technologies/apache-impala/</link><description> &lt;p&gt;An MPP query engine that supports the execution of SQL queries over in HDFS, HBase, Kudu and S3 based on tables defined in the Hive Metastore. Focus is on analytical (OLAP) use cases, and more specifically on low latency interactive queries (rather than long running batch queries), with some support for batch inserts of data. Supports DDL statements for updating the Hive Metastore, uses (broadly) the same SQL syntax as Hive (including UDFs and a range of aggregate and analytical functions), as well as the same JDBC / ODBC drivers, and is therefore compatible with any Hive query tool (such as Beeline). Supports querying over data in Parquet, Text, Avro, RCFile and SequenceFile formats, with the ability to write Parquet and Text data. Support Kerberos and LDAP authentication, and integration with Apache Sentry for authorisation. Includes a shell (Impala Shell) that supports some shell only commands for tuning performance and diagnosing problems. Created by Cloudera, started in May 2011 and first announced in October 2012, with a 1.0 GA release in May 2013. Donated to the Apache Foundation in December 2015, graduating in November 2017, and is still under active development.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Impala&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;May 2018 - v3.0&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;Cloudera CDH&lt;/a&gt;, &lt;a href=&quot;/technologies/mapr-expansion-pack/&quot;&gt;MapR Expansion Pack&lt;/a&gt;, &lt;a href=&quot;/technologies/cloudera-altus/data-warehouse/&quot;&gt;Cloudera Altus Data Warehouse&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.9&lt;/td&gt; &lt;td&gt;2017-06-17&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://impala.apache.org/docs/changelog-2.9.html&quot;&gt;changelog&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.10&lt;/td&gt; &lt;td&gt;2017-09-15&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://impala.apache.org/docs/changelog-2.10.html&quot;&gt;changelog&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.11&lt;/td&gt; &lt;td&gt;2018-01-18&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://impala.apache.org/docs/changelog-2.11.html&quot;&gt;changelog&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.12&lt;/td&gt; &lt;td&gt;2018-05-01&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://impala.apache.org/docs/changelog-2.12.html&quot;&gt;changelog&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;td&gt;2018-05-09&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://impala.apache.org/docs/changelog-3.0.html&quot;&gt;changelog&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://impala.apache.org/&quot;&gt;https://impala.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/foundation/entry/the-apache-software-foundation-announces24&quot;&gt;https://blogs.apache.org/foundation/entry/the-apache-software-foundation-announces24&lt;/a&gt; - graduation announcement&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/products/open-source/apache-hadoop/impala.html&quot;&gt;https://www.cloudera.com/products/open-source/apache-hadoop/impala.html&lt;/a&gt; - Cloudera homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/documentation/enterprise/latest/topics/impala.html&quot;&gt;https://www.cloudera.com/documentation/enterprise/latest/topics/impala.html&lt;/a&gt; - Cloudera documentation&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Sentry</title><link>https://ondataengineering.net/technologies/apache-sentry/</link><description> &lt;p&gt;A centralised security framework for managing access to data in Hadoop. Supports integration with LDAP and Active Directory for user authentication, a central policy server/store, and plugins for Hadoop components (including Hive, Solr, Impala and HDFS, with support for Kafka and Sqoop2 in preview) to manage authorisation of user access to data, although HDFS support is limited to Hive data only. Also supports row level filtering policies for Solr, and historical support for defining policies in files per service (Sentry Policy Files). Integrates with the Hue security app (to manage permissions) and with Cloudera Navigator (for authorisation audit events). Started in 2012 as Cloudera Access, with an initial 1.0 release in 2013 as Sentry. Donated to the Apache Foundation in August 2013, graduating in March 2016. &lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Sentry&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;March 2017 - v1.7&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;Cloudera CDH&lt;/a&gt;, &lt;a href=&quot;/technologies/mapr-expansion-pack/&quot;&gt;MapR Expansion Pack&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://sentry.apache.org/index.html&quot;&gt;https://sentry.apache.org/index.html&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/products/open-source/apache-hadoop/apache-sentry.html&quot;&gt;https://www.cloudera.com/products/open-source/apache-hadoop/apache-sentry.html&lt;/a&gt; - Cloudera homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/documentation/enterprise/latest/topics/sg_sentry_overview.html&quot;&gt;https://www.cloudera.com/documentation/enterprise/latest/topics/sg_sentry_overview.html&lt;/a&gt; - Cloudera documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/SENTRY/Documentation&quot;&gt;https://cwiki.apache.org/confluence/display/SENTRY/Documentation&lt;/a&gt; - Apache documentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/sentry/&quot;&gt;https://blogs.apache.org/sentry/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Other updates via the Cloudera blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Cloudera Search</title><link>https://ondataengineering.net/technologies/cloudera-search/</link><description> &lt;p&gt;A distribution of Apache Solr that also includes a number of tools for integrating with Solr using Morphlines. Includes two utilities for loading data from HDFS, the Crunch Indexer Tool (direct Solr inserts using Crunch over Spark or MapReduce), and the MapReduce Indexer Tool (creates Solr index files using Map Reduce, optionally putting these live), plus two utilities for loading data from HBase based on the Lily HBase Indexer, the Batch Indexer (for batch loads) and the NRT (Near Real Time) Indexer (for continuous replication of HBase events). First released in June 2013, with a GA release in September 2013 as part of CDH 4.3. Included tools are open sourced under an Apache 2.0 licence.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/cloudera/&quot;&gt;Cloudera&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;March 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Packages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-solr/&quot;&gt;Apache Solr&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;Cloudera CDH&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/documentation/enterprise/latest/topics/search.html&quot;&gt;https://www.cloudera.com/documentation/enterprise/latest/topics/search.html&lt;/a&gt; - Cloudera Documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/cloudera/search/&quot;&gt;https://github.com/cloudera/search/&lt;/a&gt; - HDFS utilities code repository (note that the default branch isn’t the current branch and the README is out of date)&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/cloudera/hbase-indexer&quot;&gt;https://github.com/cloudera/hbase-indexer&lt;/a&gt; - HBase utilities code repository (note that the default branch isn’t the current branch and the README is out of date)&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;News via the Cloudera blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Kudu</title><link>https://ondataengineering.net/technologies/apache-kudu/</link><description> &lt;p&gt;Columnar storage technology for tables of structured data, supporting low latency reads, updates and deletes by primary key, as well as analytical column/table scans. Provides Java, C++ and Python APIs, is queryable via Impala and Spark SQL, and provides Spark, Flume and MapReduce connectors. Supports cluster deployments (including co-existence with Hadoop), with tables partitioned into tablets (configurable on a per table basis), with tablets then replicated and distributed across the cluster, using the Raft Consensus Algorithm for consistency. Also supports variable column encoding (including bit shuffle, run length, dictionary and prefix encoding) and compression. Includes a web UI for reporting operational information, and metrics available from the command line, via HTTP or via a log file. Started in November 2012, with a initial beta release in September 2015. Donated to the Apache Foundation in December 2015, graduating in July 2016, with a 1.0 release in September 2016. Implemented in C++.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Kudu&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;March 2018 - v1.7&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;Cloudera CDH&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.4&lt;/td&gt; &lt;td&gt;2017-06-13&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://kudu.apache.org/releases/1.4.0/docs/release_notes.html&quot;&gt;summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.5&lt;/td&gt; &lt;td&gt;2017-09-08&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://kudu.apache.org/releases/1.5.0/docs/release_notes.html&quot;&gt;summary&lt;/a&gt;; &lt;a href=&quot;https://kudu.apache.org/2017/09/08/apache-kudu-1-5-0-released.html&quot;&gt;blog post&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.6&lt;/td&gt; &lt;td&gt;2017-12-07&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://kudu.apache.org/releases/1.6.0/docs/release_notes.html&quot;&gt;summary&lt;/a&gt;; &lt;a href=&quot;https://kudu.apache.org/2017/12/08/apache-kudu-1-6-0-released.html&quot;&gt;blog post&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.7&lt;/td&gt; &lt;td&gt;2018-03-23&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://kudu.apache.org/releases/1.7.0/docs/release_notes.html&quot;&gt;summary&lt;/a&gt;; &lt;a href=&quot;https://kudu.apache.org/2018/03/23/apache-kudu-1-7-0-released.html&quot;&gt;blog post&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://kudu.apache.org/&quot;&gt;https://kudu.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://kudu.apache.org/docs/&quot;&gt;https://kudu.apache.org/docs/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://kudu.apache.org/kudu.pdf&quot;&gt;http://kudu.apache.org/kudu.pdf&lt;/a&gt; - whitepaper including comparison to Parquet/HDFS and Phoenix/HBase&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/products/open-source/apache-hadoop/apache-kudu.html&quot;&gt;https://www.cloudera.com/products/open-source/apache-hadoop/apache-kudu.html&lt;/a&gt; - Cloudera information&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/documentation/kudu/latest.html&quot;&gt;https://www.cloudera.com/documentation/kudu/latest.html&lt;/a&gt; - Cloudera documentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://kudu.apache.org/releases/&quot;&gt;https://kudu.apache.org/releases/&lt;/a&gt; - details of new releases&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://kudu.apache.org/blog/&quot;&gt;https://kudu.apache.org/blog/&lt;/a&gt; - Apache Kudu blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/category/kudu/&quot;&gt;http://blog.cloudera.com/blog/category/kudu/&lt;/a&gt; - Cloudera blog posts on Kudu&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>RecordService</title><link>https://ondataengineering.net/technologies/recordservice/</link><description> &lt;p&gt;Abstraction layer for accessing structured data in Hadoop that enforces fine grained access control (via Apache Sentry). Started in January 2015 and announced with an initial beta release in September 2015 and a stated plan to donate it to the Apache Foundation, however all code and documentation were taken down at the end of 2017, with the download page on Cloudera's website now simply stating that 'RecordService is in development'. Functionality that was available included support for reading data from HDFS and S3 in Parquet, Text, Sequence File, RC and Avro formats via a Hive table/view definition or a file path, with support for HBase and Kudu planned, direct access to data via C++ and Java APIs plus integration with MapReduce, Spark, Impala and Pig, with support for Hive planned, and support for the Apache Sentry security model, including table, view, file (via grants on uris to create external tables) and column level security, with row level filtering and data masking planned.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/cloudera/&quot;&gt;Cloudera&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Discontinued&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;March 2017 - v0.3&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://vision.cloudera.com/introducing-recordservice-the-future-of-security-authorization-in-hadoop/&quot;&gt;http://vision.cloudera.com/introducing-recordservice-the-future-of-security-authorization-in-hadoop/&lt;/a&gt; - vision for RecordService&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/downloads/beta/record-service.html&quot;&gt;https://www.cloudera.com/downloads/beta/record-service.html&lt;/a&gt; - Current Cloudera holding page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://recordservice.io/&quot;&gt;http://recordservice.io/&lt;/a&gt; - old homepage and documentation, now redirects to Cloudera holding page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/cloudera/recordservice&quot;&gt;https://github.com/cloudera/recordservice&lt;/a&gt; - source code repo, now removed&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/cloudera/RecordServiceClient/&quot;&gt;https://github.com/cloudera/RecordServiceClient/&lt;/a&gt; - client libraries source code repo, now removed&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://recordservice.io/ReleaseNotes/&quot;&gt;http://recordservice.io/ReleaseNotes/&lt;/a&gt; - RecordService release details, now redirects to Cloudera holding page&lt;/li&gt; &lt;li&gt;Other updates through Cloudera Engineering blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Cloudera Manager</title><link>https://ondataengineering.net/technologies/cloudera-manager/</link><description> &lt;p&gt;Platform for installing, managing and monitoring Cloudera CDH Hadoop clusters. Supports creation of clusters using step by step wizards, plus cluster templates for creating multiple clusters with the same configuration (e.g. dev, test and production), using either native OS packages or parcels (a Cloudera Manager distribution format that has a number of advantages over packages). Also supports the administration and configuration of clusters (including user and resource management, and the ability to manage multiple clusters); the automated Kerberization of clusters; monitoring of cluster, host and service statuses, health and metrics; generation of events and the use of custom triggers to take action on these; the visualisation of metrics; centralised log management; HDFS reports and automatic replication of data to a backup/DR cluster. Also integrates directly with Cloudera Support to enable proactive support. Web based, with a REST API and a full security model with auditing of all actions, and the ability to add support for custom services. Introduced in January 2012 as a replacement for the Cloudera Management Suite (CMS). Available for free without some enterprise features, or as part of a Cloudera CDH subscription.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/cloudera/&quot;&gt;Cloudera&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;September 2018 - v6.0&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Manages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;Cloudera CDH&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.11&lt;/td&gt; &lt;td&gt;2017-04-18&lt;/td&gt; &lt;td&gt;See &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt; 5.11 release links&lt;/td&gt; &lt;td&gt;CDH 5.11&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.12&lt;/td&gt; &lt;td&gt;2017-07-13&lt;/td&gt; &lt;td&gt;See &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt; 5.12 release links&lt;/td&gt; &lt;td&gt;CDH 5.12&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.13&lt;/td&gt; &lt;td&gt;2017-10-12&lt;/td&gt; &lt;td&gt;See &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt; 5.13 release links&lt;/td&gt; &lt;td&gt;CDH 5.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.14&lt;/td&gt; &lt;td&gt;2018-01-26&lt;/td&gt; &lt;td&gt;See &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt; 5.14 release links&lt;/td&gt; &lt;td&gt;CDH 5.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.15&lt;/td&gt; &lt;td&gt;2018-06-15&lt;/td&gt; &lt;td&gt;See &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt; 5.15 release links&lt;/td&gt; &lt;td&gt;CDH 5.15&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6.0&lt;/td&gt; &lt;td&gt;2018-08-30&lt;/td&gt; &lt;td&gt;See &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt; 6.0 release links&lt;/td&gt; &lt;td&gt;CDH 6.0&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/products/product-components/cloudera-manager.html&quot;&gt;https://www.cloudera.com/products/product-components/cloudera-manager.html&lt;/a&gt; - Cloudera Manager homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/documentation/enterprise/latest/topics/cm_intro_primer.html&quot;&gt;https://www.cloudera.com/documentation/enterprise/latest/topics/cm_intro_primer.html&lt;/a&gt; - Documentation - link is to the Cloudera Manager introduction, with the rest of Cloudera documentation (e.g. Installation, Upgrade, Administration, Operation and Security documents) also referencing Cloudera Manager&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cloudera.github.io/cm_api/&quot;&gt;https://cloudera.github.io/cm_api/&lt;/a&gt; - Cloudera Manager API documentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Blog updates via the Cloudera blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Cloudera Navigator</title><link>https://ondataengineering.net/technologies/cloudera-navigator/</link><description> &lt;p&gt;A suite of solutions including Navigator Data Management (technical metadata management, lineage, cluster activity and analytics, cluster audit and automated policy actions), Navigator Encryption (filesystem level encryption, key management and integration with HDFS transparent encryption), and Navigator Optimizer (a solution for identifying SQL workloads that are candidates for migration to Hadoop and then optimising these once on Hadoop)) built around the Cloudera CDH Hadoop distribution. All products are commercial closed source products, that are only available with an appropriate Cloudera Enterprise licence.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/cloudera/&quot;&gt;Cloudera&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;February 2018&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Sub-projects&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/cloudera-navigator/data-management/&quot;&gt;Cloudera Navigator Data Management&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Solution for managing data on a CDH Hadoop cluster. Automatically extracts metadata relating to HDFS, Hive, Impala, MapReduce, Oozie, Pig, S3, Spark, Sqoop and YARN, including data structures (databases, tables and columns) and jobs (relating to data transformation) based on activity within a cluster (rather than statically analysing code), allowing it to be searched, filtered and viewed, including displaying lineage diagrams showing how data moves through the system, a Data Stewardship dashboard of key data management information (including statistics on the data held in the cluster and the activity relating to this data), analytics on the data held in HDFS, and a full audit capability of all activity on the cluster. Allows custom metadata to be added to objects, including descriptions, key-value pairs and tags, with the option to define metadata namespaces and data types / value constraints (managed metadata), plus the ability to pre-set custom attributes (via job properties for MapReduce jobs and JSON .navigator files for HDFS files), and the ability to define data lifecycle management policies (allowing actions to be specified based on metadata, e.g. to archive any files that haven't been accessed for six months). Web based, with a full user security model, and a REST API and Java SDK for integrating external applications with metadata held in Navigator. Initial 1.0 release was in February 2013.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/cloudera-navigator/data-encryption/&quot;&gt;Cloudera Navigator Data Encryption&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A suite of products that complement HDFS transparent encryption to provide data at rest encryption across an Hadoop cluster. Includes Navigator Encrypt (a solution for encrypting Linux filesystems, with access granted to approved processes), Navigator Key Trustee Server (a software based solution for managing encryption keys), Navigator Key HSM (allows Navigator Key Trustee Server to use a Hardware Security Module as the root of trust for keys), Navigator Key Trustee KMS (an Hadoop Key Management Service that uses Navigator Key Trustee Server as the underlying key store) and Navigator HSM KMS (an hadoop Key Management Service backed by an HSM where encryption zone keys originate on and never leave the HSM). First released in 2014 following the acquisition of Gazzang.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;https://ondataengineering.net/technologies/cloudera-navigator/optimizer/&quot;&gt;Cloudera Navigator Optimizer&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A web based hosted service for analysing SQL logs from a range of relational databases to provide guidance on offloading workloads to Hadoop, and from Hive and Impala to provide guidance on optimising workloads running on Hadoop. Can analyse query logs, query metadata, schemas and statistics, and includes a Java utility to mask literal values in SQl queries and logs, and to encrypting schema identifiers before files are uploaded. Provides analytics on the overall query workload (including by similarity and risk, as well as by uploaded metrics such as cpu usage, memory usage and file system reads/writes) and recommendations for improvements to queries (to reduce risk, and to make external queries Hadoop compatible), with risk representing the level of Hadoop compatibility. Formally Xplain.io which was founded in 2013, acquired by Cloudera in February 2015, with a GA release as Cloudera Navigator Optimizer in July 2016.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Add on to&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;Cloudera CDH&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;p&gt;See sub-project pages - each component is separately versioned&lt;/p&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;p&gt;See sub-project pages - product information and documentation is separate for each component&lt;/p&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/category/navigator/&quot;&gt;http://blog.cloudera.com/blog/category/navigator/&lt;/a&gt; - Cloudera blog posts&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Cloudera Navigator Data Management</title><link>https://ondataengineering.net/technologies/cloudera-navigator/data-management/</link><description> &lt;p&gt;Solution for managing data on a CDH Hadoop cluster. Automatically extracts metadata relating to HDFS, Hive, Impala, MapReduce, Oozie, Pig, S3, Spark, Sqoop and YARN, including data structures (databases, tables and columns) and jobs (relating to data transformation) based on activity within a cluster (rather than statically analysing code), allowing it to be searched, filtered and viewed, including displaying lineage diagrams showing how data moves through the system, a Data Stewardship dashboard of key data management information (including statistics on the data held in the cluster and the activity relating to this data), analytics on the data held in HDFS, and a full audit capability of all activity on the cluster. Allows custom metadata to be added to objects, including descriptions, key-value pairs and tags, with the option to define metadata namespaces and data types / value constraints (managed metadata), plus the ability to pre-set custom attributes (via job properties for MapReduce jobs and JSON .navigator files for HDFS files), and the ability to define data lifecycle management policies (allowing actions to be specified based on metadata, e.g. to archive any files that haven't been accessed for six months). Web based, with a full user security model, and a REST API and Java SDK for integrating external applications with metadata held in Navigator. Initial 1.0 release was in February 2013.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/cloudera/&quot;&gt;Cloudera&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;September 2018 - v6.0&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.10&lt;/td&gt; &lt;td&gt;2017-04-18&lt;/td&gt; &lt;td&gt;See &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt; 5.11 release links&lt;/td&gt; &lt;td&gt;CDH 5.11&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.11&lt;/td&gt; &lt;td&gt;2017-07-13&lt;/td&gt; &lt;td&gt;See &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt; 5.12 release links&lt;/td&gt; &lt;td&gt;CDH 5.12&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.12&lt;/td&gt; &lt;td&gt;2017-10-12&lt;/td&gt; &lt;td&gt;See &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt; 5.13 release links&lt;/td&gt; &lt;td&gt;CDH 5.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.13&lt;/td&gt; &lt;td&gt;2018-01-26&lt;/td&gt; &lt;td&gt;See &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt; 5.14 release links&lt;/td&gt; &lt;td&gt;CDH 5.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.14&lt;/td&gt; &lt;td&gt;2018-06-15&lt;/td&gt; &lt;td&gt;See &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt; 5.15 release links&lt;/td&gt; &lt;td&gt;CDH 5.15&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6.0&lt;/td&gt; &lt;td&gt;2018-08-30&lt;/td&gt; &lt;td&gt;See &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt; 6.0 release links&lt;/td&gt; &lt;td&gt;CDH 6.0&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/products/product-components/cloudera-navigator.html&quot;&gt;https://www.cloudera.com/products/product-components/cloudera-navigator.html&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/resources/datasheet/cloudera-navigator-datasheet.html&quot;&gt;https://www.cloudera.com/resources/datasheet/cloudera-navigator-datasheet.html&lt;/a&gt; - datasheet&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/documentation/enterprise/latest/topics/cn_iu_introduce_navigator.html&quot;&gt;https://www.cloudera.com/documentation/enterprise/latest/topics/cn_iu_introduce_navigator.html&lt;/a&gt; - introduction&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/documentation/enterprise/latest/topics/datamgmt.html&quot;&gt;https://www.cloudera.com/documentation/enterprise/latest/topics/datamgmt.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_cn_dm_6_release_notes.html&quot;&gt;https://www.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_cn_dm_6_release_notes.html&lt;/a&gt; - release note&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://cloudera.github.io/navigator/apidocs/v3/&quot;&gt;http://cloudera.github.io/navigator/apidocs/v3/&lt;/a&gt; - REST API documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/cloudera/navigator-sdk&quot;&gt;https://github.com/cloudera/navigator-sdk&lt;/a&gt; - Java SDK&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Cloudera Navigator Data Encryption</title><link>https://ondataengineering.net/technologies/cloudera-navigator/data-encryption/</link><description> &lt;p&gt;A suite of products that complement HDFS transparent encryption to provide data at rest encryption across an Hadoop cluster. Includes Navigator Encrypt (a solution for encrypting Linux filesystems, with access granted to approved processes), Navigator Key Trustee Server (a software based solution for managing encryption keys), Navigator Key HSM (allows Navigator Key Trustee Server to use a Hardware Security Module as the root of trust for keys), Navigator Key Trustee KMS (an Hadoop Key Management Service that uses Navigator Key Trustee Server as the underlying key store) and Navigator HSM KMS (an hadoop Key Management Service backed by an HSM where encryption zone keys originate on and never leave the HSM). First released in 2014 following the acquisition of Gazzang.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Navigator Encrypt, Navigator Key Trustee Server, Navigator Key HSM, Navigator Key Trustee KMS, Navigator HSM KMS&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/cloudera/&quot;&gt;Cloudera&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;September 2018 - v6.0&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.14 (1.11 Key HSM; 3.14 Encrypt)&lt;/td&gt; &lt;td&gt;2018-01-26&lt;/td&gt; &lt;td&gt;See &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt; 5.14 release links&lt;/td&gt; &lt;td&gt;CDH 5.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.15 (1.11 Key HSM; 3.15 Encrypt)&lt;/td&gt; &lt;td&gt;2018-06-15&lt;/td&gt; &lt;td&gt;See &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt; 5.15 release links&lt;/td&gt; &lt;td&gt;CDH 5.15&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6.0&lt;/td&gt; &lt;td&gt;2018-08-30&lt;/td&gt; &lt;td&gt;See &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt; 6.0 release links&lt;/td&gt; &lt;td&gt;CDH 6.0&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/documentation/enterprise/latest/topics/navigator_encryption.html&quot;&gt;https://www.cloudera.com/documentation/enterprise/latest/topics/navigator_encryption.html&lt;/a&gt; - introduction&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/documentation/enterprise/latest/topics/security.html&quot;&gt;https://www.cloudera.com/documentation/enterprise/latest/topics/security.html&lt;/a&gt; - documentation index&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_cn_6_encryption_release_notes.html&quot;&gt;https://www.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_cn_6_encryption_release_notes.html&lt;/a&gt; - release notes index&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Cloudera Navigator Optimizer</title><link>https://ondataengineering.net/technologies/cloudera-navigator/optimizer/</link><description> &lt;p&gt;A web based hosted service for analysing SQL logs from a range of relational databases to provide guidance on offloading workloads to Hadoop, and from Hive and Impala to provide guidance on optimising workloads running on Hadoop. Can analyse query logs, query metadata, schemas and statistics, and includes a Java utility to mask literal values in SQl queries and logs, and to encrypting schema identifiers before files are uploaded. Provides analytics on the overall query workload (including by similarity and risk, as well as by uploaded metrics such as cpu usage, memory usage and file system reads/writes) and recommendations for improvements to queries (to reduce risk, and to make external queries Hadoop compatible), with risk representing the level of Hadoop compatibility. Formally Xplain.io which was founded in 2013, acquired by Cloudera in February 2015, with a GA release as Cloudera Navigator Optimizer in July 2016.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/cloudera/&quot;&gt;Cloudera&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;September 2018&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://optimizer.cloudera.com/&quot;&gt;https://optimizer.cloudera.com/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/products/product-components/cloudera-navigator-optimizer.html&quot;&gt;https://www.cloudera.com/products/product-components/cloudera-navigator-optimizer.html&lt;/a&gt; - product information&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/documentation/enterprise/latest/topics/navopt.html&quot;&gt;https://www.cloudera.com/documentation/enterprise/latest/topics/navopt.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Cloudera Altus Director</title><link>https://ondataengineering.net/technologies/cloudera-altus/director/</link><description> &lt;p&gt;Solution for deploying and managing Cloudera CDH Hadoop clusters on cloud infrastructure based on automatically provisioned infrastructure with Hadoop provisioned on top via Cloudera Manager. Includes out of the box support for Amazon Web Services, Microsoft Azure and Google Cloud Platform, with support for vSphere available from VMWare, with a Service Provider Interface (SPI) for adding support for new providers. Server component must be manually deployed via an RPM. Supports the ability to scale clusters up and down, clone clusters, run post deployment scripts, and create Kerberized and highly available clusters. Manageable through a web UI, a REST API (with Python and Java APIs) and a CLI. Released as Cloudera Director at 1.0 in October 2014 as part of Cloudera Enterprise 5.2, being renamed to Cloudera Altus Director in September 2018 as part of CDH 6. Free to download and use, with commercial support available as part of a Cloudera Enterprise subscription.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Cloudera Director&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/cloudera-altus/&quot;&gt;Cloudera Altus&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;September 2018 - v6.0&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Manages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;Cloudera CDH&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;further-information&quot;&gt;Further Information&lt;/h2&gt; &lt;p&gt;Also available on AWS as &lt;a href=&quot;https://aws.amazon.com/quickstart/architecture/cloudera/&quot;&gt;Cloudera EDH AWS Quick Start&lt;/a&gt;, and as &lt;a href=&quot;https://azuremarketplace.microsoft.com/en-us/marketplace/apps/cloudera.director-on-azure?tab=Overview&quot;&gt;Cloudera Director on the Azure Marketplace&lt;/a&gt;.&lt;/p&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.4&lt;/td&gt; &lt;td&gt;2017-04-18&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2017/04/whats-new-in-cloudera-director-2-4/&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt;CDH 5.11&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.5&lt;/td&gt; &lt;td&gt;2017-07-14&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2017/07/whats-new-in-cloudera-director-2-5/&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;http://blog.cloudera.com/blog/2017/08/cloudera-director-and-spot-instances-resilience-and-repair/&quot;&gt;AWS Spot instance support&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.6&lt;/td&gt; &lt;td&gt;2017-10-12&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://community.cloudera.com/t5/Community-News-Release/ANNOUNCE-Cloudera-Director-2-6-0-Released/td-p/60880&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;http://blog.cloudera.com/blog/2017/10/whats-new-in-cloudera-director-2-6/&quot;&gt;Blog Post&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.7&lt;/td&gt; &lt;td&gt;2018-01-26&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://community.cloudera.com/t5/Community-News-Release/ANNOUNCE-Cloudera-Director-2-7-0-Released/m-p/64054#M214&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;http://blog.cloudera.com/blog/2018/01/whats-new-in-cloudera-director-2-7/&quot;&gt;Blog Post&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.8&lt;/td&gt; &lt;td&gt;2018-06-15&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://community.cloudera.com/t5/Community-News-Release/ANNOUNCE-Cloudera-Director-2-8-0-Released/td-p/69229&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;http://blog.cloudera.com/blog/2018/06/whats-new-in-cloudera-director-2-8/&quot;&gt;Blog Post&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6.0&lt;/td&gt; &lt;td&gt;2018-09-13&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://community.cloudera.com/t5/Community-News-Release/ANNOUNCE-Cloudera-Altus-Director-6-0-0-Released/td-p/79802&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;http://blog.cloudera.com/blog/2018/09/whats-new-in-cloudera-altus-director-6-0/&quot;&gt;Blog Post&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Renamed from Cloudera Director to Cloudera Altus Director&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/products/product-components/cloudera-director.html&quot;&gt;https://www.cloudera.com/products/product-components/cloudera-director.html&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/documentation/director/latest.html&quot;&gt;https://www.cloudera.com/documentation/director/latest.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/cloudera?q=director&quot;&gt;https://github.com/cloudera?q=director&lt;/a&gt; - plugins&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.vmware.com/vsphere/2016/06/an-exciting-innovation-in-big-data-a-new-cloudera-director-plugin-for-vmware-vsphere.html&quot;&gt;https://blogs.vmware.com/vsphere/2016/06/an-exciting-innovation-in-big-data-a-new-cloudera-director-plugin-for-vmware-vsphere.html&lt;/a&gt; - VMWare plugin announcement&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>MapR Expansion Pack</title><link>https://ondataengineering.net/technologies/mapr-expansion-pack/</link><description> &lt;p&gt;A package of open source Hadoop projects certified to work together against one or more versions of the MapR Converged Data Platform. Has new major releases roughtly once a quarter, with most components kept resonably up to date with the open source version, with any patching done publically in GitHub. Available as RPMs, and installable via the MapR Installer. These components were originally bundled as part of the MapR Converged Data Platform, but were broken out as the MapR Ecosystem Pack in September 2016 to allow them to be released independantly. Renamed to the MapR Expansion Pack as of version 4.0.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;MEP, MapR Ecosystem Pack&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/mapr/&quot;&gt;MapR&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;October 2018 - v6.0&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Packages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-drill/&quot;&gt;Apache Drill&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-flume/&quot;&gt;Apache Flume&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Apache Hive&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-impala/&quot;&gt;Apache Impala&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-kafka/kafka-connect/&quot;&gt;Kafka Connect&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-kafka/kafka-streams/&quot;&gt;Kafka Streams&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-mahout/&quot;&gt;Apache Mahout&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-myriad/&quot;&gt;Apache Myriad&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-oozie/&quot;&gt;Apache Oozie&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-pig/&quot;&gt;Apache Pig&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-sentry/&quot;&gt;Apache Sentry&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-sqoop/&quot;&gt;Apache Sqoop&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-storm/&quot;&gt;Apache Storm&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-tez/&quot;&gt;Apache Tez&lt;/a&gt;, &lt;a href=&quot;/technologies/hue/&quot;&gt;Hue&lt;/a&gt;, MapR Object Store, MapR-DB HBase Clients and Tools, MapR-ES Kafka Clients and Tools&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/mapr-converged-data-platform/&quot;&gt;MapR Converged Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;further-information&quot;&gt;Further Information&lt;/h2&gt; &lt;p&gt;The documentation homepage provides a list of the components included, however this omits a number of components including Mahout, Storm and Tez, and includes Cascading which isn’t actually bundled. The release notes has a more complete list, along with detailed release nodes and package details for each of the components.&lt;/p&gt; &lt;p&gt;Some key components to call out include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Hive&lt;/a&gt; - includes Hive on Tez, but not Hive on Spark or LLAP. HCatalog is also included.&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/hue/&quot;&gt;Hue&lt;/a&gt; - includes &lt;a href=&quot;/technologies/apache-livy/&quot;&gt;Livy&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-spark&quot;&gt;Spark&lt;/a&gt; - includes Spark 2.x only&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-sqoop/&quot;&gt;Sqoop&lt;/a&gt; - includes both Squoop1 and Squoop2.&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/mapr-db/&quot;&gt;MapR-DB&lt;/a&gt; &lt;a href=&quot;/technologies/apache-hbase/&quot;&gt;HBase&lt;/a&gt; Clients and Tools - this includes the standard client, REST gateway and Thrift1 gateway from HBase, and the asynchronous client from &lt;a href=&quot;https://github.com/OpenTSDB/asynchbase&quot;&gt;https://github.com/OpenTSDB/asynchbase&lt;/a&gt;, all of which can be used to manipulate data in MapR-DB binary tables. Prior to MEP 4.0 (MapR 6.0) the entirety of HBase was bundled, however in MEP 4.0 the HBase Master and HBase Regionserver are no longer distributed.&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/mapr-es/&quot;&gt;MapR-ES&lt;/a&gt; &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Kafka&lt;/a&gt; Clients and Tools - this includes the librdkafka C client, the Confluent Python client, &lt;a href=&quot;/technologies/apache-kafka/kafka-connect/&quot;&gt;Kafka Connect&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-kafka/kafka-streams/&quot;&gt;Kafka Streams&lt;/a&gt;, KSQL and the Kafka REST proxy, all of which can be used to manipulate data in MapR-ES. Introduced in MEP 2.0 - see &lt;a href=&quot;https://mapr.com/blog/kafka-connect-and-kafka-rest-api-mapr-streaming-just-became-whole-lot-easier/&quot;&gt;blog post&lt;/a&gt;&lt;/li&gt; &lt;li&gt;MapR Object Store provides an S3 compatible API over &lt;a href=&quot;/technologies/mapr-fs/&quot;&gt;MapR-FS&lt;/a&gt;&lt;/li&gt; &lt;li&gt;The open source components for &lt;a href=&quot;/technologies/mapr-monitoring/&quot;&gt;MapR Montioring&lt;/a&gt; - collectd, OpenTSDB, Grafana, FluentD, Elasticsearch and Kibana, are actually distributed as part of the MEP.&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;2017-11-21&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://community.mapr.com/community/products/blog/2017/11/21/announcing-mep-40-released&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;4.1&lt;/td&gt; &lt;td&gt;2018-02-02&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://community.mapr.com/community/products/blog/2018/02/08/announcing-mapr-expansion-pack-mep-41-released&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.0&lt;/td&gt; &lt;td&gt;2018-04-06&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://community.mapr.com/community/products/blog/2018/04/06/announcement-mapr-601-mep-50&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;https://maprdocs.mapr.com/60/EcosystemRN/MEP5.0.0.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Drill 1.13; Flume 1.8; Impala 2.10; Spark 2.2.1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6.0&lt;/td&gt; &lt;td&gt;2018-10-03&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://mapr.com/docs/61/MEPs/whats_new_MEP_6.0.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Kafka Streams; KSQL; Spark 2.3.1; Hive 2.3; Hue 4.2&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/products/mapr-ecosystem-pack/&quot;&gt;https://mapr.com/products/mapr-ecosystem-pack/&lt;/a&gt; - homepage (as of 23/02/18 not updated for MEP 4.0)&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://maprdocs.mapr.com/home/c_ecosystem_intro.html&quot;&gt;http://maprdocs.mapr.com/home/c_ecosystem_intro.html&lt;/a&gt; - documentation homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://maprdocs.mapr.com/home/EcosystemRN/EcoPackRN.html&quot;&gt;http://maprdocs.mapr.com/home/EcosystemRN/EcoPackRN.html&lt;/a&gt; - release notes&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://maprdocs.mapr.com/home/InteropMatrix/r_release_mep_dates.html&quot;&gt;http://maprdocs.mapr.com/home/InteropMatrix/r_release_mep_dates.html&lt;/a&gt; - release history&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://maprdocs.mapr.com/home/AdvancedInstallation/InstallingEcoWithoutInstaller.html&quot;&gt;http://maprdocs.mapr.com/home/AdvancedInstallation/InstallingEcoWithoutInstaller.html&lt;/a&gt; - installation docs&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/mapr&quot;&gt;https://github.com/mapr&lt;/a&gt; - component source ode&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://package.mapr.com/releases/MEP/&quot;&gt;http://package.mapr.com/releases/MEP/&lt;/a&gt; - package repository&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Announcements via the MapR product announcements blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Drill</title><link>https://ondataengineering.net/technologies/apache-drill/</link><description> &lt;p&gt;An MPP query engine that supports queries over one or more underlying databases or datasets without first defining a schema and with the ability to join data from multiple datastores together. Supports a range of underlying technologies including HDFS, NAS, HBase, MongoDB, MapR-DB, MapR-FS, Kafka, OpenTSDB, Amazon S3, Azure Blob Storage, Google Cloud Storage, JDBC, Avro, JSON and Parquet. Pushes queries down to underlying datastores where possible, and supports an in-memory columnar datastore based on a schema free JSON document model for performing cross datastore query operations. Supports dynamic schema discovery, with support for complex and nested types, including a number of SQL extensions. Supports standard SQL, UDFs (including Hive UDFs) and comes with JDBC and ODBC drivers, a REST API, plus a shell, web console and C++ API. Designed to be horizontally scalable and to support high throughput and low latency use cases, and can run over YARN. Supports Kerberos and username/password authentication, plus a full authorisation model. Created by MapR Based on Google's Dremel paper, donated to the Apache Foundation in September 2012, graduating in November 2014, with a 1.0 release in May 2015, and is still under active development&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Drill&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commerical Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;August 2018 - v1.14&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/mapr-expansion-pack/&quot;&gt;MapR Expansion Pack&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.11&lt;/td&gt; &lt;td&gt;2017-07-31&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://drill.apache.org/blog/2017/07/31/drill-1.11-released/&quot;&gt;announcement&lt;/a&gt; &lt;a href=&quot;https://mapr.com/blog/apache-drill-version-111-on-mapr-release-overview/&quot;&gt;MapR blog&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.12&lt;/td&gt; &lt;td&gt;2017-12-15&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://drill.apache.org/blog/2017/12/15/drill-1.12-released/&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;https://mapr.com/blog/apache-drill-1-12-on-mapr-6-0-release-highlights/&quot;&gt;MapR blog&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Kafka and OpenTSDB support&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.13&lt;/td&gt; &lt;td&gt;2018-03-18&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://drill.apache.org/blog/2018/03/18/drill-1.13-released/&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;https://mapr.com/blog/apache-drill-1-13-on-mapr-6-0-1-release-highlights/&quot;&gt;MapR blog&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Drill-on-YARN; Drill on MapR Event Streams alpha&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.14&lt;/td&gt; &lt;td&gt;2018-08-05&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://drill.apache.org/blog/2018/08/05/drill-1.14-released/&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Docker support&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://drill.apache.org/&quot;&gt;http://drill.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://drill.apache.org/docs/&quot;&gt;http://drill.apache.org/docs/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://maprdocs.mapr.com/51/Drill/intro_drill_on_yarn.html&quot;&gt;http://maprdocs.mapr.com/51/Drill/intro_drill_on_yarn.html&lt;/a&gt; - Drill on YARN information&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://drill.apache.org/blog/&quot;&gt;http://drill.apache.org/blog/&lt;/a&gt; - blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Myriad</title><link>https://ondataengineering.net/technologies/apache-myriad/</link><description> &lt;p&gt;Tool that allows YARN applications to run over Apache Mesos, allowing them to co-exist and share cluster resources. Consists of Myriad Executor, a Mesos managed task that in turns manages a YARN Node Manager, and Myriad Scheduler, a plugin for the YARN Resource Manager that delegates resource negotiation to Mesos (and launches Myriad Executor processes on required nodes via Mesos). Supports fixed resource allocation to YARN Node Managers, as well as fine-grained scaling where resources are dynamically requested from Mesos. Includes a web based user interface and REST API that includes support for scaling YARN resources when using fixed resource allocation. Originally created by eBay, MapR and Mesosphere and dondated to the Apache Foundation in March 2015. Has not yet graduated or reached a 1.0 release, with development activity seeming very quiet since October 2016.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Myriad&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;April 2017 - v0.2&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Compatible with&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/yarn/&quot;&gt;YARN&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/mapr-expansion-pack/&quot;&gt;MapR Expansion Pack&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Add on to&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-mesos/&quot;&gt;Apache Mesos&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://myriad.apache.org/&quot;&gt;http://myriad.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://myriad.apache.org/docs/&quot;&gt;http://myriad.apache.org/docs/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/products/product-overview/apache-myriad/&quot;&gt;https://mapr.com/products/product-overview/apache-myriad/&lt;/a&gt; - MapR information&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://myriad.apache.org/news/&quot;&gt;http://myriad.apache.org/news/&lt;/a&gt; - news&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>MapR Monitoring</title><link>https://ondataengineering.net/technologies/mapr-monitoring/</link><description> &lt;p&gt;A collection of open source components used to capture, store and visualise metrics and log messages across a MapR Converged Data Platform. Components used include collectd (to capture metrics), OpenTSDB (a time-series database that runs on top of MapR-DB to store metrics), Grafana (to visualise and graph metrics into dashboards), FluentD (to collect and parse log messages), Elasticsearch (to store and index log messages for search) and Kibana (to search and view log messages). Metrics captured include cpu, disk, memory and network metrics, plus metrics for Drill, YARN and the MapR components. Log messages are captured from system logs, plus YARN, ZooKeeper, Drill, Hbase, Hive, Oozie, Spark, the MapR component logs, and the logs for the MapR Monitoring components. Both Grafana and Kibana include starter sample dashboards. First released in June 2016 as part of the Spyglass initiative.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/mapr/&quot;&gt;MapR&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;April 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Packages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/elasticsearch/&quot;&gt;Elasticsearch&lt;/a&gt;, &lt;a href=&quot;/technologies/opentsdb/&quot;&gt;OpenTSDB&lt;/a&gt;, FluentD, Grafana, Kibana, collectd&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Manages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/mapr-converged-data-platform/&quot;&gt;MapR Converged Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/products/spyglass-initiative/&quot;&gt;https://mapr.com/products/spyglass-initiative/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/resources/mapr-monitoring/&quot;&gt;https://mapr.com/resources/mapr-monitoring/&lt;/a&gt; - introduction&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://maprdocs.mapr.com/home/AdministratorGuide/Monitoring.html&quot;&gt;http://maprdocs.mapr.com/home/AdministratorGuide/Monitoring.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://maprdocs.mapr.com/home/EcosystemRN/MapRMonitoringRN.html&quot;&gt;http://maprdocs.mapr.com/home/EcosystemRN/MapRMonitoringRN.html&lt;/a&gt; - release notes&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Announcements via the MapR product announcements blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>MapR-FS</title><link>https://ondataengineering.net/technologies/mapr-fs/</link><description> &lt;p&gt;Resilient distributed cluster file system that supports HDFS and NFS/POSIX (v3/v4) access. An S3 compatible API is provided by the MapR Object Store gateway bundled as part of the MapR Expansion Pack. Supports POSIX compliance, arbitrary in place updates to files (unlike HDFS which is append only), distributed metadata (it has no equivalent of the HDFS Name Node), block level mirroring to a remote cluster for DR or load balancing, encryption at rest, automatic storage tiering (including to external object storage) and snapshots (which provide point in time read only views). Data is stored in containers (which manage data blocks and the replication of these over the cluster), and logically organised into volumes (which manage files, directories and block allocation across one or more containers), which also provide multi-tenancy support, with administrative control, data placement, job execution, snapshots and mirroring all configurable against a volume. Supports encrypted communications, full auditing capabilities, Kerberos and Linux PAM for authentication, authorisation via ACLs (against clusters, volumes and job queues), POSIX file permissions (against files and directories) and Access Control Expressions (ACEs, arbitrary boolean expressions against volumes, files and directories). First releases as part of MapR v1.0 in 2010.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;MapR File System&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/mapr/&quot;&gt;MapR&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;October 2018 - 6.1&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is used by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/mapr-db/&quot;&gt;MapR-DB&lt;/a&gt;, &lt;a href=&quot;/technologies/mapr-es/&quot;&gt;MapR-ES&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/mapr-converged-data-platform/&quot;&gt;MapR Converged Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.2&lt;/td&gt; &lt;td&gt;2016-08-19&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6.0&lt;/td&gt; &lt;td&gt;2017-11-21&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6.1&lt;/td&gt; &lt;td&gt;2018-10-03&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt;S3 compatible API; &lt;a href=&quot;https://mapr.com/blog/data-tiering-capacity-performance-juxtaposition/&quot;&gt;storage tiering&lt;/a&gt;; NFSv4 support; encryption at rest&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;See &lt;a href=&quot;/technologies/mapr-converged-data-platform/&quot;&gt;MapR Converged Data Platform&lt;/a&gt; for further release information&lt;/p&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/products/mapr-fs/&quot;&gt;https://mapr.com/products/mapr-fs/&lt;/a&gt; - product home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/products/mapr-distribution-including-apache-hadoop/&quot;&gt;https://mapr.com/products/mapr-distribution-including-apache-hadoop/&lt;/a&gt; - links to whitepapers and further detail on key features&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://maprdocs.mapr.com/home/MapROverview/c_overview_intro.html&quot;&gt;http://maprdocs.mapr.com/home/MapROverview/c_overview_intro.html&lt;/a&gt; - overview&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://maprdocs.mapr.com/home/MapROverview/c_maprfs.html&quot;&gt;http://maprdocs.mapr.com/home/MapROverview/c_maprfs.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/blog/mapr-fs-vs-hdfs-5-minute-guide-understanding-their-differences-whiteboard-walkthrough/&quot;&gt;https://mapr.com/blog/mapr-fs-vs-hdfs-5-minute-guide-understanding-their-differences-whiteboard-walkthrough/&lt;/a&gt; - comparison to HDFS&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Updates via MapR release announcements and blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>MapR-DB</title><link>https://ondataengineering.net/technologies/mapr-db/</link><description> &lt;p&gt;NoSQL database built over MapR-FS, supporting wide column and JSON document tables and HBase and OJAI (Open JSON application interface) APIs. Tables are stored as first class objects in MapR-FS volumes, and are sharded into table regions / tablets. JSON document tables are schemaless, support read and write access to individual document fields, subsets of fields or whole documents, finding documents by id or native secondary indexes, a set of atomic operations for mutating documents, a change data capture API, and integration with Spark, Hive and MapReduce. Wide column (binary) tables are largely equivalent to HBase tables, and partially support the HBase API, but without support for custom HBase filters or co-processors. Supports replication at the table, column family or column level, either synchronously or asynchronously, and in either master-master or master-slave configurations, with support for replicating to Elasticsearch. Authentication is managed through access control expressions (ACEs) at the field level (for JSON document tables) or at the column level (for wide column tables). Introduced in MapR v4.0 in Sept 2014, with document supported added in MapR 5.1 in Feb 2016.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/mapr/&quot;&gt;MapR&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;October 2018 - 6.1&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Add ons&lt;/td&gt;&lt;td&gt;MapR-DB HBase Clients and Tools&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Uses&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/mapr-fs/&quot;&gt;MapR-FS&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/mapr-converged-data-platform/&quot;&gt;MapR Converged Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;NOTE: See &lt;a href=&quot;/technologies/mapr-expansion-pack/&quot;&gt;MapR Expansion Pack&lt;/a&gt; for details on MapR-DB HBase Clients and Tools&lt;/p&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.2&lt;/td&gt; &lt;td&gt;2016-08-19&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6.0&lt;/td&gt; &lt;td&gt;2017-11-21&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://mapr.com/blog/mapr-db-database-for-global-data-intensive-applications/&quot;&gt;blog post&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Native secondary indexes, change data capture API, Spark/Hive support&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6.1&lt;/td&gt; &lt;td&gt;2018-10-03&lt;/td&gt; &lt;td&gt;Complex datatypes in JSON&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;See &lt;a href=&quot;/technologies/mapr-converged-data-platform/&quot;&gt;MapR Converged Data Platform&lt;/a&gt; for further release information&lt;/p&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/products/mapr-db-in-hadoop-nosql/&quot;&gt;https://mapr.com/products/mapr-db-in-hadoop-nosql/&lt;/a&gt; - product homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://maprdocs.mapr.com/home/MapROverview/maprDB-overview.html&quot;&gt;http://maprdocs.mapr.com/home/MapROverview/maprDB-overview.html&lt;/a&gt; - overview&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://maprdocs.mapr.com/home/MapR-DB/developing_client_applications_for_mapr_db.html&quot;&gt;http://maprdocs.mapr.com/home/MapR-DB/developing_client_applications_for_mapr_db.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>MapR-ES</title><link>https://ondataengineering.net/technologies/mapr-es/</link><description> &lt;p&gt;Technology for buffering and storing real-time streams of data, built over MapR-FS, with support for a Kafka compatible API. Messages (key/value pairs where the key is optional) are organised into topics, which are partitioned and stored as first class objects within MapR-FS volumes, with topics then grouped into streams. Supports encryption of streams, automatic deletion of messages (via a time to live), consumer groups, authorisation using ACEs (access control expressions), plus replication of topics to one or more remote MapR-ES instances either synchronously or asynchronously, including support for Kafka's MirrorMaker. Comes with Java, C and Python libraries and includes a Kafka compatible API. Introduced in MapR 5.1 in Feb 2016. Previously called MapR Streams.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;MapR-Streams, MapR Streams&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/mapr/&quot;&gt;MapR&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;October 2018 - 6.1&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Add ons&lt;/td&gt;&lt;td&gt;MapR-ES Kafka Clients and Tools&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Uses&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/mapr-fs/&quot;&gt;MapR-FS&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/mapr-converged-data-platform/&quot;&gt;MapR Converged Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;NOTE: See &lt;a href=&quot;/technologies/mapr-expansion-pack/&quot;&gt;MapR Expansion Pack&lt;/a&gt; for details on MapR-ES Kafka Clients and Tools&lt;/p&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.2&lt;/td&gt; &lt;td&gt;2016-08-19&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6.0&lt;/td&gt; &lt;td&gt;2017-11-21&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6.1&lt;/td&gt; &lt;td&gt;2018-10-03&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;See &lt;a href=&quot;/technologies/mapr-converged-data-platform/&quot;&gt;MapR Converged Data Platform&lt;/a&gt; for further release information&lt;/p&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/products/mapr-streams/&quot;&gt;https://mapr.com/products/mapr-streams/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://maprdocs.mapr.com/home/MapROverview/c_mapr_streams.html&quot;&gt;http://maprdocs.mapr.com/home/MapROverview/c_mapr_streams.html&lt;/a&gt; - overview&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://maprdocs.mapr.com/home/MapR_Streams/mapr_streams.html&quot;&gt;http://maprdocs.mapr.com/home/MapR_Streams/mapr_streams.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/blog/kafka-vs-mapr-streams-why-mapr/&quot;&gt;https://mapr.com/blog/kafka-vs-mapr-streams-why-mapr/&lt;/a&gt; - comparison to Kafka&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>MapR Converged Data Platform</title><link>https://ondataengineering.net/technologies/mapr-converged-data-platform/</link><description> &lt;p&gt;A data platform built that provides Hadoop compatibility (via YARN and the MapR-FS HDFS compatible API), NoSQL and streaming data storage via MapR-DB and MapR-ES respectively, and a bundle of open source Hadoop projects via the MapR Ecosystem Pack. Comes with an installer (MapR Installer), a web based user interface for management (MapR Control System), and a monitoring and alerting solution (MapR Monitoring). Available as a free community edition (which excludes some enterprise features such as snapshots, high availability, disaster recovery and replication), a full commercial edition, and as MapR Edge (a small footprint edition that can run on low power and embedded hardware close to data sources to perform initial data filtering and processing before forwarding data on to a central cluster via MapR replication), MapR-XD (an edition that focuses on MapR-FS plus the Orbit Cloud Suite to provide web scale file and container storage), MapR Converged Data Platform for Docker (a marketing name for using the Converged Data Platform as persistent storage for docker containers) and MapR Data Fabric for Kubernetes (ditto but for Kubernetes). Supports a number of add-ons, including the Persistent Application Client Container (PACC, a docker image containing the client libraries required to connect to a MapR Converged Data Platform), MapR Orbit Cloud Suite (which adds support for deployment of cloud infrastructure along with MapR, integration with cloud object stores, plus mirroring and replication, with support for multi-tenancy, object tiering and OpenStack integration announced) and MapR Data Science Refinery (a docker based analytics notebook powered by Apache Zeppelin that fully integrates with the MapR Converged Data Platform). Supports deployment in the cloud (AWS and Azure), and is available as a managed service. First released as MapR v1.0 in 2010&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;MapR Edge, MapR-XD, MapR Converged Data Platform for Docker, MapR Data Fabric for Kubernetes&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/mapr/&quot;&gt;MapR&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Categories&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-categories/hadoop-distributions/&quot;&gt;Hadoop Distributions&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;October 2018 - 6.1&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Packages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/map-reduce/&quot;&gt;MapReduce&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hadoop/yarn/&quot;&gt;YARN&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-zookeeper/&quot;&gt;Apache ZooKeeper&lt;/a&gt;, &lt;a href=&quot;/technologies/mapr-db/&quot;&gt;MapR-DB&lt;/a&gt;, &lt;a href=&quot;/technologies/mapr-es/&quot;&gt;MapR-ES&lt;/a&gt;, &lt;a href=&quot;/technologies/mapr-expansion-pack/&quot;&gt;MapR Expansion Pack&lt;/a&gt;, &lt;a href=&quot;/technologies/mapr-fs/&quot;&gt;MapR-FS&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Manageable via&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/mapr-monitoring/&quot;&gt;MapR Monitoring&lt;/a&gt;, MapR Control System, MapR Installer&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Add ons&lt;/td&gt;&lt;td&gt;MapR Data Science Refinery, MapR Orbit Cloud Suite, Persistent Application Client Container&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.2&lt;/td&gt; &lt;td&gt;2016-08-19&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6.0&lt;/td&gt; &lt;td&gt;2017-11-21&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://mapr.com/products/whats-new/6-0/&quot;&gt;what’s new&lt;/a&gt;; &lt;a href=&quot;https://mapr.com/docs/60/ReleaseNotes/whatsnew_60.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt;New MapR Control System; MapR-DB new features&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6.1&lt;/td&gt; &lt;td&gt;2018-10-03&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://mapr.com/blog/mapr-6-1-new-horizons/&quot;&gt;blog post&lt;/a&gt;; &lt;a href=&quot;https://mapr.com/blog/mapr-6-1-release-with-mep-6-0-is-now-generally-available/&quot;&gt;GA announcement&lt;/a&gt;; &lt;a href=&quot;https://mapr.com/products/whats-new/6-1/&quot;&gt;what’s new&lt;/a&gt;; &lt;a href=&quot;https://mapr.com/docs/home/ReleaseNotes/whatsnew.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt;S3 compatible API; object tiering; enryption at rest&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/products/mapr-converged-data-platform/&quot;&gt;https://mapr.com/products/mapr-converged-data-platform/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://maprdocs.mapr.com/home/MapROverview/c_overview_intro.html&quot;&gt;http://maprdocs.mapr.com/home/MapROverview/c_overview_intro.html&lt;/a&gt; - overview&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://maprdocs.mapr.com/home/ReleaseNotes/c_relnotes_intro.html&quot;&gt;http://maprdocs.mapr.com/home/ReleaseNotes/c_relnotes_intro.html&lt;/a&gt; - release notes&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://maprdocs.mapr.com/home/InteropMatrix/r_release_dates.html&quot;&gt;http://maprdocs.mapr.com/home/InteropMatrix/r_release_dates.html&lt;/a&gt; - release history&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://doc.mapr.com/&quot;&gt;http://doc.mapr.com/&lt;/a&gt; - documentation for previous releases (prior to 5.0)&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/services/managed-services/&quot;&gt;https://mapr.com/services/managed-services/&lt;/a&gt; - managed service homepage&lt;/li&gt; &lt;li&gt;MapR Control System &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/products/mapr-control-system/&quot;&gt;https://mapr.com/products/mapr-control-system/&lt;/a&gt; - MapR Control System home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/blog/introducing-new-mapr-control-system/&quot;&gt;https://mapr.com/blog/introducing-new-mapr-control-system/&lt;/a&gt; - MapR Control System intro blog post&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;MapR Edge &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/products/edge&quot;&gt;https://mapr.com/products/edge&lt;/a&gt; - MapR Edge home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://community.mapr.com/community/products/blog/2017/03/14/introducing-mapr-edge&quot;&gt;https://community.mapr.com/community/products/blog/2017/03/14/introducing-mapr-edge&lt;/a&gt; - introduction to MapR Edge&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;MapR-XD &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/products/mapr-xd/&quot;&gt;https://mapr.com/products/mapr-xd/&lt;/a&gt; - MapR-XD home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/blog/intro-mapr-xd/&quot;&gt;https://mapr.com/blog/intro-mapr-xd/&lt;/a&gt; - introduction to MapR-XD&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;MapR Converged Data Platform for Docker &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://community.mapr.com/community/products/blog/2017/02/07/persistence-in-the-age-of-microservices-introducing-mapr-converged-data-platform-for-docker&quot;&gt;https://community.mapr.com/community/products/blog/2017/02/07/persistence-in-the-age-of-microservices-introducing-mapr-converged-data-platform-for-docker&lt;/a&gt; - introduction to MapR Converged Data Platform for Docker&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;MapR Data Fabric for Kubernetes &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/solutions/data-fabric/kubernetes/&quot;&gt;https://mapr.com/solutions/data-fabric/kubernetes/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://community.mapr.com/community/products/blog/2018/03/06/announcing-mapr-data-fabric-for-kubernetes&quot;&gt;https://community.mapr.com/community/products/blog/2018/03/06/announcing-mapr-data-fabric-for-kubernetes&lt;/a&gt; - announcement&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Persistent Application Client Container &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/products/persistent-application-client-container/&quot;&gt;https://mapr.com/products/persistent-application-client-container/&lt;/a&gt; - PACC homepage&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;MapR Orbit Cloud Suite &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/products/orbit-cloud/&quot;&gt;https://mapr.com/products/orbit-cloud/&lt;/a&gt; - Orbit Cloud Suite home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://community.mapr.com/community/products/blog/2017/08/29/introducing-the-mapr-orbit-cloud-suite&quot;&gt;https://community.mapr.com/community/products/blog/2017/08/29/introducing-the-mapr-orbit-cloud-suite&lt;/a&gt; - introduction to Orbit Cloud Suite&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;MapR Data Science Refinery &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/products/data-science-refinery/&quot;&gt;https://mapr.com/products/data-science-refinery/&lt;/a&gt; - Data Science Refinery home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/blog/introducing-mapr-data-science-refinery/&quot;&gt;https://mapr.com/blog/introducing-mapr-data-science-refinery/&lt;/a&gt; - Data Science Refinery intro blog post&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Announcements via the MapR product announcements blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Cloudera Data Science Workbench</title><link>https://ondataengineering.net/technologies/cloudera-data-science-workbench/</link><description> &lt;p&gt;A web based notebook for interactive data analytics that uses docker to provide custom execution environments for each notebook. Supports Python, R and Scala interpreters, plus remote execution of Spark with out of the box support for Hadoop security. Notebook code is run within a docker container in a managed Kubernetes instance, allowing different libraries to be installed and used by different notebooks, and other dependancies to be installed via terminal access to the container or via custom Docker images. Also includes support for version control (via git), tracking of model tests (Experiments), automatic deployment of models and all dependancies behind a REST endpoint (Models), collaboration via shared projects, sharing of notebooks via HTTP URLs, publishing of notebooks as HTML and scheduled execution of notebooks via workflows (including dependancies on other jobs). Originally created by Sense.io, which was acquired by Cloudera in March 2016. Initial GA release was 1.0 in April 2017.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/cloudera/&quot;&gt;Cloudera&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;June 2018 - v1.4&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;td&gt;2017-04-26&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://community.cloudera.com/t5/Community-News-Release/Announce-Cloudera-Data-Science-Workbench-is-now-available/m-p/54177#M173&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;https://www.cloudera.com/documentation/data-science-workbench/latest/topics/cdsw_release_notes.html#rel_100&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Initial release&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.1&lt;/td&gt; &lt;td&gt;2017-07-18&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://community.cloudera.com/t5/Community-News-Release/Announce-Cloudera-Data-Science-Workbench-Release-1-1/m-p/57605#M187&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;https://www.cloudera.com/documentation/data-science-workbench/latest/topics/cdsw_release_notes.html#rel_110&quot;&gt;release notes&lt;/a&gt;; &lt;a href=&quot;http://vision.cloudera.com/cloudera-data-science-workbench-release-1-1/&quot;&gt;blog post&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.2&lt;/td&gt; &lt;td&gt;2017-10-20&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.cloudera.com/documentation/data-science-workbench/latest/topics/cdsw_release_notes.html#rel_120&quot;&gt;release notes&lt;/a&gt;; &lt;a href=&quot;http://vision.cloudera.com/now-available-cloudera-data-science-workbench-release-1-2/&quot;&gt;blog post&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2017/10/new-in-cloudera-data-science-workbench-1-2-usage-monitoring-for-administrators/&quot;&gt;new usage monitoring&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.3&lt;/td&gt; &lt;td&gt;2018-01-26&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://community.cloudera.com/t5/Community-News-Release/ANNOUNCE-Cloudera-Data-Science-Workbench-1-3-Released/td-p/64065&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;https://www.cloudera.com/documentation/data-science-workbench/latest/topics/cdsw_release_notes.html#rel_130&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.4&lt;/td&gt; &lt;td&gt;2018-06-15&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://community.cloudera.com/t5/Community-News-Release/ANNOUNCE-Cloudera-Data-Science-Workbench-1-4-Released/td-p/69325&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;http://vision.cloudera.com/announcing-cloudera-data-science-workbench-release-1-4/&quot;&gt;blog post&lt;/a&gt;; &lt;a href=&quot;https://www.cloudera.com/documentation/data-science-workbench/latest/topics/cdsw_release_notes.html#rel_140&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/products/data-science-and-engineering/data-science-workbench.html&quot;&gt;https://www.cloudera.com/products/data-science-and-engineering/data-science-workbench.html&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://vision.cloudera.com/cloudera-data-science-workbench-self-service-data-science-for-the-enterprise/&quot;&gt;http://vision.cloudera.com/cloudera-data-science-workbench-self-service-data-science-for-the-enterprise/&lt;/a&gt; - introductory blog post&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/documentation/data-science-workbench/latest.html&quot;&gt;https://www.cloudera.com/documentation/data-science-workbench/latest.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Blog updates via the Cloudera blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Amazon S3</title><link>https://ondataengineering.net/technologies/amazon-s3/</link><description> &lt;p&gt;An object store service with eventual consistency, focusing on massive durability and scalability, with support for multiple storage tiers (including Amazon Glacier) and deep integration to the AWS ecosystem. Objects are organised into buckets and indexed by string, with the option to list objects by prefix and to summarise results based on a delimiter allowing a filesystem to be approximated. Metadata against objects is managed via S3 Object Tags, key-value pairs applied to objects that can be added, modified or deleted at any time. Lifecycle management policies can be assigned to name prefixes or object tags to automatically delete objects or move them between storage tiers. Supports versioning of objects, access control (at the bucket or object level), retrieving subsets of objects via server side queries (S3/Glacier select), replication of objects and metadata to a bucket in a different AWS region (cross-region replication), encryption of objects and support for SSL connections, immutable blobs (via Glacier Vault Lock), full auditing of all object operations, analytics on object operations, multi-part uploads, multi-object deletions, a flat-file output of object names and metadata (S3 Inventory), downloads via the bittorrent protocol, static website hosting and time limited object download URLs. Quotes a 99.999999999% guarentee that data won't be lost, with data stored redundantly across multiple devices and facilities within the chosen region, and scalability past trillions of objects. Provides a web based management console, mobile management app, a REST API and SDKs for a wide range of languages. First launched in March 2006.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Simple Storage Service, S3, Glacier, Amazon Glacier&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/amazon-web-services/&quot;&gt;Amazon Web Services&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;December 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/s3&quot;&gt;https://aws.amazon.com/s3&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/s3/details/&quot;&gt;https://aws.amazon.com/s3/details/&lt;/a&gt; - details&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/documentation/s3/&quot;&gt;https://aws.amazon.com/documentation/s3/&lt;/a&gt; - documentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;See &lt;a href=&quot;/tech-vendors/amazon-web-services/&quot;&gt;Amazon vendor page&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Microsoft Azure Blob Storage</title><link>https://ondataengineering.net/technologies/microsoft-azure-blob-storage/</link><description> &lt;p&gt;An object store service with strong consistency, with support for multiple blob types (block, page and append), multiple storage tiers (premium, hot, cold and archive) and deep integration to the Azure ecosystem. Block blobs are comprised of one or more blocks with operations done at the block level with changes made visible via a final commit; page blobs are collections of 512-byte pages optimised for random read and write operations against one or more pages; and append blobs only support modification via the addition of new data to the end of the blob. Objects are organised into containers and indexed by string, with the option to list objects by prefix and to summarise results based on a delimiter allowing a filesystem to be approximated. Supports name-value pair metadata against containers and objects, both optimistic and pessimistic (lock based) concurrency, snapshots (providing read only access to objects as they were when the snapshot was taken), soft deletes (allowing previous versions of objects to be recovered), immutable blobs, lifecycle management (in public preview), access control via access tokens (shared access signatures), public access to containers, configurable geo redundancy, encryption of objects (Azure Storage Service Encryption - SSE) and support for SSL connections, multi-part uploads, the use of custom domains, and logging and metrics (Azure Storage Analytics). Provides a REST API, web app (Azure Storage Explorer), a range of SDKs, a CLI and PowerShell integration.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Azure Blob Storage&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;Microsoft&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;June 2018&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://azure.microsoft.com/en-us/services/storage/blobs/&quot;&gt;https://azure.microsoft.com/en-us/services/storage/blobs/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/azure/storage/storage-introduction&quot;&gt;https://docs.microsoft.com/en-us/azure/storage/storage-introduction&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/rest/api/storageservices/&quot;&gt;https://docs.microsoft.com/en-us/rest/api/storageservices/&lt;/a&gt; - REST API documentation&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Google Cloud Storage</title><link>https://ondataengineering.net/technologies/google-cloud-storage/</link><description> &lt;p&gt;An object store service with strong consistency, multiple storage tiers and deep integration to the Google Cloud ecosystem. Objects are organised into buckets and indexed by string, with the option to list objects by prefix and to summarise results based on a delimiter allowing a filesystem to be approximated. Storage tiers supported include multi-regional (data is distributed across regions in a geo area), regional, nearline and coldline (designed for data accessed less than once per month/year respectively). Supports object lifecycle management allowing for automatic deletion or moving of objects between storage tiers. Supports versioning of objects, access control (via Google Cloud IAM, bucket and object ACLs and time-limited access via signed URLs), encryption of objects and support for SSL connections, auditing of object operations via Google Cloud Audit, gzip uncompression on read, custom domains, multi-part uploads via merging of objects after upload (Composite Objects), acccess and storage logs as downloadable CSV files and batching of request. Quotes a 99.999999999% guarentee that data won't be lost, and availability of 99.9% for regional and 99.95% for multi-regional storage tiers. Provides a web based management console (Google Cloud Platform Console), CLI (gsutil), JSON and XML REST API and SDKs for a wide range of languages.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/google-cloud-platform/&quot;&gt;Google Cloud Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;June 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/storage/&quot;&gt;https://cloud.google.com/storage/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/storage/docs/&quot;&gt;https://cloud.google.com/storage/docs/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/storage/docs/concepts&quot;&gt;https://cloud.google.com/storage/docs/concepts&lt;/a&gt; - key concepts&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/storage/release-notes&quot;&gt;https://cloud.google.com/storage/release-notes&lt;/a&gt; - release notes&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>OpenStack Swift</title><link>https://ondataengineering.net/technologies/openstack-swift/</link><description> &lt;p&gt;An open source object store with eventual consistency, that's available from a number of vendors as both an on site solution and a cloud based service offering. Objects are organised into containers and indexed by string, with the option to list objects by prefix and to summarise results based on a delimiter allowing a filesystem to be approximated. Supports configurable storage policies (each using a different storage ring allowing for differing hardware and replication levels to be used), erasure coding as well as standard replication (with erase coding providing smaller storage overheads at the code of higher CPU and read and write data), and multi-region clusters (based on configuring affinity for local operations). Also supports container and object metadata, object versioning, container to container mirroring via background synchronisation, authorisation via tokens from OpenStack Keystone, access control via container ACLs, support for large objects via segmentation (multi-part uploads combined with a special manifest file), scheduled and bulk object deletion, time limited access URLs,and encryption of data at rest. Provides a REST API and client SDKs. Originally created by Rackspace in 2009, becoming one of the first OpenStack technologies, with contributors now including SwiftStack, RedHat, HP, Intel, IBM among others.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;OpenStack&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;June 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.openstack.org/software/releases/ocata/components/swift&quot;&gt;https://www.openstack.org/software/releases/ocata/components/swift&lt;/a&gt; - project summary&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.openstack.org/developer/swift/&quot;&gt;https://docs.openstack.org/developer/swift/&lt;/a&gt; - documentation homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.openstack.org/developer/swift/overview_architecture.html&quot;&gt;https://docs.openstack.org/developer/swift/overview_architecture.html&lt;/a&gt; - architecture overview&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://developer.openstack.org/api-ref/object-storage/&quot;&gt;https://developer.openstack.org/api-ref/object-storage/&lt;/a&gt; - API reference&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Scality RING</title><link>https://ondataengineering.net/technologies/scality-ring/</link><description> &lt;p&gt;A massively scalable commercial object store available as software for deployment on premises on commodity hardware. Based around a native object store core, but with POSIX filesystem support, and support for a range of APIs including file based (NFS, SMB and Linux FUSE), object based (S3 compatible and native Scality REST APIs), and OpenStack compatible (Swift, Cinder, Glance and Manila). Supports both variable level data replication and erasure coding, object encryption, file and object versioning, multi-site support (via synchronous and asynchronous replication, including support for replicating to Amazon S3), data location control, support for arbitrarily large objects, rolling upgrades and full authentication and access controls (including support for LDAP, Active Directory, AWS IAM and Kerberos). Comes with a CLI and web based GUI, and an add on solution (Scality Cloud Monitor) is available for monitoring and management. Sold by Scality, who were founded in 2010 and who focus on selling and supporting Scality RING, but who have also open sourced their S3 API as Zenko Cloudserver (previously S3 Server).&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;Scality&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;June 2018 - 7.4&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;7.4&lt;/td&gt; &lt;td&gt;2018-06-05&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.scality.com/blog/ring-7-4-new-standard-for-easy-software-defined-storage/&quot;&gt;summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://www.scality.com/products/ring/&quot;&gt;http://www.scality.com/products/ring/&lt;/a&gt; - Scality RING homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.scality.com/products/scality-cloud-monitor/&quot;&gt;http://www.scality.com/products/scality-cloud-monitor/&lt;/a&gt; - Scality Cloud Monitor homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://storage.scality.com/rs/963-KAI-434/images/Scality%20RING7%20Data%20Sheet.pdf&quot;&gt;http://storage.scality.com/rs/963-KAI-434/images/Scality%20RING7%20Data%20Sheet.pdf&lt;/a&gt; - data sheet&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Microsoft Azure Data Lake Store</title><link>https://ondataengineering.net/technologies/microsoft-azure-data-lake-store/</link><description> &lt;p&gt;Massively scalable HDFS compatible filesystem as a service, based on Microsoft's Cosmos technology. Claims support for up to trillions of files and single files larger than one petabyte, with no limits on account sizes, file sizes or the amount of data that can be stored, and optimisation of parallel analytics workloads, with high throughput and IOPS performance. Supports user authentication via Azure Active Directory (AAD) (combined with OAuth and OpenID), role based access control for account management, POSIX ACLs for controlling access to data, encryption for both stored data and data in transit over the network,and built in auditing (of both data access and account management activities). Supports a standard WebHDFS API, an HDFS compatible interface (adl://) that's bundled with Apache Hadoop, a web UI (Data Explorer) and SDKs for a range of languages. Does not natively support geo-replication with filesystems limited to a region, but data can be manually replicated via a number of routes if required. First announced in April 2015, with a general availability release in November 2016.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Azure Data Lake Store&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;Microsoft&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;June 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://azure.microsoft.com/en-us/services/data-lake-store/&quot;&gt;https://azure.microsoft.com/en-us/services/data-lake-store/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-overview&quot;&gt;https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-overview&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.msdn.microsoft.com/azuredatalake/2016/11/17/azure-data-lake-store-is-now-generally-available/&quot;&gt;https://blogs.msdn.microsoft.com/azuredatalake/2016/11/17/azure-data-lake-store-is-now-generally-available/&lt;/a&gt; - announcement post&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-comparison-with-blob-storage&quot;&gt;https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-comparison-with-blob-storage&lt;/a&gt; - comparison of Blob Storage and Data Lake Store&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Druid (Incubating)</title><link>https://ondataengineering.net/technologies/apache-druid/</link><description> &lt;p&gt;An open source distributed database built to support sub-second OLAP / star schema style queries on both real-time and historical data, based on columnar storage and inverted indexes. All data must have a timestamp, one or more dimension fields, and then one or more measures, with data being aggregated by timestamp and dimension fields on ingest. Comes with a batch ingestor (with support for reading from HDFS, S3 and local files), a streaming ingestor (with support for local files and an HTTP endpoint), and a streaming data endpoint (Tranquility, with support for Kafka, Storm and Spark Streaming and an API for use with other systems), with real-time ingests not guaranteed under failure, but with supports hybrid architectures whereby real-time data ingests are replaced with batch refreshes when available. Architecture based on a number of different node types - historical nodes (which serve queries against a local cache of data that's been persisted in S3 or HDFS), real-time nodes (which support ingest and querying of streaming data, with data persisted and handed over to an historical node once aged), and broker nodes (which distribute queries to appropriate real-time and historical nodes and then collate the results). All data is segmented by date and time, with a metadata database (e.g. MySQL, PostgreSQL, or Derby) tracking segments and which nodes are serving them, and Apache ZooKeeper used for co-ordination and communication between nodes. Supports low latency lock free ingestion, a JSON REST endpoint for queries (with support for a range of query types including timeseries, TopN, groupBy and select), a range of SDKs, approximate and exact computations, multiple storage tiers (including data lifecycle rules on tiering and dropping data), metrics (for queries, ingestion, and coordination), rolling upgrades, HTTP authentication (including Kerberos, but no further security controls), and a number of experimental features including small dimension lookups (note that general joins are not supported), multi-value dimension fields and a SQL interface based on Apache Calcite. Started in 2011 by Metamarkets, open sourced under the GPL licence in October 2012, moving to an Apache licence in February 2015 and donated to the Apache Incubator in February 2018. Has a wide range of companies listed on the Druid website as users, and natively supported by Apache Superset and Grafana (via a plugin). Commercial support available from Imply (which distribute their own product based on Druid including a SQL interface and a data exploration tool called Pivot), and currently in tech preview as part of the Hortonworks Data Platform, where it's being integrated with Apache Hive.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Apache Druid, Druid&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;, Imply&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;June 2018 - v0.12&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;, &lt;a href=&quot;/technologies/streaming-analytics-manager/&quot;&gt;Streaming Analytics Manager&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.11&lt;/td&gt; &lt;td&gt;2017-12-04&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://druid.io/blog/2017/12/04/druid-0-11-0.html&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.12&lt;/td&gt; &lt;td&gt;2018-03-08&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://druid.io/blog/2018/03/08/druid-0-12-0.html&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://druid.incubator.apache.org/&quot;&gt;http://druid.incubator.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://druid.io/blog/2011/04/30/introducing-druid.html&quot;&gt;http://druid.io/blog/2011/04/30/introducing-druid.html&lt;/a&gt; - introductory blog post&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://imply.io/&quot;&gt;https://imply.io/&lt;/a&gt; - Imply homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/apache/druid/&quot;&gt;https://hortonworks.com/apache/druid/&lt;/a&gt; - Hortonworks information&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.4/bk_data-access/content/ch_using-druid.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.4/bk_data-access/content/ch_using-druid.html&lt;/a&gt; - Hortonworks 2.6.4 documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/druid-io/tranquility&quot;&gt;https://github.com/druid-io/tranquility&lt;/a&gt; - Tranquility repo&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://druid.io/blog/&quot;&gt;http://druid.io/blog/&lt;/a&gt; - Druid blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Superset (incubating)</title><link>https://ondataengineering.net/technologies/apache-superset/</link><description> &lt;p&gt;Web based tool for interactive exploration for OLAP style data, supporting interactive drag and drop querying, composable dashboards and a SQL workspace (SQL Lab). Originally built to query Druid, but now supports a wide range of SQL (and NoSQL) databases, with a lightweight semantic layer allowing control of how data sources are displayed in the UI and which fields can be filtered and aggregated. Users can create Slices (a visualisation of the results of an OLAP style query, with support for a range of visualisations including charts, heat maps, maps, pivot tables, and word clouds amongst others, the ability to configure the query using UI controls, and the ability to configure and customise the visualisation), with multiple slides then composable into a Dashboard (that also support interative filters that connect to multiple slices). Also supports a full SQL IDE (SQL Lab) that supports multiple tabs, a full query history, the ability to apply any data visualisation to results and to browse database metadata, and support for long-running queries using a backend query handler and results store. Other features include query results caching, a plug-in and extensibility framework, the ability to brand and skin the web application, and a robust security model for controlling access to slices, dashboards and data, with support for a range of authentication methods including OpenID, LDAP and OAuth. Originally developed by AirBnB in 2015 as Panoramix, before being renamed to Caravel and then to Superset. Donated to the Apache Foundation in June 2017 and still incubating, with development now led by AirBnB and Hortonworks.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Superset, Apache Superset&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Open Source - Active&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;June 2017 - v0.10&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/streaming-analytics-manager/&quot;&gt;Streaming Analytics Manager&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://superset.incubator.apache.org/&quot;&gt;https://superset.incubator.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://medium.com/airbnb-engineering/caravel-airbnb-s-data-exploration-platform-15a72aa610e5&quot;&gt;https://medium.com/airbnb-engineering/caravel-airbnb-s-data-exploration-platform-15a72aa610e5&lt;/a&gt; - introductory blog post&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Schema Registry</title><link>https://ondataengineering.net/technologies/schema-registry/</link><description> &lt;p&gt;A centralised registry for data schemas with support for NiFi, Kafka and Streaming Analytics Manager, allowing schemas to be defined and versioned centrally and removing the need to attach schema to every piece of data. Supports versioning of schemas (with a definable compatibility policy that validates that schemas are forward compatible, backward compatible, both or none), the ability to store and serve JAR files for serialising and de-serialising data, a REST API, Java SDK and web based user interface for managing schemas. NiFi integration supports record level operations (via RecordReaders and RecordSetWriters); Kafka integration supports Kafka Producers and Consumers. Requires a MySQL backend for schema storage, and either local of HDFS storage for serialiser/de-serialiser JAR files. Stated plan is to support a wider range of schema types (currently only Avro schemas are support), a range of other registry requirements (e.g. templates, machine learning models or business rules), and for integration with Apache Atlas and Ranger. Started by Hortonworks in October 2016, with an initial release as part of HDF 3.0 in June 2017.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/hortonworks/&quot;&gt;Hortonworks&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;June 2017 - v0.3&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-flow/&quot;&gt;Hortonworks DataFlow&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/hortonworks/registry&quot;&gt;https://github.com/hortonworks/registry&lt;/a&gt; - GitHub repo&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://registry-project.readthedocs.io/en/latest/&quot;&gt;http://registry-project.readthedocs.io/en/latest/&lt;/a&gt; - Documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/open-source/schema-registry/&quot;&gt;https://hortonworks.com/open-source/schema-registry/&lt;/a&gt; - Hortonworks homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDF3/HDF-3.1.0/bk_schema-registry-user-guide/content/index.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDF3/HDF-3.1.0/bk_schema-registry-user-guide/content/index.html&lt;/a&gt; - Hortonworks documentation (HDF 3.1)&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/part-2-hdf-blog-series-shared-schema-registry-important/&quot;&gt;https://hortonworks.com/blog/part-2-hdf-blog-series-shared-schema-registry-important/&lt;/a&gt; - intro blog post&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/hortonworks/registry/releases&quot;&gt;https://github.com/hortonworks/registry/releases&lt;/a&gt; - GitHub releases page&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Streaming Analytics Manager</title><link>https://ondataengineering.net/technologies/streaming-analytics-manager/</link><description> &lt;p&gt;A suite of open source web based tools to develop and operate stream analytics solutions and analyse the results, with pluggable support for the underlying streaming engine. Consists of Stream Builder (a web based GUI for building streaming data flows), Stream Operations (a web based management and operations tools for streaming applications) and Stream Insight (a bundling of Druid and Apache Superset to serve and analyse the results of streaming applications). Stream Builder supports creation of streaming flows using a drag and drop GUI, with support for a range of sources (including Kafka and HDFS), processors (including joins, window/aggregate functions, normalisation/projection and PMML model execution), and sinks (including email, HDFS, HBase, Hive, JDBC, Druid, Cassandra, Kafka, OpenTSDB and Solr), as well as support for custom sources, processors, sinks and functions (including window functions), and the ability to automatically deploy and execute applications. Stream Operations supports the management of multiple execution environments, the deployment, execution and management of applications within an environment, the capture of stream metrics via pluggable metrics storage (with support for Ambari and OpenTSDB), and web based dashboards to monitor applications and visualise key metrics. Started by Hortonworks in May 2015, with an initial release as part of HDF 3.0 in June 2017.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;SAM, Streamline&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/hortonworks/&quot;&gt;Hortonworks&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;June 2017 - v0.5&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Packages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-druid/&quot;&gt;Apache Druid (Incubating)&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-superset/&quot;&gt;Apache Superset (incubating)&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-flow/&quot;&gt;Hortonworks DataFlow&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/hortonworks/streamline&quot;&gt;https://github.com/hortonworks/streamline&lt;/a&gt; - GitHub repo&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/open-source/streaming-analytics-manager/&quot;&gt;https://hortonworks.com/open-source/streaming-analytics-manager/&lt;/a&gt; - Hortonworks homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDF3/HDF-3.1.0/bk_streaming-analytics-manager-user-guide/content/index.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDF3/HDF-3.1.0/bk_streaming-analytics-manager-user-guide/content/index.html&lt;/a&gt; - Hortonworks documentation (HDF 3.1)&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/hortonworks-dataflow-3-0/&quot;&gt;https://hortonworks.com/blog/hortonworks-dataflow-3-0/&lt;/a&gt; - intro blog post&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Cloudera Altus</title><link>https://ondataengineering.net/technologies/cloudera-altus/</link><description> &lt;p&gt;Platform for accessing individual CDH capabilities as services. Currently supports the deployment and management of CDH clusters on cloud infrastructure (Director, previously Cloudera Director), the execution of Spark, MapReduce or Hive over Spark or MapReduce jobs (Altus Data Engineering), the dynamic provisioning of Impala clusters (Altus Data Warehouse), with a stated future plan for R- and Python-based machine learning workloads (Altus Data Science) and an HBase based operational database service. Runs on Amazon Web Services or Microsoft Azure over external data in Amazon S3 or Azure Data Lake Storage, with a stated plan to expand support to other cloud service providers (specifically the Google Cloud Platform) in the future. Includes Altus SDX, allowing metadata (e.g. Hive table definitions) to be automatically persisted across transient workloads, referenced via a namespace. Supports a web based UI, a (Python) CLI and a Java SDK, with full user authentication and role based access management, and integration with AWS and Azure security. Launched in May 2017, with a per node / per hour pricing model.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Altus&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;September 2018&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Sub-projects&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; Cloudera Altus&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/cloudera-altus/director/&quot;&gt;Cloudera Altus Director&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Solution for deploying and managing Cloudera CDH Hadoop clusters on cloud infrastructure based on automatically provisioned infrastructure with Hadoop provisioned on top via Cloudera Manager. Includes out of the box support for Amazon Web Services, Microsoft Azure and Google Cloud Platform, with support for vSphere available from VMWare, with a Service Provider Interface (SPI) for adding support for new providers. Server component must be manually deployed via an RPM. Supports the ability to scale clusters up and down, clone clusters, run post deployment scripts, and create Kerberized and highly available clusters. Manageable through a web UI, a REST API (with Python and Java APIs) and a CLI. Released as Cloudera Director at 1.0 in October 2014 as part of Cloudera Enterprise 5.2, being renamed to Cloudera Altus Director in September 2018 as part of CDH 6. Free to download and use, with commercial support available as part of a Cloudera Enterprise subscription.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Cloudera Altus&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/cloudera-altus/data-engineering/&quot;&gt;Cloudera Altus Data Engineering&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Managed service for the execution of Spark, MapReduce or Hive (over MapReduce or Spark) jobs using managed CDH clusters on AWS and Azure cloud infrastructure over data in Amazon S3 or Azure Data Lake Storage (ADLS). Jobs run on clusters within a defined AWS or Azure environment, which can be transient (created and terminated on demand) or persistent, with each cluster supporting one service type (Hive, Spark, MapReduce) with a fixed node count. Jobs can then be queued individually or in batch for execution against an existing cluster or against a dynamically created cluster, with jobs specified either by uploading a JAR to S3 (for Spark or MapReduce) or via a Hive script (either directly uploaded or uploaded to S3), and the ability to either halt or continue the queue on job failure. Supports access to clusters via SSH, read only access to Cloudera Manager, a SOCKS proxy to cluster web UIs (including the CM admin console, YARN history server and Spark history server), and access to server and workload logs (including the ability to write these to S3 for access after clusters have been terminated). All nodes managed by Altus are tagged with the cluster name and node role (master, worker or Cloudera Manager) and bootstrap scripts can be specified for execution on nodes after cluster startup.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Cloudera Altus&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/cloudera-altus/data-warehouse/&quot;&gt;Cloudera Altus Data Warehouse&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Impala as a managed service, supporting the dynamic provisionng of Impala clusters on AWS and Azure cloud infrastructure over data in Amazon S3 or Azure Data Lake Storage (ADLS). Clusters consist of a coordinator node and multiple worker nodes, with read-only access to a Cloudera Manager instance, with the node count fixed on creation. Supports JDBC and ODBC access to data, along with access to clusters via SSH, read only access to Cloudera Manager and a SOCKS proxy for access to the Impala web UIs. Previously known as Cloudera Altus Analytical DB.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Uses&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;Cloudera CDH&lt;/a&gt;, Amazon Web Services, Microsoft Azure&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;history&quot;&gt;History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;2017-05-24&lt;/td&gt; &lt;td&gt;Initial GA release (Data Engineering) - &lt;a href=&quot;http://vision.cloudera.com/simplifying-big-data-in-the-cloud/&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;http://blog.cloudera.com/blog/2017/05/data-engineering-with-cloudera-altus/&quot;&gt;blog&lt;/a&gt;; &lt;a href=&quot;https://community.cloudera.com/t5/Community-News-Release/Announce-Cloudera-Altus-is-now-available/m-p/55007&quot;&gt;details&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2017-06-22&lt;/td&gt; &lt;td&gt;Addition of workload analytics - &lt;a href=&quot;http://vision.cloudera.com/announcing-workload-analytics-for-cloudera-altus/&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2017-09-27&lt;/td&gt; &lt;td&gt;Support for Azure added - &lt;a href=&quot;http://vision.cloudera.com/introducing-cloudera-altus-on-microsoft-azure/&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;http://blog.cloudera.com/blog/2017/09/cloudera-altus-on-microsoft-azure/&quot;&gt;blog&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2017-11-28&lt;/td&gt; &lt;td&gt;Beta support for Analytical DB added - &lt;a href=&quot;http://vision.cloudera.com/introducing-cloudera-altus-analytic-db-beta-for-cloud-based-data-warehousing/&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2018-03-06&lt;/td&gt; &lt;td&gt;Support for SDX added - &lt;a href=&quot;http://vision.cloudera.com/altus-sdx-shared-services-for-cloud-based-analytics/&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2018-05-22&lt;/td&gt; &lt;td&gt;Data Engineering GA and Analytics DB beta on Azure - &lt;a href=&quot;http://vision.cloudera.com/cloudera-altus-is-now-available-on-azure/&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2018-07-24&lt;/td&gt; &lt;td&gt;Cloudera Altus SDX Beta - &lt;a href=&quot;http://blog.cloudera.com/blog/2018/07/introducing-cloudera-altus-sdx-beta/&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2018-08-02&lt;/td&gt; &lt;td&gt;Analytical DB renamed to Data Warehouse - &lt;a href=&quot;http://vision.cloudera.com/a-new-era-in-data-warehousing/&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2018-09-12&lt;/td&gt; &lt;td&gt;Cloudera Director added as Cloudera Altus Director&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/products/altus.html&quot;&gt;https://www.cloudera.com/products/altus.html&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/documentation/altus.html&quot;&gt;https://www.cloudera.com/documentation/altus.html&lt;/a&gt; - documentation (currently only covers Altus Data Engineering)&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Pravega</title><link>https://ondataengineering.net/technologies/pravega/</link><description> &lt;p&gt;Technology for the buffering and long term storage of streaming data, designed for low latency and high throughput, with support for exactly once semantics, durable writes, strict ordering, dynamic scaling, transactions and long term storage backed by HDFS. Data is stored in named streams (continuous streams of bytes with serialisation and de-serialisation done in clients), with streams partitioned by a Routing Key into stream segments. Data is stored in two tiers, the first using Apache BookKeeper for recent data, the second using HDFS for long term storage, with automatic ageing of data and seamless reads across tiers. Operates on a publish/subscribe model, with subscribers able to select any point in history to read from. Supports automatic scaling of streams (dynamically increasing or decreasing the number of stream segments based on the operations per second on the stream), exactly once semantics (ensuring records are read once and once only even after failure), durable writes (data is persisted before write operations are acknowledged), transactions (multiple events can be committed in a single operation), ordered streams (events will always be read in the same order they're written), ReaderGroups (allows multiple subscribers to co-ordinate reads from a single stream) and a state synchroniser API (allowing multiple clients to synchronise arbitrary state through Pravega). Supports a Java SDK and out of the box integration with Flink, along with support for deployment using docker swarm, dc/os and AWS (all currently in development). Open sourced under an Apache 2.0 licence, started in July 2016 within Dell EMC, and does not yet have a first formal release, but is under active development by a wider range of contributors. Stated plans for future functionality include automatic deletion of data based on a retention period, support for other tier 2 storage technologies, access control, runtime metrics and Spark support.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Open Source - Active&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;June 2018 - v0.3&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.1&lt;/td&gt; &lt;td&gt;2017-09-14&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://github.com/pravega/pravega/releases/tag/v0.1.0&quot;&gt;GitHub release page&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.2&lt;/td&gt; &lt;td&gt;2017-12-21&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://github.com/pravega/pravega/releases/tag/v0.2.0&quot;&gt;GitHub release page&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.3&lt;/td&gt; &lt;td&gt;2018-06-22&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://github.com/pravega/pravega/releases/tag/v0.3 .0&quot;&gt;GitHub release page&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://pravega.io/&quot;&gt;http://pravega.io/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://pravega.io/docs/&quot;&gt;http://pravega.io/docs/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://pravega.io/docs/pravega-concepts/&quot;&gt;http://pravega.io/docs/pravega-concepts/&lt;/a&gt; - key concepts&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/pravega/pravega&quot;&gt;https://github.com/pravega/pravega&lt;/a&gt; - GitHub repo&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://blog.pravega.io/&quot;&gt;http://blog.pravega.io/&lt;/a&gt; - blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/pravega/pravega/releases&quot;&gt;https://github.com/pravega/pravega/releases&lt;/a&gt; - release history&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Confluent Open Source</title><link>https://ondataengineering.net/technologies/confluent-open-source/</link><description> &lt;p&gt;A package of open source projects built around Apache Kafka with the addition of the Confluent Schema Registry, Kafka REST Proxy, a number of connectors for Kafka Connect and a number of Kafka clients (language SDKs). The Schema Registry allows Kafka message schemas to be defined and versioned centrally, with schemas stored in a Kafka topic, a REST interface for managing schemas, support for schema evolution (with support for backwards, forwards and full compatibility between versions), plugins for Kafka clients to serialise / deserialise messages using the schemas, and support for running as a distributed service. The REST Proxy provides a REST interface onto a Kafka cluster, with support for viewing cluster metadata (covering brokers, topics, partitions and configuration) and both submitting and consuming messages, with support for JSON, JSON-encoded Avro and base64 messages, and integration to the Schema Registry for Avro messages. Bundled connectors for Kafka Connect include HDFS, JDBC, Elasticsearch and S3. Bundled client libraries (all open source) include those for C/C++, Go, .NET and Python. Also includes a Version Collector that reports version information to Confluent. Used to include Camus, a tool for unloading Kafka topics to HDFS, but this has now been deprecated in favour of Kafka Connect. Development of the open source projects is led by Confluent, who then bundle and distribute them for free as the Confluent Open Source version of their Confluent Platform, with the Confluent Enterprise version adding a number of closed source features and commercial support for all open and closed source products. Available as a zip, tar, deb or rpm package from Confluent, with all source code hosted on GitHub. First GA release was version 1.0 of the Confluent Platform in February 2015.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;Confluent&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;August 2018 - v5.0&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Packages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/confluent-enterprise/&quot;&gt;Confluent Enterprise&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;3.3&lt;/td&gt; &lt;td&gt;2017-08-01&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.confluent.io/blog/we-will-say-exactly-confluent-platform-3-3-available-now/&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;2017-11-28&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.confluent.io/blog/introducing-confluent-platform-4-0/&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;4.1&lt;/td&gt; &lt;td&gt;2018-04-17&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.confluent.io/blog/confluent-platform-4-1-with-production-ready-ksql-now-available/&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.0&lt;/td&gt; &lt;td&gt;2018-07-31&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.confluent.io/blog/introducing-confluent-platform-5-0/&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;https://www.zdnet.com/article/confluent-release-adds-enterprise-developer-iot-savvy-to-apache-kafka/&quot;&gt;ZDNet view&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/product/confluent-open-source/&quot;&gt;https://www.confluent.io/product/confluent-open-source/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://docs.confluent.io/current/&quot;&gt;http://docs.confluent.io/current/&lt;/a&gt; - Confluent Platform documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/confluentinc&quot;&gt;https://github.com/confluentinc&lt;/a&gt; - Confluent GitHub home&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/blog/&quot;&gt;https://www.confluent.io/blog/&lt;/a&gt; - Confluent blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Confluent Enterprise</title><link>https://ondataengineering.net/technologies/confluent-enterprise/</link><description> &lt;p&gt;A package of software built around Apache Kafka and the Confluent Open Source product, with the addition of a number of commercial closed source products including a JMS client, Control Centre (for managing Kafka clusters), Multi DC Replication (active-active replication between Kafka clusters) and Auto Data Balancing. The JMS client is an implementation of the standard JMS provider interface over a Kafka topic. Control Centre is a web based UI that supports system health monitoring (broker and topic metrics and statuses based on information from the Confluent Metrics Reporter, a plugin for Kafka clusters that reports metrics to a Kafka topic), real time stream monitoring (statistics on the production and consumption of messages including the level of consumption and latency based on statistics from Confluent Monitoring Interceptors, a plugin for Kafka producers and consumers that reports statistics to a Kafka topic), the GUI based creation of Kafka connect pipelines, viewing of cluster and topic information, and e-mail alerting based on custom triggers on on topic, consumer group or broker metrics. Multi DC Replication is an optional licenced connector for Kafka connect that enables replication between two remote Kafka clusters, including active-active synchronisation. Auto Data Balancing is a tool for re-balancing topic partitions across cluster nodes, recommending moves based on information form the Confluent Metrics Reporter and rack awareness to ensure load is distributed evenly across the cluster, and easily allowing for the additional or removal of nodes. Also includes the Confluent Support Metrics features which collects broker and cluster metadata and metrics and forwards these onto Confluent for proactive support. Confluent Enterprise is the commercial version of their Confluent Platform, with an open source version also available as Confluent Open Source. Includes full commercial support for all open and closed source products. First GA release was version 1.0 of the Confluent Platform in February 2015.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;Confluent&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;August 2018 - v5.0&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Packages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt;, &lt;a href=&quot;/technologies/confluent-open-source/&quot;&gt;Confluent Open Source&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;3.3&lt;/td&gt; &lt;td&gt;2017-08-01&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.confluent.io/blog/we-will-say-exactly-confluent-platform-3-3-available-now/&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;2017-11-28&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.confluent.io/blog/introducing-confluent-platform-4-0/&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;4.1&lt;/td&gt; &lt;td&gt;2018-04-17&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.confluent.io/blog/confluent-platform-4-1-with-production-ready-ksql-now-available/&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.0&lt;/td&gt; &lt;td&gt;2018-07-31&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.confluent.io/blog/introducing-confluent-platform-5-0/&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;https://www.zdnet.com/article/confluent-release-adds-enterprise-developer-iot-savvy-to-apache-kafka/&quot;&gt;ZDNet view&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/product/confluent-enterprise/&quot;&gt;https://www.confluent.io/product/confluent-enterprise/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://docs.confluent.io/current/&quot;&gt;http://docs.confluent.io/current/&lt;/a&gt; - Confluent Platform documentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/blog/&quot;&gt;https://www.confluent.io/blog/&lt;/a&gt; - Confluent blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Elasticsearch</title><link>https://ondataengineering.net/technologies/elasticsearch/</link><description> &lt;p&gt;A distributed search server built on Apache Lucene that supports a number of advanced analytics over search results. Data is stored in indexes, with each index able to support multiple schemas (types), with the data itself sharded to support distributed parallel queries, with multiple replicas of each shard providing resilience and redundancy. Supports both pre-defined and schemaless types, all standard Lucene functionality (including faceting, grouping, clustering, hit highlighting, geo support, near real time indexing), the ability to update and delete documents (by id or query), upsert operations, batch operations, re-indexing (from one index into a second index), generated or calculated fields, document versioning and optimistic concurrency control, nested searches based on sub-documents or explicit parent-child document links, templated searches, a range of aggregations (include support for metrics, bucketing results, matrix calculations and custom aggregations using pipelines), custom analysers for indexing data, custom transformation pipelines prior to indexing (via an ingest node), the ability to query across clusters (cross cluster search), a plugin framework, registered queries that are executed against newly indexed data (percolation) and the ability to snapshot and restore indexes using HDFS, S3, Azure and Google Cloud. Also now includes a number of features that were previously bundled separately in the Elastic X-Pack, including Security (encryption of data and links, authentication via LDAP and Active Directory, authorisation at the cluster, index, document and field level, and full audit logging), Monitoring (export of cluster, nod and index metrics), Alerting (via Watcher, allowing registration of scheduled queries over monitoring data that can perform a number of extensible actions), Graph (APIs for working with relationships, with connections between indexed terms generated on the fly using Elasticsearch aggregations and relevance scoring), SQL access (via a REST API, CLI or JDBC interface), Machine Learning (support for automated anomaly detection jobs over time-series data run on the ElasticSearch cluster) and Rollup (aggregation of historical data), the majority of which require a commercial licence from Elastic in order to be enabled. Comes with a REST API, with clients available for a range of languages including Java, C#, Python, JavaScript, PHP, Perl and Ruby. First released in February 2010, with a 1.0 release in February 2014. Open source under the Apache licence, with the exception of the X-Pack components which are under an Elastic licence following the open sourcing of X-Pack in version 6.3. Development is led by Elastic, who were formed in 2012 by the creator of Elasticsearch and a lead Lucene contributor, and who provide commercial support, licences to enable the commercial X-Pack features, and an on-site or public cloud service offering (Elastic Cloud).&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;Elastic&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;August 2018 - v6.4&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Add ons&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/elasticsearch-hadoop/&quot;&gt;Elasticsearch-Hadoop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Manageable via&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/elastic-cloud/&quot;&gt;Elastic Cloud&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/mapr-monitoring/&quot;&gt;MapR Monitoring&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;further-information&quot;&gt;Further Information&lt;/h2&gt; &lt;p&gt;A list of some add-ons that are an alternative to the X-Pack commercial features is available from Sematext at &lt;a href=&quot;https://sematext.com/blog/x-pack-alternatives/&quot;&gt;https://sematext.com/blog/x-pack-alternatives/&lt;/a&gt;&lt;/p&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.5&lt;/td&gt; &lt;td&gt;2017-07-06&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.elastic.co/blog/elasticsearch-5-5-0-released&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/reference/5.5/release-notes-5.5.0.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Windows installer&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.6&lt;/td&gt; &lt;td&gt;2017-09-11&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.elastic.co/blog/elasticsearch-5-6-0-released&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/reference/5.6/release-notes-5.6.0.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.elastic.co/blog/the-elasticsearch-java-high-level-rest-client-is-out&quot;&gt;High Level REST Client&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6.0&lt;/td&gt; &lt;td&gt;2017-11-14&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.elastic.co/blog/elasticsearch-6-0-0-released&quot;&gt;announcement&lt;/a&gt; ; &lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/reference/6.0/release-notes-6.0.0.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.elastic.co/blog/removal-of-mapping-types-elasticsearch&quot;&gt;Removal of mapping types&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6.1&lt;/td&gt; &lt;td&gt;2017-12-13&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.elastic.co/blog/elasticsearch-6-1-0-released&quot;&gt;announcement&lt;/a&gt; ; &lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/reference/6.1/release-notes-6.1.0.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6.2&lt;/td&gt; &lt;td&gt;2018-02-06&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.elastic.co/blog/elasticsearch-6-2-0-released&quot;&gt;announcement&lt;/a&gt; ; &lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/reference/6.2/release-notes-6.2.0.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6.3&lt;/td&gt; &lt;td&gt;2018-06-13&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.elastic.co/blog/elasticsearch-6-3-0-released&quot;&gt;announcement&lt;/a&gt; ; &lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/reference/6.3/release-notes-6.3.0.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Experimental SQL &amp;amp; pre-generated rollups support; addition of &lt;a href=&quot;/technologies/elastic-x-pack/&quot;&gt;X-Pack&lt;/a&gt; components&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6.4&lt;/td&gt; &lt;td&gt;2018-08-23&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.elastic.co/blog/elasticsearch-6-4-0-released&quot;&gt;announcement&lt;/a&gt; ; &lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/reference/6.4/release-notes-6.4.0.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.elastic.co/products/elasticsearch&quot;&gt;https://www.elastic.co/products/elasticsearch&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.elastic.co/guide/index.html&quot;&gt;https://www.elastic.co/guide/index.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.elastic.co/products/x-pack/open&quot;&gt;https://www.elastic.co/products/x-pack/open&lt;/a&gt; - information on the open sourcing of X-Pack&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.elastic.co/subscriptions&quot;&gt;https://www.elastic.co/subscriptions&lt;/a&gt; - details of commercial x-patch features&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/elastic/elasticsearch&quot;&gt;https://github.com/elastic/elasticsearch&lt;/a&gt; - GitHub repo&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.elastic.co/blog&quot;&gt;https://www.elastic.co/blog&lt;/a&gt; - Elastic blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Elastic X-Pack</title><link>https://ondataengineering.net/technologies/elastic-x-pack/</link><description> &lt;p&gt;A set of commercial extensions to the Elastic open source products (Elasticsearch, Kibana and Logstash) that were discontinued in June 2018 with the release of version 6.3 of the Elastic stack, with the individual components now open sourced under an Elastic licence and bundled with the relevent Elastic open source products, although the majority still require a commercial licence from Elastic to be enabled. Included Security (formally Shield), Alerting (formally Watcher), Monitoring (formally Marvel), Reporting, Graph, Machine Learning and Application Performance Monitoring (APM).&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;X-Pack&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;Elastic&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Discontinued&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;June 2018 - v6.2&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.5&lt;/td&gt; &lt;td&gt;2017-07-06&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.elastic.co/guide/en/x-pack/5.5/xpack-change-list.html#xpack-5.5.0&quot;&gt;change notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Machine Learning GA&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.6&lt;/td&gt; &lt;td&gt;2017-09-11&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.elastic.co/guide/en/x-pack/5.6/xpack-change-list.html#release-notes-5.6.0&quot;&gt;change notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6.0&lt;/td&gt; &lt;td&gt;2017-11-14&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.elastic.co/guide/en/x-pack/6.0/xpack-change-list.html&quot;&gt;change notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6.1&lt;/td&gt; &lt;td&gt;2017-12-13&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.elastic.co/guide/en/x-pack/6.1/xpack-change-list.html&quot;&gt;change notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.elastic.co/blog/elastic-apm-beta-released&quot;&gt;APM&lt;/a&gt; Beta; &lt;a href=&quot;https://www.elastic.co/blog/machine-learning-6-1-0-released&quot;&gt;New ML features&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6.2&lt;/td&gt; &lt;td&gt;2018-02-06&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.elastic.co/guide/en/x-pack/6.2/xpack-change-list.html&quot;&gt;change notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.elastic.co/blog/elastic-apm-ga-released&quot;&gt;APM GA&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Discontinued&lt;/td&gt; &lt;td&gt;2018-06-13&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.elastic.co/blog/doubling-down-on-open&quot;&gt;blog post&lt;/a&gt;; &lt;a href=&quot;https://www.elastic.co/products/x-pack/open&quot;&gt;info page&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.elastic.co/products/x-pack&quot;&gt;https://www.elastic.co/products/x-pack&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.elastic.co/guide/en/x-pack/current/index.html&quot;&gt;https://www.elastic.co/guide/en/x-pack/current/index.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.elastic.co/subscriptions&quot;&gt;https://www.elastic.co/subscriptions&lt;/a&gt; - subscription offerings&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Elastic Cloud</title><link>https://ondataengineering.net/technologies/elastic-cloud/</link><description> &lt;p&gt;Solution for provisioning, managing and monitoring Elasticsearch clusters (including Kibana and X-Pack) either on AWS or Google Cloud Platform as a managed service under a subscription model (Elastic Cloud) or on your own physical or cloud infrastructure (Elastic Cloud Enterprise). Includes support for scripting and plugins, high availability across multiple zones, and supports automated security configuration, upgrades, scaling and backups through a management web UI (Elastic Cloud Console / Cloud UI). Elastic Cloud Enterprise supports the same capability on your own infrastructure, and includes an API in addition to the web UI for configuring and managing clusters, with Elasticsearch and Kibana provisioned using Docker containers. Elastic Cloud is available under a range of subscription licence tiers with differing levels of support and some feature differences; Elastic Cloud Enterprise is freely available but requires you to provide your own Elasticsearch licences. Elastic Cloud was launched in July 2015; Elastic Cloud Enterprise was first released as alpha in December 2016, with a 1.0 GA release in May 2017. Elastic Cloud is the only Elasticsearch service offering that includes the Elastic X-Pack features.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;Elastic&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;November 2017 - v1.1&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Manages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/elasticsearch/&quot;&gt;Elasticsearch&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;history&quot;&gt;History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.1&lt;/td&gt; &lt;td&gt;2017-11-14&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.elastic.co/blog/elastic-cloud-enterprise-1-1-0-released&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.elastic.co/cloud&quot;&gt;https://www.elastic.co/cloud&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.elastic.co/guide/en/cloud/current/index.html&quot;&gt;https://www.elastic.co/guide/en/cloud/current/index.html&lt;/a&gt; - Elastic Cloud documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.elastic.co/guide/en/cloud-enterprise/current/index.html&quot;&gt;https://www.elastic.co/guide/en/cloud-enterprise/current/index.html&lt;/a&gt; - Elastic Cloud Enterprise documentation&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Elasticsearch-Hadoop</title><link>https://ondataengineering.net/technologies/elasticsearch-hadoop/</link><description> &lt;p&gt;A suite of open source components for querying and writing documents to Elasticsearch from a range of Hadoop technologies, including MapReduce, Hive, Pig, Spark, Cascading and Storm. Specific functionality includes InputFormat and OutputFormat libraries for MapReduce, a Hive storage handler allowing external tables to be defined over Elasticsearch indexes, read and write functions for Pig, Java and Scala RDD based libraries for Spark, Spark SQL support, Spark Streaming support, an Elasticsearch Tap for Cascading and a dedicated Spout and Bolt for Storm. Used to include functionality for writing snapshots of Elasticsearch indexes to HDFS which is now part of the Snapshot and Restore functionality in Elasticsearch. Certified with CDH, MapR and HDP.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;ES-Hadoop&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;Elastic&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;August 2018 - v6.4&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Add on to&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/elasticsearch/&quot;&gt;Elasticsearch&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.5&lt;/td&gt; &lt;td&gt;2017-07-06&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.elastic.co/blog/es-hadoop-5-5-0-released&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/hadoop/5.5/eshadoop-5.5.0.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Hadoop 1.x and Elasticsearch of YARN Beta deprecated&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;5.6&lt;/td&gt; &lt;td&gt;2017-07-06&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/hadoop/5.6/eshadoop-5.6.0.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6.0&lt;/td&gt; &lt;td&gt;2017-11-14&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.elastic.co/blog/es-hadoop-6-0-0-released&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/hadoop/6.0/eshadoop-6.0.0.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Spark streaming support and removal of Elasticsearch on YARN beta&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6.1&lt;/td&gt; &lt;td&gt;2017-12-13&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/hadoop/6.1/eshadoop-6.1.0.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt;One bug fix!&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6.2&lt;/td&gt; &lt;td&gt;2018-02-06&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.elastic.co/blog/es-hadoop-6-2-0-released&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/hadoop/6.2/eshadoop-6.2.0.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Custom error handlers&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6.3&lt;/td&gt; &lt;td&gt;2018-06-13&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.elastic.co/blog/es-hadoop-6-3-0-released&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/hadoop/6.3/eshadoop-6.3.0.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Spark 2.3 support&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6.4&lt;/td&gt; &lt;td&gt;2018-08-23&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.elastic.co/blog/es-hadoop-6-4-0-released&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/hadoop/6.4/eshadoop-6.4.0.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.elastic.co/products/hadoop&quot;&gt;https://www.elastic.co/products/hadoop&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/hadoop/current/index.html&quot;&gt;https://www.elastic.co/guide/en/elasticsearch/hadoop/current/index.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/elastic/elasticsearch-hadoop&quot;&gt;https://github.com/elastic/elasticsearch-hadoop&lt;/a&gt; - GitHub repo&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Cassandra</title><link>https://ondataengineering.net/technologies/apache-cassandra/</link><description> &lt;p&gt;Distributed wide-column datastore based on Amazon Dynamo and Google BigTable. Focuses on fault tolerance, linear scalability and operational simplicity with zero downtime based on a distributed masterless node and peer-to-peer design. Supports high availability using network topology aware data replication to avoid single points of failure, fast real-time and durable ingestion of data using an append-only log, strong query performance based on an in-memory index (log-structured merge-tree) that is persisted as a sorted string table (SST) for fast sequential retrieval, and tunable consistency (between strong and eventual) allowing availability (number of replicas on which a write must succeed), data accuracy (number of replicas must respond to a read request before returning data) and performance to be traded off on a global or per-operation basis. Does not support joins nor subqueries, rather, emphasises denormalisation through features like collections. Comes with a command line shell (cqlsh) for using Cassandra Query Language (resembling SQL), a wide number of drivers for many languages including Java, Python, Ruby, C++ and Go, and Nodetool, a CLI for cluster management. Metrics can be queried via JMX or pushed to external monitoring systems, SSL encryption provides secure communication, authentication and authorisation is provided based on internally controlled rolename/passwords and object permission management. An Apache project, graduating in February 2010, having been originally opened sourced in July 2008 by Facebook. Written in Java and under active development with major contributions from DataStax who distribute it as a part of their DataStax Enterprise offering. Other commercial vendors include Instaclustr and Winguzone who provide hosted and managed Apache Cassandra as a service on a number of major cloud providers.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Cassandra&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;, DataStax&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;July 2017 - 3.11&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://cassandra.apache.org&quot;&gt;https://cassandra.apache.org&lt;/a&gt; - Cassandra homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cassandra.apache.org/doc/latest&quot;&gt;https://cassandra.apache.org/doc/latest&lt;/a&gt; - Cassandra documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.datastax.com/products/datastax-enterprise&quot;&gt;http://www.datastax.com/products/datastax-enterprise&lt;/a&gt; - DataStax Enterprise&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://docs.datastax.com&quot;&gt;http://docs.datastax.com&lt;/a&gt; - DataStax Enterprise documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://academy.datastax.com/courses&quot;&gt;https://academy.datastax.com/courses&lt;/a&gt; - Free Self-Paced Courses on Apache Cassandra and DataStax Enterprise&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://www.datastax.com/dev/blog&quot;&gt;http://www.datastax.com/dev/blog&lt;/a&gt; - DataStax blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Mesos</title><link>https://ondataengineering.net/technologies/apache-mesos/</link><description> &lt;p&gt;Open source cluster manager for providing efficient resource utilization across a cluster of servers through resource sharing and isolation. Allows a cluster of servers to be shared across diverse cluster computing frameworks so that different distributed workloads such as container orchestration, machine learning, analytics and stateful big data technologies can be run without interfering with each other. Has the ability to dynamically allocate resources across the servers as needed and delegates control over scheduling to the frameworks through an abstraction layer called a resource offer to support a wide array of computing frameworks. Resource isolation is implemented using a universal containeriser, supporting numerous containers including native Mesos containers and Docker containers. Fault tolerance of the Mesos instance in control of the cluster is implemented using ZooKeeper. Started as a research project in the UC Berkeley RAD Lab, open sourced in 2011, with a v1.0 release in July 2016, which, included the 'unified containeriser' and GPU-based scheduling. Written in C++, uses Google Protocol Buffers for messaging and serialization to allow frameworks to be written in a variety of languages including C++, Java, Python, Go, Haskell, and Scala. Under active development, open sourced under the Apache 2.0 license, hosted on the Apache git repository and mirrored on GitHub. Software startup Mesosphere sells the Datacenter Operating System, a distributed operating system, based on Apache Mesos.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Mesos, DC/OS&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;, &lt;a href=&quot;/tech-vendors/mesosphere/&quot;&gt;Mesosphere&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;July 2017 - v1.3&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Add ons&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-aurora/&quot;&gt;Apache Aurora&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-myriad/&quot;&gt;Apache Myriad&lt;/a&gt;, &lt;a href=&quot;/technologies/chronos/&quot;&gt;Chronos&lt;/a&gt;, &lt;a href=&quot;/technologies/mesosphere-marathon/&quot;&gt;Mesosphere Marathon&lt;/a&gt;, Apache REEF&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Uses&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-zookeeper/&quot;&gt;Apache ZooKeeper&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Is used by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/mesosphere-dcos/&quot;&gt;Mesosphere DC/OS&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://mesos.apache.org/&quot;&gt;http://mesos.apache.org/&lt;/a&gt; - product home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://mesos.apache.org/documentation/&quot;&gt;http://mesos.apache.org/documentation/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://mesos.berkeley.edu/mesos_tech_report.pdf&quot;&gt;http://mesos.berkeley.edu/mesos_tech_report.pdf&lt;/a&gt; - Mesos: A Platform for Fine-Grained Resource Sharing the the Data Centre&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://mesos.apache.org/blog/&quot;&gt;http://mesos.apache.org/blog/&lt;/a&gt; - blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Aurora</title><link>https://ondataengineering.net/technologies/apache-aurora/</link><description> &lt;p&gt;A service scheduler for defining and managing bundled tasks as jobs across a cluster of servers using Mesos, leveraging Mesos for resource allocation and isolation at the task level. Operates as a Mesos framework, a Python based domain specific language (DSL) for job template definition, an executor for carrying out the workload described in the DSL, an associated command line interface for schedule management and a web interface providing read-only status of jobs and associated diagnostic information. Defines a fine-grained task state model to support resource allocation, rolling upgrades, health checking, priority-based scheduling and application maintenance. Handles cross-cutting concerns like observability and log collection. Supports priority-based scheduling, using pre-emption so that when resources are low, lower priority jobs can be stopped to make room for the higher priority tasks. An Apache project, originally created at Twitter, donated to the Apache Foundation in October 2013, graduating in March 2015 (0.8.0 Released). Hasn't yet reached a v1.0 milestone, however still under development from a range of contributors.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Aurora&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Open Source - Active&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;July 2017 - v0.18&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Add on to&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-mesos/&quot;&gt;Apache Mesos&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://aurora.apache.org/&quot;&gt;http://aurora.apache.org/&lt;/a&gt; - product home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://aurora.apache.org/documentation/latest/&quot;&gt;http://aurora.apache.org/documentation/latest/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://aurora.apache.org/documentation/latest/getting-started/overview/&quot;&gt;http://aurora.apache.org/documentation/latest/getting-started/overview/&lt;/a&gt; - Aurora System Overview&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=asd_h6VzaJc/&quot;&gt;https://www.youtube.com/watch?v=asd_h6VzaJc/&lt;/a&gt; - Introduction to Apache Aurora&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://aurora.apache.org/blog/&quot;&gt;http://aurora.apache.org/blog/&lt;/a&gt; - blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/apache/aurora/releases/&quot;&gt;https://github.com/apache/aurora/releases/&lt;/a&gt; - details of releases&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Chronos</title><link>https://ondataengineering.net/technologies/chronos/</link><description> &lt;p&gt;A framework for Apache Mesos to schedule and orchestrate jobs to periodically run at fixed times, dates or intervals in a clustered environment. Leverages Mesos for resource allocation and isolation and provides a REST API and web interface for job definition and job management. Reoccurring jobs are defined using ISO8601 repeating interval notation and may also be triggered by the completion of other jobs to create dependency based jobs. Uses ZooKeeper for state management and typically deployed as a service under Marathon for high-availability. Supports writing and exporting of job metrics to various systems for further analysis and notifications to various endpoints such as email and chat messaging systems. Originally created at AirBnB and written in Scala, opened sourced in March 2013 under the Apache 2.0 license, hosted under the Apache Mesos Community Projects group-owned repositories on GitHub.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Chronos&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Open Source - Active&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;July 2017 - v3.0&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Uses&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-zookeeper/&quot;&gt;Apache ZooKeeper&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Add on to&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-mesos/&quot;&gt;Apache Mesos&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://mesos.github.io/chronos/&quot;&gt;https://mesos.github.io/chronos/&lt;/a&gt; - product home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/mesos/chronos/&quot;&gt;https://github.com/mesos/chronos/&lt;/a&gt; - source code&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://mesos.github.io/chronos/docs/&quot;&gt;https://mesos.github.io/chronos/docs/&lt;/a&gt; - documentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/mesos/chronos/releases/&quot;&gt;https://github.com/mesos/chronos/releases/&lt;/a&gt; - details of releases&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Mesosphere Marathon</title><link>https://ondataengineering.net/technologies/mesosphere-marathon/</link><description> &lt;p&gt;A framework for Apache Mesos and Mesosphere's Datacenter Operating System (DC/OS) to launch long-running services in a clustered environment and ensure that they continue to run in the event of a hardware or software failure. Implemented as a Mesos framework, leverages Mesos for resource allocation and isolation and provides a REST API and web interface for service definition, discovery and management. Provides constraints control to support service placement for high-available and locality, an event bus and health checking to support rolling deployments and upgrades. Provides local and external persistent storage and resurrection on the same node in the event of a failure to support stateful services (in beta). Often used as an orchestrator for other applications and services, can be run in highly-available mode by running multiple copies of the framework and using ZooKeeper to perform leader election in the event on an failure. Written in Scala, open sourced under the Apache 2.0 license, hosted on GitHub, with development led by Mesosphere who also distribute it as part of their Mesosphere's Datacenter Operating System (DC/OS) commercial offering.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Marathon&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;October 2017 - v1.5&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Uses&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-zookeeper/&quot;&gt;Apache ZooKeeper&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Add on to&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-mesos/&quot;&gt;Apache Mesos&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/mesosphere-dcos/&quot;&gt;Mesosphere DC/OS&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.5&lt;/td&gt; &lt;td&gt;2017-09-12&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://github.com/mesosphere/marathon/releases/tag/v1.5.0&quot;&gt;release notes&lt;/a&gt;; &lt;a href=&quot;https://mesosphere.com/blog/marathon-1_5/&quot;&gt;blog post&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://mesosphere.github.io/marathon/&quot;&gt;https://mesosphere.github.io/marathon/&lt;/a&gt; - product home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/mesosphere/marathon/&quot;&gt;https://github.com/mesosphere/marathon/&lt;/a&gt; - source code&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://mesosphere.github.io/marathon/docs/&quot;&gt;https://mesosphere.github.io/marathon/docs/&lt;/a&gt; - documentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/mesosphere/marathon/releases/&quot;&gt;https://github.com/mesosphere/marathon/releases/&lt;/a&gt; - details of releases&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Mesosphere DC/OS</title><link>https://ondataengineering.net/technologies/mesosphere-dcos/</link><description> &lt;p&gt;A distributed, hybrid-cloud operating system for elastic stateless micro services running in containers and stateful big data services, ensuring high datacenter utilization. At its core, Apache Mesos handles job scheduling, resource management and abstraction, high availability, infrastructure-level processes and pluggable containerizers for both Docker and native Mesos containers. Combined with Marathon, provides a container orchestration platform with support for launching, managing, scaling and networking containers. Focused on ease of use, provides an app-store-like service catalog (Universe) to install complex distributed systems including HDFS, Apache Spark, Apache Kafka, Apache Cassandra, CI/CD applications such as Jenkins, all of which have been optimised to run on Apache Mesos and a web interface for monitoring and management. Comes in two flavors; a free community edition for installation in the cloud and a commercial enterprise edition for on-premises, in the cloud, or across a hybrid environment and includes monitoring tools, support for enterprise security and compliance tools, advanced networking, and load balancing features. Offered via a subscription license, the enterprise edition also includes product support. Open sourced in April 2016 under the Apache 2.0 license, under active development led by Mesosphere with a range of contributors including Microsoft.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;DC/OS&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;October 2017 - v1.10&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Uses&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-mesos/&quot;&gt;Apache Mesos&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Packages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/mesosphere-marathon/&quot;&gt;Mesosphere Marathon&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.10&lt;/td&gt; &lt;td&gt;2017-09-08&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://dcos.io/releases/1.10.0/&quot;&gt;Release Notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://dcos.io/&quot;&gt;https://dcos.io/&lt;/a&gt; - product home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/dcos/dcos/&quot;&gt;https://github.com/dcos/dcos/&lt;/a&gt; - source code&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://dcos.io/docs/&quot;&gt;https://dcos.io/docs/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://mesosphere.com/blog/2016/04/19/open-source-dcos/&quot;&gt;https://mesosphere.com/blog/2016/04/19/open-source-dcos/&lt;/a&gt; - Introduction to open source DC/OS&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://dcos.io/blog/&quot;&gt;https://dcos.io/blog/&lt;/a&gt; - blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://dcos.io/releases/&quot;&gt;https://dcos.io/releases/&lt;/a&gt; - details of releases&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Kylin</title><link>https://ondataengineering.net/technologies/apache-kylin/</link><description> &lt;p&gt;An open source distributed analytic engine built to support sub-second OLAP / star schema style queries using SQL on extremely large datasets on Hadoop. Data is read from a star schema data model in Hive to build a data cube of pre-calculated metrics by dimensions using MapReduce or Spark with the results stored in a key-value datastore (HBase). SQL queries can be submitted to the query engine, with results returned with sub-second latency if the required data exists in an HBase cube, otherwise the query is optionally routed back to its original source on Hadoop. Supports compression of large datasets by dictionary encoding cube data using a triple data structure, combination pruning and aggregation grouping of dimensions for efficient data storage, and uses approximation query capability (HyperLogLog) to estimate distinct items and TopN to answer top-k queries. Row keys are composed by dimension encoded values and HBase's fuzzy row filtering is performed directly on the storage nodes to implement low latency lookups. Simple additive and aggregation operations (sum, count or like) are also performed on the storage nodes using HBase coprocessors to provide efficient computational parallelism and minimise network latency. Uses Apache Calcite for SQL parsing and optimisation, comes with an ODBC driver, a JDBC driver and a REST API to integrate with third party business intelligence tools such as Tableau, Microsoft Excel and PowerBI. Includes a web interface and REST API for model building and cube design (with support for hierarchy, joint and derived dimensions), job management (full, incremental and streaming builds) and monitoring and permission management (providing security at a project or cube level). New beta features include building cubes from Kafka streaming data and cube building using Spark instead of MapReduce. Originally developed at Ebay, donated to the Apache Foundation in November 2014, graduating in November 2015, with a 1.0 release in September 2015, and still under active development. Commercial support available from Kyligence, who distribute their own product based on Kylin replacing HBase with a custom columnar storage engine (with cell level access control and integration with LDAP), along with a web based BI tool for self service analysis and a dashboard for Kylin cluster management.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Kylin&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;, Kyligence&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;September 2018 - v2.5&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.2&lt;/td&gt; &lt;td&gt;2017-11-04&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://mail-archives.apache.org/mod_mbox/www-announce/201711.mbox/%3CCA+LQBaRUR4KR_BT+KC+M7M77NXAPfy+mT39bWQBo=dLVYsB5yQ@mail.gmail.com%3E&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.3&lt;/td&gt; &lt;td&gt;2018-03-04&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://mail-archives.us.apache.org/mod_mbox/www-announce/201803.mbox/%3CCABh5zFxxo1NKXZT8i0W-Xu7iDc=tZeXRBNBSXgnsYM3T+XaQEg@mail.gmail.com%3E&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;http://kylin.apache.org/blog/2018/03/04/release-v2.3.0/&quot;&gt;blog post&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.4&lt;/td&gt; &lt;td&gt;2018-06-25&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://mail-archives.us.apache.org/mod_mbox/www-announce/201806.mbox/%3CCANfpUcsJX5a3dTZwbvHkCPxv_BKcUxD2YB_6KttGU5bfXW=FkQ@mail.gmail.com%3E&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.5&lt;/td&gt; &lt;td&gt;2018-09-19&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/www-announce/201809.mbox/%3CCANfpUcvMr=PG9pphR=uS4zpkvT9YzNudnvHtQSkO3g+bHbWcrw@mail.gmail.com%3E&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;http://kylin.apache.org/blog/2018/09/20/release-v2.5.0/&quot;&gt;blog post&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://kylin.apache.org&quot;&gt;http://kylin.apache.org&lt;/a&gt; - Kylin homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://kylin.apache.org/docs20&quot;&gt;http://kylin.apache.org/docs20&lt;/a&gt; - Kylin documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.ebaytechblog.com/2014/10/20/announcing-kylin-extreme-olap-engine-for-big-data&quot;&gt;http://www.ebaytechblog.com/2014/10/20/announcing-kylin-extreme-olap-engine-for-big-data&lt;/a&gt; - open source announcement&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://kyligence.io/products&quot;&gt;http://kyligence.io/products&lt;/a&gt; - Kylience Analytics Platform&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://manual.kyligence.io/v2.4/en&quot;&gt;http://manual.kyligence.io/v2.4/en&lt;/a&gt; - Kylience Analytics Platform documentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://kylin.apache.org/blog&quot;&gt;http://kylin.apache.org/blog&lt;/a&gt; - Kylin blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://kylin.apache.org/download/&quot;&gt;http://kylin.apache.org/download/&lt;/a&gt; - list of recent versions&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Beam</title><link>https://ondataengineering.net/technologies/apache-beam/</link><description> &lt;p&gt;Unified batch and streaming programming model to define portable data processing pipelines and execute these using a range of different engines. Originating from the Google Dataflow model, focuses on unifying both styles of processing by treating static data sets as streams (which happen to have a beginning and an end), while achieving data correctness and the ability to handle late-arriving data through a set of abstractions and concepts that give users control over estimated quality of arrived data (completeness), duration to wait for results (latency) and how much speculative/redundant computation to do (cost). Allows business logic, data characteristics and trade-off strategies to be defined via different programming languages through pluggable language SDKs (with out of the box support for Java and Python). Supports a range of pluggable runtime platforms through pipeline runners, with support for a direct runner (for development and testing pipelines in a non-distributed environment), Apache Apex, Flink, Spark, and (under development) Gearpump runners, and a Google Cloud Dataflow runner. Also supports a growing set of connectors that allow pipelines to read and write data to various data storage systems (IOs). An Apache project, opened sourced by Google in January 2016, graduated in January 2017, with a first stable release (2.0) in May 2017. Written in Java and Python and under active development with a large number of contributors including Google, data Artisans, Talend and PayPal.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Beam&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;October 2018 - 2.7&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.1&lt;/td&gt; &lt;td&gt;2017-08-23&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12340528&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.2&lt;/td&gt; &lt;td&gt;2017-12-02&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12341044&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.3&lt;/td&gt; &lt;td&gt;2018-02-19&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://beam.apache.org/blog/2018/02/19/beam-2.3.0.html&quot;&gt;blog post&lt;/a&gt;; &lt;a href=&quot;https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12341608&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.4&lt;/td&gt; &lt;td&gt;2018-03-20&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12341608&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.5&lt;/td&gt; &lt;td&gt;2018-06-26&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://beam.apache.org/blog/2018/06/26/beam-2.5.0.html&quot;&gt;blog post&lt;/a&gt;; &lt;a href=&quot;https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12342847&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.6&lt;/td&gt; &lt;td&gt;2018-08-08&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://beam.apache.org/blog/2018/08/10/beam-2.6.0.html&quot;&gt;blog post&lt;/a&gt;; &lt;a href=&quot;https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12343392&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.7&lt;/td&gt; &lt;td&gt;2018-10-02&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://beam.apache.org/blog/2018/10/03/beam-2.7.0.html&quot;&gt;blog post&lt;/a&gt;; &lt;a href=&quot;https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12343654&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://beam.apache.org&quot;&gt;https://beam.apache.org&lt;/a&gt; - product home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://beam.apache.org/documentation/&quot;&gt;https://beam.apache.org/documentation/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://beam.apache.org/documentation/runners/capability-matrix/&quot;&gt;https://beam.apache.org/documentation/runners/capability-matrix/&lt;/a&gt; - defines capabilities of individual runners&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/blog/big-data/2016/05/why-apache-beam-a-google-perspective&quot;&gt;https://cloud.google.com/blog/big-data/2016/05/why-apache-beam-a-google-perspective&lt;/a&gt; - motivation behind Beam&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://beam.apache.org/blog/&quot;&gt;https://beam.apache.org/blog/&lt;/a&gt; - blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://beam.apache.org/get-started/downloads/&quot;&gt;https://beam.apache.org/get-started/downloads/&lt;/a&gt; - details of releases&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>REX-Ray</title><link>https://ondataengineering.net/technologies/rex-ray/</link><description> &lt;p&gt;Open source, storage management solution providing containers to access external storage systems outside of the container's host thus enabling stateful applications such as databases to be run inside containers. Allows applications to save data beyond the lifecycle of a container and provides high-availability features for container restarts across hosts. Operates as a command line interface and lightweight agent that can be integrated into container runtimes (e.g. Docker, Mesos) to provide storage functionality such as volume creation, attaching, and mounting processes as well as container orchestrators (e.g. Docker Swarm, Kubernetes, or Marathon for Mesos) to attach a volume to a new host and resume state in the event of a host failure. Built on top of the libStorage library (also from Dell EMC), provides a storage plugin framework that allows access to multiple storage providers and platforms (Amazon EBS, EFS, S3FS, Dell EMC ScaleIO, Isilon etc.) and a flexible architecture that allows it to be deployed in a standalone, decentralised fashion on each container host or as a centralised service for easier management at large scale. Written in Go, open sourced under the Apache 2.0 licence, hosted on GitHub, with development led by Dell EMC. Has not yet reached a v1.0 milestone, but is still under active development.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;Dell EMC&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Open Source - Active&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;October 2017 - 0.10&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.10&lt;/td&gt; &lt;td&gt;2017-09-11&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://rexray.readthedocs.io/en/stable/about/release-notes/#version-0100-20170911&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://rexray.codedellemc.com/&quot;&gt;https://rexray.codedellemc.com/&lt;/a&gt; - Home&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://rexray.readthedocs.io/&quot;&gt;https://rexray.readthedocs.io/&lt;/a&gt; - Documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/codedellemc/rexray/&quot;&gt;https://github.com/codedellemc/rexray/&lt;/a&gt; - Code&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hub.docker.com/u/rexray/&quot;&gt;https://hub.docker.com/u/rexray/&lt;/a&gt; - Docker volume plugin for various storage providers&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/codedellemc/rexray/releases/&quot;&gt;https://github.com/codedellemc/rexray/releases/&lt;/a&gt; - details of releases&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Zenko</title><link>https://ondataengineering.net/technologies/zenko/</link><description> &lt;p&gt;A distributed and resilient Amazon S3 API compatible object storage gateway / proxy. Utilises Zenko CloudServer (previously S3 Server) to provide an S3 compatible API, to proxy requests to either Scality RING, Amazon S3, Azure Blob Storage or Google Cloud Storage, and to provide persistent local storage or transient in-memory storage. Current solution is a Docker Swarm stack of a cluster of Zenko CloudServer instances with nginx as a front end load balancer. Manageable via Zenko Orbit, a cloud based portal. Roadmap includes support for Azure Blob Storage, support for other container management systems such as Kubernetes, plus two new sub-projects - Backbeat (which will provide policy-based data workflows such as replication or migration) and Clueso (which will provide object metadata search and analytics using Apache Spark). First released in July 2017, and hosted on GitHub under an Apache 2.0 licence.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Zenko Multi-Cloud Data Controller&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;April 2018&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Sub-projects&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; Zenko&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/zenko/cloudserver/&quot;&gt;Zenko CloudServer&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Open source object storage server based on the S3 compatible API from Scality RING, with the ability to proxy requests to other S3 services (with support for Scality RING, Amazon S3, Azure Blob Storage and Google Cloud), or to store data in persistent local storage or transient in-memory storage, with support for concurrent use of multiple backends. Supports broad compatibility with the Amazon S3 API including bucket and object versioning, and has been tested against a range of Amazon S3 utilities, CLIs and SDKs. Written in Node.js, available as a Docker container, and can be deployed and used independantly of the rest of Zenko. Metadata and (locally persisted) data is managed by a data and metadata daemon (dmd), with the option to use a shared remote daemon (for example when running a cluster of CloudServers). First released in June 2016 as S3 Server before becoming being renamed to CloudServer and becoming part of Zenko in July 2017. Hosted on GitHub under an Apache 2.0 licence.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://www.zenko.io/&quot;&gt;http://www.zenko.io/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://zenko.readthedocs.io/en/latest/index.html&quot;&gt;http://zenko.readthedocs.io/en/latest/index.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/scality/Zenko&quot;&gt;https://github.com/scality/Zenko&lt;/a&gt; - GitHub repo&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.zenko.io/blog/zenko-rollout-plan/&quot;&gt;http://www.zenko.io/blog/zenko-rollout-plan/&lt;/a&gt; - Roadmap&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.scality.com/products/zenko-multi-cloud-data-controller/&quot;&gt;http://www.scality.com/products/zenko-multi-cloud-data-controller/&lt;/a&gt; - Scality Zenko product page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.scality.com/about-us/press/scality-zenko-multi-cloud-controller/&quot;&gt;http://www.scality.com/about-us/press/scality-zenko-multi-cloud-controller/&lt;/a&gt; - original press release&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://www.zenko.io/blog/&quot;&gt;http://www.zenko.io/blog/&lt;/a&gt; - blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Zenko CloudServer</title><link>https://ondataengineering.net/technologies/zenko/cloudserver/</link><description> &lt;p&gt;Open source object storage server based on the S3 compatible API from Scality RING, with the ability to proxy requests to other S3 services (with support for Scality RING, Amazon S3, Azure Blob Storage and Google Cloud), or to store data in persistent local storage or transient in-memory storage, with support for concurrent use of multiple backends. Supports broad compatibility with the Amazon S3 API including bucket and object versioning, and has been tested against a range of Amazon S3 utilities, CLIs and SDKs. Written in Node.js, available as a Docker container, and can be deployed and used independantly of the rest of Zenko. Metadata and (locally persisted) data is managed by a data and metadata daemon (dmd), with the option to use a shared remote daemon (for example when running a cluster of CloudServers). First released in June 2016 as S3 Server before becoming being renamed to CloudServer and becoming part of Zenko in July 2017. Hosted on GitHub under an Apache 2.0 licence.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;S3 Server, Scality S3 Server&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/zenko/&quot;&gt;Zenko&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;April 2018 - v7.4&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;6.4&lt;/td&gt; &lt;td&gt;2017-02-17&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://github.com/scality/S3/releases/tag/GA6.4.0&quot;&gt;GitHub release&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;7.0&lt;/td&gt; &lt;td&gt;2017-06-24&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://github.com/scality/S3/releases/tag/GA7.0.0&quot;&gt;GitHub release&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;7.1&lt;/td&gt; &lt;td&gt;2017-08-28&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://github.com/scality/S3/releases/tag/GA7.1.0&quot;&gt;GitHub release&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;7.2&lt;/td&gt; &lt;td&gt;2017-11-13&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://github.com/scality/S3/releases/tag/BETA7.2.0&quot;&gt;GitHub release&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;7.4&lt;/td&gt; &lt;td&gt;2018-04-21&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://github.com/scality/S3/releases/tag/7.4.0&quot;&gt;GitHub release&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://www.zenko.io/cloudserver/&quot;&gt;http://www.zenko.io/cloudserver/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://s3-server.readthedocs.io/en/latest/&quot;&gt;http://s3-server.readthedocs.io/en/latest/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/scality/S3&quot;&gt;https://github.com/scality/S3&lt;/a&gt; - GitHub repo&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.scality.com/about-us/press/scality-announces-s3-server/&quot;&gt;http://www.scality.com/about-us/press/scality-announces-s3-server/&lt;/a&gt; - original press release&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/scality/S3/releases&quot;&gt;https://github.com/scality/S3/releases&lt;/a&gt; - release history&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Structured Streaming</title><link>https://ondataengineering.net/technologies/apache-spark/structured-streaming/</link><description> &lt;p&gt;Extension to the Spark SQL DataFrame API to allow Spark SQL queries to be executed over streams of data, with the engine continuously updating and maintaining the result as new data arrives. Uses the full Spark SQL engine (including the Catalyst optimiser), and supports end-to-end exactly-once semantics via checkpointing when sources have sequential offsets. Supports aggregations over sliding event-time windows, including support for late data and watermarking. Introduced in Spark 2.0 with a production release in Spark 2.2.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;August 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html&quot;&gt;https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html&quot;&gt;https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html&lt;/a&gt; - intro blog post&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>OpenTSDB</title><link>https://ondataengineering.net/technologies/opentsdb/</link><description> &lt;p&gt;A time series database built on top of Apache HBase (with support for Google BigTable and Apache Cassandra recently added). Each data point consists of a metric name, a UNIX timestamp, a value (either integer or floating point) and a set of key value pair tags, where the tags define the potentially aggregations required. Data is stored with one row per metric, tag combination and hour, with all data points for that hour stored in that row under different column qualifiers based on the timestamp, allowing for more efficient in memory aggregations. Supports the recording (but not generation) of pre-aggregated data that will be used to accelerate queries, annotations (short text strings associated with timestamps and optionally time series that represent events), the organisation of metrics and tags into hierarchical trees, and the generation of statistics relating to performance, however currently does not support incrementing counters. Consists of a Time Series Daemon (TSD) (that exposes a Telnet RPC and HTTP JSON REST APIs and a simple web based UI for querying data) and a CLI (including the ability to batch import data), with each TSD opperating independantly of each other with no master or shared state allowing for horizontal scalability over a single underlying database. Supports a range of plugins, including the ability to support different deserialisation and authentication for HTTP REST calls, emmission of meta data (metrics, tags and annotations) to a search engine, real time publishing of data points to another destination and support for other RPC protocols. Open sourced on GitHub under both an LGPLv2.1+ and GPLv3+ licence, with development started in 2010, and has been adopted but a number of large organisations including MapR, Yahoo, Tumblr and ebay.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Open Source - Quiet&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;August 2017 - v2.3&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/mapr-monitoring/&quot;&gt;MapR Monitoring&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://opentsdb.net/&quot;&gt;http://opentsdb.net/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://opentsdb.net/docs/build/html/index.html&quot;&gt;http://opentsdb.net/docs/build/html/index.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.slideshare.net/HBaseCon/ecosystem-session-6&quot;&gt;https://www.slideshare.net/HBaseCon/ecosystem-session-6&lt;/a&gt; - introductory presentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/OpenTSDB/opentsdb/releases&quot;&gt;https://github.com/OpenTSDB/opentsdb/releases&lt;/a&gt; - release history&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>InfluxDB</title><link>https://ondataengineering.net/technologies/influxdb/</link><description> &lt;p&gt;A time series database implemented in Go available in both open source and enterprise editions. Each data point consists of a metric name (measurement), a UNIX nano timestamp, a set of tag key value pairs, and a set of value key value pairs, with the combination of measurement and tag keys refered to as a series. Data is stored in a custom time series index (TSI) engine which supports very large numbers of series allowing for huge cardinalities of tag and value keys. Queries are written in InfluxQL (a varient of SQL), which includes support for creating and managing databases and series, listing series metadata (including measurements, tag keys and values and field keys), managing queries, writing the results of queries back into InfluxDB into a new series, a range of analytical SQL functions including aggregations (e.g. sum, count, spread, stddev), selections (e.g. first, last, percentile, sample) and transformations (e.g. ceiling, derivative, moving_average), and support for registering continuous queries that are run automatically and periodically within a database to create aggregate tables. Also supports retention policies for the automatic deletion of historic data, basic authentication and authorisation (at the database level), HTTPS connections, service plugins that allow data to be written to InfluxDB in alternative protocols (with out of the box support for UDP, Graphite, CollectD, Prometheus and OpenTSDB protocols), snapshot backups, statistics and diagnostic information, and an HTTP API and CLI for writing and querying data. Available as an open source version (under an MIT licence but limited to a single node), and as two commercial products - InfluxEnterprise (with support for clustering, access control and incremental backups) and InfluxCloud (InfluxEnterprise as a cloud based service). Originally created in 2013, and is part of the open source TICK suite along with Telegraf (ingestion of data), Chronograf (admin UI and visualisation) and Kapacitor (streaming analytics and actions).&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;July 2018 - v1.6&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.4&lt;/td&gt; &lt;td&gt;2017-11-14&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.influxdata.com/blog/whats-new-influxdb-oss-1-4/&quot;&gt;blog&lt;/a&gt;; &lt;a href=&quot;https://docs.influxdata.com/influxdb/v1.5/about_the_project/releasenotes-changelog/#v1-4-0-2017-11-13&quot;&gt;changelog&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.influxdata.com/blog/influxdb-now-supports-prometheus-remote-read-write-natively/&quot;&gt;Prometheus compatible API&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.5&lt;/td&gt; &lt;td&gt;2018-03-06&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.influxdata.com/blog/release-announcement-influxdb-1-5-0-influxdb-enterprise-1-5-0/&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;https://docs.influxdata.com/influxdb/v1.5/about_the_project/releasenotes-changelog/#v1-5-0-2018-03-06&quot;&gt;changelog&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.influxdata.com/blog/logging-improvements-for-influxdb-1-5-0/&quot;&gt;Logging improvements&lt;/a&gt;; &lt;a href=&quot;https://www.influxdata.com/blog/new-features-in-open-source-backup-and-restore/&quot;&gt;backup and restore&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.6&lt;/td&gt; &lt;td&gt;2018-07-10&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.influxdata.com/blog/release-announcement-influxdb-1-6-0-oss/&quot;&gt;announcement&lt;/a&gt;; &lt;a href=&quot;https://docs.influxdata.com/influxdb/v1.6/about_the_project/releasenotes-changelog/#v1-6-0-2018-07-05&quot;&gt;changelog&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.influxdata.com/time-series-platform/influxdb/&quot;&gt;https://www.influxdata.com/time-series-platform/influxdb/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.influxdata.com/time-series-platform/&quot;&gt;https://www.influxdata.com/time-series-platform/&lt;/a&gt; - TICK platform overview&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.influxdata.com/products/editions/&quot;&gt;https://www.influxdata.com/products/editions/&lt;/a&gt; - comparison of the different editions&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.influxdata.com/influxdb/latest/&quot;&gt;https://docs.influxdata.com/influxdb/latest/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/influxdata/influxdb&quot;&gt;https://github.com/influxdata/influxdb&lt;/a&gt; - source code&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.influxdata.com/blog/&quot;&gt;https://www.influxdata.com/blog/&lt;/a&gt; - InfluxData blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.influxdata.com/influxdb/latest/about_the_project/releasenotes-changelog/&quot;&gt;https://docs.influxdata.com/influxdb/latest/about_the_project/releasenotes-changelog/&lt;/a&gt; - current release versions&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Denodo Platform</title><link>https://ondataengineering.net/technologies/denodo-platform/</link><description> &lt;p&gt;Data Virtualisation platform, enabling a logical schema to be defined over a range of relational, NoSQL, flat file and application / data APIs that can then be queried through a range of end points. Supported data sources include a wide range of databases (relational, in memory, MPP, Hadoop, cloud, OLAP cube and NoSQL), flat files (Hadoop, text, binary, Office, including support for (S)FTP, compression and encryption), application APIs (e.g. Salesforce, SAP, Oracle e-business, Twitter), RDF semantic repositories via SPARQL, mainframes, data APIs (SOAP and REST), JMS queues and the ability to scrape web pages, with data accessible via SQL (JDBC, ODBC), data APIs (SOAP and REST) and web widgets (Sharepoint, Java, AJAX), with the ability to transform, cleanse and combine data from multiple sources into a single semantic model using the relevant source system query language. Supports a dynamic query optimiser (which pushes query logic down to the underlying data source, with the ability to move data between sources and take advantage of data replicated in multiple sources to maximise logic pushdown), caching (either by query or by full materialisation, allowing tables in the semantic layer to be pre-generated, with support for scheduled and incremental updates and the use of external ETL tools), data writes back to source (with support for a distributed transaction manager and 2-phase commits), a full security model (role based access control at the row/column level, with authentication pass-through to data sources), resource and workload management, metadata visualisation (including search and lineage views), self service data discovery (execution of ad-hoc queries outside of the semantic layer), search (over data and metadata, including support for semantic mining and extraction of text sources), an SDK (for adding new source adapters, custom functions and stored procedures), and a graphical UI (supporting wizard-driven configuration, integration with external configuration management tools and release management). Can we deployed stand alone or as a cluster (supporting both active/active and active/passive configurations and shared caches, with support for geo-replication), and is also available for AWS and as a free Denodo Express version (with limits on the number of active queries and results). A commercial product from Denodo Technologies Inc, who were founded in 1999 with the first release of the Denodo Platform shortly afterwards.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;September 2017 - v6.0&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.denodo.com/en&quot;&gt;https://www.denodo.com/en&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://community.denodo.com/&quot;&gt;https://community.denodo.com/&lt;/a&gt; - community site, including documentation and tutorials&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://www.datavirtualizationblog.com/&quot;&gt;http://www.datavirtualizationblog.com/&lt;/a&gt; - blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Arrow</title><link>https://ondataengineering.net/technologies/apache-arrow/</link><description> &lt;p&gt;In-memory data structure specification for building columnar based data systems. Provides a standard interchange format to allow sharing of data between processes on a node without the overhead of moving or transforming the data, permits O(1) random access and has the ability to represent both flat relational structures and complex hierarchical nested data. Data is organised using a columnar structure memory-layout making it cache efficient for analytical workloads (which typically group all data relevant to a column operation together) and allows execution engines to take advantage of modern CPU SIMD (Single Instruction Multiple Data) instructions which work on multiple data values simultaneously in a single CPU clock cycle. Supports Java, C, C++, JavaScript, Python, Go, Ruby and Rust. Seeded from the Apache Drill project and promoted directly to a top level Apache project in February 2016 followed by an initial 0.1 release in October 2016. Used in a range of other projects including Drill, Spark, Impala, Kudu, Pandas and others. Has not yet reached a v1.0 milestone, but is still under active development with a range of contributors from a number of other Apache and non-Apache data projects.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Arrow&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;October 2018 - v0.11&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.8&lt;/td&gt; &lt;td&gt;2017-12-18&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://arrow.apache.org/blog/2017/12/18/0.8.0-release/&quot;&gt;blog post&lt;/a&gt;; &lt;a href=&quot;https://arrow.apache.org/release/0.8.0.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.9&lt;/td&gt; &lt;td&gt;2018-03-21&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://arrow.apache.org/blog/2018/03/22/0.9.0-release/&quot;&gt;blog post&lt;/a&gt;; &lt;a href=&quot;https://arrow.apache.org/release/0.9.0.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.10&lt;/td&gt; &lt;td&gt;2018-08-07&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://arrow.apache.org/blog/2018/08/07/0.10.0-release/&quot;&gt;blog post&lt;/a&gt;; &lt;a href=&quot;https://arrow.apache.org/release/0.10.0.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.11&lt;/td&gt; &lt;td&gt;2018-10-09&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://arrow.apache.org/blog/2018/10/09/0.11.0-release/&quot;&gt;blog post&lt;/a&gt;; &lt;a href=&quot;https://arrow.apache.org/release/0.11.0.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h1 id=&quot;links&quot;&gt;Links&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://arrow.apache.org/&quot;&gt;https://arrow.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href=&quot;http://git.apache.org/arrow.git/&quot;&gt;http://git.apache.org/arrow.git/&lt;/a&gt; - source code&lt;/p&gt; &lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://info.dataengconf.com/the-future-of-column-oriented-data-processing-with-arrow-and-parquet&quot;&gt;http://info.dataengconf.com/the-future-of-column-oriented-data-processing-with-arrow-and-parquet&lt;/a&gt; - introduction to Arrow&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces87&quot;&gt;https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces87&lt;/a&gt; - top level project announcement&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://arrow.apache.org/blog/&quot;&gt;https://arrow.apache.org/blog/&lt;/a&gt; - blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://arrow.apache.org/release/&quot;&gt;https://arrow.apache.org/release/&lt;/a&gt; - release and change summary&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache CarbonData</title><link>https://ondataengineering.net/technologies/apache-carbondata/</link><description> &lt;p&gt;Unified storage solution for Hadoop based on an indexed columnar data format, focusing on providing efficient processing and querying capabilities for disparate data access patterns. Data is loaded in batch, encoded, indexed using multiple strategies, compressed and written to HDFS using a columnar file format. Provides a number of highly configurable indexes (multi-dimensional key, min/max index, and inverted index), global dictionary encoding and column grouping to support interactive style OLAP queries, high throughput scan queries, low latency point queries and individual record queries. Also supports batch updates and deletes using delta bitmap files and compaction. Written in Java using Apache Thrift, supports all common primitive data types and complex nested data types including array and structures. Consists of several modules, the format specification and core implementation (columnar storage, indexing, compression, encoding), Hadoop input/output format interface, deep integration with Spark, interfacing to Spark SQL and the DataFrame API and connectors for Hive and Presto. Started back in 2013 at Huawei's India R&amp;D center, donated to the Apache Foundation in 2015, graduated in April 2017, with a stable (1.1.0) release in May 2017, and under active development.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;CarbonData&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;June 2018 - 1.4&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.3&lt;/td&gt; &lt;td&gt;2017-02-03&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/CARBONDATA/Apache+CarbonData+1.3.0+Release&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.4&lt;/td&gt; &lt;td&gt;2018-06-04&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/CARBONDATA/Apache+CarbonData+1.4.0+Release&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://carbondata.apache.org&quot;&gt;http://carbondata.apache.org&lt;/a&gt; - CarbonData homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://carbondata.apache.org/mainpage.html&quot;&gt;http://carbondata.apache.org/mainpage.html&lt;/a&gt; - CarbonData documentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/carbondata&quot;&gt;https://blogs.apache.org/carbondata&lt;/a&gt; - CarbonData blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/CARBONDATA/Releases&quot;&gt;https://cwiki.apache.org/confluence/display/CARBONDATA/Releases&lt;/a&gt; - details of releases&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache ORC</title><link>https://ondataengineering.net/technologies/apache-orc/</link><description> &lt;p&gt;Self-describing, type-aware, columnar file format to enable efficient querying and storage of data on Hadoop. Provides built-in storage indexes, column statistics and bloom filters to allow execution engines to implement predicate and projection push-down, partition pruning and cost based optimisation for low latency reads. Uses multi-version concurrency control to support ACID transactions and allow Hive to implement bulk insert, update, delete and streaming ingest (micro batch) use cases. Implements type-aware encoding for efficient compression (run-length for integer and dictionary for string). Schema definition is stored along side the data and supports all primitive data types and complex nested data structures. Uses protocol buffers to store meta data. Comes with a Java library for reading and writing the file format and includes a MapReduce compatible API, a C++ library for reading the file format (donated by Vertica) and a set of Java and C++ tools for inspecting and benchmarking ORC files. Created by Hortonworks in January 2013 as part of the initiative to massively speed up Hive and improve the storage efficiency of data stored in Hadoop, split off from Apache Hive to become a separate top level Apache project in April 2015 with a 1.0 release in January 2016.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;ORC&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;May 2018 - v1.5&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.5&lt;/td&gt; &lt;td&gt;2018-05-14&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://orc.apache.org/docs/releases.html#current-release---150&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://orc.apache.org/&quot;&gt;https://orc.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://git-wip-us.apache.org/repos/asf/orc.git/&quot;&gt;https://git-wip-us.apache.org/repos/asf/orc.git/&lt;/a&gt; - source code&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://orc.apache.org/docs/&quot;&gt;https://orc.apache.org/docs/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/100x-faster-hive/&quot;&gt;https://hortonworks.com/blog/100x-faster-hive/&lt;/a&gt; - initial announcement&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/apache-orc-launches-as-a-top-level-project/&quot;&gt;https://hortonworks.com/blog/apache-orc-launches-as-a-top-level-project/&lt;/a&gt; - top level project announcement, including summary of technology that support it&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://orc.apache.org/news/&quot;&gt;https://orc.apache.org/news/&lt;/a&gt; - news page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://orc.apache.org/docs/releases.html&quot;&gt;https://orc.apache.org/docs/releases.html&lt;/a&gt; - details of releases&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hortonworks DataPlane</title><link>https://ondataengineering.net/technologies/hortonworks-dataplane/</link><description> &lt;p&gt;An extensible platform for managing data ecosystems, with capabilities delivered through plugable applications. Supports the registration and management of DataPlane applications and the registration of Ambari managed clusters that are then accessible to these applications. Supports role based access control, with LDAP integration for users and groups and support for app specific roles. Runs on docker, with state held in an external database, and integrates with Knox (for SSO and access to clusters). Future services referenced include Cloudbreak and IBM DSX. Stated plan is for this to be a cloud service, however this is not currently generally available, and the documentation currently details installation steps for a local machine. First released in November 2017.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Hortonworks DataPlane Service&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/hortonworks/&quot;&gt;Hortonworks&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;September 2018 - 1.2&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Sub-projects&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; Hortonworks DataPlane&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/hortonworks-dataplane/data-analytics-studio/&quot;&gt;Data Analytics Studio&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A DataPlane application for running Hive queries, managing Hive tables, and diagnosing Hive query performance issues. Supports a query editor (with autocomplete, a visual explain plan, performance improvement recommendations, saved queries and results downloading), a query search tool (with pre-defined queries for expensive, long running, non-optimised and failed queries, a range of filters and saved searches), a database management tool (supporting searching, browsing, interrogation, creation and modification of databases, tables, partitions and columns as well as uploading of data from local storage or HDFS) and table impact reporting (showing reads, writes, projections, aggregations, filters and joins by table and column, with support for dynamic heatmaps overlaid on entity relationship diagrams). Requires a Ambari mangement pack (the DAS engine) to be installed on all clusters.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Hortonworks DataPlane&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/hortonworks-dataplane/data-lifecycle-manager/&quot;&gt;Data Lifecycle Manager&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A DataPlane application for replicating HDFS and Hive data between two clusters along with any associated metadata and security policies. Clusters already registered with DataPlane can be paired, at which point replication policies can be defined, which result in replication jobs running at the selected interval. Supports replicating between HDFS and cloud object storage (with some caveats around replication of security policies), replication of encrypted HDFS data, TLS encryption of replication streams, reporting on and management of replication jobs and HDFS snapshottable directories, with jobs executed by DLM Engine processes on the appropriate cluster. Stated future plans include support for automatic tiering of data between clusters and point in time backup and restore.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Hortonworks DataPlane&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/hortonworks-dataplane/data-steward-studio/&quot;&gt;Data Steward Studio&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A DataPlane application for viewing and understanding data assets, with supported data assets currently limited to Hive tables on clusters with Atlas and Ranger installed. Supports viewing metadata associated with data assets (including properties, lineage, security policies and audit logs), profiling of data (with profiling performed by a background Spark process, with support for data summarisation, identifying sensitive/personal data and profiling user access to data), grouping of data assets into asset collections, taging and rating of data assets and collections and dashboard views of metadata by cluster and collection.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Hortonworks DataPlane&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;https://ondataengineering.net/technologies/hortonworks-dataplane/streams-messaging-manager/&quot;&gt;Streams Messaging Manager&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A DataPlane application for monitoring Apache Kafka clusters. Provides an overview view of producers, topics (and their partitions), brokers and consumer groups, showing key statistics and the connections between them, with the ability to propagate filters based on these connections. Also provides detail views, profiles and historic graphs for each producer, topic, broker and consumer group, with the ability to link out to Atlas to see end to end lineage and Ambari Grafana for detailed metrics. Metrics and status information is also provided over a REST API, with a REST Server Agent running on each cluster being monitored.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;td&gt;2017-11-01&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://hortonworks.com/blog/hdp-2-6-3-dataplane-service/&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.1&lt;/td&gt; &lt;td&gt;2018-05-21&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/DP/DP-1.1.0/release-notes/content/dps_whats_new_in_this_release.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Significantly expanded docs&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.2&lt;/td&gt; &lt;td&gt;2018-08-25&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/DP/DP-1.2.0/release-notes/content/dps_whats_new_in_this_release.html&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Support for HDP 3.0 and HDF 3.2&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/products/dataplane/&quot;&gt;https://hortonworks.com/products/dataplane/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/DP/DP-1.2.0/index.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/DP/DP-1.2.0/index.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/category-emerges-introducing-hortonworks-dataplane-service/&quot;&gt;https://hortonworks.com/blog/category-emerges-introducing-hortonworks-dataplane-service/&lt;/a&gt; - intro blog post&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/hortonworks/dps_platform&quot;&gt;https://github.com/hortonworks/dps_platform&lt;/a&gt; - source code&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://docs.hortonworks.com/index.html&quot;&gt;http://docs.hortonworks.com/index.html&lt;/a&gt; - shows latest Hortonworks release version&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/&quot;&gt;https://hortonworks.com/blog/&lt;/a&gt; - Hortonworks blog posts&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>StreamSets Data Collector</title><link>https://ondataengineering.net/technologies/streamsets-data-collector/</link><description> &lt;p&gt;General purpose technology for the movement of data between systems, including the ingestion of batch and streaming data into an analytical platform. Pipelines are configured in a graphical user interface, and consist of a single origin, one or more processor stages and then one or more destinations, with support for a wide range of source/destination technologies and processor transformations. Supports a wide range of data formats, executors (tasks that can be triggered based on events from pipelines, e.g. to send e-mails or run a shell script), handling of erroroneous records, support for CDC CRUD records, previewing of data within the editor UI, real-time reporting and alerting on a range of execution and data quality metrics, the ability to dynamically handle changes to schemas and the semantic meaning of data and a full Python SDK. Can run in standalone mode (as a single process, with the option to run single or multi-threaded), as a Spark Straming or MapReduce job on a cluster, or in an ultralight agent (StreamSets Data Collector Edge). Java based, Open Source under the Apache 2.0 licence, hosted on GitHub, with development led by StreamSets who also provide commercial support and a number of commercial add-ons, including Control Hub (cloud service for developing and managing pipelines), Dataflow Performance Manager (for managing data metrics) and Data Protector (for managing senstive data). Started in October 2014, with a v1.0 release in September 2015.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;StreamSets&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;October 2018 - v3.5&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;td&gt;2017-12-15&lt;/td&gt; &lt;td&gt;See 3.0 notes on &lt;a href=&quot;https://streamsets.com/documentation-page/&quot;&gt;documentation and release page&lt;/a&gt;; &lt;a href=&quot;https://streamsets.com/blog/announcing-streamsets-data-collector-version-3-0/&quot;&gt;blog post&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;3.1&lt;/td&gt; &lt;td&gt;2017-03-30&lt;/td&gt; &lt;td&gt;See 3.1 notes on &lt;a href=&quot;https://streamsets.com/documentation-page/&quot;&gt;documentation and release page&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;3.2&lt;/td&gt; &lt;td&gt;2018-05-11&lt;/td&gt; &lt;td&gt;See 3.2 notes on &lt;a href=&quot;https://streamsets.com/documentation-page/&quot;&gt;documentation and release page&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;3.3&lt;/td&gt; &lt;td&gt;2018-05-24&lt;/td&gt; &lt;td&gt;See 3.3 notes on &lt;a href=&quot;https://streamsets.com/documentation-page/&quot;&gt;documentation and release page&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;3.4&lt;/td&gt; &lt;td&gt;2018-08-10&lt;/td&gt; &lt;td&gt;See 3.4 notes on &lt;a href=&quot;https://streamsets.com/documentation-page/&quot;&gt;documentation and release page&lt;/a&gt;; &lt;a href=&quot;https://streamsets.com/blog/streamsets-enhances-its-dataops-platform/&quot;&gt;blog post&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;2018-10-01&lt;/td&gt; &lt;td&gt;See 3.5 notes on &lt;a href=&quot;https://streamsets.com/documentation-page/&quot;&gt;documentation and release page&lt;/a&gt;; &lt;a href=&quot;https://streamsets.com/blog/streamsets-announces-control-hub-version-3-4-0-and-streamsets-data-collector-version-3-5-0/&quot;&gt;blog post&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://streamsets.com/product/&quot;&gt;https://streamsets.com/product/&lt;/a&gt; - product homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://streamsets.com/docs/&quot;&gt;https://streamsets.com/docs/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/streamsets/datacollector&quot;&gt;https://github.com/streamsets/datacollector&lt;/a&gt; - source code&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://streamsets.com/blog&quot;&gt;https://streamsets.com/blog&lt;/a&gt; - blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://streamsets.com/documentation-page/&quot;&gt;https://streamsets.com/documentation-page/&lt;/a&gt; - Documentation and release history&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Gobblin (Incubating)</title><link>https://ondataengineering.net/technologies/apache-gobblin/</link><description> &lt;p&gt;Java based framework for ingesting data into Hadoop. Ingestion jobs are defined through job configuration files, and are made up of a number of stages - a Source identifies work to be done and generates Work Units which are then processed by Tasks, with Tasks consisting of an Extractor (reads the records to be processed), one or more Converters (a 1:N transformation of records), a Quality Checker (covers both record and file checks), a Fork Operator (allows data to be written to multiple targets) and a Writer (writes out completed records), with the output of a completed task being committed by a Publisher. Gobblin ships with a number of standard components, including support for a range of sources and targets, as well as supporting custom implementations of any stage. Jobs can be run using a number of frameworks, including MapReduce (with all tasks running as mapper only jobs), YARN, and as Java threads within a single JVM, with some modes also supporting an internal scheduler and job management engine. Supports job locks (to ensure multiple instances of the same job don't run at the same time), job history metadata (via a job execution history store that supports a REST API that can be used to monitor jobs), exactly-once processing support (via Publisher commits), failure handling (retrying both within and across jobs), capture and forwarding of execution and data quality metrics, post processing of data (e.g. to remove duplicates or generate aggregations), partitioned writers, job configuration file templates, Hive table registration, high availability, data retention management (automatically deleting old data according to a number of retention rules), and data purging (Gobblin Compliance). Developed at LinkedIn from late 2013, first announced in November 2014 and open sourced shortly afterwards, before being donated to the Apache Foundation in February 2017, and with stated deployments at a number of large organisations.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Apache Gobblin, Gobblin&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Open Source - Active&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;September 2018 - v0.13&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.5&lt;/td&gt; &lt;td&gt;2015-09-28&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://engineering.linkedin.com/big-data/bridging-batch-and-streaming-data-ingestion-gobblin&quot;&gt;Annoucement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.6&lt;/td&gt; &lt;td&gt;2015-12-21&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://github.com/apache/incubator-gobblin/releases/tag/gobblin_0.6.0&quot;&gt;GitHub release page&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.7&lt;/td&gt; &lt;td&gt;2016-05-18&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://github.com/apache/incubator-gobblin/releases/tag/gobblin_0.7.0&quot;&gt;GitHub release page&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://engineering.linkedin.com/blog/2016/06/announcing-gobblin-0-7-0--going-beyond-ingestion&quot;&gt;Announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Dataset lifecycle features&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.8&lt;/td&gt; &lt;td&gt;2016-09-03&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://github.com/apache/incubator-gobblin/releases/tag/gobblin_0.8.0&quot;&gt;GitHub release page&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.9&lt;/td&gt; &lt;td&gt;2016-12-19&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://github.com/apache/incubator-gobblin/releases/tag/gobblin_0.9.0&quot;&gt;GitHub release page&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.10&lt;/td&gt; &lt;td&gt;2017-05-05&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://github.com/apache/incubator-gobblin/releases/tag/gobblin_0.10.0&quot;&gt;GitHub release page&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt;First Apache release&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.11&lt;/td&gt; &lt;td&gt;2017-07-20&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://github.com/apache/incubator-gobblin/releases/tag/gobblin_0.11.0&quot;&gt;GitHub release page&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.12&lt;/td&gt; &lt;td&gt;2018-07-02&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://github.com/apache/incubator-gobblin/releases/tag/release-0.12.0&quot;&gt;GitHub release page&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.13&lt;/td&gt; &lt;td&gt;2018-09-20&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://github.com/apache/incubator-gobblin/releases/tag/release-0.13.0&quot;&gt;GitHub release page&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://gobblin.apache.org/&quot;&gt;http://gobblin.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://gobblin.readthedocs.io/en/latest/l&quot;&gt;http://gobblin.readthedocs.io/en/latest/l&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://engineering.linkedin.com/data-ingestion/gobblin-big-data-ease&quot;&gt;https://engineering.linkedin.com/data-ingestion/gobblin-big-data-ease&lt;/a&gt; - announcement&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.slideshare.net/ShirshankaDas/apache-gobblin-bridging-batch-and-streaming-data-integration-big-data-meetup-2017&quot;&gt;https://www.slideshare.net/ShirshankaDas/apache-gobblin-bridging-batch-and-streaming-data-integration-big-data-meetup-2017&lt;/a&gt; - presentation from May 2017&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/apache/incubator-gobblin/releases&quot;&gt;https://github.com/apache/incubator-gobblin/releases&lt;/a&gt; - release announcements / history&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Cloudera Altus Data Engineering</title><link>https://ondataengineering.net/technologies/cloudera-altus/data-engineering/</link><description> &lt;p&gt;Managed service for the execution of Spark, MapReduce or Hive (over MapReduce or Spark) jobs using managed CDH clusters on AWS and Azure cloud infrastructure over data in Amazon S3 or Azure Data Lake Storage (ADLS). Jobs run on clusters within a defined AWS or Azure environment, which can be transient (created and terminated on demand) or persistent, with each cluster supporting one service type (Hive, Spark, MapReduce) with a fixed node count. Jobs can then be queued individually or in batch for execution against an existing cluster or against a dynamically created cluster, with jobs specified either by uploading a JAR to S3 (for Spark or MapReduce) or via a Hive script (either directly uploaded or uploaded to S3), and the ability to either halt or continue the queue on job failure. Supports access to clusters via SSH, read only access to Cloudera Manager, a SOCKS proxy to cluster web UIs (including the CM admin console, YARN history server and Spark history server), and access to server and workload logs (including the ability to write these to S3 for access after clusters have been terminated). All nodes managed by Altus are tagged with the cluster name and node role (master, worker or Cloudera Manager) and bootstrap scripts can be specified for execution on nodes after cluster startup.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/cloudera-altus/&quot;&gt;Cloudera Altus&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;May 2018&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Packages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/map-reduce/&quot;&gt;MapReduce&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Apache Hive&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/products/altus/altus-data-engineering.html&quot;&gt;https://www.cloudera.com/products/altus/altus-data-engineering.html&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/documentation/altus/topics/altde_overview.html&quot;&gt;https://www.cloudera.com/documentation/altus/topics/altde_overview.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2017/05/data-engineering-with-cloudera-altus/&quot;&gt;http://blog.cloudera.com/blog/2017/05/data-engineering-with-cloudera-altus/&lt;/a&gt; - intro blog post&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Cloudera Altus Data Warehouse</title><link>https://ondataengineering.net/technologies/cloudera-altus/data-warehouse/</link><description> &lt;p&gt;Impala as a managed service, supporting the dynamic provisionng of Impala clusters on AWS and Azure cloud infrastructure over data in Amazon S3 or Azure Data Lake Storage (ADLS). Clusters consist of a coordinator node and multiple worker nodes, with read-only access to a Cloudera Manager instance, with the node count fixed on creation. Supports JDBC and ODBC access to data, along with access to clusters via SSH, read only access to Cloudera Manager and a SOCKS proxy for access to the Impala web UIs. Previously known as Cloudera Altus Analytical DB.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Cloudera Altus Analytical DB&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/cloudera-altus/&quot;&gt;Cloudera Altus&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;September 2019&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Packages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-impala/&quot;&gt;Apache Impala&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/products/altus/altus-data-warehouse.html&quot;&gt;https://www.cloudera.com/products/altus/altus-data-warehouse.html&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/documentation/altus/topics/altdw_overview.html&quot;&gt;https://www.cloudera.com/documentation/altus/topics/altdw_overview.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://vision.cloudera.com/introducing-cloudera-altus-analytic-db-beta-for-cloud-based-data-warehousing/&quot;&gt;http://vision.cloudera.com/introducing-cloudera-altus-analytic-db-beta-for-cloud-based-data-warehousing/&lt;/a&gt; - intro blog post&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2018/02/a-technical-overview-of-cloudera-altus-analytic-db/&quot;&gt;http://blog.cloudera.com/blog/2018/02/a-technical-overview-of-cloudera-altus-analytic-db/&lt;/a&gt; - technical overview&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>NiFi Registry</title><link>https://ondataengineering.net/technologies/apache-nifi/registry/</link><description> &lt;p&gt;A solution for the configuration management of NiFi flows. Integrates with NiFi to allow users to store, retrieve and upgrade flows, keeping a full history of all changes to a flow committed to the registry, with flows stored and organised by buckets. Supports local users and groups, or authentication via certificates, LDAP or Kerberos, with access control policies allowing read, write and delete permissions to be specified for buckets, users and groups. Has a Web based UI and a REST interface for managing buckets, local users and groups, viewing flow history and for managing access control. First released in January 2018.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-nifi/&quot;&gt;Apache NiFi&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;October 2018 - v0.3&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-flow/&quot;&gt;Hortonworks DataFlow&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.1&lt;/td&gt; &lt;td&gt;2018-01-01&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/NIFI/Release+Notes#ReleaseNotes-NiFiRegistry0.1.0&quot;&gt;summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.2&lt;/td&gt; &lt;td&gt;2018-06-18&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/NIFI/Release+Notes#ReleaseNotes-NiFiRegistry0.2.0&quot;&gt;summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.3&lt;/td&gt; &lt;td&gt;2018-09-25&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/NIFI/Release+Notes#ReleaseNotes-NiFiRegistry0.3.0&quot;&gt;summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://nifi.apache.org/registry.html&quot;&gt;https://nifi.apache.org/registry.html&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://nifi.apache.org/docs/nifi-registry-docs/index.html&quot;&gt;https://nifi.apache.org/docs/nifi-registry-docs/index.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDF3/HDF-3.1.0/index.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDF3/HDF-3.1.0/index.html&lt;/a&gt; - Hortonworks documentation (as part of HDF 3.1)&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>OpenLink Virtuoso Universal Server</title><link>https://ondataengineering.net/technologies/openlink-virtuoso-universal-server/</link><description> &lt;p&gt;Multi-model database (RDBMS, VDBMS) supporting tabular relational (SQL), graph relational (SPARQL), hybrid (SPARQL-in-SQL a/k/a SPASQL), XML (XPath, XQuery, XSLT), filesystem/objects, and other forms of data; First shipped in 1999, available as Open Source or Enterprise Edition; various add-ons available for Enterprise Edition; virtualizes local and/or remote tabular relational databases and/or other data sources as RDF semantic web data sources.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;OpenLink&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;March 2018&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://virtuoso.openlinksw.com/&quot;&gt;https://virtuoso.openlinksw.com/&lt;/a&gt; - product home page&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>HDDS</title><link>https://ondataengineering.net/technologies/apache-hadoop/hdds/</link><description> &lt;p&gt;A common distributed and resilient block storage layer that will eventually underpin HDFS and Ozone, delivering increased scalability. Implemented as a Storage Container Manager (SCM) service (that performs block management) and DataNode services (inherited from HDFS that run on storage nodes and manage block IO). Blocks are arranged into containers (with the replication strategy defined at the container level). Currently under active development as part of the development of Ozone. Previously known as HDSL (Hadoop Distributed Storage Layer)&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Hadoop Distributed Data Storage&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Apache Hadoop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;September 2018&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is used by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/ozone/&quot;&gt;Ozone&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-7240&quot;&gt;https://issues.apache.org/jira/browse/HDFS-7240&lt;/a&gt; - original JIRA case&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.slideshare.net/Hadoop_Summit/ozone-scaling-hdfs-to-trillions-of-objects-103004375&quot;&gt;https://www.slideshare.net/Hadoop_Summit/ozone-scaling-hdfs-to-trillions-of-objects-103004375&lt;/a&gt; - HDDS presentation&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Ozone</title><link>https://ondataengineering.net/technologies/apache-hadoop/ozone/</link><description> &lt;p&gt;An object store built on top of the new Hadoop HDDS block storage layer that can co-exist with HDFS. Implemented as an Ozone Manager (OM) service that manages the object store namespace, utilising the HDDS Storage Container Manager for block management. Objects are arranged into buckets, which themselves are arranged into volumes. Supports consistent writes, an RPC API, an Amazon S3 compatible REST API, a CLI, a load generation tool (Freon, previously Corona), and an Hadoop Compatible File System (OzoneFS), with a stated plan for mountable LUN storage (Quadra). Originally announced in October 2014, re-invigorated under the Hortonwworks Open Hybrid Architecture Initiative in September 2018, and currently under active development with a suggested release as part of HDP 3.2.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Apache Hadoop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;October 2018 - v0.2-alpha&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Uses&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/hdds/&quot;&gt;HDDS&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.2-alpha&lt;/td&gt; &lt;td&gt;2018-10-01&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://hadoop.apache.org/news/2018-10-01-ozone-0.2.1-alpha.html&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://ozone.hadoop.apache.org/&quot;&gt;http://ozone.hadoop.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/ozone-object-store-hdfs/&quot;&gt;https://hortonworks.com/blog/ozone-object-store-hdfs/&lt;/a&gt; - initial blog post&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.slideshare.net/Hadoop_Summit/ozone-and-hdfsozone-and-hdfss-evolution&quot;&gt;https://www.slideshare.net/Hadoop_Summit/ozone-and-hdfsozone-and-hdfss-evolution&lt;/a&gt; - Ozone, HDDS and Quadra presentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/apache/hadoop/tree/trunk/hadoop-ozone/docs/content&quot;&gt;https://github.com/apache/hadoop/tree/trunk/hadoop-ozone/docs/content&lt;/a&gt; - documentation source&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-13074&quot;&gt;https://issues.apache.org/jira/browse/HDFS-13074&lt;/a&gt; - original JIRA case&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Data Analytics Studio</title><link>https://ondataengineering.net/technologies/hortonworks-dataplane/data-analytics-studio/</link><description> &lt;p&gt;A DataPlane application for running Hive queries, managing Hive tables, and diagnosing Hive query performance issues. Supports a query editor (with autocomplete, a visual explain plan, performance improvement recommendations, saved queries and results downloading), a query search tool (with pre-defined queries for expensive, long running, non-optimised and failed queries, a range of filters and saved searches), a database management tool (supporting searching, browsing, interrogation, creation and modification of databases, tables, partitions and columns as well as uploading of data from local storage or HDFS) and table impact reporting (showing reads, writes, projections, aggregations, filters and joins by table and column, with support for dynamic heatmaps overlaid on entity relationship diagrams). Requires a Ambari mangement pack (the DAS engine) to be installed on all clusters.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-dataplane/&quot;&gt;Hortonworks DataPlane&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;September 2018 - 1.0&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;td&gt;2018-09-12&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt;Initial GA release&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/products/dataplane/data-analytics-studio/&quot;&gt;https://hortonworks.com/products/dataplane/data-analytics-studio/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/DAS/DAS-1.0.0/index.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/DAS/DAS-1.0.0/index.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/announcing-general-availability-data-analytics-studio/&quot;&gt;https://hortonworks.com/blog/announcing-general-availability-data-analytics-studio/&lt;/a&gt; - intro blog post&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Data Lifecycle Manager</title><link>https://ondataengineering.net/technologies/hortonworks-dataplane/data-lifecycle-manager/</link><description> &lt;p&gt;A DataPlane application for replicating HDFS and Hive data between two clusters along with any associated metadata and security policies. Clusters already registered with DataPlane can be paired, at which point replication policies can be defined, which result in replication jobs running at the selected interval. Supports replicating between HDFS and cloud object storage (with some caveats around replication of security policies), replication of encrypted HDFS data, TLS encryption of replication streams, reporting on and management of replication jobs and HDFS snapshottable directories, with jobs executed by DLM Engine processes on the appropriate cluster. Stated future plans include support for automatic tiering of data between clusters and point in time backup and restore.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-dataplane/&quot;&gt;Hortonworks DataPlane&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;September 2018 - 1.1&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;td&gt;2017-11-01&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt;Alongside DataPlane 1.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.1&lt;/td&gt; &lt;td&gt;unknown&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/DLM1/DLM-1.1.0/release-notes/content/dlm_whats_new_in_this_release.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/DLM1/DLM-1.1.0/release-notes/content/dlm_whats_new_in_this_release.html&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Object store, encryption and automatic snapshot support&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/products/dataplane/data-lifecycle-manager/&quot;&gt;https://hortonworks.com/products/dataplane/data-lifecycle-manager/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/DLM1/DLM-1.1.2/index.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/DLM1/DLM-1.1.2/index.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/painless-disaster-recovery-using-hortonworks-data-lifecycle-manager/&quot;&gt;https://hortonworks.com/blog/painless-disaster-recovery-using-hortonworks-data-lifecycle-manager/&lt;/a&gt; - intro blog post&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/step-step-guide-hdfs-replication/&quot;&gt;https://hortonworks.com/blog/step-step-guide-hdfs-replication/&lt;/a&gt; - walkthrough guide&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/hortonworks/dlm_engine&quot;&gt;https://github.com/hortonworks/dlm_engine&lt;/a&gt; - engine source code&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/hortonworks/dlm_app&quot;&gt;https://github.com/hortonworks/dlm_app&lt;/a&gt; - app source code&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Data Steward Studio</title><link>https://ondataengineering.net/technologies/hortonworks-dataplane/data-steward-studio/</link><description> &lt;p&gt;A DataPlane application for viewing and understanding data assets, with supported data assets currently limited to Hive tables on clusters with Atlas and Ranger installed. Supports viewing metadata associated with data assets (including properties, lineage, security policies and audit logs), profiling of data (with profiling performed by a background Spark process, with support for data summarisation, identifying sensitive/personal data and profiling user access to data), grouping of data assets into asset collections, taging and rating of data assets and collections and dashboard views of metadata by cluster and collection.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-dataplane/&quot;&gt;Hortonworks DataPlane&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;September 2018 - 1.2&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;td&gt;2017-11-01&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt;Technical Preview alongside DataPlane 1.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.2&lt;/td&gt; &lt;td&gt;unknown&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/DSS1/DSS-1.2.0/release-notes/content/dss_whats_new_in_this_release.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/DSS1/DSS-1.2.0/release-notes/content/dss_whats_new_in_this_release.html&lt;/a&gt;&lt;/td&gt; &lt;td&gt;GA release, dashboard views, sensitive data profiler&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/products/dataplane/data-steward-studio/&quot;&gt;https://hortonworks.com/products/dataplane/data-steward-studio/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/DSS1/DSS-1.2.1/index.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/DSS1/DSS-1.2.1/index.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/discover-data-steward-studio-dss-understand-hybrid-data-lakes-exploit-business-value-part-2/&quot;&gt;https://hortonworks.com/blog/discover-data-steward-studio-dss-understand-hybrid-data-lakes-exploit-business-value-part-2/&lt;/a&gt; - walkthrough blog post&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/hortonworks/dss_app&quot;&gt;https://github.com/hortonworks/dss_app&lt;/a&gt; - app source code&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/hortonworks/profiler_service&quot;&gt;https://github.com/hortonworks/profiler_service&lt;/a&gt; - profiler source code&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Streams Messaging Manager</title><link>https://ondataengineering.net/technologies/hortonworks-dataplane/streams-messaging-manager/</link><description> &lt;p&gt;A DataPlane application for monitoring Apache Kafka clusters. Provides an overview view of producers, topics (and their partitions), brokers and consumer groups, showing key statistics and the connections between them, with the ability to propagate filters based on these connections. Also provides detail views, profiles and historic graphs for each producer, topic, broker and consumer group, with the ability to link out to Atlas to see end to end lineage and Ambari Grafana for detailed metrics. Metrics and status information is also provided over a REST API, with a REST Server Agent running on each cluster being monitored.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-dataplane/&quot;&gt;Hortonworks DataPlane&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;September 2018 - 1.0&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Manages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;td&gt;2018-08-23&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt;Initial GA release&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/products/dataplane/streams-messaging-manager/&quot;&gt;https://hortonworks.com/products/dataplane/streams-messaging-manager/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/SMM/SMM-1.0.0/index.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/SMM/SMM-1.0.0/index.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/introducing-hortonworks-streams-messaging-manager-smm/&quot;&gt;https://hortonworks.com/blog/introducing-hortonworks-streams-messaging-manager-smm/&lt;/a&gt; - introduction&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/solving-needs-devops-developers-streams-messaging-manager/&quot;&gt;https://hortonworks.com/blog/solving-needs-devops-developers-streams-messaging-manager/&lt;/a&gt; - walkthrough&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/hortonworks/smm_server&quot;&gt;https://github.com/hortonworks/smm_server&lt;/a&gt; - service source code&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/hortonworks/smm_app&quot;&gt;https://github.com/hortonworks/smm_app&lt;/a&gt; - app source code&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Presto</title><link>https://ondataengineering.net/technologies/presto/</link><description> &lt;p&gt;An MPP query engine that supports queries over one or more underlying databases with the ability to join data from multiple datastores together. Supports a range of underlying technologies including Accumulo, Cassandra, Hive (HDFS), Kafka, Kudu, Redshift and a number of relational databases, with schemas read from the underlying database and cached within Presto. Architecture consists of a co-coordinator node that parses and plan queries, and worker nodes that execute tasks and process data. Extensible for new database connectors, data types, functions, access control schemas and event listeners. Supports resource management, spilling to disks when processing large results sets, a cost based optimiser, Kerberos and LDAP authentication, a CLI, and web interface for monitoring and managing queries and JDBC and ODBC drivers. Created at Facebook, announced and open sourced in 2013. Commercial support was originally provided by Hadapt, which was acquired by Teradata in 2015, before being spun out as Starburst in late 2017, who now provide an enterprise distribution and commercial support and services.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;Starburst&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commerical Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;October 2018&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/amazon-emr/&quot;&gt;Amazon EMR&lt;/a&gt;, &lt;a href=&quot;/technologies/qubole-data-service/&quot;&gt;Qubole Data Service&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://prestodb.io/&quot;&gt;https://prestodb.io/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.starburstdata.com/&quot;&gt;https://www.starburstdata.com/&lt;/a&gt; - Starburst homepage&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.starburstdata.com/category/news/&quot;&gt;https://www.starburstdata.com/category/news/&lt;/a&gt; - Starbust news&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Amazon EMR</title><link>https://ondataengineering.net/technologies/amazon-emr/</link><description> &lt;p&gt;Service for dynamically provisioning Hadoop clusters on Amazon EC2 infrastructure, with one of more Hadoop based services pre-installed and configured. Supports selection of EC2 instance types, EC2 spot and reserved instances, programmatic execution of service jobs (steps), persistent or transient (terminate after pre-defined steps have been executed) clusters, automatic or manual scaling of live clusters, cloning of clusters, HDFS on local (EBS) node storage, an HDFS compatible filesystem (EMR File System - EMRFS) for accessing Amazon S3 storage (that supports consistency using DynamoDB for metadata), automatic configuration of Hadoop clusters and firewalls, integration with AWS CloudWatch and AWS Identity and Access Management, Hadoop encryption and Kerberos authentication, persistent storage of Hive metadata in AWS Glue Data Catalog, and bootstrap actions for custom configuration or installation of other services (with a GitHub repo of open source bootstrap action extensions). Manageable via the AWS Management Console, the AWS CLI, a REST API and a range of SDKs. Priced at an hourly rate (charged per second) based on the EC2 instance types being used, which is in addition to any EC2 or EBS charges.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;EMR; Elastic Map Reduce&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;October 2018 - v5.17&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Packages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-flink/&quot;&gt;Apache Flink&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Apache Hadoop&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hbase/&quot;&gt;Apache HBase&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Apache Hive&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hive/hcatalog/&quot;&gt;HCatalog&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-livy/&quot;&gt;Apache Livy&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-mahout/&quot;&gt;Apache Mahout&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-oozie/&quot;&gt;Apache Oozie&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-phoenix/&quot;&gt;Apache Phoenix&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-pig/&quot;&gt;Apache Pig&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-sqoop/&quot;&gt;Apache Sqoop&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-tez/&quot;&gt;Apache Tez&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-zeppelin/&quot;&gt;Apache Zeppelin&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-zookeeper/&quot;&gt;Apache ZooKeeper&lt;/a&gt;, &lt;a href=&quot;/technologies/hue/&quot;&gt;Hue&lt;/a&gt;, &lt;a href=&quot;/technologies/presto/&quot;&gt;Presto&lt;/a&gt;, Ganglia, JupyterHub, MXNet, TensorFlow&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/emr/&quot;&gt;https://aws.amazon.com/emr/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.aws.amazon.com/emr/index.html&quot;&gt;https://docs.aws.amazon.com/emr/index.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/awslabs/emr-bootstrap-actions&quot;&gt;https://github.com/awslabs/emr-bootstrap-actions&lt;/a&gt; - GitHub repo of bootstrap actions&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://docs.aws.amazon.com/emr/latest/ReleaseGuide/amazon-emr-release-notes.rss&quot;&gt;https://docs.aws.amazon.com/emr/latest/ReleaseGuide/amazon-emr-release-notes.rss&lt;/a&gt; - RSS release notes feed&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Azure HDInsight</title><link>https://ondataengineering.net/technologies/azure-hdinsight/</link><description> &lt;p&gt;Service for dynamically provisioning Hadoop clusters on Azure Virtual Machines based on a set of pre-defined cluster templates for Hadoop, Spark, HBase, Storm, Hive LLAP, Kafka or Machine Learning. Based on the Hortonworks HDP distribution of Hadoop, with support for Azure Blob Storage and Azure Data Lake Storage (both strongly consistent) but not local HDFS. Supports manual scaling of in-flight clusters, integration with Azure Log Analytics, encryption, use of external SQL database for Hive metadata and script actions (scripts that can be run during or after cluster creation). Comes with an Enterprise Security Package add-on that adds integration with Azure Active Directory, role based access control for Hive and Spark using Apache Ranger and security audit logs. Manageable via the Azure Portal, Powershell, a REST API and integrates with a number of development IDEs (e.g. for interactive development of Spark jobs). Priced at an hourly rate (billed per minute) based on the VM instance types being used, in addition to any Virtual Machine charges.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;HDInsight&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;October 2018 - v4.0&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Packages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;bundled-technologies&quot;&gt;Bundled Technologies&lt;/h2&gt; &lt;p&gt;Note that HDInsight is largely based on HDP releases, however it doesn’t include some components (Atlas, Accumulo, Knox, Solr).&lt;/p&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;4.0 (preview)&lt;/td&gt; &lt;td&gt;2018-09-25&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://azure.microsoft.com/en-gb/blog/deep-dive-into-azure-hdinsight-4-0/&quot;&gt;blog post&lt;/a&gt;&lt;/td&gt; &lt;td&gt;HDP 3.0; Hive LLAP; HBase 2.0&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://azure.microsoft.com/en-us/services/hdinsight/&quot;&gt;https://azure.microsoft.com/en-us/services/hdinsight/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/azure/hdinsight/&quot;&gt;https://docs.microsoft.com/en-us/azure/hdinsight/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-release-notes&quot;&gt;https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-release-notes&lt;/a&gt; - release notes&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-component-versioning&quot;&gt;https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-component-versioning&lt;/a&gt; - component versions&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;p&gt;See &lt;a href=&quot;/tech-vendors/microsoft-azure/&quot;&gt;Azure&lt;/a&gt; updates&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Google Cloud DataProc</title><link>https://ondataengineering.net/technologies/google-cloud-dataproc/</link><description> &lt;p&gt;Service for dynamically provisioning Hadoop clusters on Google Compute Engine based on a single standard set of Hadoop services. Supports selection of virtual machines (including custom machine types and machines with GPUs), usage of custom VM images, a claimed cluster startup time of less than 90 seconds, local storage and HDFS filesystem, programmatic execution of jobs, workflows (parameterisable operations that create clusters, run jobs and then delete the cluster), manual and automatic scaling, initialisation actions (to install extra services or run scripts, with a set of open source actions available), optional components (automatic addition of extra services), automatic deletion of clusters (based on time, usage or idleness), integration with Stackdriver Logging and Monitoring and encryption of data in HDFS and Cloud Storage. Manageable via the Google Cloud Console Web UI and SDK plus an RPC and REST API. Priced an an hourly rate (charged per second) based on the specification of the VMs being used, which is in addition to any Compute Engine or Persistent Disk charges.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Google DataProc, DataProc&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;October 2018 - v1.3&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Packages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Apache Hadoop&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Apache Hive&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-pig/&quot;&gt;Apache Pig&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-tez/&quot;&gt;Apache Tez&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/dataproc/&quot;&gt;https://cloud.google.com/dataproc/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions&quot;&gt;https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions&lt;/a&gt; - bundle services version list&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/dataproc/docs/release-notes&quot;&gt;https://cloud.google.com/dataproc/docs/release-notes&lt;/a&gt; - release notes&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/dataproc/docs/&quot;&gt;https://cloud.google.com/dataproc/docs/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/GoogleCloudPlatform/dataproc-initialization-actions&quot;&gt;https://github.com/GoogleCloudPlatform/dataproc-initialization-actions&lt;/a&gt; - open source initialization actions&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;p&gt;See &lt;a href=&quot;/tech-vendors/google-cloud-platform/&quot;&gt;Google Cloud Platform&lt;/a&gt; updates&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Qubole Data Service</title><link>https://ondataengineering.net/technologies/qubole-data-service/</link><description> &lt;p&gt;Hadoop as a managed service over AWS, Azure and Oracle Cloud. Supports Airflow, Hadoop, Presto and Spark cluster types, automatic management (starting, stopping and scaling) of clusters based on workload, automatic shared Hive metastores within accounts, role based access control (to accounts, clusters and UI/API functionality, with Hive authorisation to manage access to data), connectivity to external databases (Data Stores), labelling of clusters and routing of commands by label (allowing graceful cluster upgrades), custom node bootstrap commands, encryption, auditing, data caching (on AWS only via open source Rubix project), ODBC/JDBC drives. Has a rich web based user interface that supports exploration of data (in Hadoop, object stores and connected external databases), a command composer with auto completion (supporting Hive, Presto, Pig, Shell, Spark and Worklow commands) with auto completion and command history, parameterisable command templates, data management (import, export and upload), a visual query builder (Smart Query), Zeppelin based notebooks (including publication of public read only notebook views), command schedulers, cluster management and a range of usage and cluster metrics and graphs. Also supports a REST API. Priced per hour based on the cloud infrastructure being used, which is in addition to any cloud vendor costs. Launched in 2013.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;QDS&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;October 2018 - R53&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Packages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Apache Hadoop&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Apache Hive&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-pig/&quot;&gt;Apache Pig&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-sqoop/&quot;&gt;Apache Sqoop&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-tez/&quot;&gt;Apache Tez&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-zeppelin/&quot;&gt;Apache Zeppelin&lt;/a&gt;, &lt;a href=&quot;/technologies/presto/&quot;&gt;Presto&lt;/a&gt;, Airflow, TensorFlow&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.qubole.com/&quot;&gt;https://www.qubole.com/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.qubole.com/en/latest/&quot;&gt;https://docs.qubole.com/en/latest/&lt;/a&gt; - docs&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.qubole.com/en/latest/admin-guide/osversionsupport.html&quot;&gt;https://docs.qubole.com/en/latest/admin-guide/osversionsupport.html&lt;/a&gt; - supported component versions&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://rubix.readthedocs.io/en/latest/&quot;&gt;https://rubix.readthedocs.io/en/latest/&lt;/a&gt; - Rubix&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.qubole.com/blog/&quot;&gt;https://www.qubole.com/blog/&lt;/a&gt; - Qubole blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.qubole.com/en/latest/release-notes/index.html&quot;&gt;https://docs.qubole.com/en/latest/release-notes/index.html&lt;/a&gt; - release notes&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> </channel> </rss>
