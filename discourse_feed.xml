<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
	<channel>
		<title>OnDataEngineering</title>
		<description>A collaborative site for independent, critical and technical thinking on the use cases, architectural patterns and technologies relating to the transformation and preparation of data for exploitation.</description>
		<link>http://ondataengineering.net/</link>
		
			<item>
				<title>Content License</title>
				<link>http://ondataengineering.net/site/content-license/</link>
				<description>&lt;p&gt;The content of the OnDataEngineering.net site is copyright the relevant author and is licensed under a &lt;a href=&quot;http://creativecommons.org/licenses/by/4.0/&quot;&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Unless otherwise stated or granted, any code snippets or samples included in the content of the OnDataEngineering.net site are copyright their respective authors and are licensed under the &lt;a href=&quot;http://mit-license.org/&quot;&gt;MIT Licence&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The content of the OnDataEngineering.net site is defined as the main body of the pages in the &lt;a href=&quot;http://ondataengineering.net/technologies/&quot;&gt;Technologies&lt;/a&gt;, &lt;a href=&quot;/tech-categories/&quot;&gt;Tech Categories&lt;/a&gt;, &lt;a href=&quot;/tech-vendors/&quot;&gt;Tech Vendors&lt;/a&gt; and &lt;a href=&quot;http://ondataengineering.net/blog/&quot;&gt;Blog&lt;/a&gt; sections of the site.&lt;/p&gt;

&lt;p&gt;For clarity, the CC BY 4.0 licence means that this content can be used for any commercial or non commercial purpose, as long as OnDataEngineering.net is clearly credited as the source of the information. Please see the &lt;a href=&quot;http://creativecommons.org/licenses/by/4.0/&quot;&gt;CC BY 4.0 licence summary&lt;/a&gt; for further details.&lt;/p&gt;
</description>
				<discourse_author>Peter</discourse_author>
			</item>
		
			<item>
				<title>Contributing</title>
				<link>http://ondataengineering.net/site/contributing/</link>
				<description>&lt;p&gt;&lt;a href=&quot;http://ondataengineering.net&quot;&gt;OnDataEngineering&lt;/a&gt; is an open collaborative site, and lives on your contributions and participation in the community around the site.  So welcome, it?s good to see you - can we suggest that a good place to start is the &lt;a href=&quot;/site/&quot;&gt;site information&lt;/a&gt; for more details on who we are and what we?re trying to do.&lt;/p&gt;

&lt;p&gt;And before you go any further, can we ask you to sign up to our &lt;a href=&quot;http://discourse.ondataengineering.net&quot;&gt;Discourse forums&lt;/a&gt;. You?ll get updates on changes to the site, notifications of popular discussions, and can use this to discuss anything relating to this site including how you can help and contribute.  While you?re there, please take a read of the &lt;a href=&quot;http://discourse.ondataengineering.net/t/welcome-to-discourse&quot;&gt;welcome post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Once you?ve done that, we?d welcome your help in building out the content on this site and making it a fantastic resource.  All submission of content is welcomed, and we pride ourselves on being friendly and welcoming place if you want to get involved. The following are a number of ways you can contribute:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Discuss the contents of any page or blog post by clicking on the discuss button next to the page title or the link in the sidebar. Your thoughts, comments and feedback on what?s written on this site will make it?s content better.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you see typos, errors or omissions, or have information to add, edit the contents of any page by clicking on the edit button next to the page title or the link in the sidebar.  This is literally a 30 second process for submitting changes, and will help make the contents of this site better.  These edit links will take you to a GitHub edit page (you?ll need to login or create a login to GitHub first) - make your changes, fill in a description of your change in the form at the bottom of the page and click ?Propose the change?, and then on the next screen click ?Create pull request? twice and you?re done.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As well as editing pages, you can also create new pages by clicking on the create button next to the page title or the link in the sidebar on the main index pages (for example &lt;a href=&quot;/technologies&quot;&gt;here&lt;/a&gt;).  The process is exactly the same as editing a page.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you have any industry news you think we should be aware of then please send it to us as &lt;a href=&quot;mailto:news@ondataengineering.net&quot;&gt;news@ondataengineering.net&lt;/a&gt;. We follow the blogs and web pages listed in the News sections on each technology page, but there?s always a chance we?ll miss stuff, and we may not hear about technologies not yet listed on the site.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;With blog posts, feel free to correct typos or mistakes. If you have a blog post you?d like us to host please feel free to send us one via a pull request, however it?s worth getting in touch first via e-mail or the discussion forums, and we reserve the right to only take blog posts we feel are a good fit (posts trying to push specific technologies or copies of posts from elsewhere are generally not). Blog posts should be thought pieces; if you have factual information that fits elsewhere in the site please contribute it there.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If the GitHub create and edit pages are a bit limited for you, using git and your favourite text editor is surprisingly straightforward. The &lt;a href=&quot;https://guides.github.com/&quot;&gt;GitHub guides&lt;/a&gt; (especially the Hello World and Forking Projects ones) and the &lt;a href=&quot;http://help.github.com/&quot;&gt;GitHub help pages&lt;/a&gt; are great places to start, and we use Visual Studio Code (which is completely free and has great git integration) with the markdownlint and SpellChcker plugins.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;All the content of this site is written in &lt;a href=&quot;https://en.wikipedia.org/wiki/Markdown&quot;&gt;Markdown&lt;/a&gt;.  If you?re new to it, &lt;a href=&quot;http://www.markdowntutorial.com/&quot;&gt;Mardown Tutorial&lt;/a&gt; is a great place to start.  We use the kramdown flavour - see the &lt;a href=&quot;http://kramdown.gettalong.org/quickref.html&quot;&gt;quick reference&lt;/a&gt; and &lt;a href=&quot;http://kramdown.gettalong.org/syntax.html&quot;&gt;syntax&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Oh, and we reserve the right to use the (correct) British spellings of words here, although we won?t object to people using new fangled alternative spellings.&lt;/p&gt;
</description>
				<discourse_author>Peter</discourse_author>
			</item>
		
			<item>
				<title>Site Information</title>
				<link>http://ondataengineering.net/site/</link>
				<description>&lt;p&gt;&lt;a href=&quot;http://ondataengineering.net&quot;&gt;OnDataEngineering&lt;/a&gt; is a site focused on how to make the right data available in the right form in the right place at the right time to allow users to efficiently exploit it.  This covers a range of use cases (including the acquisition and staging of data, ?big data? preparation and data warehousing) to support any type of exploitation (be it reporting, big data analytics, machine learning or any one of a dozen similar capabilities).&lt;/p&gt;

&lt;p&gt;This site currently hosts a catalog of technologies that fit within this space, organised &lt;a href=&quot;/technologies/&quot;&gt;alphabetically&lt;/a&gt;, &lt;a href=&quot;/tech-categories/&quot;&gt;by category&lt;/a&gt; and &lt;a href=&quot;/tech-vendors/&quot;&gt;by vendor&lt;/a&gt;.  We also have a &lt;a href=&quot;/blog/&quot;&gt;blog&lt;/a&gt; that covers updates to the site as well as industry news, and a set of &lt;a href=&quot;http://discourse.ondataengineering.net&quot;&gt;forums&lt;/a&gt; .&lt;/p&gt;

&lt;p&gt;In the future it is planned for this site to also look at key use cases for different technologies within the Data Engineering space along with a set of independent, technical and critical evaluations of various technologies and architectural patterns against these. Please see the &lt;a href=&quot;/blog/2016/12/12/the-plan/&quot;&gt;introduction&lt;/a&gt; blog post for further details.&lt;/p&gt;

&lt;p&gt;This is a community owned and authored site.  All the content on this site is licensed under a Creative Commons Attribution license (see &lt;a href=&quot;/site/content-license/&quot;&gt;here&lt;/a&gt; for details) and the content of the site is hosted in a public GitHub repository &lt;a href=&quot;https://github.com/OnDataEngineering/OnDataEngineeringContent&quot;&gt;here&lt;/a&gt;, with contributions welcomed (see &lt;a href=&quot;/site/contributing/&quot;&gt;here&lt;/a&gt; for details).&lt;/p&gt;

&lt;p&gt;To get in contact with the site administrators please e-mail &lt;a href=&quot;mailto:admin@ondataengineering.net&quot;&gt;admin@ondataengineering.net&lt;/a&gt;&lt;/p&gt;
</description>
				<discourse_author>Peter</discourse_author>
			</item>
		
			<item>
				<title>Search</title>
				<link>http://ondataengineering.net/site/search/</link>
				<description>
&lt;script language=&quot;Javascript&quot; type=&quot;text/javascript&quot;&gt;
	function google_search() {
		var query = document.getElementById(&quot;google-search&quot;).value;
		window.open(&quot;https://www.google.com/search?q=&quot; + query + &quot;+site:&quot; + &quot;http%3A%2F%2Fondataengineering.net%2F&quot;);
	}
&lt;/script&gt;

&lt;form id=&quot;search&quot; onsubmit=&quot;google_search(); return false;&quot;&gt;
	&lt;input type=&quot;text&quot; id=&quot;google-search&quot; placeholder=&quot;Enter search term and hit enter&quot; /&gt;
&lt;/form&gt;
&lt;noscript&gt;
	Search &lt;a href=&quot;https://www.google.com/search?q=site:http%3A%2F%2Fondataengineering.net%2F&quot; target=&quot;_blank&quot;&gt;Google&lt;/a&gt; for:
	&lt;pre&gt;&lt;code&gt;search-term site:http://ondataengineering.net/&lt;/code&gt;&lt;/pre&gt;
&lt;/noscript&gt;

</description>
				<discourse_author>Peter</discourse_author>
			</item>
		
		
			<item>
				<title>Welcome</title>
				<link>http://ondataengineering.net/blog/2016/12/07/welcome/</link>
				<description>&lt;p&gt;For me, one of the biggest challenges in exploiting data (be that through reporting, big data analytics, machine learning or any one of a dozen similar capabilities) is making sure you have the right data in the right place at the right time to allow you to do this efficiently.
&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;For example, &lt;a href=&quot;http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html&quot;&gt;this&lt;/a&gt; article from the New York Times and &lt;a href=&quot;http://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/&quot;&gt;this&lt;/a&gt; more recent one from Forbes talks about how the analysis of big data promises unique business insights, but for big-data scientists there is significant manual ?janitor work? (up to 80% of their time) required to prepare data, and although the research is sponsored by Data Wrangling tool vendors, the conclusions will resonate with many data scientists.  Combine this with the historical cost, delivery speed and agility issues typically associated with delivery data warehouse or reporting solutions, and for me it?s never been clearer that we need to get smarter at how we prepare and manage data.&lt;/p&gt;

&lt;p&gt;Part of the solution to this is better Data Engineering, ensuring the processes, tools, technologies, data platforms, regular data feeds and their data preparation jobs are in place to allow the data to be exploited in an efficient, reliable and repeatable way.  The aim of this site is therefore to try to offer independent, critical and technical thinking on the technologies, architectural patterns and delivery capabilities that can help address this.&lt;/p&gt;

&lt;p&gt;My hope is that this becomes a community owned and authored site of trusted reference material on these topics.  To that end, all the content on this site is licensed under the Creative Commons Attribution 4.0 International License and hosted in a public GitHub repository, and there are a set of Discourse forums for discussions. Details of how to contribute and get involved can be found on every page.&lt;/p&gt;
</description>
				<discourse_author>Peter</discourse_author>
			</item>
			
			<item>
				<title>Big Data</title>
				<link>http://ondataengineering.net/blog/2016/12/09/big-data/</link>
				<description>&lt;p&gt;Before we get stuck in, a short digression to talk about Big Data.
&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;There are many different definitions of Big Data, but for arguments sake let?s say this refers to the exploitation of data that would have previously been uneconomical due to the volume of data, the structure or format of the data (e.g. unstructured, semi-structured, or complex file formats such as video and audio) and the types of analytics required (e.g. path, graph or time series analysis)&lt;/p&gt;

&lt;p&gt;I?ve a couple of comments to make on this topic.&lt;/p&gt;

&lt;p&gt;Firstly, in my (humble) option Big Data is a marketing term (supported by new technologies - primarily Hadoop) that?s been exploited to sell these technologies and to give the industry something to talk about.  However, I think this has been a broadly positive thing, in that it?s brought data analytics to the mainstream, spurred uptake of new technologies and encouraged companies to invest in analytics and data processing that they may not have done previously. In any case it?s probably just about run its course now (as demonstrated as it?s fall down the far side of the hype curve), and has definitely resulted in the devaluation of other (perhaps more traditional) analytical capabilities which still have a role to play and in many cases deliver capabilities that Big Data technologies can?t yet match.&lt;/p&gt;

&lt;p&gt;Secondly, I think there?s been a lot of misinformation about Big Data and Big Data technologies. It?s not a replacement for existing BI/MI and analytical capabilities, and in fact needs to coexist and integrate with these in order to deliver on its promises. It?s not always cheaper or more performant than existing technologies, and won?t always reduce the timescales and costs for analytics or data exploration. And it?s not a new or innovative technology -  I know of companies that were analysing multi-petabyte data stores and doing real time analytics over ten years ago, parallel distributed file systems have been around for a lot longer than that, and there are many established technologies that have data processing capabilities that the new technologies are only just starting to catch up to.&lt;/p&gt;

&lt;p&gt;In terms of this site the plan is to look at the wider picture and take an holistic view of data transformation and exploitation.  I?ve therefore no plans to talk explicitly about Big Data, but in looking at the wider picture we will absolutely cover everything relating to it (the technologies and the new use cases these enable) alongside coverage of other new technologies, more established technologies and capabilities, and the interesting intersection between them all.&lt;/p&gt;
</description>
				<discourse_author>Peter</discourse_author>
			</item>
			
			<item>
				<title>The Plan</title>
				<link>http://ondataengineering.net/blog/2016/12/12/the-plan/</link>
				<description>&lt;p&gt;One more post before we get started.&lt;/p&gt;

&lt;p&gt;The following are my current thoughts for some of the topics I?d like to cover on this site, both as a reference for my future self to look back at my naive optimism, but also if anyone wants to start contributing to any of these now, or to start a discussion on any the later topics to start framing and exploring them.
&lt;!--more--&gt;&lt;/p&gt;

&lt;h2 id=&quot;theme-1---the-technology-catalogue&quot;&gt;Theme 1 - the technology catalogue&lt;/h2&gt;

&lt;p&gt;The plan here is to start building up a technology catalogue by looking at the key vendors in the Data Engineering space. This will only be a start on the technologies I?d expect to see in our catalogue however, so once this is done the plan is to then go through by technology category to complete the catalogue.&lt;/p&gt;

&lt;p&gt;I?d also like to look at providing a concise yet detailed introduction to some technologies that describes exactly what it is, how it works, and what the key features are.  So much material that can be found on the internet is marketing material that glosses over the information I?m interested in knowing to understand whether a technology might meet my use cases and integrate into my environment, and my hope is that I can use this site to address that.&lt;/p&gt;

&lt;h2 id=&quot;theme-2---data-engineering-use-cases&quot;&gt;Theme 2 - data engineering use cases&lt;/h2&gt;

&lt;p&gt;One thing I don?t want to do on this site is define another data ecosystem architecture - there are too many already, most of them are designed to sell specific technologies, and none of them will fit the range of different requirements and constraints that different organisations will have.&lt;/p&gt;

&lt;p&gt;However, what I do want to do is look at the range of different of different use cases that you might use data engineering technologies for, from a Data Lake (and we?ll look at what that overloaded term actually means) to a Data Warehouse (and why they?re still relevant), from the acquisition of data to the preparation of a Query Focused Dataset, and from the management of a data catalogue to the monitoring of data quality metrics.&lt;/p&gt;

&lt;p&gt;I?d then like to look at how different technologies and architectural patterns can support these use cases - how do you implement a Data Lake using Hadoop, what technologies support data governance and data catalogues, and how do the various streaming frameworks compare.&lt;/p&gt;

&lt;p&gt;As part of this I also want to look at the core principles behind Data Transformation, what state of the art in this space looks like, and how the established enterprise technologies compare to the new Open Source upstarts.&lt;/p&gt;

&lt;h2 id=&quot;theme-3---delivery&quot;&gt;Theme 3 - delivery&lt;/h2&gt;

&lt;p&gt;As if the above isn?t already massively ambitious enough, I?d also like to talk about the delivery of data solutions, including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;How we can use best practice delivery concepts (e.g. configuration management, continuous integration and testing, automated deployment, infrastructure and database management) and what these mean within a data solution&lt;/li&gt;
  &lt;li&gt;How we can bring some the new best practices from Lean and Agile into the data space, and what data transformation tools need to do in order to be able to support this&lt;/li&gt;
  &lt;li&gt;Why data projects can have a reputation for late delivery, cost overruns, poor quality data and a high cost of change, and what can be done about this&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I think that will more than do us.  Getting through that lot will take some time, but with help and contributions I think this site could be hugely valuable.&lt;/p&gt;
</description>
				<discourse_author>Peter</discourse_author>
			</item>
			
			<item>
				<title>The Technology Catalogue</title>
				<link>http://ondataengineering.net/blog/2016/12/14/the-technology-catalogue/</link>
				<description>&lt;p&gt;The first step in this journey is going to be creation of a catalogue of the technologies that are going to be of interest to us as we explore the world of data engineering.
&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;The aim is to provide a valuable reference for anyone starting a technology evaluation to understand what technology options they might have for a given situation, or to understand if and how a technology might fit into an existing ecosystem.&lt;/p&gt;

&lt;p&gt;For each technology, the plan is therefore to provide a short summary describing the technology, along with its background and current status.  As mentioned in my previous post, for some technologies, I also want to do a deep dive to provide a longer summary with more detail that gives a solid introduction to the technology, and my hope is that we?ll get contributions to provide these for the vast majority of the technologies that I won?t get time to look at.&lt;/p&gt;

&lt;p&gt;As part of this we?ll need to look at providing a categorisation of technologies, although making this useful is going to be challenging given that multiple categories of technologies could be used to meet a given use case.  The technologies we?ll look at broadly fall into three groups however:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Data Transformation tools, be they one of the established commercial products or a new Open Source technology&lt;/li&gt;
  &lt;li&gt;Data Platforms, be they a traditional relational database, an Hadoop based data platform, a real time broker such as Kafka, or a NoSQL data platform&lt;/li&gt;
  &lt;li&gt;Technologies that address the other supporting capabilities around these, such as data catalogues or metadata management&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The plan is to start by looking at the key vendors in the Data Engineering space including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The pure play Hadoop distributions - Apache Big Top, Hortonworks, Cloudera and MapR&lt;/li&gt;
  &lt;li&gt;The rest of the Apache Data Engineering ecosystem&lt;/li&gt;
  &lt;li&gt;The major Cloud vendors - Amazon, Google and Azure&lt;/li&gt;
  &lt;li&gt;The big multi play vendors - IBM, Oracle, Teradata, Microsoft, Pivotal, SAP and SAS&lt;/li&gt;
  &lt;li&gt;The big specialist commercial data integration vendors - Ab Initio and Informatica&lt;/li&gt;
  &lt;li&gt;Other commercial data integration and data platform vendors - perhaps based at least partially on looking at the latest Gartner and Forrester reports&lt;/li&gt;
  &lt;li&gt;The major open source cloud scale companies, if only to see what they do in this space - Facebook, Netflix, LinkedIn, Google, Yahoo, eBay and Twitter&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This will only be a start on the technologies I?d expect to see in our catalogue however, so once this is done the plan is to then go through by technology category.&lt;/p&gt;

&lt;p&gt;This is obviously going to take some time given the vast range of technologies, so if you?re interested in contributing in whatever form, if you spot any issues or omissions, or if you have any comments you?d like to add, then please do share your thoughts.&lt;/p&gt;
</description>
				<discourse_author>Peter</discourse_author>
			</item>
			
			<item>
				<title>Apache Hadoop</title>
				<link>http://ondataengineering.net/blog/2016/12/16/apache-hadoop/</link>
				<description>&lt;p&gt;And so we begin our journey through the jungle of Data Engineering technologies by looking at the technology du jour - Apache Hadoop.
&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;There?s an entire ecosystem here that we?ll start to explore by looking at the major Hadoop vendors, but the first entries in our technology catalogue are the &lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Apache Hadoop&lt;/a&gt; project itself, along with its sub-projects: &lt;a href=&quot;/technologies/apache-hadoop/yarn/&quot;&gt;YARN&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hadoop/hdfs/&quot;&gt;HDFS&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-hadoop/map-reduce/&quot;&gt;MapReduce&lt;/a&gt;. Click on the links to view the technology information I?ve added to the site.&lt;/p&gt;

&lt;p&gt;This also brings our first technology vendor - the &lt;a href=&quot;/tech-vendors/apache/&quot;&gt;Apache Software Foundation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And with that it?s time for Christmas!  We?ll be back in three weeks with the first of the core technologies within the Hadoop space.&lt;/p&gt;
</description>
				<discourse_author>Peter</discourse_author>
			</item>
			
			<item>
				<title>Core Hadoop Technologies (pt1)</title>
				<link>http://ondataengineering.net/blog/2017/01/06/core-hadoop-technologies/</link>
				<description>&lt;p&gt;And we?re back - Happy New Year!&lt;/p&gt;

&lt;p&gt;Having started with the core Apache Hadoop project, we?re now going to look at the ?core? technologies within the Hadoop space, based on those included in multiple distributions (many thanks to Merv Adrian from Gartner for his useful &lt;a href=&quot;http://blogs.gartner.com/merv-adrian/2016/07/30/hadoop-project-commercial-support-tracker-july-2016/&quot;&gt;tracker&lt;/a&gt;)
&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;There?s three Apache technologies added to the catalogue this week - &lt;a href=&quot;/technologies/apache-flume/&quot;&gt;Flume&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hbase/&quot;&gt;HBase&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Hive&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There?s no much to say have Flume or HBase right now, but we?ll take a more detailed look at both of these in the future.  Hive however, is more interesting.&lt;/p&gt;

&lt;p&gt;Firstly, it?s a hugely popular and important project that?s a corner stone of the Hadoop ecosystem, which in its short life has seen enormous change - a classic example of an Open Source technology that has mutated, evolved, consumed other projects and been pulled in multiple directions over time.  I plan to dig into the history of Hive in the not too distant future as I think it?s a great example of how an Open Source project can evolve.&lt;/p&gt;

&lt;p&gt;Secondly, it?s not one thing, but a collection of different components with very distinct roles all bundled together, which is why I?ve taken the decision to break it out into a number of sub-projects (&lt;a href=&quot;/technologies/apache-hive/hive-metastore/&quot;&gt;Hive Metastore&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hive/hive-server/&quot;&gt;Hive Server&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-hive/hcatalog/&quot;&gt;HCatalog&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;As before, click on the links above to see the information added to the site.&lt;/p&gt;

&lt;p&gt;That?s it for this week - next up is Solr, Sqoop and Spark.&lt;/p&gt;
</description>
				<discourse_author>Peter</discourse_author>
			</item>
			
			<item>
				<title>Core Hadoop Technologies (pt2)</title>
				<link>http://ondataengineering.net/blog/2017/01/13/core-hadoop-technologies-pt2/</link>
				<description>&lt;p&gt;And onwards with our look at the core Hadoop technologies.
&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;Today, I?ve added &lt;a href=&quot;/technologies/apache-solr/&quot;&gt;Solr&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-sqoop/&quot;&gt;Sqoop&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Spark&lt;/a&gt;, along with the Spark sub-projects &lt;a href=&quot;/technologies/apache-spark/spark-sql/&quot;&gt;Spark SQL&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-spark/spark-streaming/&quot;&gt;Spark Streaming&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-spark/mllib/&quot;&gt;MLlib&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-spark/graphx/&quot;&gt;GraphX&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Solr is one of the big two search technologies along with Elasticsearch, and although there?s debate around which is best (Elastic is probably slightly more developer friendly and supports slightly better analytics capabilities, whereas Solr has the more open development model being an Apache project), they?re both great technologies.&lt;/p&gt;

&lt;p&gt;Sqoop is interesting because of where it?s going - whereas the current version of Sqoop focuses on integration with structured databases, the pending version 2 (which admittedly has been in development for a long time now) evolves it slightly to support the batch ingest of any data into Hadoop.  It?s going to have stiff competition from Apache NiFi however if and when it?s finally released.&lt;/p&gt;

&lt;p&gt;And so to Spark - which claims to be the most active Open Source project in Big Data (as well as many other things).  What is clear however is it?s the one next gen data processing and transformation language that managed to catch significant momentum and adoption.  We can argue the toss on whether it?s the best technology, but it?s now bundled with all the Hadoop distributions, has a rapidly growing base of trained and experienced developers, and a rich ecosystem, which means it?s becoming the default answer to a whole bunch of use cases.&lt;/p&gt;
</description>
				<discourse_author>Peter</discourse_author>
			</item>
			
			<item>
				<title>The Apache Software Foundation</title>
				<link>http://ondataengineering.net/tech-vendors/apache/</link>
				<description>&lt;p&gt;The Apache Software Foundation is a non-profit organisation that supports a wide range of open source projects, including providing and mandating a standard governance model (including the use of the Apache license), holding all trademarks for project names and logos, and providing legal protection to developers.  It was founded in 1999 and now oversees nearly 200 projects.&lt;/p&gt;&lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.apache.org/&quot;&gt;https://www.apache.org/&lt;/a&gt; - homepage&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.apache.org/foundation/how-it-works.html&quot;&gt;https://www.apache.org/foundation/how-it-works.html&lt;/a&gt; - information on the foundation&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://apache.org/foundation/mailinglists.html#foundation-announce&quot;&gt;http://apache.org/foundation/mailinglists.html#foundation-announce&lt;/a&gt; - the Apache Foundation announcements mailing list&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/&quot;&gt;https://blogs.apache.org/&lt;/a&gt;; &lt;a href=&quot;https://blogs.apache.org/planet/feed/entries/rss&quot;&gt;https://blogs.apache.org/planet/feed/entries/rss&lt;/a&gt; - The set of Apache Foundation blogs&lt;/li&gt;
&lt;/ul&gt;
</description>
				<discourse_author>Peter</discourse_author>
			</item>
			
			<item>
				<title>Apache Hadoop</title>
				<link>http://ondataengineering.net/technologies/apache-hadoop/</link>
				<description>&lt;p&gt;A distributed storage and compute platform consisting of a distributed filesystem (HDFS) and a cluster workload and resource management layer (YARN), along with MapReduce, a solution built on HDFS and YARN for massive scale parallel processing of data. Has an extensive ecosystem of compatible technologies. An Apache Open Source project, started in January 2006 as a Lucene sub-project, becoming a top level project in January 2008, with a 1.0 release in December 2011 (containing HDFS and MapReduce), and a 2.2 release (the first 2.x GA release) in October 2013 (adding YARN). Very active, with a deep and broad range of contributors, and backing from multiple commercial vendors.&lt;/p&gt;&lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://hadoop.apache.org/&quot;&gt;http://hadoop.apache.org/&lt;/a&gt; - Project Homepage&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://hadoop.apache.org/releases.html&quot;&gt;http://hadoop.apache.org/releases.html&lt;/a&gt; - full release history&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@markobonaci/the-history-of-hadoop-68984a11704&quot;&gt;https://medium.com/@markobonaci/the-history-of-hadoop-68984a11704&lt;/a&gt; - history of Hadoop&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blogs.gartner.com/merv-adrian/2016/07/30/hadoop-project-commercial-support-tracker-july-2016/&quot;&gt;http://blogs.gartner.com/merv-adrian/2016/07/30/hadoop-project-commercial-support-tracker-july-2016/&lt;/a&gt; - summary of the technologies in the common Hadoop distributions&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://hadoop.apache.org/&quot;&gt;http://hadoop.apache.org/&lt;/a&gt; - project updates and releases&lt;/li&gt;
&lt;/ul&gt;
</description>
				<discourse_author>Peter</discourse_author>
			</item>
			
			<item>
				<title>HDFS</title>
				<link>http://ondataengineering.net/technologies/apache-hadoop/hdfs/</link>
				<description>&lt;p&gt;A highly resilient distributed cluster file system proven at extreme scale that supports user authentication, extended ACLs, snapshots, quotas, central caching, a REST API, an NFS gateway, rolling upgrades, transparent encryption and heterogeneous storage. Part of the original Hadoop code base, becoming an Apache Hadoop sub-project in July 2009.&lt;/p&gt;&lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html&quot;&gt;http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html&lt;/a&gt; - HDFS documentation home&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://bradhedlund.com/2011/09/10/understanding-hadoop-clusters-and-the-network/&quot;&gt;http://bradhedlund.com/2011/09/10/understanding-hadoop-clusters-and-the-network/&lt;/a&gt; - good intro the the architecture of HDFS&lt;/li&gt;
&lt;/ul&gt;
</description>
				<discourse_author>Peter</discourse_author>
			</item>
			
			<item>
				<title>MapReduce</title>
				<link>http://ondataengineering.net/technologies/apache-hadoop/map-reduce/</link>
				<description>&lt;p&gt;A data transformation and aggregation technology proven at extreme scale that works on key value pairs and consists of three transformation stages - map (a general transformation of the input key value pairs), shuffle (brings all pairs with the same key together) and reduce (an aggregation of all pairs with the same key). Part of the original Hadoop code base, becoming an Apache Hadoop sub-project in July 2009.&lt;/p&gt;&lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html&quot;&gt;http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html&lt;/a&gt; - MapReduce tutorial and documentation&lt;/li&gt;
&lt;/ul&gt;
</description>
				<discourse_author>Peter</discourse_author>
			</item>
			
			<item>
				<title>YARN</title>
				<link>http://ondataengineering.net/technologies/apache-hadoop/yarn/</link>
				<description>&lt;p&gt;Resource management and job scheduling &amp; monitoring for the Hadoop ecosystem.  Includes support for capacity guarantees amongst other scheduling options. Added as an Apache Hadoop sub-project as part of Hadoop 2.x (with a GA release as part of 2.2 in October 2013) having been started in January 2008.&lt;/p&gt;&lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html&quot;&gt;http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html&lt;/a&gt; - YARN architecture overview and documentation&lt;/li&gt;
&lt;/ul&gt;
</description>
				<discourse_author>Peter</discourse_author>
			</item>
			
			<item>
				<title>Apache Flume</title>
				<link>http://ondataengineering.net/technologies/apache-flume/</link>
				<description>&lt;p&gt;Specialist technology for the continuous movement of data using a set of independent agents connected together into pipelines.  Supports a wide range of sources, targets and buffers (channels), along with the ability to chain agents together and to modify and drop events in-flight.  Designed to be highly reliable, and to support reconfiguration without the need for a restart.  Heavily integrated with the Hadoop ecosystem.  An Apache project, donated by Cloudera in June 2011, graduating in June 2012, with a v1.2 release (the first considered ready for production use) in July 2012. Java based, with commercial support available as part of most Hadoop distributions.&lt;/p&gt;&lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://flume.apache.org/&quot;&gt;http://flume.apache.org/&lt;/a&gt; - home page&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://flume.apache.org/FlumeUserGuide.html&quot;&gt;http://flume.apache.org/FlumeUserGuide.html&lt;/a&gt; - user guide&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://flume.apache.org/&quot;&gt;http://flume.apache.org/&lt;/a&gt; - project updates and releases&lt;/li&gt;
&lt;/ul&gt;
</description>
				<discourse_author>Peter</discourse_author>
			</item>
			
			<item>
				<title>Apache HBase</title>
				<link>http://ondataengineering.net/technologies/apache-hbase/</link>
				<description>&lt;p&gt;NoSQL wide-column datastore based on Google BigTable. Focuses on random real-time access to data, and supports horizontal scalability, consistent reads and writes, versioning and fine grained security controls.  Runs on Hadoop and HDFS, and is heavily integrated with the Hadoop ecosystem.  An Apache project, first released as part of Hadoop 0.15 in October 2007 before graduating as a top level project in May 2010.  Java based, with commercial support available as part of most Hadoop distributions.&lt;/p&gt;&lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://hbase.apache.org/&quot;&gt;http://hbase.apache.org/&lt;/a&gt; - home page&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://hbase.apache.org/book.html&quot;&gt;http://hbase.apache.org/book.html&lt;/a&gt; - documentation&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://jimbojw.com/wiki/index.php?title=Understanding_Hbase_and_BigTable&quot;&gt;http://jimbojw.com/wiki/index.php?title=Understanding_Hbase_and_BigTable&lt;/a&gt; - HBase primer&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/hbase/entry/start_of_a_new_era&quot;&gt;https://blogs.apache.org/hbase/entry/start_of_a_new_era&lt;/a&gt; - introduction to HBase versioning scheme&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/hbase/&quot;&gt;https://blogs.apache.org/hbase/&lt;/a&gt; - Apache HBase Blog&lt;/li&gt;
  &lt;li&gt;HBase release announcements only appear to be available via the Apache announcements mailing list&lt;/li&gt;
&lt;/ul&gt;
</description>
				<discourse_author>Peter</discourse_author>
			</item>
			
			<item>
				<title>Apache Hive</title>
				<link>http://ondataengineering.net/technologies/apache-hive/</link>
				<description>&lt;p&gt;Technology that supports the exposure of data in Hadoop as structured tables and the execution of analytical SQL queries over these.  Consists of a number of distinct components (that we treat as sub-projects) including Hive Metastore (stores the definitions of the structured tables), Hive Server (supports the execution of analytical SQL queries as MapReduce, Spark or Tez jobs) and HCatalog (allows MapReduce and Pig jobs to read and write Hive tables).  First released by Facebook as an Hadoop contrib module in September 2008, becoming an Hadoop sub-project in November 2008, and a top level Apache project in September 2010, following a first official stable release (0.3) in April 2009.  Java based, under active development from a number of large commercial sponsors, with commercial support available as part of most Hadoop distributions.&lt;/p&gt;&lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://hive.apache.org/&quot;&gt;http://hive.apache.org/&lt;/a&gt; - home page&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/Home&quot;&gt;https://cwiki.apache.org/confluence/display/Hive/Home&lt;/a&gt; - documentation&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://hive.apache.org/downloads.html&quot;&gt;http://hive.apache.org/downloads.html&lt;/a&gt; - details of new releases&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/category/hive/&quot;&gt;http://blog.cloudera.com/blog/category/hive/&lt;/a&gt; - Cloudera Hive News&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://hortonworks.com/blog/category/hive/&quot;&gt;http://hortonworks.com/blog/category/hive/&lt;/a&gt; - Hortonworks Hive News&lt;/li&gt;
&lt;/ul&gt;
</description>
				<discourse_author>Peter</discourse_author>
			</item>
			
			<item>
				<title>HCatalog</title>
				<link>http://ondataengineering.net/technologies/apache-hive/hcatalog/</link>
				<description>&lt;p&gt;Libraries for MapReduce and Pig to read and write data to and from Hive tables, albeit with some limitations. Also supports a CLI for querying and updating the Hive Metastore, however this doesn't support the full range of Hive DDL commands.  Includes WebHCat, a REST API over the HCatalog CLI that also supports the execution of MapReduce, Pig, Hive and Sqoop jobs.  Donated to the Apache foundation by Yahoo in March 2011, had WebHCat folded in in July 2012, graduating as a top level project in February 2013, but then almost immediately was folded into Hive in March 2013 as part of the Hive 0.11 release.  Has seem limited development since this time.&lt;/p&gt;&lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/HCatalog&quot;&gt;https://cwiki.apache.org/confluence/display/Hive/HCatalog&lt;/a&gt; - HCatalog documentation home&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/WebHCat&quot;&gt;https://cwiki.apache.org/confluence/display/Hive/WebHCat&lt;/a&gt; - WebHCat documentation home&lt;/li&gt;
&lt;/ul&gt;
</description>
				<discourse_author>Peter</discourse_author>
			</item>
			
			<item>
				<title>Hive Metastore</title>
				<link>http://ondataengineering.net/technologies/apache-hive/hive-metastore/</link>
				<description>&lt;p&gt;A metadata service that allows structured tables to be defined over files in HDFS (and also HBase or Accumulo), providing an API that allows the metadata to be queried and updated by other tools including Impala, Spark SQL or RecordService.  Supports partitioned and clustered tables, as well as complex field types such as arrays, maps and structs.  Backed by a relational database (either MySQL, Postgres and Oracle).  Part of the original Hive code base.&lt;/p&gt;&lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin&quot;&gt;https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin&lt;/a&gt; - documentation home&lt;/li&gt;
&lt;/ul&gt;
</description>
				<discourse_author>Peter</discourse_author>
			</item>
			
			<item>
				<title>Hive Server</title>
				<link>http://ondataengineering.net/technologies/apache-hive/hive-server/</link>
				<description>&lt;p&gt;Supports the execution of SQL queries over data in HDFS based on tables defined in the Hive Metastore, as well as DDL to query and update the Hive Metastore.  Focus is on analytical (OLAP) use cases, with some support for batch updates to data.  Originally executed queries as MapReduce jobs, but significant investment from has seen support for executing queries as Spark and as Tez jobs, with work underway to support sub second query times using Tez.  Recent changes have also seen it achieve significant SQL compliance, with support for SQL:2011 analytical functions on-going.  Accepts queries over an API with JDBC and ODBC drivers available, and includes Beeline, a command line JDBC client.  Technically referred to as Hive Server 2, and was introduced in Hive 0.11 as a replacement for the original Hive Server to address a number of concurrency and security issues.&lt;/p&gt;&lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2013/07/how-hiveserver2-brings-security-and-concurrency-to-apache-hive/&quot;&gt;http://blog.cloudera.com/blog/2013/07/how-hiveserver2-brings-security-and-concurrency-to-apache-hive/&lt;/a&gt; - introduction to Hive Server 2&lt;/li&gt;
&lt;/ul&gt;
</description>
				<discourse_author>Peter</discourse_author>
			</item>
			
			<item>
				<title>Apache Solr</title>
				<link>http://ondataengineering.net/technologies/apache-solr/</link>
				<description>&lt;p&gt;A search server built on Apache Lucene with a REST-like API for loading and searching data.  Supports a distributed deployment (SolrCloud) that can run over HDFS on an Hadoop cluster.  Includes an administration web interface, an extensible plugin architecture, support for schemaless indexing, faceted, grouped and clustered results, hit highlighting, geo-spacial and graph searches, near real time indexing and searching, (experimental) streaming expressions for parallel compute (including support for MapReduce and SQL) and broad authentication and security capabilities.  A sub-project of the Apache Lucene project, originally donated to the Apache foundation by CNET Networks in January 2006, graduating as a top level project in January 2007, before merging with the Lucene project in March 2010. Java based, with commercial support available as part of most Hadoop distributions (although this is bundled as Cloudera Search with CDH and HDP Search with HDP), as well as from Lucidworks.&lt;/p&gt;&lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://lucene.apache.org/solr&quot;&gt;http://lucene.apache.org/solr&lt;/a&gt; - home page&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://lucene.apache.org/solr/features.html&quot;&gt;http://lucene.apache.org/solr/features.html&lt;/a&gt; - good summary of Solr features&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.apache.org/dyn/closer.lua/lucene/solr/ref-guide/&quot;&gt;https://www.apache.org/dyn/closer.lua/lucene/solr/ref-guide/&lt;/a&gt; - PDF download of documentation for latest release&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/solr/&quot;&gt;https://cwiki.apache.org/confluence/display/solr/&lt;/a&gt; - online working version of documentation&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Apache_Solr&quot;&gt;https://en.wikipedia.org/wiki/Apache_Solr&lt;/a&gt; - history of Solr&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://lucene.apache.org/solr/news.html&quot;&gt;http://lucene.apache.org/solr/news.html&lt;/a&gt; - project news&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://yonik.com/&quot;&gt;http://yonik.com/&lt;/a&gt; - details of new features from the creator of Solr&lt;/li&gt;
&lt;/ul&gt;
</description>
				<discourse_author>Peter</discourse_author>
			</item>
			
			<item>
				<title>Apache Spark</title>
				<link>http://ondataengineering.net/technologies/apache-spark/</link>
				<description>&lt;p&gt;A high performance general purpose distributed data processing engine based on directed acyclic graphs that primarily runs in memory, but can spill to disk if required, and which supports processing applications written in Java, Scala, Python and R.  Includes a number of sub-projects that support more specialised analytics including Spark SQL, Spark Streaming, MLlib (machine learning) and GraphX (graph analytics).  Requires a cluster manager (YARN, EC2 and Mesos are supported as well as standalone clusters) and can access data in a wide range of technologies (including HDFS, other Hadoop data sources, relational databases and NoSQL databases).  An Apache project, originally started at UC Berkley in 2009, open sourced in 2010, and donated to the Apache foundation in June 2013, graduating in February 2014.  v1.0 was released in May 2014, with a v2.0 release in July 2016.  Java based, with development led by Databricks (who sell a Spark hosted service), and with commercial support available as part of most Hadoop distributions.&lt;/p&gt;&lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/&quot;&gt;http://spark.apache.org/&lt;/a&gt; - home page&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/&quot;&gt;http://spark.apache.org/docs/latest/&lt;/a&gt; - documentation&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/news/index.html&quot;&gt;http://spark.apache.org/news/index.html&lt;/a&gt; - project news&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://databricks.com/blog/category/engineering&quot;&gt;https://databricks.com/blog/category/engineering&lt;/a&gt; - Databricks engineering blog&lt;/li&gt;
&lt;/ul&gt;
</description>
				<discourse_author>Peter</discourse_author>
			</item>
			
			<item>
				<title>GraphX</title>
				<link>http://ondataengineering.net/technologies/apache-spark/graphx/</link>
				<description>&lt;p&gt;Spark library for processing graphs and running graph algorithms, based on graph model that supports directional edges with properties on both vertices and edges.  Graphs are constructed from a pair of collections representing the edges and vertex, either directly from data on disk using builders, or prepared using other Spark functionality, with the ability to also view the graph as a set of triples.  Supports a range of graph operations, as well as an optimised variant of the Pregel API, and a set of out of the box algorithms (including PageRank, connected components and triangle count).  First introduced in Spark 0.9, with a production release as part of Spark 1.2, however has seen almost no new functionality since then.&lt;/p&gt;&lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/graphx/&quot;&gt;http://spark.apache.org/graphx/&lt;/a&gt; - home page&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/graphx-programming-guide.html&quot;&gt;http://spark.apache.org/docs/latest/graphx-programming-guide.html&lt;/a&gt; - documentation&lt;/li&gt;
&lt;/ul&gt;
</description>
				<discourse_author>Peter</discourse_author>
			</item>
			
			<item>
				<title>MLlib</title>
				<link>http://ondataengineering.net/technologies/apache-spark/mllib/</link>
				<description>&lt;p&gt;Spark library for running Machine Learning algorithms.  Supports a range of algorithms (including classifications, regressions, decision trees, recommendations, clustering and topic modelling), including iterative algorithms.  First introduced in Spark 0.8 after being collaboratively developed with the UC Berkeley MLbase project.&lt;/p&gt;&lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/mllib//&quot;&gt;http://spark.apache.org/mllib//&lt;/a&gt; - home page&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/ml-guide.html&quot;&gt;http://spark.apache.org/docs/latest/ml-guide.html&lt;/a&gt; - documentation&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.mlbase.org/&quot;&gt;http://www.mlbase.org/&lt;/a&gt; - the UC Berkeley MLbase project&lt;/li&gt;
&lt;/ul&gt;
</description>
				<discourse_author>Peter</discourse_author>
			</item>
			
			<item>
				<title>Spark SQL</title>
				<link>http://ondataengineering.net/technologies/apache-spark/spark-sql/</link>
				<description>&lt;p&gt;Spark library for processing structured data, using either SQL statements or a DataFrame API.  Supports querying and writing to local datasets (including JSON, Parquet, Avro, Orc and CSV) as well as external data sources (including Hive and JDBC), including the ability to query across data sources.  Includes Catalyst, a cost based optimiser that turns high level operations into low level Spark DAGs for execution.  Also includes a Hive compatible Thrift JDBC/ODBC server that's compatible with Beeline and the Hive JDBC and ODBC drivers, and a REPL CLI for interactive queries.  First introduced in Spark 1.0, with a production release as part of Spark 1.3.&lt;/p&gt;&lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/sql/&quot;&gt;http://spark.apache.org/sql/&lt;/a&gt; - home page&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/sql-programming-guide.html&quot;&gt;http://spark.apache.org/docs/latest/sql-programming-guide.html&lt;/a&gt; - documentation&lt;/li&gt;
&lt;/ul&gt;
</description>
				<discourse_author>Peter</discourse_author>
			</item>
			
			<item>
				<title>Spark Streaming</title>
				<link>http://ondataengineering.net/technologies/apache-spark/spark-streaming/</link>
				<description>&lt;p&gt;Spark library for continuous stream processing, that allows stream and batch processing (including Spark SQL and MLlib operations) to be combined. Uses a micro-batch execution model, leveraging core Spark to process each micro-batch, and provides fault tolerance through exactly-once processing semantics.  Supports a number of data sources (including HDFS, sockets, Flume, Kafka, Kinesis and messaging buses), as well as functions to maintain state and to execute windowed operations. First introduced in Spark 0.7, with a production release as part of Spark 0.9.&lt;/p&gt;&lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/sql/&quot;&gt;http://spark.apache.org/sql/&lt;/a&gt; - home page&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/sql-programming-guide.html&quot;&gt;http://spark.apache.org/docs/latest/sql-programming-guide.html&lt;/a&gt; - documentation&lt;/li&gt;
&lt;/ul&gt;
</description>
				<discourse_author>Peter</discourse_author>
			</item>
			
			<item>
				<title>Apache Sqoop</title>
				<link>http://ondataengineering.net/technologies/apache-sqoop/</link>
				<description>&lt;p&gt;Specialist technology for moving bulk data between Hadoop and structured (relational) databases.  Command line based, with the ability to import and export data between a range of databases (including mainframe partitioned datasets) and HDFS, Hive, HBase and Accumulo.  Supports parallel partitioned unloads, writing to Avro, Sequence File, Parquet and text files, incremental imports and saved jobs that can be shared via a simple metadata store.  An Apache project, started in May 2009 as an Hadoop contrib module, migrating to a Cloudera GitHub project in April 2010 (with a v1.0 release shortly after), before being donated to the Apache foundation in June 2011, graduating in March 2012.  The last major release (v1.4) was in November 2011, with only minor releases since then.  However in January 2012 a significant re-write was announced as part of a proposed v2.0 release to address a number of usability, security and architectural issues.  This will introduce a new Sqoop Server and Metadata Repository, supporting both a CLI and web UI, centralising job definitions, database connections and credentials, as well as enabling support for a wider range of connectors including NoSQL databases, Kafka and (S)FTP folders.  Java based, with commercial support available as part of most Hadoop distributions.&lt;/p&gt;&lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://sqoop.apache.org&quot;&gt;http://sqoop.apache.org&lt;/a&gt; - home page&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://sqoop.apache.org/docs/&quot;&gt;http://sqoop.apache.org/docs/&lt;/a&gt; - documentation&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/sqoop/entry/apache_sqoop_highlights_of_sqoop&quot;&gt;https://blogs.apache.org/sqoop/entry/apache_sqoop_highlights_of_sqoop&lt;/a&gt; - introduction to Sqoop 2&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/sqoop/entry/apache_sqoop_graduates_from_incubator&quot;&gt;https://blogs.apache.org/sqoop/entry/apache_sqoop_graduates_from_incubator&lt;/a&gt; - early history of Sqoop&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://sqoop.apache.org/&quot;&gt;http://sqoop.apache.org/&lt;/a&gt; - details latest release, and hosts release notes for v1.4.0 onwards&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/sqoop/&quot;&gt;https://blogs.apache.org/sqoop/&lt;/a&gt; - project blog&lt;/li&gt;
&lt;/ul&gt;
</description>
				<discourse_author>Peter</discourse_author>
			</item>
			
	</channel>
</rss>
