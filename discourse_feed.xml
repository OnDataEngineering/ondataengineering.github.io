<?xml version="1.0" encoding="UTF-8"?> <rss version="2.0"> <channel><title>OnDataEngineering</title><description>A collaborative site for independent, critical and technical thinking on the use cases, architectural patterns and technologies relating to the transformation and preparation of data for exploitation.</description><link>http://ondataengineering.net/</link><item><title>Content License</title><link>http://ondataengineering.net/site/content-license/</link><description>&lt;p&gt;The content of the OnDataEngineering.net site is copyright the relevant author and is licensed under a &lt;a href=&quot;http://creativecommons.org/licenses/by/4.0/&quot;&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Unless otherwise stated or granted, any code snippets or samples included in the content of the OnDataEngineering.net site are copyright their respective authors and are licensed under the &lt;a href=&quot;http://mit-license.org/&quot;&gt;MIT Licence&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The content of the OnDataEngineering.net site is defined as the main body of the pages in the &lt;a href=&quot;http://ondataengineering.net/technologies/&quot;&gt;Technologies&lt;/a&gt;, &lt;a href=&quot;/tech-categories/&quot;&gt;Tech Categories&lt;/a&gt;, &lt;a href=&quot;/tech-vendors/&quot;&gt;Tech Vendors&lt;/a&gt; and &lt;a href=&quot;http://ondataengineering.net/blog/&quot;&gt;Blog&lt;/a&gt; sections of the site.&lt;/p&gt; &lt;p&gt;For clarity, the CC BY 4.0 licence means that this content can be used for any commercial or non commercial purpose, as long as OnDataEngineering.net is clearly credited as the source of the information. Please see the &lt;a href=&quot;http://creativecommons.org/licenses/by/4.0/&quot;&gt;CC BY 4.0 licence summary&lt;/a&gt; for further details.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Contributing</title><link>http://ondataengineering.net/site/contributing/</link><description>&lt;p&gt;&lt;a href=&quot;http://ondataengineering.net&quot;&gt;OnDataEngineering&lt;/a&gt; is an open collaborative site, and lives on your contributions and participation in the community around the site. So welcome, it?s good to see you - can we suggest that a good place to start is the &lt;a href=&quot;/site/&quot;&gt;site information&lt;/a&gt; for more details on who we are and what we?re trying to do.&lt;/p&gt; &lt;p&gt;And before you go any further, can we ask you to sign up to our &lt;a href=&quot;http://discourse.ondataengineering.net&quot;&gt;Discourse forums&lt;/a&gt;. You?ll get updates on changes to the site, notifications of popular discussions, and can use this to discuss anything relating to this site including how you can help and contribute. While you?re there, please take a read of the &lt;a href=&quot;http://discourse.ondataengineering.net/t/welcome-to-discourse&quot;&gt;welcome post&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Once you?ve done that, we?d welcome your help in building out the content on this site and making it a fantastic resource. All submission of content is welcomed, and we pride ourselves on being friendly and welcoming place if you want to get involved. The following are a number of ways you can contribute:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;Discuss the contents of any page or blog post by clicking on the discuss button next to the page title or the link in the sidebar. Your thoughts, comments and feedback on what?s written on this site will make it?s content better.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;If you see typos, errors or omissions, or have information to add, edit the contents of any page by clicking on the edit button next to the page title or the link in the sidebar. This is literally a 30 second process for submitting changes, and will help make the contents of this site better. These edit links will take you to a GitHub edit page (you?ll need to login or create a login to GitHub first) - make your changes, fill in a description of your change in the form at the bottom of the page and click ?Propose the change?, and then on the next screen click ?Create pull request? twice and you?re done.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;As well as editing pages, you can also create new pages by clicking on the create button next to the page title or the link in the sidebar on the main index pages (for example &lt;a href=&quot;/technologies&quot;&gt;here&lt;/a&gt;). The process is exactly the same as editing a page.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;If you have any industry news you think we should be aware of then please send it to us as &lt;a href=&quot;mailto:news@ondataengineering.net&quot;&gt;news@ondataengineering.net&lt;/a&gt;. We follow the blogs and web pages listed in the News sections on each technology page, but there?s always a chance we?ll miss stuff, and we may not hear about technologies not yet listed on the site.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;With blog posts, feel free to correct typos or mistakes. If you have a blog post you?d like us to host please feel free to send us one via a pull request, however it?s worth getting in touch first via e-mail or the discussion forums, and we reserve the right to only take blog posts we feel are a good fit (posts trying to push specific technologies or copies of posts from elsewhere are generally not). Blog posts should be thought pieces; if you have factual information that fits elsewhere in the site please contribute it there.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;If the GitHub create and edit pages are a bit limited for you, using git and your favourite text editor is surprisingly straightforward. The &lt;a href=&quot;https://guides.github.com/&quot;&gt;GitHub guides&lt;/a&gt; (especially the Hello World and Forking Projects ones) and the &lt;a href=&quot;http://help.github.com/&quot;&gt;GitHub help pages&lt;/a&gt; are great places to start, and we use Visual Studio Code (which is completely free and has great git integration) with the markdownlint and SpellChcker plugins.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;All the content of this site is written in &lt;a href=&quot;https://en.wikipedia.org/wiki/Markdown&quot;&gt;Markdown&lt;/a&gt;. If you?re new to it, &lt;a href=&quot;http://www.markdowntutorial.com/&quot;&gt;Mardown Tutorial&lt;/a&gt; is a great place to start. We use the kramdown flavour - see the &lt;a href=&quot;http://kramdown.gettalong.org/quickref.html&quot;&gt;quick reference&lt;/a&gt; and &lt;a href=&quot;http://kramdown.gettalong.org/syntax.html&quot;&gt;syntax&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Oh, and we reserve the right to use the (correct) British spellings of words here, although we won?t object to people using new fangled alternative spellings.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Site Information</title><link>http://ondataengineering.net/site/</link><description>&lt;p&gt;&lt;a href=&quot;http://ondataengineering.net&quot;&gt;OnDataEngineering&lt;/a&gt; is a site focused on how to make the right data available in the right form in the right place at the right time to allow users to efficiently exploit it. This covers a range of use cases (including the acquisition and staging of data, ?big data? preparation and data warehousing) to support any type of exploitation (be it reporting, big data analytics, machine learning or any one of a dozen similar capabilities).&lt;/p&gt; &lt;p&gt;This site currently hosts a catalog of technologies that fit within this space, organised &lt;a href=&quot;/technologies/&quot;&gt;alphabetically&lt;/a&gt;, &lt;a href=&quot;/tech-categories/&quot;&gt;by category&lt;/a&gt; and &lt;a href=&quot;/tech-vendors/&quot;&gt;by vendor&lt;/a&gt;. We also have a &lt;a href=&quot;/blog/&quot;&gt;blog&lt;/a&gt; that covers updates to the site as well as industry news, and a set of &lt;a href=&quot;http://discourse.ondataengineering.net&quot;&gt;forums&lt;/a&gt; .&lt;/p&gt; &lt;p&gt;In the future it is planned for this site to also look at key use cases for different technologies within the Data Engineering space along with a set of independent, technical and critical evaluations of various technologies and architectural patterns against these. Please see the &lt;a href=&quot;/blog/2016/12/12/the-plan/&quot;&gt;introduction&lt;/a&gt; blog post for further details.&lt;/p&gt; &lt;p&gt;This is a community owned and authored site. All the content on this site is licensed under a Creative Commons Attribution license (see &lt;a href=&quot;/site/content-license/&quot;&gt;here&lt;/a&gt; for details) and the content of the site is hosted in a public GitHub repository &lt;a href=&quot;https://github.com/OnDataEngineering/OnDataEngineeringContent&quot;&gt;here&lt;/a&gt;, with contributions welcomed (see &lt;a href=&quot;/site/contributing/&quot;&gt;here&lt;/a&gt; for details).&lt;/p&gt; &lt;p&gt;For details on how to get new information added to the site delivered straight to you via e-mail, RSS or Twitter, please see &lt;a href=&quot;/site/subscribe&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;To get in contact with the site administrators please e-mail &lt;a href=&quot;mailto:admin@ondataengineering.net&quot;&gt;admin@ondataengineering.net&lt;/a&gt;&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Search</title><link>http://ondataengineering.net/site/search/</link><description> &lt;script language=&quot;Javascript&quot; type=&quot;text/javascript&quot;&gt; function google_search() { var query = document.getElementById(&quot;google-search&quot;).value; window.open(&quot;https://www.google.com/search?q=&quot; + query + &quot;+site:&quot; + &quot;http%3A%2F%2Fondataengineering.net%2F&quot;); } &lt;/script&gt; &lt;form id=&quot;search&quot; onsubmit=&quot;google_search(); return false;&quot;&gt; &lt;input type=&quot;text&quot; id=&quot;google-search&quot; placeholder=&quot;Enter search term and hit enter&quot; /&gt; &lt;/form&gt; &lt;noscript&gt; Search &lt;a href=&quot;https://www.google.com/search?q=site:http%3A%2F%2Fondataengineering.net%2F&quot; target=&quot;_blank&quot;&gt;Google&lt;/a&gt; for: &lt;pre&gt;&lt;code&gt;search-term site:http://ondataengineering.net/&lt;/code&gt;&lt;/pre&gt; &lt;/noscript&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Subscribe</title><link>http://ondataengineering.net/site/subscribe/</link><description>&lt;p&gt;Want to get updates to the site - both blog posts and new / changed content - delivered direct to you?&lt;/p&gt; &lt;h3 id=&quot;by-twitter&quot;&gt;By Twitter&lt;/h3&gt; &lt;p&gt;Follow us - we?re &lt;a href=&quot;https://twitter.com/OnDataEng&quot;&gt;@OnDataEng&lt;/a&gt;.&lt;/p&gt; &lt;h3 id=&quot;by-e-mail&quot;&gt;By E-mail&lt;/h3&gt; &lt;p&gt;You can sign up to our updates e-mail &lt;a href=&quot;http://eepurl.com/cyQSqv&quot;&gt;here&lt;/a&gt; (and un-subscribe &lt;a href=&quot;http://ondataengineering.us15.list-manage1.com/unsubscribe?u=2641f8b7b450d6b8685c38076&amp;amp;id=29bd4f4db6&quot;&gt;here&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;There?s generally new content on the site every weekday, and you have the option of choosing to get the update e-mail daily or weekly.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Please note - we will only ever use your e-mail address to deliver you the e-mail update; we will never share or sell this.&lt;/em&gt;&lt;/p&gt; &lt;h3 id=&quot;by-rss--atom-feed&quot;&gt;By RSS / Atom Feed&lt;/h3&gt; &lt;p&gt;Point your favourite feed reader at our &lt;a href=&quot;http://ondataengineering.net/atom.xml&quot;&gt;atom feed&lt;/a&gt;. If you don?t like atom, we have an &lt;a href=&quot;http://ondataengineering.net/feed.xml&quot;&gt;RSS feed&lt;/a&gt; as well.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Welcome</title><link>http://ondataengineering.net/blog/2016/12/07/welcome/</link><description> &lt;p&gt;For me, one of the biggest challenges in exploiting data (be that through reporting, big data analytics, machine learning or any one of a dozen similar capabilities) is making sure you have the right data in the right place at the right time to allow you to do this efficiently. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;For example, &lt;a href=&quot;http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html&quot;&gt;this&lt;/a&gt; article from the New York Times and &lt;a href=&quot;http://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/&quot;&gt;this&lt;/a&gt; more recent one from Forbes talks about how the analysis of big data promises unique business insights, but for big-data scientists there is significant manual ?janitor work? (up to 80% of their time) required to prepare data, and although the research is sponsored by Data Wrangling tool vendors, the conclusions will resonate with many data scientists. Combine this with the historical cost, delivery speed and agility issues typically associated with delivery data warehouse or reporting solutions, and for me it?s never been clearer that we need to get smarter at how we prepare and manage data.&lt;/p&gt; &lt;p&gt;Part of the solution to this is better Data Engineering, ensuring the processes, tools, technologies, data platforms, regular data feeds and their data preparation jobs are in place to allow the data to be exploited in an efficient, reliable and repeatable way. The aim of this site is therefore to try to offer independent, critical and technical thinking on the technologies, architectural patterns and delivery capabilities that can help address this.&lt;/p&gt; &lt;p&gt;My hope is that this becomes a community owned and authored site of trusted reference material on these topics. To that end, all the content on this site is licensed under the Creative Commons Attribution 4.0 International License and hosted in a public GitHub repository, and there are a set of Discourse forums for discussions. Details of how to contribute and get involved can be found on every page.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Big Data</title><link>http://ondataengineering.net/blog/2016/12/09/big-data/</link><description> &lt;p&gt;Before we get stuck in, a short digression to talk about Big Data. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;There are many different definitions of Big Data, but for arguments sake let?s say this refers to the exploitation of data that would have previously been uneconomical due to the volume of data, the structure or format of the data (e.g. unstructured, semi-structured, or complex file formats such as video and audio) and the types of analytics required (e.g. path, graph or time series analysis)&lt;/p&gt; &lt;p&gt;I?ve a couple of comments to make on this topic.&lt;/p&gt; &lt;p&gt;Firstly, in my (humble) option Big Data is a marketing term (supported by new technologies - primarily Hadoop) that?s been exploited to sell these technologies and to give the industry something to talk about. However, I think this has been a broadly positive thing, in that it?s brought data analytics to the mainstream, spurred uptake of new technologies and encouraged companies to invest in analytics and data processing that they may not have done previously. In any case it?s probably just about run its course now (as demonstrated as it?s fall down the far side of the hype curve), and has definitely resulted in the devaluation of other (perhaps more traditional) analytical capabilities which still have a role to play and in many cases deliver capabilities that Big Data technologies can?t yet match.&lt;/p&gt; &lt;p&gt;Secondly, I think there?s been a lot of misinformation about Big Data and Big Data technologies. It?s not a replacement for existing BI/MI and analytical capabilities, and in fact needs to coexist and integrate with these in order to deliver on its promises. It?s not always cheaper or more performant than existing technologies, and won?t always reduce the timescales and costs for analytics or data exploration. And it?s not a new or innovative technology - I know of companies that were analysing multi-petabyte data stores and doing real time analytics over ten years ago, parallel distributed file systems have been around for a lot longer than that, and there are many established technologies that have data processing capabilities that the new technologies are only just starting to catch up to.&lt;/p&gt; &lt;p&gt;In terms of this site the plan is to look at the wider picture and take an holistic view of data transformation and exploitation. I?ve therefore no plans to talk explicitly about Big Data, but in looking at the wider picture we will absolutely cover everything relating to it (the technologies and the new use cases these enable) alongside coverage of other new technologies, more established technologies and capabilities, and the interesting intersection between them all.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Plan</title><link>http://ondataengineering.net/blog/2016/12/12/the-plan/</link><description> &lt;p&gt;One more post before we get started.&lt;/p&gt; &lt;p&gt;The following are my current thoughts for some of the topics I?d like to cover on this site, both as a reference for my future self to look back at my naive optimism, but also if anyone wants to start contributing to any of these now, or to start a discussion on any the later topics to start framing and exploring them. &lt;!--more--&gt;&lt;/p&gt; &lt;h2 id=&quot;theme-1---the-technology-catalogue&quot;&gt;Theme 1 - the technology catalogue&lt;/h2&gt; &lt;p&gt;The plan here is to start building up a technology catalogue by looking at the key vendors in the Data Engineering space. This will only be a start on the technologies I?d expect to see in our catalogue however, so once this is done the plan is to then go through by technology category to complete the catalogue.&lt;/p&gt; &lt;p&gt;I?d also like to look at providing a concise yet detailed introduction to some technologies that describes exactly what it is, how it works, and what the key features are. So much material that can be found on the internet is marketing material that glosses over the information I?m interested in knowing to understand whether a technology might meet my use cases and integrate into my environment, and my hope is that I can use this site to address that.&lt;/p&gt; &lt;h2 id=&quot;theme-2---data-engineering-use-cases&quot;&gt;Theme 2 - data engineering use cases&lt;/h2&gt; &lt;p&gt;One thing I don?t want to do on this site is define another data ecosystem architecture - there are too many already, most of them are designed to sell specific technologies, and none of them will fit the range of different requirements and constraints that different organisations will have.&lt;/p&gt; &lt;p&gt;However, what I do want to do is look at the range of different of different use cases that you might use data engineering technologies for, from a Data Lake (and we?ll look at what that overloaded term actually means) to a Data Warehouse (and why they?re still relevant), from the acquisition of data to the preparation of a Query Focused Dataset, and from the management of a data catalogue to the monitoring of data quality metrics.&lt;/p&gt; &lt;p&gt;I?d then like to look at how different technologies and architectural patterns can support these use cases - how do you implement a Data Lake using Hadoop, what technologies support data governance and data catalogues, and how do the various streaming frameworks compare.&lt;/p&gt; &lt;p&gt;As part of this I also want to look at the core principles behind Data Transformation, what state of the art in this space looks like, and how the established enterprise technologies compare to the new Open Source upstarts.&lt;/p&gt; &lt;h2 id=&quot;theme-3---delivery&quot;&gt;Theme 3 - delivery&lt;/h2&gt; &lt;p&gt;As if the above isn?t already massively ambitious enough, I?d also like to talk about the delivery of data solutions, including:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How we can use best practice delivery concepts (e.g. configuration management, continuous integration and testing, automated deployment, infrastructure and database management) and what these mean within a data solution&lt;/li&gt; &lt;li&gt;How we can bring some the new best practices from Lean and Agile into the data space, and what data transformation tools need to do in order to be able to support this&lt;/li&gt; &lt;li&gt;Why data projects can have a reputation for late delivery, cost overruns, poor quality data and a high cost of change, and what can be done about this&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I think that will more than do us. Getting through that lot will take some time, but with help and contributions I think this site could be hugely valuable.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Technology Catalogue</title><link>http://ondataengineering.net/blog/2016/12/14/the-technology-catalogue/</link><description> &lt;p&gt;The first step in this journey is going to be creation of a catalogue of the technologies that are going to be of interest to us as we explore the world of data engineering. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;The aim is to provide a valuable reference for anyone starting a technology evaluation to understand what technology options they might have for a given situation, or to understand if and how a technology might fit into an existing ecosystem.&lt;/p&gt; &lt;p&gt;For each technology, the plan is therefore to provide a short summary describing the technology, along with its background and current status. As mentioned in my previous post, for some technologies, I also want to do a deep dive to provide a longer summary with more detail that gives a solid introduction to the technology, and my hope is that we?ll get contributions to provide these for the vast majority of the technologies that I won?t get time to look at.&lt;/p&gt; &lt;p&gt;As part of this we?ll need to look at providing a categorisation of technologies, although making this useful is going to be challenging given that multiple categories of technologies could be used to meet a given use case. The technologies we?ll look at broadly fall into three groups however:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Data Transformation tools, be they one of the established commercial products or a new Open Source technology&lt;/li&gt; &lt;li&gt;Data Platforms, be they a traditional relational database, an Hadoop based data platform, a real time broker such as Kafka, or a NoSQL data platform&lt;/li&gt; &lt;li&gt;Technologies that address the other supporting capabilities around these, such as data catalogues or metadata management&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The plan is to start by looking at the key vendors in the Data Engineering space including:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The pure play Hadoop distributions - Apache Big Top, Hortonworks, Cloudera and MapR&lt;/li&gt; &lt;li&gt;The rest of the Apache Data Engineering ecosystem&lt;/li&gt; &lt;li&gt;The major Cloud vendors - Amazon, Google and Azure&lt;/li&gt; &lt;li&gt;The big multi play vendors - IBM, Oracle, Teradata, Microsoft, Pivotal, SAP and SAS&lt;/li&gt; &lt;li&gt;The big specialist commercial data integration vendors - Ab Initio and Informatica&lt;/li&gt; &lt;li&gt;Other commercial data integration and data platform vendors - perhaps based at least partially on looking at the latest Gartner and Forrester reports&lt;/li&gt; &lt;li&gt;The major open source cloud scale companies, if only to see what they do in this space - Facebook, Netflix, LinkedIn, Google, Yahoo, eBay and Twitter&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This will only be a start on the technologies I?d expect to see in our catalogue however, so once this is done the plan is to then go through by technology category.&lt;/p&gt; &lt;p&gt;This is obviously going to take some time given the vast range of technologies, so if you?re interested in contributing in whatever form, if you spot any issues or omissions, or if you have any comments you?d like to add, then please do share your thoughts.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Hadoop</title><link>http://ondataengineering.net/blog/2016/12/16/apache-hadoop/</link><description> &lt;p&gt;And so we begin our journey through the jungle of Data Engineering technologies by looking at the technology du jour - Apache Hadoop. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;There?s an entire ecosystem here that we?ll start to explore by looking at the major Hadoop vendors, but the first entries in our technology catalogue are the &lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Apache Hadoop&lt;/a&gt; project itself, along with its sub-projects: &lt;a href=&quot;/technologies/apache-hadoop/yarn/&quot;&gt;YARN&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hadoop/hdfs/&quot;&gt;HDFS&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-hadoop/map-reduce/&quot;&gt;MapReduce&lt;/a&gt;. Click on the links to view the technology information I?ve added to the site.&lt;/p&gt; &lt;p&gt;This also brings our first technology vendor - the &lt;a href=&quot;/tech-vendors/apache/&quot;&gt;Apache Software Foundation&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;And with that it?s time for Christmas! We?ll be back in three weeks with the first of the core technologies within the Hadoop space.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Core Hadoop Technologies (pt1)</title><link>http://ondataengineering.net/blog/2017/01/06/core-hadoop-technologies/</link><description> &lt;p&gt;And we?re back - Happy New Year!&lt;/p&gt; &lt;p&gt;Having started with the core Apache Hadoop project, we?re now going to look at the ?core? technologies within the Hadoop space, based on those included in multiple distributions (many thanks to Merv Adrian from Gartner for his useful &lt;a href=&quot;http://blogs.gartner.com/merv-adrian/2016/07/30/hadoop-project-commercial-support-tracker-july-2016/&quot;&gt;tracker&lt;/a&gt;) &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;There?s three Apache technologies added to the catalogue this week - &lt;a href=&quot;/technologies/apache-flume/&quot;&gt;Flume&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hbase/&quot;&gt;HBase&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Hive&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;There?s no much to say have Flume or HBase right now, but we?ll take a more detailed look at both of these in the future. Hive however, is more interesting.&lt;/p&gt; &lt;p&gt;Firstly, it?s a hugely popular and important project that?s a corner stone of the Hadoop ecosystem, which in its short life has seen enormous change - a classic example of an Open Source technology that has mutated, evolved, consumed other projects and been pulled in multiple directions over time. I plan to dig into the history of Hive in the not too distant future as I think it?s a great example of how an Open Source project can evolve.&lt;/p&gt; &lt;p&gt;Secondly, it?s not one thing, but a collection of different components with very distinct roles all bundled together, which is why I?ve taken the decision to break it out into a number of sub-projects (&lt;a href=&quot;/technologies/apache-hive/hive-metastore/&quot;&gt;Hive Metastore&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hive/hive-server/&quot;&gt;Hive Server&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-hive/hcatalog/&quot;&gt;HCatalog&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;As before, click on the links above to see the information added to the site.&lt;/p&gt; &lt;p&gt;That?s it for this week - next up is Solr, Sqoop and Spark.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Core Hadoop Technologies (pt2)</title><link>http://ondataengineering.net/blog/2017/01/13/core-hadoop-technologies-pt2/</link><description> &lt;p&gt;And onwards with our look at the core Hadoop technologies. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Today, I?ve added &lt;a href=&quot;/technologies/apache-solr/&quot;&gt;Solr&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-sqoop/&quot;&gt;Sqoop&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Spark&lt;/a&gt;, along with the Spark sub-projects &lt;a href=&quot;/technologies/apache-spark/spark-sql/&quot;&gt;Spark SQL&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-spark/spark-streaming/&quot;&gt;Spark Streaming&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-spark/mllib/&quot;&gt;MLlib&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-spark/graphx/&quot;&gt;GraphX&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Solr is one of the big two search technologies along with Elasticsearch, and although there?s debate around which is best (Elastic is probably slightly more developer friendly and supports slightly better analytics capabilities, whereas Solr has the more open development model being an Apache project), they?re both great technologies.&lt;/p&gt; &lt;p&gt;Sqoop is interesting because of where it?s going - whereas the current version of Sqoop focuses on integration with structured databases, the pending version 2 (which admittedly has been in development for a long time now) evolves it slightly to support the batch ingest of any data into Hadoop. It?s going to have stiff competition from Apache NiFi however if and when it?s finally released.&lt;/p&gt; &lt;p&gt;And so to Spark - which claims to be the most active Open Source project in Big Data (as well as many other things). What is clear however is it?s the one next gen data processing and transformation language that managed to catch significant momentum and adoption. We can argue the toss on whether it?s the best technology, but it?s now bundled with all the Hadoop distributions, has a rapidly growing base of trained and experienced developers, and a rich ecosystem, which means it?s becoming the default answer to a whole bunch of use cases.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Core Hadoop Technologies (pt3)</title><link>http://ondataengineering.net/blog/2017/01/20/core-hadoop-technologies-pt3/</link><description> &lt;p&gt;Up today, our final look at the core technologies within the Hadoop ecosystem. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;First up are &lt;a href=&quot;/technologies/apache-avro&quot;&gt;Avro&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-parquet&quot;&gt;Parquet&lt;/a&gt;, both of which are key data formats used within the Hadoop ecosystem, but with different and contrasting focuses.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/technologies/apache-kafka&quot;&gt;Kafka&lt;/a&gt;?s a hot technology at the moment - deliverying high bandwidth low latency storage and processing of data streams, with reference cases handling millions of events per second. If you?re looking at doing anything with streaming data it?s probably well worth a look. Note that I?ve broken out &lt;a href=&quot;/technologies/apache-kafka/kafka-connect&quot;&gt;Kafka Connect&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-kafka/kafka-streams&quot;&gt;Kafka Streams&lt;/a&gt; as sub-projects.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/technologies/apache-pig&quot;&gt;Pig&lt;/a&gt; was one of the first technologies to provide a ore user friendly abstraction over MapReduce for developing Hadoop jobs. It?s starting to show it?s age however, and although Hortonworks and Yahoo (who are heavy Pig users) seem to be investing heavily in Pig on Tez, and Cloudera seems to be supporting Pig on Spark (mirroring their Hive strategies), it?s difficult to see newcomers to Hadoop who don?t have an existing investment in Pig using it over Spark and other newer tools.&lt;/p&gt; &lt;p&gt;And finally &lt;a href=&quot;/technologies/apache-oozie&quot;&gt;Oozie&lt;/a&gt; - a job scheduling an orchestration engine. It?s been a staple of most Hadoop distributions for a while now, however it?s difficult to find many big references cases for it?s use, and it?s not the most user friendly tool. Orchestration and management of data transformation pipelines feels like a huge technology gap at the moment - if anyone knows of any great technologies in this space please shout.&lt;/p&gt; &lt;p&gt;As before - click on the links to see the technology information added to the site.&lt;/p&gt; &lt;p&gt;That?s it for this week, and for the core Hadoop technologies - it?s been fun. We?ll be back on Monday to start looking at Apache Bigtop, along with a change of pace?&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Daily Technology Summary</title><link>http://ondataengineering.net/blog/2017/01/23/the-daily-technology-summary/</link><description> &lt;p&gt;Up until today, I?ve been publishing new technology summaries in bulk every Friday. However I want to change up the pace a little, and so going forward I?m going to try to publish a new technology summary every week day.&lt;/p&gt; &lt;!--more--&gt; &lt;p&gt;Blog posts will be less frequent, and will usually be used to introduce and conclude a specific set of technologies we?re looking it. Because of this, you?ll notice that the site home page now supports a snazzy new summary of recent changes to content on the right hand side next to the summary of recent blog posts. In addition, the RSS and Atom feeds now include content updates as well as blog posts.&lt;/p&gt; &lt;p&gt;Also, in the coming days there will be new options to get the daily technology summary (and blog posts) delivered to your inbox via e-mail, and we?ll also start publishing these on Twitter (@OnDataEng) as well - just as soon as it?s all setup.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hadoop Distros: Apache Bigtop</title><link>http://ondataengineering.net/blog/2017/01/23/hadoop-distros-apache-bigtop/</link><description> &lt;p&gt;And so to our first (hopefully of many) daily technology summary.&lt;/p&gt; &lt;p&gt;I want to continue our wander through the Apache Hadoop ecosystem by looking at the common Hadoop Distributions, starting with &lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, and with this introducing our first technology category - the &lt;a href=&quot;/tech-categories/hadoop_distributions/&quot;&gt;Hadoop Distribution&lt;/a&gt;. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Apache Bigtop is interesting for a couple of reasons - firstly because it?s the only true open source Hadoop distribution (meaning that it includes many components that the commercial distributions don?t, and the components it includes are often more up to date, assuming you?re happy to use the latest snapshot builds), and secondly because of it?s history as Cloudera?s attempt to create a common base Hadoop distribution with all the associated integration testing and packaging (there are links in the &lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Bigtop page&lt;/a&gt; that provide further reading on this).&lt;/p&gt; &lt;p&gt;You?ll see that the &lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Bigtop page&lt;/a&gt; links to the Hadoop technologies we?re already covered that it packages (and vice-versa), and the aim is that over the coming days we?ll complete this list as we add technology summaries for the rest of the technologies it packages.&lt;/p&gt; &lt;p&gt;And I shouldn?t let our first technology category go uncommented - as we?re going to be looking at the common Hadoop Distributions we?ll start collecting these together under an &lt;a href=&quot;/tech-categories/hadoop_distributions/&quot;&gt;Hadoop Distribution&lt;/a&gt; page and try and round these out over the coming weeks.&lt;/p&gt; &lt;p&gt;That?s it for now - we?ll speak again when we?ve worked our way through the remaining technologies bundled with Apache Bigtop.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Week That Was - 27/01/2017</title><link>http://ondataengineering.net/blog/2017/01/27/the-week-that-was/</link><description> &lt;p&gt;So rather than waiting until we?ve finished looking at all the technologies included in Apache Bigtop before talking about them, let?s try wrapping up each week with a blog post summarising what we?ve looked at, and maybe at some point summarising some of the news of the week. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;So what have we looked at this week?&lt;/p&gt; &lt;p&gt;Firstly, a couple of graph computation frameworks - &lt;a href=&quot;/technologies/apache-hama/&quot;&gt;Apache Hama&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-giraph&quot;&gt;Apache Giraph&lt;/a&gt;. Along with &lt;a href=&quot;/technologies/apache-spark/graphx/&quot;&gt;Spark GraphX&lt;/a&gt;, these are probably the big three graph computation frameworks on Hadoop. What?s interesting is that both GraphX and Hama seem to have seen very little development recently - either meaning they?re done and meet most people?s use cases, or there just isn?t the demand for them. Giraph still seems to be going strong, however this is mainly being used at extreme scale by Facebook and LinkedIn. My guess is that graph technologies (both computation frameworks and graph databases) are being pushed as hot technologies at the moment, however most organisations aren?t quite sure what to do with them.&lt;/p&gt; &lt;p&gt;We?re also looked at a couple of Hadoop in-memory storage accelerators - &lt;a href=&quot;/technologies/apache-ignite&quot;&gt;Apache Ignite&lt;/a&gt; and &lt;a href=&quot;/technologies/alluxio&quot;&gt;Alluxio&lt;/a&gt; (formally known as Tachyon). These are both interesting, promising performance boosts for Hadoop computation jobs by providing an in memory HDFS compatible filesystem, as well a bunch of other features - both support an in memory key-value store, Alluxio supports tiered storage over multiple storage layers (in-memory, local and remote disk), and Ignite provides a more general purpose compute layer that supports streaming computation and arbitrary compute. If you?ve used either of these and can talk to their benefits I?d be very interested to chat in the forums.&lt;/p&gt; &lt;p&gt;And we started the week by talking about &lt;a href=&quot;/technologies/apache-bigtop&quot;&gt;Apache Bigtop&lt;/a&gt;, which I?ve already &lt;a href=&quot;/blog/2017/01/23/hadoop-distros-apache-bigtop/&quot;&gt;talked about&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Finally - you?ll now see that we?re proudly displaying our options for &lt;a href=&quot;/site/subscribe/&quot;&gt;subscribing&lt;/a&gt; to our content on the front page. Pick your poison - we support e-mail (daily and weekly), Twitter updates plus RSS and Atom feeds.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Week That Was - 03/02/2017</title><link>http://ondataengineering.net/blog/2017/02/03/the-week-that-was/</link><description> &lt;p&gt;And another week goes by - let?s have a look back over the technologies we?ve looked at this week.&lt;/p&gt; &lt;p&gt;We kicked off the week by looking at &lt;a href=&quot;/technologies/kite/&quot;&gt;Kite&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-datafu/&quot;&gt;Apache DataFu&lt;/a&gt;, a couple of utilities designed to make working in the Hadoop ecosystem a little easier, before moving on to &lt;a href=&quot;/technologies/apache-phoenix&quot;&gt;Apache Phoenix&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-tajo&quot;&gt;Apache Tajo&lt;/a&gt;. a couple of query engines (also over the Hadoop ecosystem), and then finishing with &lt;a href=&quot;/technologies/apache-mahout&quot;&gt;Apache Mahout&lt;/a&gt;. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/technologies/kite/&quot;&gt;Kite&lt;/a&gt; is a solid concept and feels like it fills a bit of a void - how can I easily create Hive tables and load data in from a variety of sources without writing code - however it doesn?t ever seem to have gained much traction, and it looks like even Cloudera aren?t developing and maintaining it any more. It?s also the first non Apache technology we?re looked at on this site! I?m definitely planning to revisit Kite and some of it?s concepts when we talk about Data Lakes in the future however.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/technologies/apache-datafu/&quot;&gt;Apache DataFu&lt;/a&gt; is actually two things - a set of user defined functions for Pig, and a MapReduce framework for calculating aggregations over regular ingestions of data into Hadoop based on only processing the new data called Hourglass. The first of these sounds well worth a look if you any sort of significant work in Pig. The second I?m less sure about - you?ll have to be using MapReduce, and you?d have to want to follow their pattern, however as a concept or an exemplar it could well be worth a look.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/technologies/apache-phoenix&quot;&gt;Apache Phoenix&lt;/a&gt; surprised me - it appears to be an extremely active project, with excellent documentation and a great range of companies that are using it in production, and in providing a SQL query later over HBase, fills an interesting niche within the Hadoop ecosystem. Hive and Impala are great if you have batch updates (and ideally just appends), but they don?t support low latency random updates and queries (along with the full table scans) that HBase (and therefore Phoenix) does. It?s going to be interesting to see how this stacks up against Kudu as this matures and gains adoption, and how the major Hadoop distributions look to support this use case.&lt;/p&gt; &lt;p&gt;I?m not quite sure what to make of &lt;a href=&quot;/technologies/apache-tajo&quot;&gt;Apache Tajo&lt;/a&gt;. It seems like a great technology, with significant commercial backing from Gruter, however I?m not sure it?s getting much traction, and I?m not sure what niche it?s trying to target - it feels uncomfortably close to Hive and Impala. Maybe prior to Hive on Tez/Spark Tajo had some differentiation in terms of low latency queries.&lt;/p&gt; &lt;p&gt;And last (but not least) &lt;a href=&quot;/technologies/apache-mahout&quot;&gt;Apache Mahout&lt;/a&gt;. Mahout has been a staple of most Hadoop distributions for a while (probably as a result of it being one of the first machine learning technologies in the Hadoop space), but what?s interesting is that it completely reinvented itself in April 2015 to become a general purpose distributed linear algebra engine that can run over Spark (with H2O and Flink support coming), and (if running on Spark) is fully compatible with other Spark libraries such as MLlib.&lt;/p&gt; &lt;p&gt;That?s it for this week - have a great weekend.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Week That Was - 10/02/2017</title><link>http://ondataengineering.net/blog/2017/02/10/the-week-that-was/</link><description> &lt;p&gt;Wow, doesn?t time fly when you?re having fun.&lt;/p&gt; &lt;p&gt;We?re nearly at the end of our Apache Bigtop journey - this week we?ve had a look at a bunch of data processing tools (&lt;a href=&quot;/technologies/apache-apex&quot;&gt;Apache Apex&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-crunch&quot;&gt;Apache Crunch&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-flink&quot;&gt;Apache Flink&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-tez&quot;&gt;Apache Tez&lt;/a&gt;), and to round the week out &lt;a href=&quot;/technologies/apache-zookeeper&quot;&gt;Apache ZooKeeper&lt;/a&gt;. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;There are some interesting ideas in &lt;a href=&quot;/technologies/apache-apex&quot;&gt;Apache Apex&lt;/a&gt; - building low level flexible DAGs with custom operators is powerful, and their messages about minimising the amount of ceremony and focusing on business logic are great. However it feels very much like a commercial product in open source clothing (its absolutely not alone in this regard), and I?m not sure I see it making much headway. There could be potential in the graphical tools and extra stuff that comes as part of the DataTorrent commercial product however - something I?d like to take a deeper look at in the future.&lt;/p&gt; &lt;p&gt;I?m not fully informed on the history of &lt;a href=&quot;/technologies/apache-crunch/&quot;&gt;Apache Crunch&lt;/a&gt;, however I?m assuming that it was created to make working with MapReduce easier, and based on how many companies seem to have adopted it appears to have been pretty successful, with support for running over Spark presumably allowing these companies to migrate their Crunch works to Spark to gain the benefits. I?m also going to make a wild assumption that Crunch?s time has been and gone, and now that there are alternatives to MapReduce there?s little value in Crunch unless you?re already a heavy user.&lt;/p&gt; &lt;p&gt;To to &lt;a href=&quot;/technologies/apache-flink&quot;&gt;Apache Flink&lt;/a&gt;. There?s a lot to like about Flink, and it feels like it could be a serious contender as a leading stream processing platform if it can continue its growth, contributions and adoption. I?d love to hear from anyone that?s had experience with using Flink.&lt;/p&gt; &lt;p&gt;I have to say I?m conflicted about &lt;a href=&quot;/technologies/apache-tez&quot;&gt;Apache Tez&lt;/a&gt;. There?s part of me that sees it as an indulgence on Hortonworks part - the creation of new technology for Hive and Pig to use for executing queries when (maybe) other technologies existed already (not looking anywhere in particular Spark) that?s destined to become a footnote to history. However there?s another (perhaps larger) part of me that thinks there?s something here that I (and many people) don?t quite appreciate yet. I absolutely don?t see it as a competitor to Spark in the iterative analytics space, however I have a feeling that it might be a better and more scalable general purpose data processing engine that supports large Hive and Pig queries that Spark might struggle with. It?s great that Spark has set terabyte scale sorting benchmarks, but I?m not sure I?d want to use it to join terabyte sized datasets together. What?s going to be interesting is not the Spark vs Tez question, but more about how it holds up against new combined batch and streaming engines such as Flink.&lt;/p&gt; &lt;p&gt;And last but not least &lt;a href=&quot;/technologies/apache-zookeeper&quot;&gt;Apache ZooKeeper&lt;/a&gt;. You may not be aware you?re using it, but if you?re running almost any clustered Apache technology you probably are!&lt;/p&gt; &lt;p&gt;And with that we?re up to 28 technologies - only a couple of thousand to go!&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Bigtop to HDP</title><link>http://ondataengineering.net/blog/2017/02/17/apache-bigtop-to-hdp/</link><description> &lt;p&gt;And so we?ve come to the end of the technologies included in Apache Bigtop - it?s been a bit of a meandering trip, taking in some well known sights, some up and coming stuff, and some slightly odd and obscure pieces.&lt;/p&gt; &lt;p&gt;But time and tide wait for no man, so on we march, continuing our trip into the world of Hadoop Distributions. Up next, it?s the Hortonworks Data Platform (HDP) - we?ll start by looking at HDP itself, and then move on to the technologies it includes that we haven?t looked at yet.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Week That Was - 17/02/2017</title><link>http://ondataengineering.net/blog/2017/02/17/the-week-that-was/</link><description> &lt;p&gt;So let?s review the technologies we?ve looked at this week as we come to the end of our journey through the technologies bundled with Apache Bigtop.&lt;/p&gt; &lt;p&gt;The last Bigtop technologies we looked at where a couple of web based end user tools (&lt;a href=&quot;/technologies/hue&quot;&gt;Hue&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-zeppelin&quot;&gt;Apache Zeppelin&lt;/a&gt;), an HDFS compatible filesystem (&lt;a href=&quot;/technologies/quantcast-file-system&quot;&gt;Quantcast File System&lt;/a&gt;) and an MPP database (&lt;a href=&quot;/technologies/greenplum&quot;&gt;Greenplum&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;And today, we?ve started our look the &lt;a href=&quot;/technologies/hortonworks-data-platform&quot;&gt;Hortonworks Data Platform&lt;/a&gt;. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Of all the technologies we?ve looked at this week, &lt;a href=&quot;/technologies/hue&quot;&gt;Hue&lt;/a&gt; was the biggest surprise. It?s an open source project licensed under the Apache 2.0 licence, but is not an Apache Foundation project (it actually sits in a Cloudera github repository). It?s pitched as a general purpose user interface for Hadoop, and the range of functionality it includes was surprising - everything from managing data in HDFS to creating Ozzie workflows to monitoring YARN logs to running SQL and Solr queries. It?s not an analysis notebook ala Jupyter or Zeppelin (although it now has some basic functionality in this area), but a web front end onto all the common Hadoop components, and if you?re using Hadoop, I would strongly suggest it?s worth your time to take a look at it. Even Hortonworks bundle it (despite the fact it?s not an Apache project), although they obviously don?t advertise this.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/technologies/apache-zeppelin&quot;&gt;Apache Zeppelin&lt;/a&gt; is however an analytical notebook, with support for a wide range of languages and analytical tools. If you?re doing interactive or exploratory analytics it?s probably well worth a look.&lt;/p&gt; &lt;p&gt;The promise and potential of open source software is often overstated, but if you have a strong development team that are comfortable in opening and extending open source software then you can create fantastic capabilities specifically tuned to your business. &lt;a href=&quot;/technologies/quantcast-file-system&quot;&gt;Quantcast File System&lt;/a&gt; is a great example of this - an HDFS compatible filesystem based on an open source project (KFS) that allows Quantcast to operate at a scale that isn?t supported by other technologies. Companies that operate at extreme scale are fantastic breeding grounds for innovation, and we?ll definitely look at some of the technologies coming out of companies like Netflix, Facebook, eBay and LinkedIn in the future.&lt;/p&gt; &lt;p&gt;There are many reasons why companies open source projects - to pay back to the community, to accelerate development, and because a technology is no longer of significant commercial value. &lt;a href=&quot;/technologies/greenplum&quot;&gt;Greenplum&lt;/a&gt; feels like it falls into the final category - it?s been a commercial product since 2003 and has failed to gain any significant traction. It does however still seem to be under development (although primarily by Pivotal rather than by outside competitors), so maybe I?m misjudging this. I wouldn?t be &lt;a href=&quot;http://www.ness.com/big-data-101-the-rise-and-fall-of-greenplum-2/&quot;&gt;the only one&lt;/a&gt; however.&lt;/p&gt; &lt;p&gt;And today we looked at our second Hadoop distribution - the &lt;a href=&quot;/technologies/hortonworks-data-platform&quot;&gt;Hortonworks Data Platform&lt;/a&gt;. There?s a lot to like about HDP, especially it?s commitment to open source - I just hope that Hortonworks can work out a commercial model that makes them a sustainable business.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Week That Was - 24/02/2017</title><link>http://ondataengineering.net/blog/2017/02/24/the-week-that-was/</link><description> &lt;p&gt;And so we?ve started our foray into the technologies bundled with &lt;a href=&quot;/technologies/hortonworks-data-platform&quot;&gt;Hortonworks Data Platform&lt;/a&gt;. We?ve already looked at a large number of these technologies, but there?s a few here that are new.&lt;/p&gt; &lt;p&gt;First up this week were the Hortonworks candidates in the metadata management and security space - &lt;a href=&quot;/technologies/apache-atlas&quot;&gt;Apache Atlas&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-ranger&quot;&gt;Apache Ranger&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-knox&quot;&gt;Apache Knox&lt;/a&gt;. We?ve then finished the week with &lt;a href=&quot;/technologies/apache-falcon&quot;&gt;Apache Falcon&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-calcite&quot;&gt;Apache Calcite&lt;/a&gt;. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Everyone says they want metadata management, data catalogues and business glossaries, very few people actually build them, and they?re desperately un-sexy. Which is why you don?t often see open source technologies in this space. &lt;a href=&quot;/technologies/apache-atlas&quot;&gt;Apache Atlas&lt;/a&gt; is trying to buck that trend, with some significant commercial backers. It?s going to be interesting to see how far this gets and what level of adoptions it gets. At the moment it feels like it?s trailing Cloudera Navigator, but that?s a commercial product which perhaps gives Cloudera greater impetus to invest in it. One to come back to at some point I think.&lt;/p&gt; &lt;p&gt;We?ll also be coming back to look at the state of security in the Hadoop Ecosystem - Cloudera and MapR are supporting Apache Sentry, whereas Hortonworks are supporting &lt;a href=&quot;/technologies/apache-ranger&quot;&gt;Apache Ranger&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-knox&quot;&gt;Apache Knox&lt;/a&gt;. Competition and survival of the fittest in open source is one of it?s greatest strengths, however in this case it seems like the Cloudera Hortonworks rivalry (for want of a better word) is perhaps not helping to overall Hadoop ecosystem.&lt;/p&gt; &lt;p&gt;I really want to like &lt;a href=&quot;/technologies/apache-falcon&quot;&gt;Apache Falcon&lt;/a&gt;, however I think I need to get to know it better before I start professing any love. It?s trying to solve a real problem - managing and orchestrating your data pipelines and the data that moves between and through these - however it?s a difficult problem, and creating a reductive solution can create real limitations and constraints. Another one I?d like to return to in due course.&lt;/p&gt; &lt;p&gt;And finally &lt;a href=&quot;/technologies/apache-calcite&quot;&gt;Apache Calcite&lt;/a&gt;, here to claim the crown (from &lt;a href=&quot;/technologies/apache-zookeeper&quot;&gt;Apache ZooKeeper&lt;/a&gt;) of the best, most widely used technology you?ve probably never heard of. If you use an open source technology that has a SQL interface, you?re more than likely to be using Calcite - it provides SQL parsing, cost based optimisation and JDBC frameworks that are used in Hive, Drill, Storm, Apex, Druid, Kylin, Phoenix, Solr, Flink, Cascading and Samza amongst others. Creators of open source software often don?t get the acknowledgement they deserve, but Julian Hyde deserves our thanks and appreciation for creating what would become Apache Calcite.&lt;/p&gt; &lt;p&gt;Right - I?m done with this week. See you on the other side of the weekend.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Week That Was - 03/03/2017</title><link>http://ondataengineering.net/blog/2017/03/03/the-week-that-was/</link><description> &lt;p&gt;Right - we?re nearly at the end of the &lt;a href=&quot;/technologies/hortonworks-data-platform&quot;&gt;Hortonworks Data Platform&lt;/a&gt; technologies. Let?s summarise what we?ve looked at this week.&lt;/p&gt; &lt;p&gt;We started off with &lt;a href=&quot;/technologies/apache-slider&quot;&gt;Apache Slider&lt;/a&gt;, then looked at &lt;a href=&quot;/technologies/apache-storm&quot;&gt;Apache Storm&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-accumulo&quot;&gt;Apache Accumulo&lt;/a&gt;, before finishing off with &lt;a href=&quot;/technologies/livy&quot;&gt;Livy&lt;/a&gt; and &lt;a href=&quot;/technologies/hortonworks-data-platform-search/&quot;&gt;HDP Search&lt;/a&gt;. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/technologies/apache-slider&quot;&gt;Apache Slider&lt;/a&gt; is interesting. It feels like a technology that allows you to run any long running app on YARN and have it play nicely with other YARN apps should be something people care about, because isn?t the whole selling point of Hadoop that you can have on analytical cluster that supports multiple workloads that all play nicely together, but it looks like outside of Hortonworks there?s very take up. It?s been in incubation since April 2014, and it seems like the biggest barrier to graduation is that there simply aren?t any committers outside of Hortonworks.&lt;/p&gt; &lt;p&gt;If there?s any one technology that kick-started the rise of the streaming data engines is has to be &lt;a href=&quot;/technologies/apache-storm&quot;&gt;Apache Storm&lt;/a&gt; - the granddaddy of streaming technologies and still the 900 pound gorilla in the room. It?s not perfect, people have taken a lot of potshots at it over the years, and Twitter have now moved on (to Heron), however it?s been successful for a reason, and it looks like it?s been given a new lease of life after joining the Apache foundation, so if you?re looking at streaming use cases I don?t think you can afford not to look at it. Brush up on your micro batch vs record at a time considerations first however.&lt;/p&gt; &lt;p&gt;It want to look at &lt;a href=&quot;/technologies/apache-hbase&quot;&gt;Apache HBase&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-accumulo&quot;&gt;Apache Accumulo&lt;/a&gt; in more detail in the future, however Accumulo is gaining good adoption, is bundled with most Hadoop distributions, and has some interesting differentiations from HBase.&lt;/p&gt; &lt;p&gt;And on to &lt;a href=&quot;/technologies/livy&quot;&gt;Livy&lt;/a&gt; - a little piece of technology that?s come out of Cloudera that underpins the ability for analytical notebooks to run Spark code on remote clusters. I wonder how much it rankles Hortonworks to distribute Livy and Hue - both (open source Apache licenced) technologies that currently sit in a Cloudera repository in GitHub.&lt;/p&gt; &lt;p&gt;And last up for this week is &lt;a href=&quot;/technologies/hortonworks-data-platform-search/&quot;&gt;HDP Search&lt;/a&gt; - a custom bundling of Solr (along with a bunch of other technologies), built and maintained by Lucidworks and distributed as an add on to the Hortonworks Data Platform. It means that Solr doesn?t come out of the box with HDP (you have to download an extra Ambari management pack manually to install it), but it looks like a great partnership for Hortonworks - you get support from arguably the leading experts in Solr, and get Solr bundled with a bunch of other useful stuff that you don?t get with the other distributions.&lt;/p&gt; &lt;p&gt;Next week - the final HDP technologies. Have a great weekend.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Week That Was - 10/03/2017</title><link>http://ondataengineering.net/blog/2017/03/10/the-week-that-was/</link><description> &lt;p&gt;And so we come to the end of the &lt;a href=&quot;/technologies/hortonworks-data-platform&quot;&gt;Hortonworks Data Platform&lt;/a&gt; technologies. On Monday we?ll start looking at the remainder of the Hortonworks technology offerings (yes - I know we?re meant to be working our way through the Hadoop distributions - it?ll only be a short detour), but for now let?s summarise what we?ve looked at this week.&lt;/p&gt; &lt;p&gt;First up was the second add-on to HDP based on a partnership with another company - &lt;a href=&quot;/technologies/apache-hawq&quot;&gt;Hortonworks HBD&lt;/a&gt; (aka Pivotal HDB, aka Apache Hawq). We then looked at the management components of the HDP stack - &lt;a href=&quot;/technologies/apache-ambari&quot;&gt;Apache Ambari&lt;/a&gt;, &lt;a href=&quot;/technologies/cloudbreak&quot;&gt;Cloudbreak&lt;/a&gt; and &lt;a href=&quot;/technologies/hortonworks-smartsense&quot;&gt;Hortonworks SmartSense&lt;/a&gt;. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;So - &lt;a href=&quot;/technologies/apache-hawq&quot;&gt;Hortonworks HBD&lt;/a&gt;. It?s Pivotal HDB - you download it from Pivotal and the Hortonworks documentation links through to the Pivotal documentation, but with Hortonworks (according to the press release) providing customer support and professional implementation services. The press release is worth a look - as part of the deal Pivotal agreed to drop Pivotal HD (their Hadoop distribution) and resell HDP instead, and Hortonworks agreed to distribute HDB. But given their investment in Hive through the Stinger initiative, you have to wonder how interested Hortonworks are in pushing it. Which is possibly a shame, because Apache Hawq is probably the most mature SQL engine available on Hadoop today - whether that?s a good or bad thing, and how much traction it?s going to get I don?t know.&lt;/p&gt; &lt;p&gt;And on to &lt;a href=&quot;/technologies/apache-ambari&quot;&gt;Apache Ambari&lt;/a&gt; - Hortonworks competitor to Cloudera Manager. What I found most interesting about Ambari was the list of contributors - 50 from Hortonworks, 6 from IBM, 6 from Pivotal, 4 from RedHat plus some more - 82 in total. That?s a pretty significant development capacity, and probably goes to show how valuable having an easy way to provision and manage Hadoop clusters is to its adoption. What also struck me about Ambari was how much it didn?t feel like an open source technology - it only (realistically) supports the installation of HDP, the committers are all employees of large companies and the Apache documentation is pretty poor.&lt;/p&gt; &lt;p&gt;I split &lt;a href=&quot;/technologies/apache-ambari/ambari-views&quot;&gt;Ambari Views&lt;/a&gt; out from Ambari for a couple of reasons. Firstly, I ran out of time on Tuesday to include it in the Ambari technology summary, and there was probably too much to put in a single summary anyway, but also because I think Ambari Views is targeting a slightly different use case and group of users than Ambari. It feels like Hortonworks is lining this up as a competitor to Hue (they have a Hue to Ambari migration tool for starters), however it feels lightweight (in features) and heavyweight (in terms of hardware requirements) compared to Hue, and I don?t see it gaining the same traction. Perhaps if Cloudera had donated Hue to the Apache Foundation Ambari Views wouldn?t even exist.&lt;/p&gt; &lt;p&gt;And &lt;a href=&quot;/technologies/cloudbreak&quot;&gt;Cloudbreak&lt;/a&gt; - an extremely interesting technology where cloud infrastructure meets Docker meets Hadoop. It feels like early days for Cloudbreak, however Hadoop in the Cloud (on or off premesis) is seeing massive investment from both Cloudera and Hortonworks at the moment, and it?s a really interesting area that I?d love to come back to at some point.&lt;/p&gt; &lt;p&gt;Lastly to &lt;a href=&quot;/technologies/hortonworks-smartsense&quot;&gt;Hortonworks SmartSense&lt;/a&gt;. Hortonworks? business model is interesting - their commitment to open source means you can use their entire technology stack for free, you only pay for support and professional services, but this means their support and services offering has to deliver value and be worth the money (which also means that their stack can?t be too reliable or easy to manage without their help). SmartSense is their only technology that isn?t open source, and is the key piece in the value of their support offering. And the fact it appears to contain a bunch of cluster analytics that aren?t available through Ambari is an interesting facet of that.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hortonworks</title><link>http://ondataengineering.net/blog/2017/03/13/hortonworks/</link><description> &lt;p&gt;Right - we?ve finished with our probing into the &lt;a href=&quot;/technologies/hortonworks-data-platform&quot;&gt;Hortonworks Data Platform&lt;/a&gt; technologies, it?s time to move on.&lt;/p&gt; &lt;p&gt;So today, we?re going to add &lt;a href=&quot;/tech-vendors/hortonworks/&quot;&gt;Hortonworks&lt;/a&gt; to our vendor catalogue, and over the next week and a bit have a look at some of the other technology offerings they have outside of HDP. Once we?re done with that, we?ll revert to our original course of looking at all the major Hadoop distributions.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Week That Was - 17/03/2017</title><link>http://ondataengineering.net/blog/2017/03/17/the-week-that-was/</link><description> &lt;p&gt;And another week passes?&lt;/p&gt; &lt;p&gt;This week we?ve wrapped up the &lt;a href=&quot;/tech-vendors/hortonworks&quot;&gt;Hortonworks&lt;/a&gt; technology stack (give or take). On Monday we?ll review what we?ve found, and look ahead to our next destination - Cloudera.&lt;/p&gt; &lt;p&gt;So what have we looked at this week? We took a spin through &lt;a href=&quot;/technologies/hortonworks-data-flow&quot;&gt;Hortonworks DataFlow&lt;/a&gt;, Hortonworks? bundling of &lt;a href=&quot;/technologies/apache-nifi&quot;&gt;Apache NiFi&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-kafka&quot;&gt;Apache Kafka&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-storm&quot;&gt;Apache Storm&lt;/a&gt;. We?ve looked at Kafka and Storm previously, but we paused this week to look at &lt;a href=&quot;/technologies/apache-nifi&quot;&gt;Apache NiFi&lt;/a&gt; and it?s sub-project &lt;a href=&quot;/technologies/apache-nifi/minifi&quot;&gt;MiNiFi&lt;/a&gt; in more detail.&lt;/p&gt; &lt;p&gt;And we finished off by looking at &lt;a href=&quot;/technologies/hortonworks-data-cloud-for-aws/&quot;&gt;HDCloud for AWS&lt;/a&gt;. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/technologies/hortonworks-data-flow&quot;&gt;Hortonworks DataFlow&lt;/a&gt; (HDF to it?s friends) is Hortonworks? big push into analytics on data in motion, and more specifically into analytics in the Internet of Things world (or Internet of Anything as they refer to it). It?s a compelling story - the ability to deploy key real time analytical technologies independently from your Hadoop cluster (which can now focus on the batch historical analytical use cases) - and comes with the introduction of a new technology - &lt;a href=&quot;/technologies/apache-nifi&quot;&gt;Apache NiFi&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;What to say about &lt;a href=&quot;/technologies/apache-nifi&quot;&gt;Apache NiFi&lt;/a&gt;? There?s a use case here that I think NiFi fills almost unapproachably well - specifically getting batch (and probably mini batch) data to your analytical cluster. Previously you?d be looking at a bunch of technologies - Sqoop for database unloads, and some combination of shell scripts, FTP transfers, custom jobs to pull data from queues etc. etc. NiFi wraps all of this up - giving you a single solution to bring data from anywhere to a place where you can exploit it. The visualisation of the data moving through your flows, the ability to view this data, to get detailed provenance of where every file came from and when, and to perform common file level transformations just make this a great fit for this use case (although I?m never entirely convinced by the develop, test, release and configuration management story of GUI based tools, but that?s a discussion for another day). Where I think it has stiffer competition, and where I?m not as wholly convinced, is in the high volume, low latency, real-time event data space. There are a lot of well established technologies in this space (Logstash, FluentD and Heka for starters), and I?m not entirely convinced that NiFi is well architected for this use case. Do I really want provenance and record level state tracking when I?m bringing in billions of records per day - that seems like a significant overhead to me. By it?s a space NiFi is targeting, both with &lt;a href=&quot;/technologies/apache-nifi/minifi&quot;&gt;MiNiFi&lt;/a&gt; (which supports collection, transformation and forwarding out at the edge), and with some bold claims about throughput. I?m happy to accept I?ve missed something here, and I?d love to hear from anyone that can talk to this with some experience and evidence?&lt;/p&gt; &lt;p&gt;I?m going to update the Hortonworks vendor page on Monday with more information about their product offerings, as their Cloud offerings are a little more complex and convoluted that I was expecting. However &lt;a href=&quot;/technologies/hortonworks-data-cloud-for-aws/&quot;&gt;HDCloud for AWS&lt;/a&gt; is their only Hortonworks branded cloud offering - a tool that allows you to deploy and resize HDP clusters in AWS, but with a limited set of technologies, focusing on Hive, Spark and Zeppelin. It?s brand new, only coming out at the end of 2016, and it appears to overlap with a more general capability that &lt;a href=&quot;/technologies/cloudbreak&quot;&gt;Cloudbreak&lt;/a&gt; is targeting. We?ll keep an eye on these, as it feels like next year is going to see a lot of movement in the Hadoop on Cloud space.&lt;/p&gt; &lt;p&gt;Right - back to the grindstone before the escape of the weekend. See you all next week for our first looks at Cloudera?s product offerings.&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hadoop Distributions</title><link>http://ondataengineering.net/tech-categories/hadoop_distributions/</link><description> &lt;p&gt;Projects that bundle together specific versions of multiple components around the Hadoop ecosystem, certify that these work together, and deliver packages and other installation mechanisms for installing and managing these.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt; &lt;/td&gt; &lt;td&gt;An Apache open source distribution of Hadoop. Packages up a number of Apache Hadoop components, certifies their interoperability using an automated integration test suite, and packages them up as RPMs/DEBs packages for most flavours of Linux. Also includes virtual machine images and vagrant, docker and puppet recipes for deploying and working with Hadoop. Does not patch projects for distribution, but requires any fixes to be made upstream. An Apache Open Source project, started by Cloudera, donated to the Apache foundation in June 2011, graduating in September 2012, with a 1.0 release in August 2015 based on Hadoop 2.6. Since donating the project, Cloudera have backed away from it, with the project lead moving to Pivotal in December 2013. Now has a broad range of contributors, however usage by the major distributors is not clear.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A distribution of Hadoop based on a commitment to the Apache open source ecosystem. All bundled projects are Apache open source projects based on official Apache project releases, with any patches for bug fixes or new features official Apache project patches pulled from later releases of the project. Available as RPMs or can be installed using Apache Ambari (for local installs) or Cloudbreak (for installation on cloud platforms). Also comes with a number of add-ons, including ODBC and JDBC drivers for Hive and Spark SQL, HDP Search and Hortonworks HDB. Provided free of charge, with training, consultancy and support available from Hortonworks, along with their proprietary SmartSense support tool. First released in June 2012.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;!-- Tech Vendor metadata --&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>The Apache Software Foundation</title><link>http://ondataengineering.net/tech-vendors/apache/</link><description> &lt;p&gt;The Apache Software Foundation is a non-profit organisation that supports a wide range of open source projects, including providing and mandating a standard governance model (including the use of the Apache license), holding all trademarks for project names and logos, and providing legal protection to developers. It was founded in 1999 and now oversees nearly 200 projects.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Vendor Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Apache&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Vendor Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-accumulo/&quot;&gt;Apache Accumulo&lt;/a&gt; &lt;/td&gt; &lt;td&gt;NoSQL wide-column datastore based on BigTable. Supports horizontal scalability, cell based access control (based on arbitrary boolean expressions of user security labels), high availability, atomic read-modify-write operations, map reduce support (both as a source and sink), table constraints, LDAP and Kerberos integration, and replication between instances. Comes with a web based monitoring interface (Accumulo Monitor) and a CLI. Written in Java, with thrift based API allowing access from other languages including C++, Python, Ruby. Originally developed at the NSA, donated to the Apache Foundation in September 2011, before graduating in March 2012, and is still under active development.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-ambari/&quot;&gt;Apache Ambari&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Platform for installing, managing and monitoring Apache Hadoop clusters. Supports the installation of different versions of different distributions of Hadoop through Stack definitions (with support for HDP out of the box, and further stacks and add ons available through management packs), and the specification of Blueprints (cluster layouts and configuration for a given Stack) that can be used to programmatically create multiple clusters (e.g. dev, test and production). Also supports both rolling (no downtime) and express (faster but with downtime) upgrades; cluster administration (including adding and removing nodes/services, viewing the status of nodes/services, and configuring services with the versioning of configuration and the ability to rollback changes); the automated Kerberization of clusters; the collection, storage (in HBase) and visualisation (via Grafana or through dashboards in Ambari) of system and Hadoop component metrics via the Ambari Metrics System (AMS); alerting on statuses and metrics; the collection, storage (in Solr) and searching/viewing of log entries from across the Hadoop cluster (currently in technical preview); and a framework for UI components within Ambari (Ambari Views, treated here as a sub-project). Web based, with a REST API, and backed by a backend database (Oracle, MySQL or Postgres). Donated to the Apache Foundation by Hortonworks, IBM and Yahoo in August 2011 as the Hadoop Management System (HMS), graduating in December 2013 after changing it's name to Ambari. Still under active development with a large number of contributors.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-apex/&quot;&gt;Apache Apex&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Data transformation engine based on Directed Acyclic Graph (DAG) flows configured through a Java API or via JSON, with a stated focus on performance, code re-use, testability and ease of operations. Runs over YARN and HDFS with native support for both micro-batch streaming and batch uses cases, and includes a range of standard operators and connectors (called Apex Malhar). An Apache project, graduating in April 2016, having been originally donated in August 2015 by DataTorrent from their DataTorrent RTS product which launched in June 2014. Java based, with development lead by DataTorrent who distribute it as DataTorrent RTS in two editions - a Community Edition (which also includes a basic management GUI and a tool for configuring Apex for data ingestion), and an Enterprise Edition (which further includes a graphical transformation editor, a self service dashboard, security integration and commercial support, and is also available as a cloud offering).&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-atlas/&quot;&gt;Apache Atlas&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A metadata and data governance solution for Hadoop. Supports an extensible metadata model with out of the box support for Hive datasets and data lineage from Hive queries and Sqoop imports, with limited support for Falcon, Storm and Kafka. Allows datasets and data items to be tagged (and for these tags to be used for access control by Apache Ranger), and includes support for business taxonomies as a technical preview. Implemented as a graph based database using Titan (which by default uses HBase and Solr), with a web based user interface and a REST API for searching and visualising/retrieving metadata, and Kafka topics for the ingest of metadata (primarily from hooks in metadata sources such as Hive or Sqoop) and the publishing of metadata change events. An incubating Apache project, donated to the Apache Foundation in May 2015 by the Hortonworks Data Governance Initiative in partnership with Aetna, Merck, Target, Schlumberger and SAS. Has not yet reached a v1.0 milestone or graduated as a top level Apache project, but is still under active development.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-avro/&quot;&gt;Apache Avro&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Data serialisation framework that supports both messaging and data storage. Primarily uses a compact binary format but also supports a JSON format. Supports a range of data structures (including records, enumerations, arrays and maps) with APIs for a wide range of both static and dynamically typed languages. Schema based, with schemas primarily specified in JSON, and support for both code generation from schema definitions as well as dynamic runtime usage. Schemas are serialised alongside data, with support for automatic schema resolution if the schema used to read the data differs from that used to write it. Started as an Hadoop sub-project by Cloudera in April 2009, with an initial v1.0 release in July 2009, before becoming a top level Apache project in May 2010. Has seen significant adoption in the Hadoop ecosystem.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt; &lt;/td&gt; &lt;td&gt;An Apache open source distribution of Hadoop. Packages up a number of Apache Hadoop components, certifies their interoperability using an automated integration test suite, and packages them up as RPMs/DEBs packages for most flavours of Linux. Also includes virtual machine images and vagrant, docker and puppet recipes for deploying and working with Hadoop. Does not patch projects for distribution, but requires any fixes to be made upstream. An Apache Open Source project, started by Cloudera, donated to the Apache foundation in June 2011, graduating in September 2012, with a 1.0 release in August 2015 based on Hadoop 2.6. Since donating the project, Cloudera have backed away from it, with the project lead moving to Pivotal in December 2013. Now has a broad range of contributors, however usage by the major distributors is not clear.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-calcite/&quot;&gt;Apache Calcite&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A framework for building SQL based data access capabilities. Supports a SQL parser and validator, tools for the transformation and (cost based) optimisation of SQL expression trees, and an adapter framework for accessing metadata and executing queries (including out of the box adapters for a number of database technologies as well as CSV files and POJO objects), along with specific support for streaming SQL queries and optimising data cube queries to use materialised views. Also includes (as a sub-project named Avatica), a framework for building database drivers with support for a standard JDBC driver, server and wire protocols, plus a local embeddable JDBC driver. Used in a range of other projects including Drill, Flink, Hive, Kylin, Phoenix, Samza, Storm and Cascading. An Apache project, originally created by Julian Hyde in May 2012 as Optiq, donated to the Apache Foundation in May 2014, graduating in October 2015 following a v1.0 release in January 2015. Under active development with a range of contributors.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-crunch/&quot;&gt;Apache Crunch&lt;/a&gt; &lt;/td&gt; &lt;td&gt;An abstraction layer over MapReduce (and now Spark) that provides a high level Java API for creating data transformation pipelines, originally designed to make working with MapReduce easier based on the Google FlumeJava paper. Also includes connectors for HBase, Hive and Kafka, Java 8 lambda support, an experimental Scala wrapper for the API (Scrunch), and support for in memory pipelines and helper classes to support testing. Open sourced by Cloudera in October 2011, donated to the Apache Foundation in May 2012, before graduating in February 2013. Support for Spark was added as part of v0.10 in June 2014. Still being maintained, and appears to have had been adopted at a number of large companies, but with limited new development.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-datafu/&quot;&gt;Apache DataFu&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A set of libraries for working with data in Hadoop. Consists of two sub-projects - DataFu Pig (a set of Pig User Defined Functions) and DataFu Hourglass (a framework for incremental processing using MapReduce). Originally created at LinkedIn, with the Pig UDFs being open sourced in January 2012 as DataFu, with a v1.0 release in September 2013. Split into sub-projects in October 2013 when LinkedIn open sourced DataFu Hourglass and added it to the project. Donated to the Apache Foundation in January 2014, however is still incubating and has not yet graduated. Last release was v1.3 in November 2015 (albeit with a very minor v1.3.1 release in August 2016), with little development activity since this time.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-falcon/&quot;&gt;Apache Falcon&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Data feed management system for Hadoop. Supports the definition, scheduling and orchestration (including support for late data and retry policies) of data processing pipelines (referred to as processes, with support for Ozzie, Spark, Hive and Pig jobs), the management of the data produced and consumed by these pipelines (referred to as feeds, with support for data in HDFS and Hive) and the generation and visualisation of pipeline lineage information, all across multiple Hadoop clusters. Also includes the ability to mirror or replicate HDFS and Hive data between clusters, to failover processing between clusters and to import and export data using Sqoop. Supports both a web and command line interface and a REST API. An Apache project, graduating in December 2014, having been originally donated by inMobi in April 2013. Hasn't yet reached a v1.0 milestone, however still under development led by inMobi and Hortonworks with a range of other contributors.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-flink/&quot;&gt;Apache Flink&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Specialised stream processing technology inspired by the Google Data Flow model. Based on a single record (not micro batch) model, with exactly once processing semantics (for supported sources and sinks) via light weight checkpointing, and focusing on high throughput, low latency use cases. Supports both a Java and Scala API, with a fluent DataStream API for working with continuous data flows (including a flexible windowing API that supports both event time and processing time windows and support for out of order or late data), and a DataSet API for working with batch data sets (that uses the same streaming execution engine). Also supports a number of connectors and extra libraries, including experimental support for SQL expressions, a CEP library (FlinkCEP) that can be used to detect complex event patterns, a beta package for running Storm apps on Flink, a graph processing library (Gelly) and a machine learning library (FlinkML). Clustered, with support for YARN and Mesos as well as standalone clusters. Open sourced by Data Artisans in April 2013, donated to the Apache Foundation in April 2014 before graduating in August 2014. Under active development with a large number of contributors and a range of user case studies. Sold as a hosted managed service (dA Platform) by Data Artisans who also supply training.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-flume/&quot;&gt;Apache Flume&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Specialist technology for the continuous movement of data using a set of independent agents connected together into pipelines. Supports a wide range of sources, targets and buffers (channels), along with the ability to chain agents together and to modify and drop events in-flight. Designed to be highly reliable, and to support reconfiguration without the need for a restart. Heavily integrated with the Hadoop ecosystem. An Apache project, donated by Cloudera in June 2011, graduating in June 2012, with a v1.2 release (the first considered ready for production use) in July 2012. Java based, with commercial support available as part of most Hadoop distributions.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-giraph/&quot;&gt;Apache Giraph&lt;/a&gt; &lt;/td&gt; &lt;td&gt;An iterative, highly scalable graph processing system built on top of MapReduce and based on Pregel, with a number of features added including a framework for creating re-usable code (called blocks). An Apache project, graduating in May 2012, having been originally donated by Yahoo in August 2011. Java based, no commercial support available, but is mature and has been adopted by a number of companies (including LinkedIn and most famously Facebook who scaled it to process a trillion edges), and has a number of active developers.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-hadoop/&quot;&gt;Apache Hadoop&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A distributed storage and compute platform consisting of a distributed filesystem (HDFS) and a cluster workload and resource management layer (YARN), along with MapReduce, a solution built on HDFS and YARN for massive scale parallel processing of data. Has an extensive ecosystem of compatible technologies. An Apache Open Source project, started in January 2006 as a Lucene sub-project, becoming a top level project in January 2008, with a 1.0 release in December 2011 (containing HDFS and MapReduce), and a 2.2 release (the first 2.x GA release) in October 2013 (adding YARN). Very active, with a deep and broad range of contributors, and backing from multiple commercial vendors.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-hama/&quot;&gt;Apache Hama&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A general purpose BSP (Bulk Synchronous Parallel) processing engine inspired by Pregel and DistBelief that runs over Mesos or YARN. Supports BSP, graph computing and machine learning programming models, as well as Apache MRQL. An Apache project, donated in 2008, and graduated in 2012. Java based, with no commercial support available, limited case studies for it's use and limited active developers, with the last release being in June 2015.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-hawq/&quot;&gt;Apache Hawq&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A port of the Greenplum MPP database (which itself is based on PostgreSQL) to run over YARN and HDFS. Supports all the features of Greenplum (ACID transactions, broad SQL support and in database language and analytics support, including support for Apache MADLib), integrated with Apache Ambari, an Input Format for MapReduce to read Hawq tables, and both row and Parquet (column) based storage of data managed by Hawq. Also supports queries over data not managed by Hawq via external tables, with a Java based framework (PXF) for accessing external data, and out of the box support for accessing data in HDFS (text, Avro, JSON), Hive and HBase, with a number of open source connectors also available. Fault tolerant and horizontally scalable, with the ability to scale up or down on the fly. Originally created as Pivotal Hawq based on a fork of Greenplum in 2011, with an initial 1.0 release as part of Pivotal HD in July 2013. Open sourced and donated to the Apache Foundation in September 2015, becoming Apache Hawq, with the first open source release (2.0) in October 2016. Development led by Pivotal, who also distribute binaries as Pivotal HDB and provide training, consultancy and support. Pivotal HDB is also available as Hortonworks HDB.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-hbase/&quot;&gt;Apache HBase&lt;/a&gt; &lt;/td&gt; &lt;td&gt;NoSQL wide-column datastore based on Google BigTable. Focuses on random real-time access to data, and supports horizontal scalability, consistent reads and writes, versioning and fine grained security controls. Runs on Hadoop and HDFS, and is heavily integrated with the Hadoop ecosystem. An Apache project, first released as part of Hadoop 0.15 in October 2007 before graduating as a top level project in May 2010. Java based, with commercial support available as part of most Hadoop distributions.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-hive/&quot;&gt;Apache Hive&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Technology that supports the exposure of data in Hadoop as structured tables and the execution of analytical SQL queries over these. Consists of a number of distinct components (that we treat as sub-projects) including Hive Metastore (stores the definitions of the structured tables), Hive Server (supports the execution of analytical SQL queries as MapReduce, Spark or Tez jobs) and HCatalog (allows MapReduce and Pig jobs to read and write Hive tables). First released by Facebook as an Hadoop contrib module in September 2008, becoming an Hadoop sub-project in November 2008, and a top level Apache project in September 2010, following a first official stable release (0.3) in April 2009. Java based, under active development from a number of large commercial sponsors, with commercial support available as part of most Hadoop distributions.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-ignite/&quot;&gt;Apache Ignite&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A distributed in-memory data fabric/grid, supporting a number of use cases including a key value store (with SQL support), real time stream/event processing engine, arbitrary compute, long running service management, an in-memory HDFS compatible file system for acceleration of Hadoop jobs, and in-memory shared Spark RDDs. An Apache project, graduating in September 2015, having been originally donated by GridGain from their In-Memory Data Fabric product launched in 2007. Java based, with development lead by GridGain who also supply commercial support (as GridGain Professional with ongoing Q&amp;A and bug fixes before they're included in Ignite) along with GridGain Enterprise (which includes extra features such as a management GUI, enterprise security and rolling upgrades).&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Technology for buffering and storing real-time streams of data between publishers to subscribers, with a focus on high throughput at low latency. Based on a distributed, horizontally scalable architecture, with messages organised into topics which are partitioned and replicated across nodes to provide resilience and written to disk to provide persistence. Topics may have multiple publishers and subscribers, with ability to do fault tolerant reads and to load balance across subscribers. Records consist of a key, value and timestamp, with the ability to compact topics to remove updates and deletes by key. Supports a full security model, and the ability to set quotas. Comes with a Java client, but clients for a wide range of languages are also available. Has two sub-projects (Kafka Connect and Kafka Streams) that are bundled with the main product. Originally developed at LinkedIn, being open sourced in January 2011, before being donated to the Apache Foundation in July 2011. Graduated in October 2012, and although it has not had a v1.0 release is considered production quality and stable. Development is primarily led by Confluent (which was founded by the team that built Kafka at LinkedIn), who distribute a Confluent Open Source product (which includes further clients and connectors) and a subscription based Confluent Enterprise product (which includes management, replication and data balancing features and commercial support under a subscription licence). Commercial support is also available from most Hadoop vendors.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-knox/&quot;&gt;Apache Knox&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A stateless gateway for the Apache Hadoop ecosystem that provides perimeter security. Includes support for user authentication (via LDAP, Active Directory and a number of single sign on solutions), access authorisation on a per service basis, transitions to Kerberos authentication, reverse proxying and auditing, extension points for supporting new services, audit capabilities, and out of the box support for a number of Hadoop technology end points. An Apache project, started by Hortonworks in February 2013, donated to the Apache Foundation two months later in April, before graduating in February 2014. Hasn't yet reached a v1.0 milestone, however still under active development.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-mahout/&quot;&gt;Apache Mahout&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Machine learning technology comprising of a Scala based linear algebra engine (codenamed Samsara) with an R-like DSL/API that runs over Spark (with experimental support for H2O and Flink), an optimiser, a wide variety of pre-made algorithms, and a Scala REPL (based on Spark Shell) for interactive execution. Can be embedded and integrated within larger applications, for example MLlib when running over Spark. Also includes some original, now deprecated, algorithms implemented over MapReduce. Created in January 2008 as a Lucene sub-project, becoming a top level Apache project in April 2010. The original MapReduce algorithms were deprecated and Samsara introduced as part of v0.10 in April 2015. Supported by most major Hadoop distributions, and still under active development.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-nifi/&quot;&gt;Apache NiFi&lt;/a&gt; &lt;/td&gt; &lt;td&gt;General purpose technology for the movement of data between systems, including the ingestion of data into an analytical platform. Based on directed acyclic graph of Processors and Connections, with the unit of work being a FlowFile (a blob of data plus a set of key/value pair attributes). Supports guaranteed delivery of FlowFiles, with NiFi resiliently storing state (by default to a local write ahead log) and data blobs (by default a set of local partitions on disk), with all transformation logic executed via a thread pool within the NiFi instance (with the option to deploy multiple NiFi instances as a cluster). All flows are configured in a graphical user interface, which is also used for management and operations (starting/stopping individual Processors and viewing real time statuses, statistics and other information). Also supports data provenance (reporting on the processing events and lineage of individual FlowFiles), scheduling of Processor execution (based on periodic execution timers or cron specifications), multi-threaded Processor execution, configuration of Processor batch sizes (to enable low latency or high throughput), prioritised queues within Connections (allowing FlowFiles to be processed based on their age or a priority attribute as an alternative to FIFO), back pressure (based on counts or data volume against individual Connections) and pressure release (automatic discarding of FlowFiles based on their age), the ability to stream data to and from other NiFi instances and other streaming technologies, the ability to import and export flows as XML (flow templates), an expression language for setting Processor configuration and populating FlowFile attributes, Controller Services to provide shared services to processors (e.g. access to credentials, shared state), Reporting Tasks to output status and statistics information and a user security model. Extensible through the addition of custom Processors, Controller Services, Reporting Tasks and Prioritizers, and integrates with Apache Ranger and Apache Ambari. Originally developed at the NSA as &quot;Niagara Files&quot;, before being donated to the Apache Foundation in November 2014, graduating in July 2015. Java based, with development lead by Hortonworks after their aquisition of Onyara (which was set up by original NiFi developers to provide commercial support and services).&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-oozie/&quot;&gt;Apache Oozie&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Technology for managing workflows of jobs on Hadoop clusters. Primary concepts include workflows (a sequence of jobs modelled as a directed acyclic graph), coordinators (schedule the execution of workflows based on the time or the presence of data) and bundles (collections of coordinators), with all configuration specified in XML. Supports a range of technologies, including MapReduce, Pig, Hive, Sqoop, Spark, Java executables and shell scripts. Includes a server component, a metadata database for holding definitions and state (with support for a range of database technologies), a command line interface and a read only web interface for viewing the status of jobs. Also supports the parameterisation of workflows, the modelling of datasets (and the use of these to manage dependencies between workflows within coordinators), automatic retry and failure handling, and the ability to send job status notifications via HTTP or JMS. Open sourced by Yahoo in June 2010. Donated to the Apache Foundation in July 2011, graduating in August 2012. Commercial support available as part of most Hadoop distributions&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-parquet/&quot;&gt;Apache Parquet&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Data serialisation framework that supports a columnar storage format to enable efficient querying of data. Built using Apache Thrift, and supports complex nested data structures, using techniques from the Google Dremel paper. Consists of three sub-projects, the specification and Thrift definitions (Parquet Format), the Java and Hadoop libraries (Parquet MR) and the C++ implementation (Parquet CPP). Created as a joint effort between Twitter and Cloudera based on work started as part of Avro Trevni, with an initial v1.0 release in July 2013. Donated to the Apache Foundation in May 2014, graduating in April 2015. Has seen significant adoption in the Hadoop ecosystem.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-phoenix/&quot;&gt;Apache Phoenix&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A SQL query engine over Apache HBase tables that supports a subset of SQL 92 (including joins), and comes with a JDBC driver. Supports a range of features including ACID transactions (via Apache Tephra), user defined functions, secondary indexes, atomic upserts, views, multi tenancy tables (where each user or tenant can only see their data) and dynamic columns (which are only specified at query time). Supports a range of SQL DDL commands, creating and modifying underlying HBase tables as required, or can run over existing HBase tables in a read only mode. Comes with connectors to allow Spark, Hive, Pig, Flume and MapReduce to read and write Phoenix tables, and a number of utilities, including a bulk loader and a command line SQL tool. Open sourced by SalesForce in January 2013 at v1.0, donated to the Apache foundation in December 2013, before graduating in May 2014. Commercial support available through Hortonworks as part of HDP, with Cloudera making it available via Cloudera Labs without support. Active project with a range of contributors, including many from SalesForce and Hortonworks.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-pig/&quot;&gt;Apache Pig&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Technology for running analytical and data processing jobs against data in Hadoop. Jobs are written in Pig Latin (a custom procedural language that can be extended using user defined functions in a range of languages), which is then translated into Map Reduce or Tez (with Spark in development) for execution. Supports both a batch mode for running pre-defined scripts and an interactive mode, and connectors for reading and writing to HBase and Accumulo as well as HDFS. Originally developed at Yahoo in 2006 before being donated to the Apache Foundation in October 2007. Graduated as an Hadoop sub-project in October 2008, before becoming a top level project in September 2010. Although has not had a v1.0 release, has been production quality for many years. Commercial support available as part of most Hadoop distributions&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-ranger/&quot;&gt;Apache Ranger&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A centralised security framework for managing access to data in Hadoop. Supports integration with LDAP and Active Directory for user authentication, a web based administration interface, a REST API, and a central policy engine used by plugins within individual Hadoop components (including HDFS, Hive, HBase, Storm, Knox, Solr, Kafka, YARN, Atlas and NiFi). Supports data access, data masking, and row level filtering policies (with masking and row level filtering currently only supported by Hive), the ability to define policies against tags as well as directly against resources (with tags assigned to resources externally, e.g. in Apache Atlas), and the ability to use more complex conditions (e.g. denying access after an expiration date or based on a users location). Extendable with the ability to add support for new services (Ranger Stacks) and to add custom decision rules (via content enrichers and condition evaluators). Also supports a full audit capability of access requests and decisions, and a key management service for HDFS encryption keys. An incubating Apache project, donated in July 2014 by the Hortonworks following their acquisition of XA Secure. Has not yet reached a v1.0 milestone, but is still under active development with a range of contributors.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-slider/&quot;&gt;Apache Slider&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Framework for hosting long running distributed applications on YARN, allowing YARN to manage the resources these applications use. Can handle any application that supports a base set of requirements (including being able to install and run from a tarball), with experimental support for docker packaged applications. Operates as a YARN application master (the Slider AM), an associated command line interface and lightweight agents to manage running components. Supports manual scaling, automatic recovery, rolling upgrades and component placement controls, and includes out of the box configuration for a number of applications including Accumulo, HBase, Kafka, Memcached, Solr, Storm and Tomcat. An incubating Apache project, originally donated in April 2014. Hasn't yet reached a v1.0 milestone, however still under development led by Hortonworks.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-solr/&quot;&gt;Apache Solr&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A search server built on Apache Lucene with a REST-like API for loading and searching data. Supports a distributed deployment (SolrCloud) that can run over HDFS on an Hadoop cluster. Includes an administration web interface, an extensible plugin architecture, support for schemaless indexing, faceted, grouped and clustered results, hit highlighting, geo-spacial and graph searches, near real time indexing and searching, (experimental) streaming expressions for parallel compute (including support for MapReduce and SQL) and broad authentication and security capabilities. A sub-project of the Apache Lucene project, originally donated to the Apache foundation by CNET Networks in January 2006, graduating as a top level project in January 2007, before merging with the Lucene project in March 2010. Java based, with commercial support available as part of most Hadoop distributions (although this is bundled as Cloudera Search with CDH and HDP Search with HDP), as well as from Lucidworks.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A high performance general purpose distributed data processing engine based on directed acyclic graphs that primarily runs in memory, but can spill to disk if required, and which supports processing applications written in Java, Scala, Python and R. Includes a number of sub-projects that support more specialised analytics including Spark SQL, Spark Streaming, MLlib (machine learning) and GraphX (graph analytics). Requires a cluster manager (YARN, EC2 and Mesos are supported as well as standalone clusters) and can access data in a wide range of technologies (including HDFS, other Hadoop data sources, relational databases and NoSQL databases). An Apache project, originally started at UC Berkley in 2009, open sourced in 2010, and donated to the Apache foundation in June 2013, graduating in February 2014. v1.0 was released in May 2014, with a v2.0 release in July 2016. Java based, with development led by Databricks (who sell a Spark hosted service), and with commercial support available as part of most Hadoop distributions.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-sqoop/&quot;&gt;Apache Sqoop&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Specialist technology for moving bulk data between Hadoop and structured (relational) databases. Command line based, with the ability to import and export data between a range of databases (including mainframe partitioned datasets) and HDFS, Hive, HBase and Accumulo. Supports parallel partitioned unloads, writing to Avro, Sequence File, Parquet and text files, incremental imports and saved jobs that can be shared via a simple metadata store. An Apache project, started in May 2009 as an Hadoop contrib module, migrating to a Cloudera GitHub project in April 2010 (with a v1.0 release shortly after), before being donated to the Apache foundation in June 2011, graduating in March 2012. The last major release (v1.4) was in November 2011, with only minor releases since then. However in January 2012 a significant re-write was announced as part of a proposed v2.0 release to address a number of usability, security and architectural issues. This will introduce a new Sqoop Server and Metadata Repository, supporting both a CLI and web UI, centralising job definitions, database connections and credentials, as well as enabling support for a wider range of connectors including NoSQL databases, Kafka and (S)FTP folders. Java based, with commercial support available as part of most Hadoop distributions.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-storm/&quot;&gt;Apache Storm&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Specialised distributed stream processing technology based on a single record (not micro batch) model with at least once processing semantics. Processing flows are called topologies based on a directed acyclic graph of spouts (which produce unbounded streams of tuples) and bolts (which process streams and optionally produce output streams). Supports high throughput and low latency use cases, horizontal scalability, fault tolerance (failed workers are automatically restarted and failed over to new nodes if required), back pressure, windowing (with support for sliding and tumbling windows based on time or event counts), stateful bolts and a shared bolt storage cache (that's updatable from the command line). Also includes a higher level micro batch API (Trident) that supports exactly-once processing semantics, fault-tolerant state management and higher level operations including joins, aggregations and groupings, support for SQL (StormSQL) and frameworks and utilities to make defining and deploying topologies easier (Flux). Has both a graphical web based and command line interface, plus a REST API. Primarily written in Clojure, JVM based, but supports multiple languages through the use of Thrift for defining and submitting topologies, and the use of spouts that can interface to other languages using JSON over stdin/stdout. Originally created at BackType, before being open sourced in September 2011 after the acquisition of BackType by Twitter. Donated to the Apache Foundation in September 2013, graduating in September 2014, with a 1.0 release in April 2016. Has multiple reference cases for being deployed at scale, including Twitter, and is still under active development.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-tajo/&quot;&gt;Apache Tajo&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Distributed analytical database engine. Supports HDFS, Amazon S3, Google Cloud Storage, OpenStack Swift and local storage, and querying over Postgres, HBase and Hive tables. Provides a standard SQL interface, JDBC driver, and supports partitioning, compression and indexing (currently experimental). An Apache project, donated by Gruter in March 2013, and graduated in April 2014. Java based, with development lead by Gruter who also supply commercial support, a Tajo managed service, a data analytics hub (Qrytica) built on Tajo, and a Tajo Data Warehouse appliance.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-tez/&quot;&gt;Apache Tez&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Data processing framework based on Directed Acyclic Graphs (DAGs), that runs natively on YARN and was designed to be a replacement for the use of MapReduce within Hadoop analytical tools (primarily Hive and Pig), and therefore offer better performance with similar scalability. Targeted more at application developers rather than data engineers, includes a number of performance optimisations (including dynamic DAG re-configuration during execution and re-use of sessions and containers), and comes with a UI for viewing live and historic Tez job executions based on information in the YARN Application Timeline Server. Created by Hortonworks and donated to the Apache Foundation in February 2013 before graduating in July 2014. Still under active development, and now used by Cascading and Flink in addition to Hive and Pig.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-zeppelin/&quot;&gt;Apache Zeppelin&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A web based notebook for interactive data analytics. Supports a wide range of interpreters (including Spark, JDBC SQL, Pig, Elasticsearch, Beam, Flink, Shell, Python amongst many others), a range of output formats (plain text, HTML, mathematical expressions using MathJax and tabular data), a range of visualisations for tabular data (including the ability to add more via a JavaScript NPM based plugin system called Helium), forms for user entry of parameters, and an Angular API to enable dynamic and interactive functionality within notebooks. Has a plugable storage for notebooks (with out of the box support for git, S3, Azure and ZeppelinHub), support for multi-user environments and a security model. Open sourced by NFLabs (now called ZEPL) in 2013 before being donated to the Apache Foundation in December 2014, graduating in May 2016. Under active development with a wide range of contributors, led by ZEPL, who sell Zeppelin as a managed service (ZepplinHub).&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-zookeeper/&quot;&gt;Apache ZooKeeper&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Service for managing coordination (e.g. configuration information and synchronisation) of distributed and clustered systems. Based on a hierarchical key-value store, with support for things such as sequential nodes (whose names are automatically assigned a sequence number suffix), ephemeral nodes (which only exist whilst their owners session exists) and the ability to watch nodes. Guarantees that all writes are serial and ordered (i.e. all clients will see them in the same order), meaning it's more appropriate for low write high read scenarios. Can run in a high available cluster called an ensemble. Originally an Hadoop sub-project, but graduated to a top level Apache project in January 2011. Java based, still under active development, and used by a range of technologies including Hadoop, Mesos, HBase, Kafka and Solr.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;!-- Technology metadata --&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.apache.org/&quot;&gt;https://www.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.apache.org/foundation/how-it-works.html&quot;&gt;https://www.apache.org/foundation/how-it-works.html&lt;/a&gt; - information on the foundation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://apache.org/foundation/mailinglists.html#foundation-announce&quot;&gt;http://apache.org/foundation/mailinglists.html#foundation-announce&lt;/a&gt; - the Apache Foundation announcements mailing list&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/&quot;&gt;https://blogs.apache.org/&lt;/a&gt;; &lt;a href=&quot;https://blogs.apache.org/planet/feed/entries/rss&quot;&gt;https://blogs.apache.org/planet/feed/entries/rss&lt;/a&gt; - The set of Apache Foundation blogs&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hortonworks</title><link>http://ondataengineering.net/tech-vendors/hortonworks/</link><description> &lt;p&gt;Hortonworks is a commercial company focusing on products that support the exploitation of data both at rest and in motion. Their business model is to provide support and professional services for a range of Apache open source technologies which they package and distribute for free. They are therefore extreemly active in the Apache open source space, with committers on all the technologies they distribute, and with a history of donating projects to the Apache Foundation that they have either initiated or acquired. Hortonworks was formed in June 2011 by ex-Yahoo employees.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Vendor Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Vendor Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-ambari/&quot;&gt;Apache Ambari&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Platform for installing, managing and monitoring Apache Hadoop clusters. Supports the installation of different versions of different distributions of Hadoop through Stack definitions (with support for HDP out of the box, and further stacks and add ons available through management packs), and the specification of Blueprints (cluster layouts and configuration for a given Stack) that can be used to programmatically create multiple clusters (e.g. dev, test and production). Also supports both rolling (no downtime) and express (faster but with downtime) upgrades; cluster administration (including adding and removing nodes/services, viewing the status of nodes/services, and configuring services with the versioning of configuration and the ability to rollback changes); the automated Kerberization of clusters; the collection, storage (in HBase) and visualisation (via Grafana or through dashboards in Ambari) of system and Hadoop component metrics via the Ambari Metrics System (AMS); alerting on statuses and metrics; the collection, storage (in Solr) and searching/viewing of log entries from across the Hadoop cluster (currently in technical preview); and a framework for UI components within Ambari (Ambari Views, treated here as a sub-project). Web based, with a REST API, and backed by a backend database (Oracle, MySQL or Postgres). Donated to the Apache Foundation by Hortonworks, IBM and Yahoo in August 2011 as the Hadoop Management System (HMS), graduating in December 2013 after changing it's name to Ambari. Still under active development with a large number of contributors.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-hawq/&quot;&gt;Apache Hawq&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A port of the Greenplum MPP database (which itself is based on PostgreSQL) to run over YARN and HDFS. Supports all the features of Greenplum (ACID transactions, broad SQL support and in database language and analytics support, including support for Apache MADLib), integrated with Apache Ambari, an Input Format for MapReduce to read Hawq tables, and both row and Parquet (column) based storage of data managed by Hawq. Also supports queries over data not managed by Hawq via external tables, with a Java based framework (PXF) for accessing external data, and out of the box support for accessing data in HDFS (text, Avro, JSON), Hive and HBase, with a number of open source connectors also available. Fault tolerant and horizontally scalable, with the ability to scale up or down on the fly. Originally created as Pivotal Hawq based on a fork of Greenplum in 2011, with an initial 1.0 release as part of Pivotal HD in July 2013. Open sourced and donated to the Apache Foundation in September 2015, becoming Apache Hawq, with the first open source release (2.0) in October 2016. Development led by Pivotal, who also distribute binaries as Pivotal HDB and provide training, consultancy and support. Pivotal HDB is also available as Hortonworks HDB.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/cloudbreak/&quot;&gt;Cloudbreak&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Solution for deploying and managing Hadoop clusters on cloud infrastructure based on automatically provisioned infrastructure running base docker images with Hadoop provisioned on top via Apache Ambari using Blueprints. Includes out of the box support for Amazon Web Services, Microsoft Azure, Google Cloud Platform and OpenStack, plus a Service Provider Interface (SPI) for adding support for new providers. Supports automated scaling of clusters based on Ambari Metrics and Alerts (Periscope), custom scripts that can be run on hosts before or after deployment (Recipes), a number of out of the box Blueprints, plus a number of technical preview features, including the use of custom docker images, data locality specifiers, Kerberized clusters, support for external AD/LDAP servers and deployment on Mesos. Manageable through a web UI, a REST API, a CLI and an interactive shell. Originally created by SequenceIQ, with an initial beta release in July 2014, with SequenceIQ then acquired by Hortonworks in April 2015, and a 1.0 release of Cloudbreak included in HDP 2.3 in July 2015. Open sourced under the Apache 2.0 licence, with a stated plan for the code to be donated to the Apache Foundation.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/hortonworks-data-cloud-for-aws/&quot;&gt;Hortonworks Data Cloud for AWS&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Service that supports the creation and management of HDP clusters on Amazon Web Services (AWS). Management is done through a Cloud Controller AWS Product that provides a web interface and CLI for orchestrating the creation of AWS resources and the deployment of clusters using Ambari, and the subsequent scaling or cloning of the cluster. Supports a number of standard cluster types, including Data Science (Spark, Zeppelin), EDW-ETL (Hive, Spark) and EDW-Analytics (Hive, Zeppelin), with clusters also including Tez, Pig and Scoop, along with a number of standard node types, including worker nodes (that support HDFS and YARN) and computer nodes (that only support YARN). Clusters are designed to be ephemeral, however Amazon RDS can be used to provide persistent storage of Cloud Controller and Hive metadata, and Amazon S3 can be used to provide persistent cluster storage. Also supports Hortonworks SmartSense, cluster templates, the use of Spot Instances for compute nodes, and node recipes for executing custom scripts pre/post the Ambari cluster setup. Comes with free community support from Hortonworks. First launched in November 2016.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/hortonworks-data-flow/&quot;&gt;Hortonworks DataFlow&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A distribution of a set of Apache open source technologies (primarily NiFi, Kafka and Storm) for processing data, with all products integrated with Ranger for security and Ambari for management. All bundled projects are Apache open source projects based on official Apache project releases, with any patches for bug fixes or new features pulled from official Apache project patches from later releases of the project. Available as RPMs or can be installed using Apache Ambari (via a management pack). Provided free of charge, with training, consultancy and support available from Hortonworks. First released in September 2015 as a distribution of just NiFi following the acquisition by Hortonworks of Onyara,who were setup by the creators of NiFi to provided commercial support for it.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A distribution of Hadoop based on a commitment to the Apache open source ecosystem. All bundled projects are Apache open source projects based on official Apache project releases, with any patches for bug fixes or new features official Apache project patches pulled from later releases of the project. Available as RPMs or can be installed using Apache Ambari (for local installs) or Cloudbreak (for installation on cloud platforms). Also comes with a number of add-ons, including ODBC and JDBC drivers for Hive and Spark SQL, HDP Search and Hortonworks HDB. Provided free of charge, with training, consultancy and support available from Hortonworks, along with their proprietary SmartSense support tool. First released in June 2012.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/hortonworks-data-platform-search/&quot;&gt;Hortonworks Data Platform Search&lt;/a&gt; &lt;/td&gt; &lt;td&gt;An add on package to HDP that bundles up Solr, Banana, and a suite of libraries and tools for integrating with Solr from Hadoop (utilities for loading data from HDFS), Hive (a SerDe to allow Solr data to be read and written as a Hive table), Pig (store and load functions), HBase (replication of HBase event to Solr), Storm and Spark (both SDKs for integrating with Solr). Available as an add on Ambari management pack or as a set of RPMs. Built, maintained and supported by Lucidworks on behalf of Hortonworks, first announced in April 2014 as part of the introduction of Solr with HDP 2.1.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;a href=&quot;http://ondataengineering.net/technologies/hortonworks-smartsense/&quot;&gt;Hortonworks SmartSense&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Supports the capture of diagnostic information from Hadoop clusters (including configuration, metrics and logs from both Hadoop and the Operating System) into a bundle for upload (either manually or automatically) to the Hortonworks support portal to assist in the resolution of support issues and the delivery of cluster optimisation and preventative action recommendations, with support for anonymisation (including IP addresses and host names, with support for further custom rules) and encryption of information in bundles and a SmartSense gateway to proxy uploads if direct internet access isn't available. Also includes functionality to help understand and analyse cluster activity include the Activity Analyser (aggregates data from YARN, Tez, MapReduce and HDFS into Ambari Metrics) and Activity Explorer (an embedded instance of Apache Zeppelin with pre-built notebooks for exploring and visualising cluster activity). Installable and manageable through Apache Ambari. Part of the Hortonworks support offering, introduced in June 2015 as part of HDP 2.3.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;!-- Technology metadata --&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/&quot;&gt;https://hortonworks.com/&lt;/a&gt; - homepage&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/&quot;&gt;https://hortonworks.com/blog/&lt;/a&gt; - Hortonworks blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Hadoop</title><link>http://ondataengineering.net/technologies/apache-hadoop/</link><description> &lt;p&gt;A distributed storage and compute platform consisting of a distributed filesystem (HDFS) and a cluster workload and resource management layer (YARN), along with MapReduce, a solution built on HDFS and YARN for massive scale parallel processing of data. Has an extensive ecosystem of compatible technologies. An Apache Open Source project, started in January 2006 as a Lucene sub-project, becoming a top level project in January 2008, with a 1.0 release in December 2011 (containing HDFS and MapReduce), and a 2.2 release (the first 2.x GA release) in October 2013 (adding YARN). Very active, with a deep and broad range of contributors, and backing from multiple commercial vendors.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Hadoop&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;December 2016 - v2.7&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Sub-projects&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; Apache Hadoop&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-hadoop/hdfs/&quot;&gt;HDFS&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A highly resilient distributed cluster file system proven at extreme scale that supports user authentication, extended ACLs, snapshots, quotas, central caching, a REST API, an NFS gateway, rolling upgrades, transparent encryption and heterogeneous storage. Part of the original Hadoop code base, becoming an Apache Hadoop sub-project in July 2009.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Hadoop&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-hadoop/map-reduce/&quot;&gt;MapReduce&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A data transformation and aggregation technology proven at extreme scale that works on key value pairs and consists of three transformation stages - map (a general transformation of the input key value pairs), shuffle (brings all pairs with the same key together) and reduce (an aggregation of all pairs with the same key). Part of the original Hadoop code base, becoming an Apache Hadoop sub-project in July 2009.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Hadoop&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-hadoop/yarn/&quot;&gt;YARN&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Resource management and job scheduling &amp; monitoring for the Hadoop ecosystem. Includes support for capacity guarantees amongst other scheduling options. Added as an Apache Hadoop sub-project as part of Hadoop 2.x (with a GA release as part of 2.2 in October 2013) having been started in January 2008.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://hadoop.apache.org/&quot;&gt;http://hadoop.apache.org/&lt;/a&gt; - Project Homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://hadoop.apache.org/releases.html&quot;&gt;http://hadoop.apache.org/releases.html&lt;/a&gt; - full release history&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://medium.com/@markobonaci/the-history-of-hadoop-68984a11704&quot;&gt;https://medium.com/@markobonaci/the-history-of-hadoop-68984a11704&lt;/a&gt; - history of Hadoop&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blogs.gartner.com/merv-adrian/2016/07/30/hadoop-project-commercial-support-tracker-july-2016/&quot;&gt;http://blogs.gartner.com/merv-adrian/2016/07/30/hadoop-project-commercial-support-tracker-july-2016/&lt;/a&gt; - summary of the technologies in the common Hadoop distributions&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://hadoop.apache.org/&quot;&gt;http://hadoop.apache.org/&lt;/a&gt; - project updates and releases&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>HDFS</title><link>http://ondataengineering.net/technologies/apache-hadoop/hdfs/</link><description> &lt;p&gt;A highly resilient distributed cluster file system proven at extreme scale that supports user authentication, extended ACLs, snapshots, quotas, central caching, a REST API, an NFS gateway, rolling upgrades, transparent encryption and heterogeneous storage. Part of the original Hadoop code base, becoming an Apache Hadoop sub-project in July 2009.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Hadoop Distributed File System&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Apache Hadoop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;December 2016&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html&quot;&gt;http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html&lt;/a&gt; - HDFS documentation home&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://bradhedlund.com/2011/09/10/understanding-hadoop-clusters-and-the-network/&quot;&gt;http://bradhedlund.com/2011/09/10/understanding-hadoop-clusters-and-the-network/&lt;/a&gt; - good intro the the architecture of HDFS&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>MapReduce</title><link>http://ondataengineering.net/technologies/apache-hadoop/map-reduce/</link><description> &lt;p&gt;A data transformation and aggregation technology proven at extreme scale that works on key value pairs and consists of three transformation stages - map (a general transformation of the input key value pairs), shuffle (brings all pairs with the same key together) and reduce (an aggregation of all pairs with the same key). Part of the original Hadoop code base, becoming an Apache Hadoop sub-project in July 2009.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Apache Hadoop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;December 2016&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html&quot;&gt;http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html&lt;/a&gt; - MapReduce tutorial and documentation&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>YARN</title><link>http://ondataengineering.net/technologies/apache-hadoop/yarn/</link><description> &lt;p&gt;Resource management and job scheduling &amp; monitoring for the Hadoop ecosystem. Includes support for capacity guarantees amongst other scheduling options. Added as an Apache Hadoop sub-project as part of Hadoop 2.x (with a GA release as part of 2.2 in October 2013) having been started in January 2008.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Yet Another Resource Negotiator&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Apache Hadoop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;December 2016&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Add ons&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-slider/&quot;&gt;Apache Slider&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html&quot;&gt;http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html&lt;/a&gt; - YARN architecture overview and documentation&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Flume</title><link>http://ondataengineering.net/technologies/apache-flume/</link><description> &lt;p&gt;Specialist technology for the continuous movement of data using a set of independent agents connected together into pipelines. Supports a wide range of sources, targets and buffers (channels), along with the ability to chain agents together and to modify and drop events in-flight. Designed to be highly reliable, and to support reconfiguration without the need for a restart. Heavily integrated with the Hadoop ecosystem. An Apache project, donated by Cloudera in June 2011, graduating in June 2012, with a v1.2 release (the first considered ready for production use) in July 2012. Java based, with commercial support available as part of most Hadoop distributions.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Flume&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v1.7&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://flume.apache.org/&quot;&gt;http://flume.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://flume.apache.org/FlumeUserGuide.html&quot;&gt;http://flume.apache.org/FlumeUserGuide.html&lt;/a&gt; - user guide&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://flume.apache.org/&quot;&gt;http://flume.apache.org/&lt;/a&gt; - project updates and releases&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache HBase</title><link>http://ondataengineering.net/technologies/apache-hbase/</link><description> &lt;p&gt;NoSQL wide-column datastore based on Google BigTable. Focuses on random real-time access to data, and supports horizontal scalability, consistent reads and writes, versioning and fine grained security controls. Runs on Hadoop and HDFS, and is heavily integrated with the Hadoop ecosystem. An Apache project, first released as part of Hadoop 0.15 in October 2007 before graduating as a top level project in May 2010. Java based, with commercial support available as part of most Hadoop distributions.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;HBase&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v1.2&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://hbase.apache.org/&quot;&gt;http://hbase.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://hbase.apache.org/book.html&quot;&gt;http://hbase.apache.org/book.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://jimbojw.com/wiki/index.php?title=Understanding_Hbase_and_BigTable&quot;&gt;http://jimbojw.com/wiki/index.php?title=Understanding_Hbase_and_BigTable&lt;/a&gt; - HBase primer&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/hbase/entry/start_of_a_new_era&quot;&gt;https://blogs.apache.org/hbase/entry/start_of_a_new_era&lt;/a&gt; - introduction to HBase versioning scheme&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/hbase/&quot;&gt;https://blogs.apache.org/hbase/&lt;/a&gt; - Apache HBase Blog&lt;/li&gt; &lt;li&gt;HBase release announcements only appear to be available via the Apache announcements mailing list&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Hive</title><link>http://ondataengineering.net/technologies/apache-hive/</link><description> &lt;p&gt;Technology that supports the exposure of data in Hadoop as structured tables and the execution of analytical SQL queries over these. Consists of a number of distinct components (that we treat as sub-projects) including Hive Metastore (stores the definitions of the structured tables), Hive Server (supports the execution of analytical SQL queries as MapReduce, Spark or Tez jobs) and HCatalog (allows MapReduce and Pig jobs to read and write Hive tables). First released by Facebook as an Hadoop contrib module in September 2008, becoming an Hadoop sub-project in November 2008, and a top level Apache project in September 2010, following a first official stable release (0.3) in April 2009. Java based, under active development from a number of large commercial sponsors, with commercial support available as part of most Hadoop distributions.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Hive&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v2.1&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Sub-projects&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; Apache Hive&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-hive/hcatalog/&quot;&gt;HCatalog&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Libraries for MapReduce and Pig to read and write data to and from Hive tables, albeit with some limitations. Also supports a CLI for querying and updating the Hive Metastore, however this doesn't support the full range of Hive DDL commands. Includes WebHCat, a REST API over the HCatalog CLI that also supports the execution of MapReduce, Pig, Hive and Sqoop jobs. Donated to the Apache foundation by Yahoo in March 2011, had WebHCat folded in in July 2012, graduating as a top level project in February 2013, but then almost immediately was folded into Hive in March 2013 as part of the Hive 0.11 release. Has seem limited development since this time.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Hive&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-hive/hive-metastore/&quot;&gt;Hive Metastore&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A metadata service that allows structured tables to be defined over files in HDFS (and also HBase or Accumulo), providing an API that allows the metadata to be queried and updated by other tools including Impala, Spark SQL or RecordService. Supports partitioned and clustered tables, as well as complex field types such as arrays, maps and structs. Backed by a relational database (either MySQL, Postgres and Oracle). Part of the original Hive code base.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Hive&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-hive/hive-server/&quot;&gt;Hive Server&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Supports the execution of SQL queries over data in HDFS based on tables defined in the Hive Metastore, as well as DDL to query and update the Hive Metastore. Focus is on analytical (OLAP) use cases, with some support for batch updates to data. Originally executed queries as MapReduce jobs, but significant investment from has seen support for executing queries as Spark and as Tez jobs, with work underway to support sub second query times using Tez. Recent changes have also seen it achieve significant SQL compliance, with support for SQL:2011 analytical functions on-going. Accepts queries over an API with JDBC and ODBC drivers available, and includes Beeline, a command line JDBC client. Technically referred to as Hive Server 2, and was introduced in Hive 0.11 as a replacement for the original Hive Server to address a number of concurrency and security issues.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://hive.apache.org/&quot;&gt;http://hive.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/Home&quot;&gt;https://cwiki.apache.org/confluence/display/Hive/Home&lt;/a&gt; - documentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://hive.apache.org/downloads.html&quot;&gt;http://hive.apache.org/downloads.html&lt;/a&gt; - details of new releases&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/category/hive/&quot;&gt;http://blog.cloudera.com/blog/category/hive/&lt;/a&gt; - Cloudera Hive News&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://hortonworks.com/blog/category/hive/&quot;&gt;http://hortonworks.com/blog/category/hive/&lt;/a&gt; - Hortonworks Hive News&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>HCatalog</title><link>http://ondataengineering.net/technologies/apache-hive/hcatalog/</link><description> &lt;p&gt;Libraries for MapReduce and Pig to read and write data to and from Hive tables, albeit with some limitations. Also supports a CLI for querying and updating the Hive Metastore, however this doesn't support the full range of Hive DDL commands. Includes WebHCat, a REST API over the HCatalog CLI that also supports the execution of MapReduce, Pig, Hive and Sqoop jobs. Donated to the Apache foundation by Yahoo in March 2011, had WebHCat folded in in July 2012, graduating as a top level project in February 2013, but then almost immediately was folded into Hive in March 2013 as part of the Hive 0.11 release. Has seem limited development since this time.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;WebHCat&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Apache Hive&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/HCatalog&quot;&gt;https://cwiki.apache.org/confluence/display/Hive/HCatalog&lt;/a&gt; - HCatalog documentation home&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/WebHCat&quot;&gt;https://cwiki.apache.org/confluence/display/Hive/WebHCat&lt;/a&gt; - WebHCat documentation home&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hive Metastore</title><link>http://ondataengineering.net/technologies/apache-hive/hive-metastore/</link><description> &lt;p&gt;A metadata service that allows structured tables to be defined over files in HDFS (and also HBase or Accumulo), providing an API that allows the metadata to be queried and updated by other tools including Impala, Spark SQL or RecordService. Supports partitioned and clustered tables, as well as complex field types such as arrays, maps and structs. Backed by a relational database (either MySQL, Postgres and Oracle). Part of the original Hive code base.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Apache Hive&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin&quot;&gt;https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin&lt;/a&gt; - documentation home&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hive Server</title><link>http://ondataengineering.net/technologies/apache-hive/hive-server/</link><description> &lt;p&gt;Supports the execution of SQL queries over data in HDFS based on tables defined in the Hive Metastore, as well as DDL to query and update the Hive Metastore. Focus is on analytical (OLAP) use cases, with some support for batch updates to data. Originally executed queries as MapReduce jobs, but significant investment from has seen support for executing queries as Spark and as Tez jobs, with work underway to support sub second query times using Tez. Recent changes have also seen it achieve significant SQL compliance, with support for SQL:2011 analytical functions on-going. Accepts queries over an API with JDBC and ODBC drivers available, and includes Beeline, a command line JDBC client. Technically referred to as Hive Server 2, and was introduced in Hive 0.11 as a replacement for the original Hive Server to address a number of concurrency and security issues.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Apache Hive&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2013/07/how-hiveserver2-brings-security-and-concurrency-to-apache-hive/&quot;&gt;http://blog.cloudera.com/blog/2013/07/how-hiveserver2-brings-security-and-concurrency-to-apache-hive/&lt;/a&gt; - introduction to Hive Server 2&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Solr</title><link>http://ondataengineering.net/technologies/apache-solr/</link><description> &lt;p&gt;A search server built on Apache Lucene with a REST-like API for loading and searching data. Supports a distributed deployment (SolrCloud) that can run over HDFS on an Hadoop cluster. Includes an administration web interface, an extensible plugin architecture, support for schemaless indexing, faceted, grouped and clustered results, hit highlighting, geo-spacial and graph searches, near real time indexing and searching, (experimental) streaming expressions for parallel compute (including support for MapReduce and SQL) and broad authentication and security capabilities. A sub-project of the Apache Lucene project, originally donated to the Apache foundation by CNET Networks in January 2006, graduating as a top level project in January 2007, before merging with the Lucene project in March 2010. Java based, with commercial support available as part of most Hadoop distributions (although this is bundled as Cloudera Search with CDH and HDP Search with HDP), as well as from Lucidworks.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Solr&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;, Lucidworks&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v6.3&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform-search/&quot;&gt;Hortonworks Data Platform Search&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://lucene.apache.org/solr&quot;&gt;http://lucene.apache.org/solr&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://lucene.apache.org/solr/features.html&quot;&gt;http://lucene.apache.org/solr/features.html&lt;/a&gt; - good summary of Solr features&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.apache.org/dyn/closer.lua/lucene/solr/ref-guide/&quot;&gt;https://www.apache.org/dyn/closer.lua/lucene/solr/ref-guide/&lt;/a&gt; - PDF download of documentation for latest release&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/solr/&quot;&gt;https://cwiki.apache.org/confluence/display/solr/&lt;/a&gt; - online working version of documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Apache_Solr&quot;&gt;https://en.wikipedia.org/wiki/Apache_Solr&lt;/a&gt; - history of Solr&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://lucene.apache.org/solr/news.html&quot;&gt;http://lucene.apache.org/solr/news.html&lt;/a&gt; - project news&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://yonik.com/&quot;&gt;http://yonik.com/&lt;/a&gt; - details of new features from the creator of Solr&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Spark</title><link>http://ondataengineering.net/technologies/apache-spark/</link><description> &lt;p&gt;A high performance general purpose distributed data processing engine based on directed acyclic graphs that primarily runs in memory, but can spill to disk if required, and which supports processing applications written in Java, Scala, Python and R. Includes a number of sub-projects that support more specialised analytics including Spark SQL, Spark Streaming, MLlib (machine learning) and GraphX (graph analytics). Requires a cluster manager (YARN, EC2 and Mesos are supported as well as standalone clusters) and can access data in a wide range of technologies (including HDFS, other Hadoop data sources, relational databases and NoSQL databases). An Apache project, originally started at UC Berkley in 2009, open sourced in 2010, and donated to the Apache foundation in June 2013, graduating in February 2014. v1.0 was released in May 2014, with a v2.0 release in July 2016. Java based, with development led by Databricks (who sell a Spark hosted service), and with commercial support available as part of most Hadoop distributions.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Spark&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v2.1&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Sub-projects&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; Apache Spark&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-spark/graphx/&quot;&gt;GraphX&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Spark library for processing graphs and running graph algorithms, based on graph model that supports directional edges with properties on both vertices and edges. Graphs are constructed from a pair of collections representing the edges and vertex, either directly from data on disk using builders, or prepared using other Spark functionality, with the ability to also view the graph as a set of triples. Supports a range of graph operations, as well as an optimised variant of the Pregel API, and a set of out of the box algorithms (including PageRank, connected components and triangle count). First introduced in Spark 0.9, with a production release as part of Spark 1.2, however has seen almost no new functionality since then.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Spark&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-spark/mllib/&quot;&gt;MLlib&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Spark library for running Machine Learning algorithms. Supports a range of algorithms (including classifications, regressions, decision trees, recommendations, clustering and topic modelling), including iterative algorithms. First introduced in Spark 0.8 after being collaboratively developed with the UC Berkeley MLbase project.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Spark&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-spark/spark-sql/&quot;&gt;Spark SQL&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Spark library for processing structured data, using either SQL statements or a DataFrame API. Supports querying and writing to local datasets (including JSON, Parquet, Avro, Orc and CSV) as well as external data sources (including Hive and JDBC), including the ability to query across data sources. Includes Catalyst, a cost based optimiser that turns high level operations into low level Spark DAGs for execution. Also includes a Hive compatible Thrift JDBC/ODBC server that's compatible with Beeline and the Hive JDBC and ODBC drivers, and a REPL CLI for interactive queries. First introduced in Spark 1.0, with a production release as part of Spark 1.3.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Spark&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-spark/spark-streaming/&quot;&gt;Spark Streaming&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Spark library for continuous stream processing, that allows stream and batch processing (including Spark SQL and MLlib operations) to be combined. Uses a micro-batch execution model, leveraging core Spark to process each micro-batch, and provides fault tolerance through exactly-once processing semantics. Supports a number of data sources (including HDFS, sockets, Flume, Kafka, Kinesis and messaging buses), as well as functions to maintain state and to execute windowed operations. First introduced in Spark 0.7, with a production release as part of Spark 0.9.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/&quot;&gt;http://spark.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/&quot;&gt;http://spark.apache.org/docs/latest/&lt;/a&gt; - documentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/news/index.html&quot;&gt;http://spark.apache.org/news/index.html&lt;/a&gt; - project news&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://databricks.com/blog/category/engineering&quot;&gt;https://databricks.com/blog/category/engineering&lt;/a&gt; - Databricks engineering blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>GraphX</title><link>http://ondataengineering.net/technologies/apache-spark/graphx/</link><description> &lt;p&gt;Spark library for processing graphs and running graph algorithms, based on graph model that supports directional edges with properties on both vertices and edges. Graphs are constructed from a pair of collections representing the edges and vertex, either directly from data on disk using builders, or prepared using other Spark functionality, with the ability to also view the graph as a set of triples. Supports a range of graph operations, as well as an optimised variant of the Pregel API, and a set of out of the box algorithms (including PageRank, connected components and triangle count). First introduced in Spark 0.9, with a production release as part of Spark 1.2, however has seen almost no new functionality since then.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/graphx/&quot;&gt;http://spark.apache.org/graphx/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/graphx-programming-guide.html&quot;&gt;http://spark.apache.org/docs/latest/graphx-programming-guide.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>MLlib</title><link>http://ondataengineering.net/technologies/apache-spark/mllib/</link><description> &lt;p&gt;Spark library for running Machine Learning algorithms. Supports a range of algorithms (including classifications, regressions, decision trees, recommendations, clustering and topic modelling), including iterative algorithms. First introduced in Spark 0.8 after being collaboratively developed with the UC Berkeley MLbase project.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/mllib//&quot;&gt;http://spark.apache.org/mllib//&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/ml-guide.html&quot;&gt;http://spark.apache.org/docs/latest/ml-guide.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.mlbase.org/&quot;&gt;http://www.mlbase.org/&lt;/a&gt; - the UC Berkeley MLbase project&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Spark SQL</title><link>http://ondataengineering.net/technologies/apache-spark/spark-sql/</link><description> &lt;p&gt;Spark library for processing structured data, using either SQL statements or a DataFrame API. Supports querying and writing to local datasets (including JSON, Parquet, Avro, Orc and CSV) as well as external data sources (including Hive and JDBC), including the ability to query across data sources. Includes Catalyst, a cost based optimiser that turns high level operations into low level Spark DAGs for execution. Also includes a Hive compatible Thrift JDBC/ODBC server that's compatible with Beeline and the Hive JDBC and ODBC drivers, and a REPL CLI for interactive queries. First introduced in Spark 1.0, with a production release as part of Spark 1.3.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/sql/&quot;&gt;http://spark.apache.org/sql/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/sql-programming-guide.html&quot;&gt;http://spark.apache.org/docs/latest/sql-programming-guide.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Spark Streaming</title><link>http://ondataengineering.net/technologies/apache-spark/spark-streaming/</link><description> &lt;p&gt;Spark library for continuous stream processing, that allows stream and batch processing (including Spark SQL and MLlib operations) to be combined. Uses a micro-batch execution model, leveraging core Spark to process each micro-batch, and provides fault tolerance through exactly-once processing semantics. Supports a number of data sources (including HDFS, sockets, Flume, Kafka, Kinesis and messaging buses), as well as functions to maintain state and to execute windowed operations. First introduced in Spark 0.7, with a production release as part of Spark 0.9.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/sql/&quot;&gt;http://spark.apache.org/sql/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/sql-programming-guide.html&quot;&gt;http://spark.apache.org/docs/latest/sql-programming-guide.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Sqoop</title><link>http://ondataengineering.net/technologies/apache-sqoop/</link><description> &lt;p&gt;Specialist technology for moving bulk data between Hadoop and structured (relational) databases. Command line based, with the ability to import and export data between a range of databases (including mainframe partitioned datasets) and HDFS, Hive, HBase and Accumulo. Supports parallel partitioned unloads, writing to Avro, Sequence File, Parquet and text files, incremental imports and saved jobs that can be shared via a simple metadata store. An Apache project, started in May 2009 as an Hadoop contrib module, migrating to a Cloudera GitHub project in April 2010 (with a v1.0 release shortly after), before being donated to the Apache foundation in June 2011, graduating in March 2012. The last major release (v1.4) was in November 2011, with only minor releases since then. However in January 2012 a significant re-write was announced as part of a proposed v2.0 release to address a number of usability, security and architectural issues. This will introduce a new Sqoop Server and Metadata Repository, supporting both a CLI and web UI, centralising job definitions, database connections and credentials, as well as enabling support for a wider range of connectors including NoSQL databases, Kafka and (S)FTP folders. Java based, with commercial support available as part of most Hadoop distributions.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Sqoop&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v1.4&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://sqoop.apache.org&quot;&gt;http://sqoop.apache.org&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://sqoop.apache.org/docs/&quot;&gt;http://sqoop.apache.org/docs/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/sqoop/entry/apache_sqoop_highlights_of_sqoop&quot;&gt;https://blogs.apache.org/sqoop/entry/apache_sqoop_highlights_of_sqoop&lt;/a&gt; - introduction to Sqoop 2&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/sqoop/entry/apache_sqoop_graduates_from_incubator&quot;&gt;https://blogs.apache.org/sqoop/entry/apache_sqoop_graduates_from_incubator&lt;/a&gt; - early history of Sqoop&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://sqoop.apache.org/&quot;&gt;http://sqoop.apache.org/&lt;/a&gt; - details latest release, and hosts release notes for v1.4.0 onwards&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/sqoop/&quot;&gt;https://blogs.apache.org/sqoop/&lt;/a&gt; - project blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Avro</title><link>http://ondataengineering.net/technologies/apache-avro/</link><description> &lt;p&gt;Data serialisation framework that supports both messaging and data storage. Primarily uses a compact binary format but also supports a JSON format. Supports a range of data structures (including records, enumerations, arrays and maps) with APIs for a wide range of both static and dynamically typed languages. Schema based, with schemas primarily specified in JSON, and support for both code generation from schema definitions as well as dynamic runtime usage. Schemas are serialised alongside data, with support for automatic schema resolution if the schema used to read the data differs from that used to write it. Started as an Hadoop sub-project by Cloudera in April 2009, with an initial v1.0 release in July 2009, before becoming a top level Apache project in May 2010. Has seen significant adoption in the Hadoop ecosystem.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Avro&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v1.8&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://avro.apache.org/&quot;&gt;http://avro.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://avro.apache.org/docs/current/&quot;&gt;https://avro.apache.org/docs/current/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2009/11/avro-a-new-format-for-data-interchange/&quot;&gt;http://blog.cloudera.com/blog/2009/11/avro-a-new-format-for-data-interchange/&lt;/a&gt; - original introduction to Avro&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces4&quot;&gt;https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces4&lt;/a&gt; - Avro top level project announcement&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://avro.apache.org/releases.html&quot;&gt;http://avro.apache.org/releases.html&lt;/a&gt; - project releases&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Kafka</title><link>http://ondataengineering.net/technologies/apache-kafka/</link><description> &lt;p&gt;Technology for buffering and storing real-time streams of data between publishers to subscribers, with a focus on high throughput at low latency. Based on a distributed, horizontally scalable architecture, with messages organised into topics which are partitioned and replicated across nodes to provide resilience and written to disk to provide persistence. Topics may have multiple publishers and subscribers, with ability to do fault tolerant reads and to load balance across subscribers. Records consist of a key, value and timestamp, with the ability to compact topics to remove updates and deletes by key. Supports a full security model, and the ability to set quotas. Comes with a Java client, but clients for a wide range of languages are also available. Has two sub-projects (Kafka Connect and Kafka Streams) that are bundled with the main product. Originally developed at LinkedIn, being open sourced in January 2011, before being donated to the Apache Foundation in July 2011. Graduated in October 2012, and although it has not had a v1.0 release is considered production quality and stable. Development is primarily led by Confluent (which was founded by the team that built Kafka at LinkedIn), who distribute a Confluent Open Source product (which includes further clients and connectors) and a subscription based Confluent Enterprise product (which includes management, replication and data balancing features and commercial support under a subscription licence). Commercial support is also available from most Hadoop vendors.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Kafka, Confluent&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;, Confluent&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v0.10&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Sub-projects&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; Apache Kafka&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-kafka/kafka-connect/&quot;&gt;Kafka Connect&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Framework for building scalable and reliable integrations between Kafka and other technologies, either for importing or exporting data. Part of the core Apache Kafka open source technology, connectors are available for a wide range of systems, including Hadoop, relational, NoSQL and analytical databases, search technologies and message queues amongst others. Runs separately to Kafka, in either a stand-alone or distributed cluster mode, with a REST API for managing connectors.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache Kafka&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-kafka/kafka-streams/&quot;&gt;Kafka Streams&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Technology that allows stream processing to be added to a Kafka cluster, consuming and publishing events from and to Kafka topics (and potentially writing output to external systems). Based on an event-at-a-time model (i.e. not micro batch), with support for stateful processing, windowing, joining and re-processing data. Supports a low level DSL API, as well as a high level API that provides both stream and table abstractions (where tables present the latest record for each key). Introduced in Kafka 0.10.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-flow/&quot;&gt;Hortonworks DataFlow&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://kafka.apache.org&quot;&gt;http://kafka.apache.org&lt;/a&gt; - project home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://kafka.apache.org/intro&quot;&gt;http://kafka.apache.org/intro&lt;/a&gt; - great introduction to Kafka&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blog.linkedin.com/2011/01/11/open-source-linkedin-kafka&quot;&gt;https://blog.linkedin.com/2011/01/11/open-source-linkedin-kafka&lt;/a&gt; - open source announcement&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/Clients&quot;&gt;https://cwiki.apache.org/confluence/display/KAFKA/Clients&lt;/a&gt; - list of clients&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem&quot;&gt;https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem&lt;/a&gt; - associated technologies&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/product/&quot;&gt;https://www.confluent.io/product/&lt;/a&gt; - Confluent product homepage&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://kafka.apache.org/downloads&quot;&gt;http://kafka.apache.org/downloads&lt;/a&gt; - release history&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/blog/&quot;&gt;https://www.confluent.io/blog/&lt;/a&gt; - Confluent blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Kafka Connect</title><link>http://ondataengineering.net/technologies/apache-kafka/kafka-connect/</link><description> &lt;p&gt;Framework for building scalable and reliable integrations between Kafka and other technologies, either for importing or exporting data. Part of the core Apache Kafka open source technology, connectors are available for a wide range of systems, including Hadoop, relational, NoSQL and analytical databases, search technologies and message queues amongst others. Runs separately to Kafka, in either a stand-alone or distributed cluster mode, with a REST API for managing connectors.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://kafka.apache.org/documentation/#connect&quot;&gt;http://kafka.apache.org/documentation/#connect&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/product/connectors/&quot;&gt;https://www.confluent.io/product/connectors/&lt;/a&gt; - Confluent information, including list of available connectors&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/blog/how-to-build-a-scalable-etl-pipeline-with-kafka-connect/&quot;&gt;https://www.confluent.io/blog/how-to-build-a-scalable-etl-pipeline-with-kafka-connect/&lt;/a&gt; - introduction blog post&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Kafka Streams</title><link>http://ondataengineering.net/technologies/apache-kafka/kafka-streams/</link><description> &lt;p&gt;Technology that allows stream processing to be added to a Kafka cluster, consuming and publishing events from and to Kafka topics (and potentially writing output to external systems). Based on an event-at-a-time model (i.e. not micro batch), with support for stateful processing, windowing, joining and re-processing data. Supports a low level DSL API, as well as a high level API that provides both stream and table abstractions (where tables present the latest record for each key). Introduced in Kafka 0.10.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://kafka.apache.org/documentation/streams&quot;&gt;http://kafka.apache.org/documentation/streams&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/product/kafka-streams/&quot;&gt;https://www.confluent.io/product/kafka-streams/&lt;/a&gt; - Confluent information page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/&quot;&gt;https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/&lt;/a&gt; - introduction blog post&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Oozie</title><link>http://ondataengineering.net/technologies/apache-oozie/</link><description> &lt;p&gt;Technology for managing workflows of jobs on Hadoop clusters. Primary concepts include workflows (a sequence of jobs modelled as a directed acyclic graph), coordinators (schedule the execution of workflows based on the time or the presence of data) and bundles (collections of coordinators), with all configuration specified in XML. Supports a range of technologies, including MapReduce, Pig, Hive, Sqoop, Spark, Java executables and shell scripts. Includes a server component, a metadata database for holding definitions and state (with support for a range of database technologies), a command line interface and a read only web interface for viewing the status of jobs. Also supports the parameterisation of workflows, the modelling of datasets (and the use of these to manage dependencies between workflows within coordinators), automatic retry and failure handling, and the ability to send job status notifications via HTTP or JMS. Open sourced by Yahoo in June 2010. Donated to the Apache Foundation in July 2011, graduating in August 2012. Commercial support available as part of most Hadoop distributions&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Oozie&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v4.3&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://oozie.apache.org/&quot;&gt;http://oozie.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://oozie.apache.org/docs/4.3.0/index.html&quot;&gt;http://oozie.apache.org/docs/4.3.0/index.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://jaxenter.com/yahoos-hadoop-based-project-proposed-for-apache-incubator-103651.html&quot;&gt;https://jaxenter.com/yahoos-hadoop-based-project-proposed-for-apache-incubator-103651.html&lt;/a&gt; - intro interview&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Oozie release announcements only appear to be available via the Apache announcements mailing list&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Parquet</title><link>http://ondataengineering.net/technologies/apache-parquet/</link><description> &lt;p&gt;Data serialisation framework that supports a columnar storage format to enable efficient querying of data. Built using Apache Thrift, and supports complex nested data structures, using techniques from the Google Dremel paper. Consists of three sub-projects, the specification and Thrift definitions (Parquet Format), the Java and Hadoop libraries (Parquet MR) and the C++ implementation (Parquet CPP). Created as a joint effort between Twitter and Cloudera based on work started as part of Avro Trevni, with an initial v1.0 release in July 2013. Donated to the Apache Foundation in May 2014, graduating in April 2015. Has seen significant adoption in the Hadoop ecosystem.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Avro&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v1.9&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://parquet.apache.org/&quot;&gt;http://parquet.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://parquet.apache.org/documentation/latest/&quot;&gt;http://parquet.apache.org/documentation/latest/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2013/03/introducing-parquet-columnar-storage-for-apache-hadoop/&quot;&gt;http://blog.cloudera.com/blog/2013/03/introducing-parquet-columnar-storage-for-apache-hadoop/&lt;/a&gt; - initial announcement&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blog.twitter.com/2013/dremel-made-simple-with-parquet&quot;&gt;https://blog.twitter.com/2013/dremel-made-simple-with-parquet&lt;/a&gt; - good introduction to Parquet&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blog.twitter.com/2013/announcing-parquet-10-columnar-storage-for-hadoop&quot;&gt;https://blog.twitter.com/2013/announcing-parquet-10-columnar-storage-for-hadoop&lt;/a&gt; - 1.0 release announcement&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces75&quot;&gt;https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces75&lt;/a&gt; - top level project announcement, including summary of technology that support it&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/apache/parquet-mr/blob/master/CHANGES.md&quot;&gt;https://github.com/apache/parquet-mr/blob/master/CHANGES.md&lt;/a&gt; - release and change summary&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Pig</title><link>http://ondataengineering.net/technologies/apache-pig/</link><description> &lt;p&gt;Technology for running analytical and data processing jobs against data in Hadoop. Jobs are written in Pig Latin (a custom procedural language that can be extended using user defined functions in a range of languages), which is then translated into Map Reduce or Tez (with Spark in development) for execution. Supports both a batch mode for running pre-defined scripts and an interactive mode, and connectors for reading and writing to HBase and Accumulo as well as HDFS. Originally developed at Yahoo in 2006 before being donated to the Apache Foundation in October 2007. Graduated as an Hadoop sub-project in October 2008, before becoming a top level project in September 2010. Although has not had a v1.0 release, has been production quality for many years. Commercial support available as part of most Hadoop distributions&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Pig&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v0.16&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Is used by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-datafu/datafu-pig/&quot;&gt;DataFu Pig&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://pig.apache.org/&quot;&gt;https://pig.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://pig.apache.org/releases.html&quot;&gt;https://pig.apache.org/releases.html&lt;/a&gt; - release history&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://hortonworks.com/blog/announcing-apache-pig-0-14-0/&quot;&gt;http://hortonworks.com/blog/announcing-apache-pig-0-14-0/&lt;/a&gt; - Pig on Tez release announcement&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2014/09/pig-is-flying-apache-pig-on-apache-spark/&quot;&gt;http://blog.cloudera.com/blog/2014/09/pig-is-flying-apache-pig-on-apache-spark/&lt;/a&gt; - first details of Pig on Spark&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://pig.apache.org/&quot;&gt;https://pig.apache.org/&lt;/a&gt; - news and updates&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Bigtop</title><link>http://ondataengineering.net/technologies/apache-bigtop/</link><description> &lt;p&gt;An Apache open source distribution of Hadoop. Packages up a number of Apache Hadoop components, certifies their interoperability using an automated integration test suite, and packages them up as RPMs/DEBs packages for most flavours of Linux. Also includes virtual machine images and vagrant, docker and puppet recipes for deploying and working with Hadoop. Does not patch projects for distribution, but requires any fixes to be made upstream. An Apache Open Source project, started by Cloudera, donated to the Apache foundation in June 2011, graduating in September 2012, with a 1.0 release in August 2015 based on Hadoop 2.6. Since donating the project, Cloudera have backed away from it, with the project lead moving to Pivotal in December 2013. Now has a broad range of contributors, however usage by the major distributors is not clear.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Bigtop&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Categories&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-categories/hadoop_distributions/&quot;&gt;Hadoop Distributions&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Open Source - Active&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v1.1&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Packages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/alluxio/&quot;&gt;Alluxio&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-apex/&quot;&gt;Apache Apex&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-crunch/&quot;&gt;Apache Crunch&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-datafu/&quot;&gt;Apache DataFu&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-flink/&quot;&gt;Apache Flink&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-flume/&quot;&gt;Apache Flume&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-giraph/&quot;&gt;Apache Giraph&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Apache Hadoop&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hama/&quot;&gt;Apache Hama&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hbase/&quot;&gt;Apache HBase&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Apache Hive&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-ignite/&quot;&gt;Apache Ignite&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-mahout/&quot;&gt;Apache Mahout&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-oozie/&quot;&gt;Apache Oozie&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-phoenix/&quot;&gt;Apache Phoenix&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-pig/&quot;&gt;Apache Pig&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-solr/&quot;&gt;Apache Solr&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-sqoop/&quot;&gt;Apache Sqoop&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-tajo/&quot;&gt;Apache Tajo&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-tez/&quot;&gt;Apache Tez&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-zeppelin/&quot;&gt;Apache Zeppelin&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-zookeeper/&quot;&gt;Apache ZooKeeper&lt;/a&gt;, &lt;a href=&quot;/technologies/greenplum/&quot;&gt;Greenplum&lt;/a&gt;, &lt;a href=&quot;/technologies/hue/&quot;&gt;Hue&lt;/a&gt;, &lt;a href=&quot;/technologies/kite/&quot;&gt;Kite&lt;/a&gt;, &lt;a href=&quot;/technologies/quantcast-file-system/&quot;&gt;Quantcast File System&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;1.2.0&lt;/td&gt; &lt;td&gt;pending&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;to date has added Flink, Tajo, Apex, QFS and GPDB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.1.0&lt;/td&gt; &lt;td&gt;17th Feb 2016&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://blogs.apache.org/bigtop/entry/release_1_1_0_is&quot;&gt;https://blogs.apache.org/bigtop/entry/release_1_1_0_is&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.0.0&lt;/td&gt; &lt;td&gt;17th Aug 2015&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://blogs.apache.org/bigtop/entry/not_just_yet_another_release&quot;&gt;https://blogs.apache.org/bigtop/entry/not_just_yet_another_release&lt;/a&gt;&lt;/td&gt; &lt;td&gt;based on Hadoop 2.6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.8.0&lt;/td&gt; &lt;td&gt;6th Oct 2014&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://blogs.apache.org/bigtop/entry/release_of_apache_bigtop_01&quot;&gt;https://blogs.apache.org/bigtop/entry/release_of_apache_bigtop_01&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.7.0&lt;/td&gt; &lt;td&gt;6th Nov 2013&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://blogs.apache.org/bigtop/entry/release_of_apache_bigtop_0&quot;&gt;https://blogs.apache.org/bigtop/entry/release_of_apache_bigtop_0&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.6.0&lt;/td&gt; &lt;td&gt;22nd June 2013&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://blogs.apache.org/bigtop/entry/apache_bigtop_0_6_0&quot;&gt;https://blogs.apache.org/bigtop/entry/apache_bigtop_0_6_0&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.5.0&lt;/td&gt; &lt;td&gt;27th Dec 2012&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://blogs.apache.org/bigtop/entry/apache_bigtop_0_5_0&quot;&gt;https://blogs.apache.org/bigtop/entry/apache_bigtop_0_5_0&lt;/a&gt;&lt;/td&gt; &lt;td&gt;first release as TLP&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://bigtop.apache.org&quot;&gt;http://bigtop.apache.org&lt;/a&gt; - Homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/BIGTOP/Index&quot;&gt;https://cwiki.apache.org/confluence/display/BIGTOP/Index&lt;/a&gt; - The Apache Bigtop Wiki&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/apache/bigtop/blob/master/bigtop.bom&quot;&gt;https://github.com/apache/bigtop/blob/master/bigtop.bom&lt;/a&gt; - details of all included packages and their versions (as of current development snapshot)&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2012/07/update-on-apache-bigtop-incubating/&quot;&gt;http://blog.cloudera.com/blog/2012/07/update-on-apache-bigtop-incubating/&lt;/a&gt; - Cloudera introduction to Bigtop&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/bigtop/entry/bigtop_and_why_should_you&quot;&gt;https://blogs.apache.org/bigtop/entry/bigtop_and_why_should_you&lt;/a&gt; - Early introduction to Bigtop&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blog.pivotal.io/pivotal/pivotal-people/pivotal-people-roman-shaposhnik-founder-of-apache-bigtop-joins-pivotal&quot;&gt;https://blog.pivotal.io/pivotal/pivotal-people/pivotal-people-roman-shaposhnik-founder-of-apache-bigtop-joins-pivotal&lt;/a&gt; - Interview with Roman Shoposhnik on the aims of Bigtop&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/BIGTOP/?selectedTab=com.atlassian.jira.jira-projects-plugin:versions-panel&quot;&gt;https://issues.apache.org/jira/browse/BIGTOP/?selectedTab=com.atlassian.jira.jira-projects-plugin:versions-panel&lt;/a&gt; - Release list and link to release notes&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/bigtop/&quot;&gt;https://blogs.apache.org/bigtop/&lt;/a&gt; - The Apache Bigtop Blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://bigtop.apache.org/&quot;&gt;http://bigtop.apache.org/&lt;/a&gt; - details of latest release&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Ignite</title><link>http://ondataengineering.net/technologies/apache-ignite/</link><description> &lt;p&gt;A distributed in-memory data fabric/grid, supporting a number of use cases including a key value store (with SQL support), real time stream/event processing engine, arbitrary compute, long running service management, an in-memory HDFS compatible file system for acceleration of Hadoop jobs, and in-memory shared Spark RDDs. An Apache project, graduating in September 2015, having been originally donated by GridGain from their In-Memory Data Fabric product launched in 2007. Java based, with development lead by GridGain who also supply commercial support (as GridGain Professional with ongoing Q&amp;A and bug fixes before they're included in Ignite) along with GridGain Enterprise (which includes extra features such as a management GUI, enterprise security and rolling upgrades).&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Ignite, GridGain&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;, GridGain&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v1.8.0 (Ignite), v7.7 (GridGain)&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://ignite.apache.org/&quot;&gt;https://ignite.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.gridgain.com/&quot;&gt;http://www.gridgain.com/&lt;/a&gt; - Grid Gain home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.gridgain.com/products/pricing/&quot;&gt;http://www.gridgain.com/products/pricing/&lt;/a&gt; - details of GridGain editions&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://apacheignite.readme.io/docs&quot;&gt;https://apacheignite.readme.io/docs&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.infoq.com/presentations/apache-ignite&quot;&gt;https://www.infoq.com/presentations/apache-ignite&lt;/a&gt; - intro presentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://ignite.apache.org/blogs.html&quot;&gt;https://ignite.apache.org/blogs.html&lt;/a&gt; - Ingite blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.gridgain.com/resources/blog&quot;&gt;https://www.gridgain.com/resources/blog&lt;/a&gt; - GridGain blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Giraph</title><link>http://ondataengineering.net/technologies/apache-giraph/</link><description> &lt;p&gt;An iterative, highly scalable graph processing system built on top of MapReduce and based on Pregel, with a number of features added including a framework for creating re-usable code (called blocks). An Apache project, graduating in May 2012, having been originally donated by Yahoo in August 2011. Java based, no commercial support available, but is mature and has been adopted by a number of companies (including LinkedIn and most famously Facebook who scaled it to process a trillion edges), and has a number of active developers.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Giraph&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Open Source - Active&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v1.2&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://giraph.apache.org/&quot;&gt;http://giraph.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://giraph.apache.org/intro.html&quot;&gt;http://giraph.apache.org/intro.html&lt;/a&gt; - introduction&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://engineering.linkedin.com/open-source/apache-giraph-framework-large-scale-graph-processing-hadoop-reaches-01-milestone&quot;&gt;https://engineering.linkedin.com/open-source/apache-giraph-framework-large-scale-graph-processing-hadoop-reaches-01-milestone&lt;/a&gt; - v0.1 release announcement from LinkedIn&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.facebook.com/notes/facebook-engineering/scaling-apache-giraph-to-a-trillion-edges/10151617006153920/&quot;&gt;https://www.facebook.com/notes/facebook-engineering/scaling-apache-giraph-to-a-trillion-edges/10151617006153920/&lt;/a&gt; - Facebook?s story on scaling Giraph to a trillion edges&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://giraph.apache.org/&quot;&gt;http://giraph.apache.org/&lt;/a&gt; - news via homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/giraph&quot;&gt;https://blogs.apache.org/giraph&lt;/a&gt; - blog, with first entry being v1.2 announcement&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Hama</title><link>http://ondataengineering.net/technologies/apache-hama/</link><description> &lt;p&gt;A general purpose BSP (Bulk Synchronous Parallel) processing engine inspired by Pregel and DistBelief that runs over Mesos or YARN. Supports BSP, graph computing and machine learning programming models, as well as Apache MRQL. An Apache project, donated in 2008, and graduated in 2012. Java based, with no commercial support available, limited case studies for it's use and limited active developers, with the last release being in June 2015.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Hama&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Open Source - Quiet&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v0.7&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://hama.apache.org/&quot;&gt;http://hama.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://wiki.apache.org/hama/&quot;&gt;https://wiki.apache.org/hama/&lt;/a&gt; - Wiki / documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://wiki.apache.org/incubator/HamaProposal&quot;&gt;https://wiki.apache.org/incubator/HamaProposal&lt;/a&gt; - details of Apache incubation proposal&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/hama/&quot;&gt;https://blogs.apache.org/hama/&lt;/a&gt; - blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Alluxio</title><link>http://ondataengineering.net/technologies/alluxio/</link><description> &lt;p&gt;A distributed virtual storage layer, supporting key-value and filesystem interfaces (including HDFS compatibility and a FUSE driver) with support for a range of computation and storage frameworks (including Spark, MapReduce, HBase and Hive) over multiple storage layers (including in-memory, local, network, cloud and cluster file systems) with the ability to create unified and tiered storage, for example to create an in memory filesystem backed by disk to accelerate analytics jobs. Supports a POSIX like access control model, and a CLI and web interface for browsing the storage layer. Java based, Open Source under the Apache 2.0 licence, hosted on GitHub, with development led by Alluxio (with significant external contributions), although they don't appear to yet provide commercial support (but do provide training). Started in December 2012, open sourced in April 2013, with a v1.0 release in February 2016. Formally known as Tachyon.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Alluxio, Tachyon&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;Alluxio&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v1.4&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://alluxio.org/&quot;&gt;http://alluxio.org/&lt;/a&gt; - product home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/Alluxio/alluxio&quot;&gt;https://github.com/Alluxio/alluxio&lt;/a&gt; - source code&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.alluxio.org/docs/master/en/&quot;&gt;http://www.alluxio.org/docs/master/en/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.alluxio.com/2016/02/alluxio-formerly-tachyon-is-entering-a-new-era-with-1-0-release/&quot;&gt;http://www.alluxio.com/2016/02/alluxio-formerly-tachyon-is-entering-a-new-era-with-1-0-release/&lt;/a&gt; - history&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.alluxio.com/blog&quot;&gt;https://www.alluxio.com/blog&lt;/a&gt; - blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.alluxio.org/download&quot;&gt;http://www.alluxio.org/download&lt;/a&gt; - details of releases&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Kite</title><link>http://ondataengineering.net/technologies/kite/</link><description> &lt;p&gt;A set of libraries, tools, examples, and documentation focused on making it easier to build systems on top of the Hadoop ecosystem. Consists of three sub-projects - Kite Data (a logical dataset abstraction over Hadoop), Morphlines (embeddable configuration driven transformation pipelines) and Kite Maven Plugin (a Maven plugin for deploying Hadoop applications). Java based, Open Source under the Apache 2.0 licence and hosted on GitHub. First released in May 2013 by Cloudera as the Cloudera Development Kit (CDK), renamed to Kite in December 2013, and reached a v1.0 release in February 2015 with a number of external contributors. Last release was v1.1 in June 2015, with very little development activity since this time.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Cloudera Development Kit, CDK&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;Cloudera&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v1.1&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Sub-projects&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; Kite&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/kite/kite-data/&quot;&gt;Kite Data&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Library that provides a logical dataset and record abstraction over HDFS, S3, local filesystems and HBase, including support for partitioning and views (which allow datasets to be filtered and supports automatic partition pruning). Provides a command line interface and Maven plugin for managing and viewing datasets. Supports Crunch, Flume, Spark and MapReduce, and can integrate with a Hive Metastore to make datasets available through Hive and Impala. Stores data using Avro (utilising Avro schema evolution / resolution) or Parquet.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Kite&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/kite/kite-maven-plugin/&quot;&gt;Kite Maven Plugin&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A Maven plugin that supports the packaging, deployment and execution of applications onto Hadoop.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Kite&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/kite/morphlines/&quot;&gt;Morphlines&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A configuration driven in-memory transformation pipeline that can be embedded into any Java code base, with specific support for Flume, MapReduce, HBase, Spark and Solr. Supports multiple different file types including CSV, Avro, JSON, Parquet, RCFile, SequenceFile, ProtoBuf and XML plus gzip, bzip2, tar zip and jar files. Also supports a number of transformation steps out of the box, including integration with Apache Tika for reading common file formats.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://kitesdk.org/docs/current/&quot;&gt;http://kitesdk.org/docs/current/&lt;/a&gt; - homepage and documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/kite-sdk/kite&quot;&gt;https://github.com/kite-sdk/kite&lt;/a&gt; - source code&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Any updates to Kite are likely to be published on the Cloudera blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Kite Data</title><link>http://ondataengineering.net/technologies/kite/kite-data/</link><description> &lt;p&gt;Library that provides a logical dataset and record abstraction over HDFS, S3, local filesystems and HBase, including support for partitioning and views (which allow datasets to be filtered and supports automatic partition pruning). Provides a command line interface and Maven plugin for managing and viewing datasets. Supports Crunch, Flume, Spark and MapReduce, and can integrate with a Hive Metastore to make datasets available through Hive and Impala. Stores data using Avro (utilising Avro schema evolution / resolution) or Parquet.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/kite/&quot;&gt;Kite&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://kitesdk.org/docs/current/Kite-SDK-Guide.html&quot;&gt;http://kitesdk.org/docs/current/Kite-SDK-Guide.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Kite Maven Plugin</title><link>http://ondataengineering.net/technologies/kite/kite-maven-plugin/</link><description> &lt;p&gt;A Maven plugin that supports the packaging, deployment and execution of applications onto Hadoop.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/kite/&quot;&gt;Kite&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://kitesdk.org/docs/current/maven/plugin-info.html&quot;&gt;http://kitesdk.org/docs/current/maven/plugin-info.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Morphlines</title><link>http://ondataengineering.net/technologies/kite/morphlines/</link><description> &lt;p&gt;A configuration driven in-memory transformation pipeline that can be embedded into any Java code base, with specific support for Flume, MapReduce, HBase, Spark and Solr. Supports multiple different file types including CSV, Avro, JSON, Parquet, RCFile, SequenceFile, ProtoBuf and XML plus gzip, bzip2, tar zip and jar files. Also supports a number of transformation steps out of the box, including integration with Apache Tika for reading common file formats.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/kite/&quot;&gt;Kite&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://kitesdk.org/docs/current/morphlines/&quot;&gt;http://kitesdk.org/docs/current/morphlines/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://kitesdk.org/docs/current/morphlines/morphlines-reference-guide.html&quot;&gt;http://kitesdk.org/docs/current/morphlines/morphlines-reference-guide.html&lt;/a&gt; - reference guide&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2013/07/morphlines-the-easy-way-to-build-and-integrate-etl-apps-for-apache-hadoop/&quot;&gt;http://blog.cloudera.com/blog/2013/07/morphlines-the-easy-way-to-build-and-integrate-etl-apps-for-apache-hadoop/&lt;/a&gt; - intro&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache DataFu</title><link>http://ondataengineering.net/technologies/apache-datafu/</link><description> &lt;p&gt;A set of libraries for working with data in Hadoop. Consists of two sub-projects - DataFu Pig (a set of Pig User Defined Functions) and DataFu Hourglass (a framework for incremental processing using MapReduce). Originally created at LinkedIn, with the Pig UDFs being open sourced in January 2012 as DataFu, with a v1.0 release in September 2013. Split into sub-projects in October 2013 when LinkedIn open sourced DataFu Hourglass and added it to the project. Donated to the Apache Foundation in January 2014, however is still incubating and has not yet graduated. Last release was v1.3 in November 2015 (albeit with a very minor v1.3.1 release in August 2016), with little development activity since this time.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;DataFu&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017 - v1.3&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Sub-projects&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; Apache DataFu&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-datafu/datafu-hourglass/&quot;&gt;DataFu Hourglass&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A framework over MapReduce that supports the efficient generation of statistics of dated data by incrementally updating the previous days output. Supports both fixed length and fixed start point windows, and the generation of statistics by input partition or as a total over all input data.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Apache DataFu&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-datafu/datafu-pig/&quot;&gt;DataFu Pig&lt;/a&gt; &lt;/td&gt; &lt;td&gt;A set of user defined functions for Apache Pig, including support for statistical calculations, bag and set operations, sessionisation of streams of data, cardinality estimation, sampling, hashing, PageRank and others.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://datafu.incubator.apache.org&quot;&gt;https://datafu.incubator.apache.org&lt;/a&gt; - homepage&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://datafu.incubator.apache.org/blog/&quot;&gt;https://datafu.incubator.apache.org/blog/&lt;/a&gt; - blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>DataFu Hourglass</title><link>http://ondataengineering.net/technologies/apache-datafu/datafu-hourglass/</link><description> &lt;p&gt;A framework over MapReduce that supports the efficient generation of statistics of dated data by incrementally updating the previous days output. Supports both fixed length and fixed start point windows, and the generation of statistics by input partition or as a total over all input data.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-datafu/&quot;&gt;Apache DataFu&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://datafu.incubator.apache.org/docs/hourglass/getting-started.html&quot;&gt;https://datafu.incubator.apache.org/docs/hourglass/getting-started.html&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://datafu.incubator.apache.org/blog/2013/10/03/datafus-hourglass-incremental-data-processing-in-hadoop.html&quot;&gt;https://datafu.incubator.apache.org/blog/2013/10/03/datafus-hourglass-incremental-data-processing-in-hadoop.html&lt;/a&gt; - introduction (contains examples and information not available in the documentation)&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://datafu.incubator.apache.org/docs/hourglass/1.3.1/&quot;&gt;http://datafu.incubator.apache.org/docs/hourglass/1.3.1/&lt;/a&gt; - Javadoc (note link from &lt;a href=&quot;https://datafu.incubator.apache.org/docs/hourglass/javadoc.html&quot;&gt;here&lt;/a&gt; is wrong)&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>DataFu Pig</title><link>http://ondataengineering.net/technologies/apache-datafu/datafu-pig/</link><description> &lt;p&gt;A set of user defined functions for Apache Pig, including support for statistical calculations, bag and set operations, sessionisation of streams of data, cardinality estimation, sampling, hashing, PageRank and others.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-datafu/&quot;&gt;Apache DataFu&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Uses&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-pig/&quot;&gt;Apache Pig&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://datafu.incubator.apache.org/docs/datafu/getting-started.html&quot;&gt;https://datafu.incubator.apache.org/docs/datafu/getting-started.html&lt;/a&gt; - homepage (see linked blog posts for further information and examples)&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://datafu.incubator.apache.org/docs/datafu/guide.html&quot;&gt;https://datafu.incubator.apache.org/docs/datafu/guide.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Phoenix</title><link>http://ondataengineering.net/technologies/apache-phoenix/</link><description> &lt;p&gt;A SQL query engine over Apache HBase tables that supports a subset of SQL 92 (including joins), and comes with a JDBC driver. Supports a range of features including ACID transactions (via Apache Tephra), user defined functions, secondary indexes, atomic upserts, views, multi tenancy tables (where each user or tenant can only see their data) and dynamic columns (which are only specified at query time). Supports a range of SQL DDL commands, creating and modifying underlying HBase tables as required, or can run over existing HBase tables in a read only mode. Comes with connectors to allow Spark, Hive, Pig, Flume and MapReduce to read and write Phoenix tables, and a number of utilities, including a bulk loader and a command line SQL tool. Open sourced by SalesForce in January 2013 at v1.0, donated to the Apache foundation in December 2013, before graduating in May 2014. Commercial support available through Hortonworks as part of HDP, with Cloudera making it available via Cloudera Labs without support. Active project with a range of contributors, including many from SalesForce and Hortonworks.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Phoenix&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;February 2017 - v4.9&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://phoenix.apache.org/&quot;&gt;http://phoenix.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/phoenix/entry/announcing_phoenix_4_9_released&quot;&gt;https://blogs.apache.org/phoenix/entry/announcing_phoenix_4_9_released&lt;/a&gt; - 4.9 announcement (homepage doesn?t appear to have been updated)&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/phoenix/entry/apache_phoenix_released_next_major&quot;&gt;https://blogs.apache.org/phoenix/entry/apache_phoenix_released_next_major&lt;/a&gt; - 3.0/4.0 announcement&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blog.cloudera.com/blog/2015/05/apache-phoenix-joins-cloudera-labs&quot;&gt;https://blog.cloudera.com/blog/2015/05/apache-phoenix-joins-cloudera-labs&lt;/a&gt; - Cloudera labs announcement&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://phoenix-hbase.blogspot.co.uk/&quot;&gt;http://phoenix-hbase.blogspot.co.uk/&lt;/a&gt; - original blog, now superseded by the Apache blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/phoenix/&quot;&gt;https://blogs.apache.org/phoenix/&lt;/a&gt; - project blog including release announcements&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Tajo</title><link>http://ondataengineering.net/technologies/apache-tajo/</link><description> &lt;p&gt;Distributed analytical database engine. Supports HDFS, Amazon S3, Google Cloud Storage, OpenStack Swift and local storage, and querying over Postgres, HBase and Hive tables. Provides a standard SQL interface, JDBC driver, and supports partitioning, compression and indexing (currently experimental). An Apache project, donated by Gruter in March 2013, and graduated in April 2014. Java based, with development lead by Gruter who also supply commercial support, a Tajo managed service, a data analytics hub (Qrytica) built on Tajo, and a Tajo Data Warehouse appliance.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Tajo&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;, Gruter&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;February 2017 - v0.11&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://tajo.apache.org/&quot;&gt;http://tajo.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://tajo.apache.org/docs/current/&quot;&gt;http://tajo.apache.org/docs/current/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://gruter.com/technology/tajo/&quot;&gt;http://gruter.com/technology/tajo/&lt;/a&gt; - Gruter product page&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://tajo.apache.org/&quot;&gt;http://tajo.apache.org/&lt;/a&gt; - news and releases&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.gruter.com/blog/&quot;&gt;http://www.gruter.com/blog/&lt;/a&gt; - Gruter blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Mahout</title><link>http://ondataengineering.net/technologies/apache-mahout/</link><description> &lt;p&gt;Machine learning technology comprising of a Scala based linear algebra engine (codenamed Samsara) with an R-like DSL/API that runs over Spark (with experimental support for H2O and Flink), an optimiser, a wide variety of pre-made algorithms, and a Scala REPL (based on Spark Shell) for interactive execution. Can be embedded and integrated within larger applications, for example MLlib when running over Spark. Also includes some original, now deprecated, algorithms implemented over MapReduce. Created in January 2008 as a Lucene sub-project, becoming a top level Apache project in April 2010. The original MapReduce algorithms were deprecated and Samsara introduced as part of v0.10 in April 2015. Supported by most major Hadoop distributions, and still under active development.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Mahout&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;February 2017 - 0.12&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://mahout.apache.org/&quot;&gt;https://mahout.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.weatheringthroughtechdays.com/2015/04/mahout-010x-first-mahout-release-as.html&quot;&gt;http://www.weatheringthroughtechdays.com/2015/04/mahout-010x-first-mahout-release-as.html&lt;/a&gt; - introduction to new architecture introduced in v0.10&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://mahout.apache.org/general/release-notes.html&quot;&gt;https://mahout.apache.org/general/release-notes.html&lt;/a&gt; - new releases&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Apex</title><link>http://ondataengineering.net/technologies/apache-apex/</link><description> &lt;p&gt;Data transformation engine based on Directed Acyclic Graph (DAG) flows configured through a Java API or via JSON, with a stated focus on performance, code re-use, testability and ease of operations. Runs over YARN and HDFS with native support for both micro-batch streaming and batch uses cases, and includes a range of standard operators and connectors (called Apex Malhar). An Apache project, graduating in April 2016, having been originally donated in August 2015 by DataTorrent from their DataTorrent RTS product which launched in June 2014. Java based, with development lead by DataTorrent who distribute it as DataTorrent RTS in two editions - a Community Edition (which also includes a basic management GUI and a tool for configuring Apex for data ingestion), and an Enterprise Edition (which further includes a graphical transformation editor, a self service dashboard, security integration and commercial support, and is also available as a cloud offering).&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Apex, DataTorrent RTS&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;, DataTorrent&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;February 2017 - v3.5 (Apex Core), v3.6 (Apex Malhar), v3.7 (DataTorrent RTS)&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://apex.apache.org/&quot;&gt;https://apex.apache.org/&lt;/a&gt; - Apex homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://apex.apache.org/docs.html&quot;&gt;https://apex.apache.org/docs.html&lt;/a&gt; -Apex documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.datatorrent.com/products-services/datatorrent-rts/&quot;&gt;https://www.datatorrent.com/products-services/datatorrent-rts/&lt;/a&gt; - DataTorrent RTS home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://docs.datatorrent.com/&quot;&gt;http://docs.datatorrent.com/&lt;/a&gt; - DataTorrent documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.datatorrent.com/blog/introducing-apache-apex-incubating/&quot;&gt;https://www.datatorrent.com/blog/introducing-apache-apex-incubating/&lt;/a&gt; - introductory blog post&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.datatorrent.com/products-services/edition-comparison/&quot;&gt;https://www.datatorrent.com/products-services/edition-comparison/&lt;/a&gt; - DataTorrent editions comparison&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://apex.apache.org/&quot;&gt;https://apex.apache.org/&lt;/a&gt; - release announcements&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.datatorrent.com/blog/&quot;&gt;https://www.datatorrent.com/blog/&lt;/a&gt; - DataTorrent blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Crunch</title><link>http://ondataengineering.net/technologies/apache-crunch/</link><description> &lt;p&gt;An abstraction layer over MapReduce (and now Spark) that provides a high level Java API for creating data transformation pipelines, originally designed to make working with MapReduce easier based on the Google FlumeJava paper. Also includes connectors for HBase, Hive and Kafka, Java 8 lambda support, an experimental Scala wrapper for the API (Scrunch), and support for in memory pipelines and helper classes to support testing. Open sourced by Cloudera in October 2011, donated to the Apache Foundation in May 2012, before graduating in February 2013. Support for Spark was added as part of v0.10 in June 2014. Still being maintained, and appears to have had been adopted at a number of large companies, but with limited new development.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Crunch&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;February 2017 - 0.14&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://crunch.apache.org/&quot;&gt;https://crunch.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://crunch.apache.org/user-guide.html&quot;&gt;https://crunch.apache.org/user-guide.html&lt;/a&gt; - user guide&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2011/10/introducing-crunch/&quot;&gt;http://blog.cloudera.com/blog/2011/10/introducing-crunch/&lt;/a&gt; - initial intro blog post&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://crunch.apache.org/user-guide.html#motivation&quot;&gt;https://crunch.apache.org/user-guide.html#motivation&lt;/a&gt; - the motivation behind Crunch&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://crunch.apache.org/scrunch.html&quot;&gt;http://crunch.apache.org/scrunch.html&lt;/a&gt; - Scrunch page&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://crunch.apache.org/download.html&quot;&gt;https://crunch.apache.org/download.html&lt;/a&gt; - new releases only appear to be announced on download page&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Flink</title><link>http://ondataengineering.net/technologies/apache-flink/</link><description> &lt;p&gt;Specialised stream processing technology inspired by the Google Data Flow model. Based on a single record (not micro batch) model, with exactly once processing semantics (for supported sources and sinks) via light weight checkpointing, and focusing on high throughput, low latency use cases. Supports both a Java and Scala API, with a fluent DataStream API for working with continuous data flows (including a flexible windowing API that supports both event time and processing time windows and support for out of order or late data), and a DataSet API for working with batch data sets (that uses the same streaming execution engine). Also supports a number of connectors and extra libraries, including experimental support for SQL expressions, a CEP library (FlinkCEP) that can be used to detect complex event patterns, a beta package for running Storm apps on Flink, a graph processing library (Gelly) and a machine learning library (FlinkML). Clustered, with support for YARN and Mesos as well as standalone clusters. Open sourced by Data Artisans in April 2013, donated to the Apache Foundation in April 2014 before graduating in August 2014. Under active development with a large number of contributors and a range of user case studies. Sold as a hosted managed service (dA Platform) by Data Artisans who also supply training.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Flink&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;, Data Artisans&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;February 2017 - 1.2&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://flink.apache.org/&quot;&gt;http://flink.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://ci.apache.org/projects/flink/flink-docs-release-1.2&quot;&gt;http://ci.apache.org/projects/flink/flink-docs-release-1.2&lt;/a&gt; - 1.2 release documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.slideshare.net/KostasTzoumas/apache-flink-at-strata-san-jose-2016&quot;&gt;http://www.slideshare.net/KostasTzoumas/apache-flink-at-strata-san-jose-2016&lt;/a&gt; - good intro slides, including comparison to other technologies&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://data-artisans.com/&quot;&gt;http://data-artisans.com/&lt;/a&gt; - Data Artisans homepage&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://flink.apache.org/blog/&quot;&gt;https://flink.apache.org/blog/&lt;/a&gt; - flink blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://data-artisans.com/blog/&quot;&gt;http://data-artisans.com/blog/&lt;/a&gt; - Data Artisans blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Tez</title><link>http://ondataengineering.net/technologies/apache-tez/</link><description> &lt;p&gt;Data processing framework based on Directed Acyclic Graphs (DAGs), that runs natively on YARN and was designed to be a replacement for the use of MapReduce within Hadoop analytical tools (primarily Hive and Pig), and therefore offer better performance with similar scalability. Targeted more at application developers rather than data engineers, includes a number of performance optimisations (including dynamic DAG re-configuration during execution and re-use of sessions and containers), and comes with a UI for viewing live and historic Tez job executions based on information in the YARN Application Timeline Server. Created by Hortonworks and donated to the Apache Foundation in February 2013 before graduating in July 2014. Still under active development, and now used by Cascading and Flink in addition to Hive and Pig.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Tez&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;February 2017 - 0.8&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://tez.apache.org/&quot;&gt;https://tez.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://hortonworks.com/blog/apache-tez-a-new-chapter-in-hadoop-data-processing/&quot;&gt;http://hortonworks.com/blog/apache-tez-a-new-chapter-in-hadoop-data-processing/&lt;/a&gt; - introduction to Tez, and links to further documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://hortonworks.com/blog/introducing-apache-tez-0-5/&quot;&gt;http://hortonworks.com/blog/introducing-apache-tez-0-5/&lt;/a&gt; - developer documentation introduction&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.slideshare.net/HadoopSummit/yahoos-experience-running-pig-on-tez-at-scale&quot;&gt;http://www.slideshare.net/HadoopSummit/yahoos-experience-running-pig-on-tez-at-scale&lt;/a&gt; - case study of Tez at scale&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://tez.apache.org/releases/index.html&quot;&gt;https://tez.apache.org/releases/index.html&lt;/a&gt; - release history&lt;/li&gt; &lt;li&gt;Other news available via Hortonworks blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache ZooKeeper</title><link>http://ondataengineering.net/technologies/apache-zookeeper/</link><description> &lt;p&gt;Service for managing coordination (e.g. configuration information and synchronisation) of distributed and clustered systems. Based on a hierarchical key-value store, with support for things such as sequential nodes (whose names are automatically assigned a sequence number suffix), ephemeral nodes (which only exist whilst their owners session exists) and the ability to watch nodes. Guarantees that all writes are serial and ordered (i.e. all clients will see them in the same order), meaning it's more appropriate for low write high read scenarios. Can run in a high available cluster called an ensemble. Originally an Hadoop sub-project, but graduated to a top level Apache project in January 2011. Java based, still under active development, and used by a range of technologies including Hadoop, Mesos, HBase, Kafka and Solr.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;ZooKeeper&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Open Source - Active&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;February 2017 - 3.4&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-flow/&quot;&gt;Hortonworks DataFlow&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;p&gt;&lt;a href=&quot;https://zookeeper.apache.org/&quot;&gt;https://zookeeper.apache.org/&lt;/a&gt; - homepage &lt;a href=&quot;https://zookeeper.apache.org/doc/current/&quot;&gt;https://zookeeper.apache.org/doc/current/&lt;/a&gt; - current documentation &lt;a href=&quot;https://zookeeper.apache.org/doc/current/zookeeperOver.html&quot;&gt;https://zookeeper.apache.org/doc/current/zookeeperOver.html&lt;/a&gt; - excellent introduction&lt;/p&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;p&gt;&lt;a href=&quot;https://zookeeper.apache.org/releases.html&quot;&gt;https://zookeeper.apache.org/releases.html&lt;/a&gt; - release history&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hue</title><link>http://ondataengineering.net/technologies/hue/</link><description> &lt;p&gt;Web application to allow users and administrators to work with a Hadoop cluster. Features include a SQL query tool (with auto-complete, a SQL expression builder, plotting results as a graph or on a map, and the ability to refine results) over any JDBC compatible database, a Pig query tool (with auto-complete and parameterised queries), a Solr search tool (drag a drop creation of Solr dashboards with grid, timeline, graph, map and filter widgets, a tool for indexing data into Solr and a Solr index browser), a query notebook (Spark, PySpark, Scala, Hive, Impala, Pig and R queries along with visualisation of results as graphs and maps), an Oozie management tool (graphical Oozie workflow, coordinator and bundle editors and an Oozie monitoring and management dashboard), an Apache Sentry configuration tool (for managing permissions to Hive tables and Solr collections), an HDFS and S3 file browser and manager (including the ability to upload and edit data), a YARN job browser (viewing logs and statistics), a Hive Metastore manager (browse, view sample data, create and manage databases and tables), an HBase table manager (browse, view, edit, create and manage tables), a Sqoop2 manager (create, manage and execute Sqoop2 jobs), a ZooKeeper manager (list, view and edit) and a user workspace for saving work done in Hue, organising this in folders and sharing it with other users. Originally released by Cloudera as Cloudera Desktop in October 2009, before being open sourced as Hue in June 2010. Python/Django based, under active development with a wide range of contributors, and available for all major Hadoop distributions.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Hue&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;Cloudera&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;February 2017 - 3.12&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;p&gt;&lt;a href=&quot;http://gethue.com/&quot;&gt;http://gethue.com/&lt;/a&gt; - homepage &lt;a href=&quot;https://github.com/cloudera/hue&quot;&gt;https://github.com/cloudera/hue&lt;/a&gt; - code repository&lt;/p&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;p&gt;&lt;a href=&quot;http://gethue.com/blog/&quot;&gt;http://gethue.com/blog/&lt;/a&gt; - Hue blog&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Zeppelin</title><link>http://ondataengineering.net/technologies/apache-zeppelin/</link><description> &lt;p&gt;A web based notebook for interactive data analytics. Supports a wide range of interpreters (including Spark, JDBC SQL, Pig, Elasticsearch, Beam, Flink, Shell, Python amongst many others), a range of output formats (plain text, HTML, mathematical expressions using MathJax and tabular data), a range of visualisations for tabular data (including the ability to add more via a JavaScript NPM based plugin system called Helium), forms for user entry of parameters, and an Angular API to enable dynamic and interactive functionality within notebooks. Has a plugable storage for notebooks (with out of the box support for git, S3, Azure and ZeppelinHub), support for multi-user environments and a security model. Open sourced by NFLabs (now called ZEPL) in 2013 before being donated to the Apache Foundation in December 2014, graduating in May 2016. Under active development with a wide range of contributors, led by ZEPL, who sell Zeppelin as a managed service (ZepplinHub).&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Zeppelin&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;, ZEPL&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;February 2017 - 0.7&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;p&gt;&lt;a href=&quot;http://zeppelin.apache.org/&quot;&gt;http://zeppelin.apache.org/&lt;/a&gt; - homepage &lt;a href=&quot;https://zeppelin.apache.org/docs/&quot;&gt;https://zeppelin.apache.org/docs/&lt;/a&gt; - documentation by version &lt;a href=&quot;https://www.zepl.com/&quot;&gt;https://www.zepl.com/&lt;/a&gt; - ZEPL homepage &lt;a href=&quot;https://www.zeppelinhub.com/&quot;&gt;https://www.zeppelinhub.com/&lt;/a&gt; - ZepplinHub home page&lt;/p&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;p&gt;&lt;a href=&quot;https://zeppelin.apache.org/&quot;&gt;https://zeppelin.apache.org/&lt;/a&gt; - release announcements via the homepage&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Quantcast File System</title><link>http://ondataengineering.net/technologies/quantcast-file-system/</link><description> &lt;p&gt;Open source HDFS compatible distributed file system, which focuses on improving performance and scalability over HDFS. Uses erase coding (specifically Reed-Solomon error correction) allowing each data block to be stored with a 50% overhead over 9 nodes with data able to be read from any 6 (half the space required by HDFS with 3 way replication). Also supports online addition of new data (chunk) nodes, automatic re-balancing and re-replication of data, Unix style permissions support and C++ and Java client libraries. Published benchmarks suggest a 50/75% read/write performance increase over HDFS, and significantly faster metadata operations. Now also runs over Amazon S3. Built and maintained by Quantcast, who open sourced it in August 2012. An evolution of the Kosmos File System (KFS), an open source project started by Kosmix in 2005, which Quantcast first adopted in 2007. Built in C++ and released under the Apache 2.0 licence.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;QFS&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;Quantcast&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Open Source - Quiet&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;February 2017 - 1.2&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;p&gt;&lt;a href=&quot;https://quantcast.github.io/qfs/&quot;&gt;https://quantcast.github.io/qfs/&lt;/a&gt; - homepage &lt;a href=&quot;https://github.com/quantcast/qfs/&quot;&gt;https://github.com/quantcast/qfs/&lt;/a&gt; - code &lt;a href=&quot;https://github.com/quantcast/qfs/wiki/&quot;&gt;https://github.com/quantcast/qfs/wiki/&lt;/a&gt; - documentation &lt;a href=&quot;https://github.com/quantcast/qfs/wiki/Introduction-To-QFS&quot;&gt;https://github.com/quantcast/qfs/wiki/Introduction-To-QFS&lt;/a&gt; - introduction and summary of benefits &lt;a href=&quot;https://github.com/quantcast/qfs/wiki/Performance-Comparison-to-HDFS&quot;&gt;https://github.com/quantcast/qfs/wiki/Performance-Comparison-to-HDFS&lt;/a&gt; - performance comparison to HDFS &lt;a href=&quot;https://www.quantcast.com/blog/quantcast-file-system-on-amazon-s3/&quot;&gt;https://www.quantcast.com/blog/quantcast-file-system-on-amazon-s3/&lt;/a&gt; - information on running over S3 &lt;a href=&quot;https://gigaom.com/2012/09/27/quantcast-releases-bigger-faster-stronger-hadoop-file-system/&quot;&gt;https://gigaom.com/2012/09/27/quantcast-releases-bigger-faster-stronger-hadoop-file-system/&lt;/a&gt; - background information &lt;a href=&quot;http://www.odbms.org/blog/2013/03/big-data-improving-hadoop-for-petascale-processing-at-quantcast/&quot;&gt;http://www.odbms.org/blog/2013/03/big-data-improving-hadoop-for-petascale-processing-at-quantcast/&lt;/a&gt; - interview with creators&lt;/p&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;p&gt;&lt;a href=&quot;http://www.quantcast.com/feed/&quot;&gt;http://www.quantcast.com/feed/&lt;/a&gt; - occasional updates on the Quantcast blog&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Greenplum</title><link>http://ondataengineering.net/technologies/greenplum/</link><description> &lt;p&gt;A shared nothing, massively parallel processing (MPP) database optimised for analytical / OLAP workloads. Based on a fork PostgreSQL, it is essentially multiple PostgreSQL databases working together as a single logical database. Supports a cost-based query optimiser optimised for large analytical workloads, multiple storage models (including append only, columnar and heap), full ACID compliance and concurrent transactions, multiple index types, broad SQL support, a range of client connectors (including ODBC and JDBC), high capacity bulk load and unload tools, in database query language support (including Python, R, Perl, Java and C), and in database analytics support (including machine learning via Apache MADLib, geographic analytics via PostGIS and encryption via PGCrypto). Originally created by Greenplum (the company) which was founded in September 2003 before being brought by EMC in 2010, with Greenplum (the database) then spun out as part of Pivotal Software in 2013 before being open sourced in in October 2015 under the Apache 2.0 licence with the source code hosted on GitHub. Development is still led by Pivotal (with little evidence of outside contributions), who also distribute binaries as Pivotal Greenplum and provide training, consultancy and support.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Pivotal Greenplum, GPDB&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;Pivotal&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;February 2017 - 4.3&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;p&gt;&lt;a href=&quot;http://greenplum.org/&quot;&gt;http://greenplum.org/&lt;/a&gt; - open source project homepage &lt;a href=&quot;https://github.com/greenplum-db/gpdb&quot;&gt;https://github.com/greenplum-db/gpdb&lt;/a&gt; - code repository &lt;a href=&quot;https://pivotal.io/pivotal-greenplum&quot;&gt;https://pivotal.io/pivotal-greenplum&lt;/a&gt; - Pivotal Greenplum homepage &lt;a href=&quot;https://github.com/greenplum-db/greenplum-db.github.io/wiki/Greenplum-Architecture&quot;&gt;https://github.com/greenplum-db/greenplum-db.github.io/wiki/Greenplum-Architecture&lt;/a&gt; - architecture overview &lt;a href=&quot;https://content.pivotal.io/datasheets/pivotal-greenplum&quot;&gt;https://content.pivotal.io/datasheets/pivotal-greenplum&lt;/a&gt; - Pivotal Greenplum datasheet &lt;a href=&quot;http://gpdb.docs.pivotal.io/&quot;&gt;http://gpdb.docs.pivotal.io/&lt;/a&gt; - documentation &lt;a href=&quot;https://network.pivotal.io/products/pivotal-gpdb&quot;&gt;https://network.pivotal.io/products/pivotal-gpdb&lt;/a&gt; - download site&lt;/p&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;p&gt;&lt;a href=&quot;http://greenplum.org/&quot;&gt;http://greenplum.org/&lt;/a&gt; - link to Greenplum announcement mailing list&lt;/p&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hortonworks Data Platform</title><link>http://ondataengineering.net/technologies/hortonworks-data-platform/</link><description> &lt;p&gt;A distribution of Hadoop based on a commitment to the Apache open source ecosystem. All bundled projects are Apache open source projects based on official Apache project releases, with any patches for bug fixes or new features official Apache project patches pulled from later releases of the project. Available as RPMs or can be installed using Apache Ambari (for local installs) or Cloudbreak (for installation on cloud platforms). Also comes with a number of add-ons, including ODBC and JDBC drivers for Hive and Spark SQL, HDP Search and Hortonworks HDB. Provided free of charge, with training, consultancy and support available from Hortonworks, along with their proprietary SmartSense support tool. First released in June 2012.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;HDP&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/hortonworks/&quot;&gt;Hortonworks&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Categories&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-categories/hadoop_distributions/&quot;&gt;Hadoop Distributions&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;February 2017 - 2.5&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Packages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-accumulo/&quot;&gt;Apache Accumulo&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-atlas/&quot;&gt;Apache Atlas&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-calcite/&quot;&gt;Apache Calcite&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-datafu/&quot;&gt;Apache DataFu&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-falcon/&quot;&gt;Apache Falcon&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-flume/&quot;&gt;Apache Flume&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Apache Hadoop&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hbase/&quot;&gt;Apache HBase&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Apache Hive&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-knox/&quot;&gt;Apache Knox&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-mahout/&quot;&gt;Apache Mahout&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-oozie/&quot;&gt;Apache Oozie&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-phoenix/&quot;&gt;Apache Phoenix&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-pig/&quot;&gt;Apache Pig&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-ranger/&quot;&gt;Apache Ranger&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-slider/&quot;&gt;Apache Slider&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-sqoop/&quot;&gt;Apache Sqoop&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-storm/&quot;&gt;Apache Storm&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-tez/&quot;&gt;Apache Tez&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-zeppelin/&quot;&gt;Apache Zeppelin&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-zookeeper/&quot;&gt;Apache ZooKeeper&lt;/a&gt;, &lt;a href=&quot;/technologies/hue/&quot;&gt;Hue&lt;/a&gt;, &lt;a href=&quot;/technologies/livy/&quot;&gt;Livy&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Add ons&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-hawq/&quot;&gt;Apache Hawq&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-platform-search/&quot;&gt;Hortonworks Data Platform Search&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-smartsense/&quot;&gt;Hortonworks SmartSense&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Manageable via&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-ambari/&quot;&gt;Apache Ambari&lt;/a&gt;, &lt;a href=&quot;/technologies/cloudbreak/&quot;&gt;Cloudbreak&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;further-information&quot;&gt;Further Information&lt;/h2&gt; &lt;p&gt;The base Apache project versions bundled with each version of HDP are shown on the HDP home page, as well as on the first page of the release notes. Details of the features in these releases that Hortonworks don?t support, and the patches that have been applied to these releases are also available in the release notes, along with known vulnerabilities, fixes from previous versions and known issues.&lt;/p&gt; &lt;p&gt;Note that:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Apache Calcite, Apache DataFu, Apache Mahout and Hue are not referenced on the HDP home page, but are part of HDP (they are referenced in the release notes)&lt;/li&gt; &lt;li&gt;Livy is not mentioned on the HDP home page or the release notes, but is part of HDP (it?s in the HDP rpm repo and included Zeppelin installation steps)&lt;/li&gt; &lt;li&gt;Although Solr is referenced on the HDP home page and in the release note, it is only available via the HDP Search add-on to HDP&lt;/li&gt; &lt;li&gt;Hortonworks HDB is Pivotal HDB, with support and consultancy provided by Hortonworks, and is distributed as an add-on to HDP&lt;/li&gt; &lt;li&gt;Cascading is referenced in the release notes, but isn?t part of HDP (it?s not in the HDP repo and isn?t covered by the installation guide)&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://hortonworks.com/products/data-center/hdp/&quot;&gt;http://hortonworks.com/products/data-center/hdp/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.0/index.html&quot;&gt;http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.0/index.html&lt;/a&gt; - HDP 2.5.0 documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.0/bk_release-notes/content/ch_relnotes_v250.html&quot;&gt;http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.0/bk_release-notes/content/ch_relnotes_v250.html&lt;/a&gt; - HDP 2.5.0 release notes&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://hortonworks.com/blog/&quot;&gt;http://hortonworks.com/blog/&lt;/a&gt; - Hortonworks Blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Atlas</title><link>http://ondataengineering.net/technologies/apache-atlas/</link><description> &lt;p&gt;A metadata and data governance solution for Hadoop. Supports an extensible metadata model with out of the box support for Hive datasets and data lineage from Hive queries and Sqoop imports, with limited support for Falcon, Storm and Kafka. Allows datasets and data items to be tagged (and for these tags to be used for access control by Apache Ranger), and includes support for business taxonomies as a technical preview. Implemented as a graph based database using Titan (which by default uses HBase and Solr), with a web based user interface and a REST API for searching and visualising/retrieving metadata, and Kafka topics for the ingest of metadata (primarily from hooks in metadata sources such as Hive or Sqoop) and the publishing of metadata change events. An incubating Apache project, donated to the Apache Foundation in May 2015 by the Hortonworks Data Governance Initiative in partnership with Aetna, Merck, Target, Schlumberger and SAS. Has not yet reached a v1.0 milestone or graduated as a top level Apache project, but is still under active development.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Atlas&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;February 2017 - v0.7&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://atlas.incubator.apache.org/&quot;&gt;http://atlas.incubator.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://hortonworks.com/apache/atlas&quot;&gt;http://hortonworks.com/apache/atlas&lt;/a&gt; - Hortonworks background information, including links to relevant blog posts&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://atlas.incubator.apache.org/Architecture.html&quot;&gt;http://atlas.incubator.apache.org/Architecture.html&lt;/a&gt; - Architecture overview&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.3/bk_data-governance/content/ch_hdp_data_governance_overview.html&quot;&gt;http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.3/bk_data-governance/content/ch_hdp_data_governance_overview.html&lt;/a&gt; - Hortonworks documentation from 2.5.3 HDP release&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Blog updates via the Hortonworks and Apache blogs*&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Ranger</title><link>http://ondataengineering.net/technologies/apache-ranger/</link><description> &lt;p&gt;A centralised security framework for managing access to data in Hadoop. Supports integration with LDAP and Active Directory for user authentication, a web based administration interface, a REST API, and a central policy engine used by plugins within individual Hadoop components (including HDFS, Hive, HBase, Storm, Knox, Solr, Kafka, YARN, Atlas and NiFi). Supports data access, data masking, and row level filtering policies (with masking and row level filtering currently only supported by Hive), the ability to define policies against tags as well as directly against resources (with tags assigned to resources externally, e.g. in Apache Atlas), and the ability to use more complex conditions (e.g. denying access after an expiration date or based on a users location). Extendable with the ability to add support for new services (Ranger Stacks) and to add custom decision rules (via content enrichers and condition evaluators). Also supports a full audit capability of access requests and decisions, and a key management service for HDFS encryption keys. An incubating Apache project, donated in July 2014 by the Hortonworks following their acquisition of XA Secure. Has not yet reached a v1.0 milestone, but is still under active development with a range of contributors.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Ranger&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;February 2017 - v0.6&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-flow/&quot;&gt;Hortonworks DataFlow&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://ranger.incubator.apache.org/&quot;&gt;http://ranger.incubator.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://ranger.apache.org/faq.html&quot;&gt;http://ranger.apache.org/faq.html&lt;/a&gt; - FAQs&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://hortonworks.com/apache/ranger/&quot;&gt;http://hortonworks.com/apache/ranger/&lt;/a&gt; - Hortonworks information on Ranger, including tutorials and blog posts&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.3/bk_security/content/ch_hdp-security-guide-authorization.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.3/bk_security/content/ch_hdp-security-guide-authorization.html&lt;/a&gt; - Hortonworks documentation on Ranger&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/RANGER&quot;&gt;https://cwiki.apache.org/confluence/display/RANGER&lt;/a&gt; - Apache Ranger Wiki, with most information detailed by release under the Release Folders page&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Blog updates via the Hortonworks and Apache blogs&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Knox</title><link>http://ondataengineering.net/technologies/apache-knox/</link><description> &lt;p&gt;A stateless gateway for the Apache Hadoop ecosystem that provides perimeter security. Includes support for user authentication (via LDAP, Active Directory and a number of single sign on solutions), access authorisation on a per service basis, transitions to Kerberos authentication, reverse proxying and auditing, extension points for supporting new services, audit capabilities, and out of the box support for a number of Hadoop technology end points. An Apache project, started by Hortonworks in February 2013, donated to the Apache Foundation two months later in April, before graduating in February 2014. Hasn't yet reached a v1.0 milestone, however still under active development.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Knox&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;February 2017 - v0.11&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://knox.apache.org/&quot;&gt;http://knox.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://knox.apache.org/books/knox-0-11-0/user-guide.html&quot;&gt;http://knox.apache.org/books/knox-0-11-0/user-guide.html&lt;/a&gt; - extreemly comprehensive documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://hortonworks.com/apache/knox-gateway/&quot;&gt;http://hortonworks.com/apache/knox-gateway/&lt;/a&gt; - Hortonworks information on Knox&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.3/bk_security/content/perimeter_security_with_apache_knox.html&quot;&gt;http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.3/bk_security/content/perimeter_security_with_apache_knox.html&lt;/a&gt; - Hortonworks documentation on Knox&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Blog updates via the Hortonworks and Apache blogs&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/KNOX/News&quot;&gt;https://cwiki.apache.org/confluence/display/KNOX/News&lt;/a&gt; - news updates&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Falcon</title><link>http://ondataengineering.net/technologies/apache-falcon/</link><description> &lt;p&gt;Data feed management system for Hadoop. Supports the definition, scheduling and orchestration (including support for late data and retry policies) of data processing pipelines (referred to as processes, with support for Ozzie, Spark, Hive and Pig jobs), the management of the data produced and consumed by these pipelines (referred to as feeds, with support for data in HDFS and Hive) and the generation and visualisation of pipeline lineage information, all across multiple Hadoop clusters. Also includes the ability to mirror or replicate HDFS and Hive data between clusters, to failover processing between clusters and to import and export data using Sqoop. Supports both a web and command line interface and a REST API. An Apache project, graduating in December 2014, having been originally donated by inMobi in April 2013. Hasn't yet reached a v1.0 milestone, however still under development led by inMobi and Hortonworks with a range of other contributors.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Falcon&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;February 2017 - v0.10&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://falcon.apache.org/&quot;&gt;http://falcon.apache.org/&lt;/a&gt; - homepage and documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://hortonworks.com/apache/falcon/&quot;&gt;http://hortonworks.com/apache/falcon/&lt;/a&gt; - Hortonworks information on Falcon, including tutorials and blog posts&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.3/bk_data-movement-and-integration/content/index.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.3/bk_data-movement-and-integration/content/index.html&lt;/a&gt; - Hortonworks documentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Blog updates via the Hortonworks and Apache blogs&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Calcite</title><link>http://ondataengineering.net/technologies/apache-calcite/</link><description> &lt;p&gt;A framework for building SQL based data access capabilities. Supports a SQL parser and validator, tools for the transformation and (cost based) optimisation of SQL expression trees, and an adapter framework for accessing metadata and executing queries (including out of the box adapters for a number of database technologies as well as CSV files and POJO objects), along with specific support for streaming SQL queries and optimising data cube queries to use materialised views. Also includes (as a sub-project named Avatica), a framework for building database drivers with support for a standard JDBC driver, server and wire protocols, plus a local embeddable JDBC driver. Used in a range of other projects including Drill, Flink, Hive, Kylin, Phoenix, Samza, Storm and Cascading. An Apache project, originally created by Julian Hyde in May 2012 as Optiq, donated to the Apache Foundation in May 2014, graduating in October 2015 following a v1.0 release in January 2015. Under active development with a range of contributors.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Calcite, Avatica&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Open Source - Active&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;February 2017 - v1.11&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://calcite.apache.org/&quot;&gt;https://calcite.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://calcite.apache.org/docs/&quot;&gt;https://calcite.apache.org/docs/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://calcite.apache.org/docs/avatica_overview.html&quot;&gt;https://calcite.apache.org/docs/avatica_overview.html&lt;/a&gt; - Avatica overview&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://calcite.apache.org/news/&quot;&gt;https://calcite.apache.org/news/&lt;/a&gt; - Calcite news&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://calcite.apache.org/avatica/news/&quot;&gt;https://calcite.apache.org/avatica/news/&lt;/a&gt; - Avatica news&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Slider</title><link>http://ondataengineering.net/technologies/apache-slider/</link><description> &lt;p&gt;Framework for hosting long running distributed applications on YARN, allowing YARN to manage the resources these applications use. Can handle any application that supports a base set of requirements (including being able to install and run from a tarball), with experimental support for docker packaged applications. Operates as a YARN application master (the Slider AM), an associated command line interface and lightweight agents to manage running components. Supports manual scaling, automatic recovery, rolling upgrades and component placement controls, and includes out of the box configuration for a number of applications including Accumulo, HBase, Kafka, Memcached, Solr, Storm and Tomcat. An incubating Apache project, originally donated in April 2014. Hasn't yet reached a v1.0 milestone, however still under development led by Hortonworks.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Slider&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;February 2017 - v0.91&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Add on to&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/yarn/&quot;&gt;YARN&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://slider.incubator.apache.org&quot;&gt;http://slider.incubator.apache.org&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/deploying-long-running-services-on-apache-hadoop-yarn-cluster/&quot;&gt;https://hortonworks.com/blog/deploying-long-running-services-on-apache-hadoop-yarn-cluster/&lt;/a&gt; - introduction blog post&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/apache/slider/&quot;&gt;https://hortonworks.com/apache/slider/&lt;/a&gt; - Hortonworks information on slider, including links to blog posts&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.3/bk_yarn-resource-management/content/ch_slider.html&quot;&gt;http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.3/bk_yarn-resource-management/content/ch_slider.html&lt;/a&gt; - Hortonworks documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://slider.incubator.apache.org/docs/&quot;&gt;http://slider.incubator.apache.org/docs/&lt;/a&gt; - Slider documentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Blog updates via the Hortonworks and Apache blogs&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Storm</title><link>http://ondataengineering.net/technologies/apache-storm/</link><description> &lt;p&gt;Specialised distributed stream processing technology based on a single record (not micro batch) model with at least once processing semantics. Processing flows are called topologies based on a directed acyclic graph of spouts (which produce unbounded streams of tuples) and bolts (which process streams and optionally produce output streams). Supports high throughput and low latency use cases, horizontal scalability, fault tolerance (failed workers are automatically restarted and failed over to new nodes if required), back pressure, windowing (with support for sliding and tumbling windows based on time or event counts), stateful bolts and a shared bolt storage cache (that's updatable from the command line). Also includes a higher level micro batch API (Trident) that supports exactly-once processing semantics, fault-tolerant state management and higher level operations including joins, aggregations and groupings, support for SQL (StormSQL) and frameworks and utilities to make defining and deploying topologies easier (Flux). Has both a graphical web based and command line interface, plus a REST API. Primarily written in Clojure, JVM based, but supports multiple languages through the use of Thrift for defining and submitting topologies, and the use of spouts that can interface to other languages using JSON over stdin/stdout. Originally created at BackType, before being open sourced in September 2011 after the acquisition of BackType by Twitter. Donated to the Apache Foundation in September 2013, graduating in September 2014, with a 1.0 release in April 2016. Has multiple reference cases for being deployed at scale, including Twitter, and is still under active development.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Storm&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;February 2017 - v1.0&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-flow/&quot;&gt;Hortonworks DataFlow&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://storm.apache.org/&quot;&gt;http://storm.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/apache/storm/&quot;&gt;https://hortonworks.com/apache/storm/&lt;/a&gt; - Hortonworks information, including tutorials and blog posts&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://storm.apache.org/releases/1.0.3/index.html&quot;&gt;http://storm.apache.org/releases/1.0.3/index.html&lt;/a&gt; - documentation for current release&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.3/bk_storm-component-guide/content/ch_storm-overview.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.3/bk_storm-component-guide/content/ch_storm-overview.html&lt;/a&gt; - Hortonworks documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://nathanmarz.com/blog/history-of-apache-storm-and-lessons-learned.html&quot;&gt;http://nathanmarz.com/blog/history-of-apache-storm-and-lessons-learned.html&lt;/a&gt; - history of storm&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;p&gt;http://accumulo.apache.org/news/&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://storm.apache.org/index.html&quot;&gt;http://storm.apache.org/index.html&lt;/a&gt; - Storm new announced on Apache homepage&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Accumulo</title><link>http://ondataengineering.net/technologies/apache-accumulo/</link><description> &lt;p&gt;NoSQL wide-column datastore based on BigTable. Supports horizontal scalability, cell based access control (based on arbitrary boolean expressions of user security labels), high availability, atomic read-modify-write operations, map reduce support (both as a source and sink), table constraints, LDAP and Kerberos integration, and replication between instances. Comes with a web based monitoring interface (Accumulo Monitor) and a CLI. Written in Java, with thrift based API allowing access from other languages including C++, Python, Ruby. Originally developed at the NSA, donated to the Apache Foundation in September 2011, before graduating in March 2012, and is still under active development.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Accumulo&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;March 2017 - v1.8&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://accumulo.apache.org/&quot;&gt;http://accumulo.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://accumulo.apache.org/docs-archive/&quot;&gt;http://accumulo.apache.org/docs-archive/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/apache/accumulo/&quot;&gt;https://hortonworks.com/apache/accumulo/&lt;/a&gt; - Hortonworks information&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://accumulo.apache.org/news/&quot;&gt;http://accumulo.apache.org/news/&lt;/a&gt; - news page&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Livy</title><link>http://ondataengineering.net/technologies/livy/</link><description> &lt;p&gt;A service that allows Spark jobs (pre-compiled JARs) or code snippets (Scala or Python) to be executed by remote systems over a REST API or via clients for Java, Scala and Python. Supports re-use of Spark Contexts (and caching and sharing of RDDs across jobs and clients), multiple concurrent clients, secure authenticated communications and batch job submissions. Started in November 2015 based on code from Hue, with a formal announcement and first release in June 2016. Open source under the Apache 2.0 licence, hosted on GitHub with development led by Cloudera and Microsoft. Still considered to be in alpha, but under active development, and used by tools such as Hue and Zeppelin.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Open Source - Active&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;March 2017 - v0.3&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://livy.io/&quot;&gt;http://livy.io/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/cloudera/livy&quot;&gt;https://github.com/cloudera/livy&lt;/a&gt; - code repository&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/more/news-and-events/press-releases/2016-06-06-cloudera-microsoft-lead-development-open-source-project-livy-for-easy-use-spark-end-user-applications.html&quot;&gt;https://www.cloudera.com/more/news-and-events/press-releases/2016-06-06-cloudera-microsoft-lead-development-open-source-project-livy-for-easy-use-spark-end-user-applications.html&lt;/a&gt; - original announcement&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/cloudera/livy/releases&quot;&gt;https://github.com/cloudera/livy/releases&lt;/a&gt; - details of releases&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hortonworks Data Platform Search</title><link>http://ondataengineering.net/technologies/hortonworks-data-platform-search/</link><description> &lt;p&gt;An add on package to HDP that bundles up Solr, Banana, and a suite of libraries and tools for integrating with Solr from Hadoop (utilities for loading data from HDFS), Hive (a SerDe to allow Solr data to be read and written as a Hive table), Pig (store and load functions), HBase (replication of HBase event to Solr), Storm and Spark (both SDKs for integrating with Solr). Available as an add on Ambari management pack or as a set of RPMs. Built, maintained and supported by Lucidworks on behalf of Hortonworks, first announced in April 2014 as part of the introduction of Solr with HDP 2.1.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;HDP Search&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/hortonworks/&quot;&gt;Hortonworks&lt;/a&gt;, Lucidworks&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;March 2017 - v2.5&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Packages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-solr/&quot;&gt;Apache Solr&lt;/a&gt;, Banana&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Add on to&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://doc.lucidworks.com/lucidworks-hdpsearch/2.5/index.html&quot;&gt;https://doc.lucidworks.com/lucidworks-hdpsearch/2.5/index.html&lt;/a&gt; - Documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.3/bk_solr-search-installation/content/ch_hdp-search.html&quot;&gt;http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.3/bk_solr-search-installation/content/ch_hdp-search.html&lt;/a&gt; - Hortonworks installation documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/bringing-enterprise-search-enterprise-hadoop/&quot;&gt;https://hortonworks.com/blog/bringing-enterprise-search-enterprise-hadoop/&lt;/a&gt; - Partnership announcement blog post&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;News via the Hortonworks blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Hawq</title><link>http://ondataengineering.net/technologies/apache-hawq/</link><description> &lt;p&gt;A port of the Greenplum MPP database (which itself is based on PostgreSQL) to run over YARN and HDFS. Supports all the features of Greenplum (ACID transactions, broad SQL support and in database language and analytics support, including support for Apache MADLib), integrated with Apache Ambari, an Input Format for MapReduce to read Hawq tables, and both row and Parquet (column) based storage of data managed by Hawq. Also supports queries over data not managed by Hawq via external tables, with a Java based framework (PXF) for accessing external data, and out of the box support for accessing data in HDFS (text, Avro, JSON), Hive and HBase, with a number of open source connectors also available. Fault tolerant and horizontally scalable, with the ability to scale up or down on the fly. Originally created as Pivotal Hawq based on a fork of Greenplum in 2011, with an initial 1.0 release as part of Pivotal HD in July 2013. Open sourced and donated to the Apache Foundation in September 2015, becoming Apache Hawq, with the first open source release (2.0) in October 2016. Development led by Pivotal, who also distribute binaries as Pivotal HDB and provide training, consultancy and support. Pivotal HDB is also available as Hortonworks HDB.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Pivotal HDB, Hortonworks HDB&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;, Pivotal, &lt;a href=&quot;/tech-vendors/hortonworks/&quot;&gt;Hortonworks&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;March 2017 - 2.1&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Add on to&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://hawq.incubator.apache.org/&quot;&gt;http://hawq.incubator.apache.org/&lt;/a&gt; - Apache Hawq homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://hawq.incubator.apache.org/docs/userguide/latest/index.html&quot;&gt;http://hawq.incubator.apache.org/docs/userguide/latest/index.html&lt;/a&gt; - Apache Hawq documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/HAWQ/&quot;&gt;https://cwiki.apache.org/confluence/display/HAWQ/&lt;/a&gt; - Apache Hawq Wiki&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://hortonworks.com/apache/hawk/&quot;&gt;http://hortonworks.com/apache/hawk/&lt;/a&gt; - Hortonworks information on Apache Hawq&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.3/bk_hdb-quick-guide/content/ch_hdb_summary.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.3/bk_hdb-quick-guide/content/ch_hdb_summary.html&lt;/a&gt; - Hortonworks HDB documentation (links through to Pivotal HDB docs)&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://pivotal.io/pivotal-hdb&quot;&gt;https://pivotal.io/pivotal-hdb&lt;/a&gt; - Pivotal HDB homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://hdb.docs.pivotal.io/&quot;&gt;http://hdb.docs.pivotal.io/&lt;/a&gt; - Pivotal HDB documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://content.pivotal.io/blog/the-way-to-hadoop-native-sql&quot;&gt;https://content.pivotal.io/blog/the-way-to-hadoop-native-sql&lt;/a&gt; - Hawq open source announcement&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/press-releases/hortonworks-pivotal-expand-relationship-deliver-enterprise-ready-modern-data-platforms-data-management-analytics/&quot;&gt;https://hortonworks.com/press-releases/hortonworks-pivotal-expand-relationship-deliver-enterprise-ready-modern-data-platforms-data-management-analytics/&lt;/a&gt; - Hortonworks and Pivotal HDB announcement&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;News via Hortonworks and Pivotal blogs&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache Ambari</title><link>http://ondataengineering.net/technologies/apache-ambari/</link><description> &lt;p&gt;Platform for installing, managing and monitoring Apache Hadoop clusters. Supports the installation of different versions of different distributions of Hadoop through Stack definitions (with support for HDP out of the box, and further stacks and add ons available through management packs), and the specification of Blueprints (cluster layouts and configuration for a given Stack) that can be used to programmatically create multiple clusters (e.g. dev, test and production). Also supports both rolling (no downtime) and express (faster but with downtime) upgrades; cluster administration (including adding and removing nodes/services, viewing the status of nodes/services, and configuring services with the versioning of configuration and the ability to rollback changes); the automated Kerberization of clusters; the collection, storage (in HBase) and visualisation (via Grafana or through dashboards in Ambari) of system and Hadoop component metrics via the Ambari Metrics System (AMS); alerting on statuses and metrics; the collection, storage (in Solr) and searching/viewing of log entries from across the Hadoop cluster (currently in technical preview); and a framework for UI components within Ambari (Ambari Views, treated here as a sub-project). Web based, with a REST API, and backed by a backend database (Oracle, MySQL or Postgres). Donated to the Apache Foundation by Hortonworks, IBM and Yahoo in August 2011 as the Hadoop Management System (HMS), graduating in December 2013 after changing it's name to Ambari. Still under active development with a large number of contributors.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Ambari&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;, &lt;a href=&quot;/tech-vendors/hortonworks/&quot;&gt;Hortonworks&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;March 2017 - v2.4&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Sub-projects&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; Apache Ambari&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-ambari/ambari-views/&quot;&gt;Ambari Views&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Framework within Ambari that allows new applications or views to be added to Ambari, based on new client side code (HTML, JavaScript and CSS) supported by new backend code (Java) that exposes REST API end points for the UI to consume. Comes with support for a number of views out of the box, including YARN Queue Manager (supports the creation and configuration of YARN capacity schedule queues), Files (supports copying and moving, uploading and setting permissions on files in HDFS), Falcon (supports defining, scheduling and monitoring data management pipelines), Hive (supports browsing databases, executing queries and viewing explain plans, saving queries, viewing query history and uploading data to Hive tables), Pig (supports executing Pig scripts and viewing execution history), SmartSense (supports capture and download of bundles), Storm (supports viewing cluster status, monitoring topologies, perform topology management and access metrics and logs) and Tez (supports viewing and debugging Tez jobs), along with technical previews of Workflow Designer, Zeppelin and Hue migration views. Views can be deployed into a standalone Ambari instance to separate these from the primary Ambari management instance and to support scaling out.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Manages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;, &lt;a href=&quot;/technologies/hortonworks-data-flow/&quot;&gt;Hortonworks DataFlow&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://ambari.incubator.apache.org/&quot;&gt;http://ambari.incubator.apache.org/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://hortonworks.com/apache/ambari/&quot;&gt;http://hortonworks.com/apache/ambari/&lt;/a&gt; - Hortonworks information, including tutorials and blog posts&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/Ambari/Ambari-2.4.2.0/index.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/Ambari/Ambari-2.4.2.0/index.html&lt;/a&gt; - Hortonworks documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/AMBARI/Ambari&quot;&gt;https://cwiki.apache.org/confluence/display/AMBARI/Ambari&lt;/a&gt; - Apache developer level documentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Blog updates via the Hortonworks blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Ambari Views</title><link>http://ondataengineering.net/technologies/apache-ambari/ambari-views/</link><description> &lt;p&gt;Framework within Ambari that allows new applications or views to be added to Ambari, based on new client side code (HTML, JavaScript and CSS) supported by new backend code (Java) that exposes REST API end points for the UI to consume. Comes with support for a number of views out of the box, including YARN Queue Manager (supports the creation and configuration of YARN capacity schedule queues), Files (supports copying and moving, uploading and setting permissions on files in HDFS), Falcon (supports defining, scheduling and monitoring data management pipelines), Hive (supports browsing databases, executing queries and viewing explain plans, saving queries, viewing query history and uploading data to Hive tables), Pig (supports executing Pig scripts and viewing execution history), SmartSense (supports capture and download of bundles), Storm (supports viewing cluster status, monitoring topologies, perform topology management and access metrics and logs) and Tez (supports viewing and debugging Tez jobs), along with technical previews of Workflow Designer, Zeppelin and Hue migration views. Views can be deployed into a standalone Ambari instance to separate these from the primary Ambari management instance and to support scaling out.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-ambari/&quot;&gt;Apache Ambari&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;March 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Cloudbreak</title><link>http://ondataengineering.net/technologies/cloudbreak/</link><description> &lt;p&gt;Solution for deploying and managing Hadoop clusters on cloud infrastructure based on automatically provisioned infrastructure running base docker images with Hadoop provisioned on top via Apache Ambari using Blueprints. Includes out of the box support for Amazon Web Services, Microsoft Azure, Google Cloud Platform and OpenStack, plus a Service Provider Interface (SPI) for adding support for new providers. Supports automated scaling of clusters based on Ambari Metrics and Alerts (Periscope), custom scripts that can be run on hosts before or after deployment (Recipes), a number of out of the box Blueprints, plus a number of technical preview features, including the use of custom docker images, data locality specifiers, Kerberized clusters, support for external AD/LDAP servers and deployment on Mesos. Manageable through a web UI, a REST API, a CLI and an interactive shell. Originally created by SequenceIQ, with an initial beta release in July 2014, with SequenceIQ then acquired by Hortonworks in April 2015, and a 1.0 release of Cloudbreak included in HDP 2.3 in July 2015. Open sourced under the Apache 2.0 licence, with a stated plan for the code to be donated to the Apache Foundation.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/hortonworks/&quot;&gt;Hortonworks&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;March 2017 - v1.6&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Manages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://sequenceiq.com/cloudbreak-docs/latest/&quot;&gt;http://sequenceiq.com/cloudbreak-docs/latest/&lt;/a&gt; - Documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/sequenceiq/cloudbreak&quot;&gt;https://github.com/sequenceiq/cloudbreak&lt;/a&gt; - Code&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/apache/cloudbreak/&quot;&gt;https://hortonworks.com/apache/cloudbreak/&lt;/a&gt; - Hortonworks information, including blog posts&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://docs.hortonworks.com/HDPDocuments/Cloudbreak/Cloudbreak-1.6.3/index.html&quot;&gt;http://docs.hortonworks.com/HDPDocuments/Cloudbreak/Cloudbreak-1.6.3/index.html&lt;/a&gt; - Hortonworks documentation page, however links through to SequenceIQ docs page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/hortonworks-acquires-sequenceiq-to-provide-automated-deployment-of-hadoop-everywhere/&quot;&gt;https://hortonworks.com/blog/hortonworks-acquires-sequenceiq-to-provide-automated-deployment-of-hadoop-everywhere/&lt;/a&gt; - SequenceIQ acquisition announcement&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Blog updates via the Hortonworks blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hortonworks SmartSense</title><link>http://ondataengineering.net/technologies/hortonworks-smartsense/</link><description> &lt;p&gt;Supports the capture of diagnostic information from Hadoop clusters (including configuration, metrics and logs from both Hadoop and the Operating System) into a bundle for upload (either manually or automatically) to the Hortonworks support portal to assist in the resolution of support issues and the delivery of cluster optimisation and preventative action recommendations, with support for anonymisation (including IP addresses and host names, with support for further custom rules) and encryption of information in bundles and a SmartSense gateway to proxy uploads if direct internet access isn't available. Also includes functionality to help understand and analyse cluster activity include the Activity Analyser (aggregates data from YARN, Tez, MapReduce and HDFS into Ambari Metrics) and Activity Explorer (an embedded instance of Apache Zeppelin with pre-built notebooks for exploring and visualising cluster activity). Installable and manageable through Apache Ambari. Part of the Hortonworks support offering, introduced in June 2015 as part of HDP 2.3.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;SmartSense&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/hortonworks/&quot;&gt;Hortonworks&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;March 2017 - v1.3&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Add on to&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/services/support/smartsense/&quot;&gt;https://hortonworks.com/services/support/smartsense/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://docs.hortonworks.com/HDPDocuments/SS1/SmartSense-1.3.1/index.html&quot;&gt;http://docs.hortonworks.com/HDPDocuments/SS1/SmartSense-1.3.1/index.html&lt;/a&gt; - current documentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Blog updates via the Hortonworks blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hortonworks DataFlow</title><link>http://ondataengineering.net/technologies/hortonworks-data-flow/</link><description> &lt;p&gt;A distribution of a set of Apache open source technologies (primarily NiFi, Kafka and Storm) for processing data, with all products integrated with Ranger for security and Ambari for management. All bundled projects are Apache open source projects based on official Apache project releases, with any patches for bug fixes or new features pulled from official Apache project patches from later releases of the project. Available as RPMs or can be installed using Apache Ambari (via a management pack). Provided free of charge, with training, consultancy and support available from Hortonworks. First released in September 2015 as a distribution of just NiFi following the acquisition by Hortonworks of Onyara,who were setup by the creators of NiFi to provided commercial support for it.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;HDF&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/hortonworks/&quot;&gt;Hortonworks&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;March 2017 - 2.1&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Packages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-nifi/&quot;&gt;Apache NiFi&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-nifi/minifi/&quot;&gt;MiNiFi&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-ranger/&quot;&gt;Apache Ranger&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-storm/&quot;&gt;Apache Storm&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-zookeeper/&quot;&gt;Apache ZooKeeper&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Manageable via&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-ambari/&quot;&gt;Apache Ambari&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;further-information&quot;&gt;Further Information&lt;/h2&gt; &lt;p&gt;The details of the Apache projects distributed as part of Hortonworks DataFlow are detailed in the release notes, along with the specific versions included, the unsupported features, the patches pulled forward from future project releases, and the known vulnerabilities and issues.&lt;/p&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;td&gt;September 2015&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://hortonworks.com/press-releases/hortonworks-to-acquire-onyara-to-turn-internet-of-anything-data-into-actionable-insights/&quot;&gt;https://hortonworks.com/press-releases/hortonworks-to-acquire-onyara-to-turn-internet-of-anything-data-into-actionable-insights/&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Initial version consisting of just Apache NiFi&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.1&lt;/td&gt; &lt;td&gt;December 2015&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://hortonworks.com/blog/hortonworks-dataflow-1-1-released/&quot;&gt;https://hortonworks.com/blog/hortonworks-dataflow-1-1-released/&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.2&lt;/td&gt; &lt;td&gt;March 2016&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://hortonworks.com/blog/hortonworks-dataflow-1-2-released/&quot;&gt;https://hortonworks.com/blog/hortonworks-dataflow-1-2-released/&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Storm and Kafka added&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.0&lt;/td&gt; &lt;td&gt;September 2016&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://hortonworks.com/blog/hortonworks-dataflow-2-0-ga/&quot;&gt;https://hortonworks.com/blog/hortonworks-dataflow-2-0-ga/&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Ranger and Ambari support added&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.1&lt;/td&gt; &lt;td&gt;December 2016&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://hortonworks.com/blog/announcing-availability-hortonworks-dataflow-hdf-2-1/&quot;&gt;https://hortonworks.com/blog/announcing-availability-hortonworks-dataflow-hdf-2-1/&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/products/data-center/hdf/&quot;&gt;https://hortonworks.com/products/data-center/hdf/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://docs.hortonworks.com/HDPDocuments/HDF2/HDF-2.1.0/index.html&quot;&gt;http://docs.hortonworks.com/HDPDocuments/HDF2/HDF-2.1.0/index.html&lt;/a&gt; - HDF 2.1.0 documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://docs.hortonworks.com/HDPDocuments/HDF2/HDF-2.1.0/bk_dataflow-release-notes/content/index.html&quot;&gt;http://docs.hortonworks.com/HDPDocuments/HDF2/HDF-2.1.0/bk_dataflow-release-notes/content/index.html&lt;/a&gt; - HDF 2.1.2 release notes&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/category/hdf/&quot;&gt;https://hortonworks.com/blog/category/hdf/&lt;/a&gt; - blog posts&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Apache NiFi</title><link>http://ondataengineering.net/technologies/apache-nifi/</link><description> &lt;p&gt;General purpose technology for the movement of data between systems, including the ingestion of data into an analytical platform. Based on directed acyclic graph of Processors and Connections, with the unit of work being a FlowFile (a blob of data plus a set of key/value pair attributes). Supports guaranteed delivery of FlowFiles, with NiFi resiliently storing state (by default to a local write ahead log) and data blobs (by default a set of local partitions on disk), with all transformation logic executed via a thread pool within the NiFi instance (with the option to deploy multiple NiFi instances as a cluster). All flows are configured in a graphical user interface, which is also used for management and operations (starting/stopping individual Processors and viewing real time statuses, statistics and other information). Also supports data provenance (reporting on the processing events and lineage of individual FlowFiles), scheduling of Processor execution (based on periodic execution timers or cron specifications), multi-threaded Processor execution, configuration of Processor batch sizes (to enable low latency or high throughput), prioritised queues within Connections (allowing FlowFiles to be processed based on their age or a priority attribute as an alternative to FIFO), back pressure (based on counts or data volume against individual Connections) and pressure release (automatic discarding of FlowFiles based on their age), the ability to stream data to and from other NiFi instances and other streaming technologies, the ability to import and export flows as XML (flow templates), an expression language for setting Processor configuration and populating FlowFile attributes, Controller Services to provide shared services to processors (e.g. access to credentials, shared state), Reporting Tasks to output status and statistics information and a user security model. Extensible through the addition of custom Processors, Controller Services, Reporting Tasks and Prioritizers, and integrates with Apache Ranger and Apache Ambari. Originally developed at the NSA as &quot;Niagara Files&quot;, before being donated to the Apache Foundation in November 2014, graduating in July 2015. Java based, with development lead by Hortonworks after their aquisition of Onyara (which was set up by original NiFi developers to provide commercial support and services).&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;NiFi&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;March 2017 - v1.1&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Sub-projects&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; Apache NiFi&amp;nbsp;&gt;&amp;nbsp; &lt;a href=&quot;http://ondataengineering.net/technologies/apache-nifi/minifi/&quot;&gt;MiNiFi&lt;/a&gt; &lt;/td&gt; &lt;td&gt;Lightweight headless version of NiFi used to collect and process data at it's source, before forwarding it on for centralised processing. Supports all key NiFi functionality including all NiFi processors, guaranteed delivery, data buffering (including back pressure and pressure release) and prioritised queuing, however flows are specified in configuration files, status information and statistics are only available via Reporting Tasks or via a CLI, and provenance can only be viewed by exporting events via Reporting Tasks to log files or a full NiFi instance. Supports warm re-deployments, automatically restarting to load a new configuration written to disk or pushed or pulled over HTTP. Available as a Java or Native C++ executable. Started in March 2016, with a 0.1 release in December 2016.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-flow/&quot;&gt;Hortonworks DataFlow&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://nifi.apache.org/&quot;&gt;http://nifi.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://nifi.apache.org/docs.html&quot;&gt;http://nifi.apache.org/docs.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/apache/nifi/&quot;&gt;https://hortonworks.com/apache/nifi/&lt;/a&gt; - Hortonworks information, including tutorials and blog posts&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://docs.hortonworks.com/HDPDocuments/HDF2/HDF-2.1.0/index.html&quot;&gt;http://docs.hortonworks.com/HDPDocuments/HDF2/HDF-2.1.0/index.html&lt;/a&gt; - Hortonworks documentation (as part of HDF)&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/NIFI/Release+Notes&quot;&gt;https://cwiki.apache.org/confluence/display/NIFI/Release+Notes&lt;/a&gt; - release notes&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Blog updates via the Hortonworks blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>MiNiFi</title><link>http://ondataengineering.net/technologies/apache-nifi/minifi/</link><description> &lt;p&gt;Lightweight headless version of NiFi used to collect and process data at it's source, before forwarding it on for centralised processing. Supports all key NiFi functionality including all NiFi processors, guaranteed delivery, data buffering (including back pressure and pressure release) and prioritised queuing, however flows are specified in configuration files, status information and statistics are only available via Reporting Tasks or via a CLI, and provenance can only be viewed by exporting events via Reporting Tasks to log files or a full NiFi instance. Supports warm re-deployments, automatically restarting to load a new configuration written to disk or pushed or pulled over HTTP. Available as a Java or Native C++ executable. Started in March 2016, with a 0.1 release in December 2016.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Sub-Project&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Parent Project&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-nifi/&quot;&gt;Apache NiFi&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;March 2017 - v0.1&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-flow/&quot;&gt;Hortonworks DataFlow&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://nifi.apache.org/minifi/index.html&quot;&gt;http://nifi.apache.org/minifi/index.html&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://nifi.apache.org/minifi/system-admin-guide.html&quot;&gt;http://nifi.apache.org/minifi/system-admin-guide.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/apache/nifi/#section_4&quot;&gt;https://hortonworks.com/apache/nifi/#section_4&lt;/a&gt; - Hortonworks information&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://docs.hortonworks.com/HDPDocuments/HDF2/HDF-2.1.0/index.html&quot;&gt;http://docs.hortonworks.com/HDPDocuments/HDF2/HDF-2.1.0/index.html&lt;/a&gt; - Hortonworks documentation (as part of HDF)&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/MINIFI/Release+Notes&quot;&gt;https://cwiki.apache.org/confluence/display/MINIFI/Release+Notes&lt;/a&gt; - release notes&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> <item><title>Hortonworks Data Cloud for AWS</title><link>http://ondataengineering.net/technologies/hortonworks-data-cloud-for-aws/</link><description> &lt;p&gt;Service that supports the creation and management of HDP clusters on Amazon Web Services (AWS). Management is done through a Cloud Controller AWS Product that provides a web interface and CLI for orchestrating the creation of AWS resources and the deployment of clusters using Ambari, and the subsequent scaling or cloning of the cluster. Supports a number of standard cluster types, including Data Science (Spark, Zeppelin), EDW-ETL (Hive, Spark) and EDW-Analytics (Hive, Zeppelin), with clusters also including Tez, Pig and Scoop, along with a number of standard node types, including worker nodes (that support HDFS and YARN) and computer nodes (that only support YARN). Clusters are designed to be ephemeral, however Amazon RDS can be used to provide persistent storage of Cloud Controller and Hive metadata, and Amazon S3 can be used to provide persistent cluster storage. Also supports Hortonworks SmartSense, cluster templates, the use of Spot Instances for compute nodes, and node recipes for executing custom scripts pre/post the Ambari cluster setup. Comes with free community support from Hortonworks. First launched in November 2016.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;HDCloud for AWS&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/hortonworks/&quot;&gt;Hortonworks&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;March 2017 - v1.11&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/products/cloud/aws/&quot;&gt;https://hortonworks.com/products/cloud/aws/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://docs.hortonworks.com/HDPDocuments/HDCloudAWS/HDCloudAWS-1.11.1/bk_hdcloud-aws/content/index.html&quot;&gt;http://docs.hortonworks.com/HDPDocuments/HDCloudAWS/HDCloudAWS-1.11.1/bk_hdcloud-aws/content/index.html&lt;/a&gt; - v1.11.1 documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/press-releases/availability-hortonworks-data-cloud-aws/&quot;&gt;https://hortonworks.com/press-releases/availability-hortonworks-data-cloud-aws/&lt;/a&gt; - original press release&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/marketplace/pp/B01LXOQBOU&quot;&gt;https://aws.amazon.com/marketplace/pp/B01LXOQBOU&lt;/a&gt; - AWS Cloud Controller product page&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Blog updates via the Hortonworks blog&lt;/li&gt; &lt;/ul&gt; </description> <discourse_author>Peter</discourse_author> </item> </channel> </rss>
