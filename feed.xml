<?xml version="1.0" encoding="UTF-8"?> <rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"> <channel><title>OnDataEngineering</title><description>A collaborative site for independent, critical and technical thinking on the use cases, architectural patterns and technologies relating to the transformation and preparation of data for exploitation.</description><link>http://ondataengineering.net/</link><atom:link href="http://ondataengineering.net/feed.xml" rel="self" type="application/rss+xml" /> <item><title>The Mid Week News - 07/02/2018</title><link>http://ondataengineering.net/blog/2018/02/07/the-mid-week-news/</link><pubDate>Wed, 07 Feb 2018 07:30:00 +0000</pubDate> <description> &lt;p&gt;No news next week, but back the week after, hopefully with a resumption of updates.&lt;/p&gt; &lt;p&gt;In the meantime, let’s have this weeks news… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/hortonworks-data-flow/&quot;&gt;Hortonworks Data Flow (HDF)&lt;/a&gt; has seen a 3.1 release&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-carbondata/&quot;&gt;Apache CarbonData&lt;/a&gt; has seen a 1.3 release&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Coral has entered the Apache Incubator - it’s a runtime for data processing languages (currently supporting &lt;a href=&quot;/technologies/apache-beam/&quot;&gt;Apache Beam&lt;/a&gt; with &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt; support coming) that dynamically adjusts to the runtime environment and uses Apache REEF to run over &lt;a href=&quot;/technologies/apache-hadoop/yarn/&quot;&gt;YARN&lt;/a&gt; or &lt;a href=&quot;/technologies/apache-mesos/&quot;&gt;Mesos&lt;/a&gt; - &lt;a href=&quot;https://snuspl.github.io/coral/&quot;&gt;homepage&lt;/a&gt;; &lt;a href=&quot;https://wiki.apache.org/incubator/CoralProposal&quot;&gt;Apache proposal&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From MapR on &lt;a href=&quot;/tech-vendors/mapr/&quot;&gt;MapR&lt;/a&gt;, part 1 of 4 on why their technologies are the best - &lt;a href=&quot;https://mapr.com/blog/mapr-data-technologies-gloves-off-series-1-4-storage/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Datanami - 2018 will be the year of the Data Engineer - &lt;a href=&quot;https://www.datanami.com/2018/02/05/2018-will-year-data-engineer/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;An intro to NiFi Registry (that enables configuration management of flows) from Hortonworks - &lt;a href=&quot;https://hortonworks.com/blog/hdf-3-1-blog-series-part-2-introducing-nifi-registry/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;There’s a new Gartner Critical Capabilities for Object Storage report out, and The Register have a summary - &lt;a href=&quot;https://www.theregister.co.uk/2018/02/02/gartner_object_storage_report/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Pivotal, graph processing on Greenplum using MADLib - &lt;a href=&quot;https://content.pivotal.io/blog/graph-processing-on-greenplum-database-using-apache-madlib&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2018/02/07/the-mid-week-news/</guid> </item> <item><title>The Mid Week News - 31/01/2018</title><link>http://ondataengineering.net/blog/2018/01/31/the-mid-week-news/</link><pubDate>Wed, 31 Jan 2018 07:30:00 +0000</pubDate> <description> &lt;p&gt;It’s Wednesday, which means it’s news time… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt; has seen a 5.14 release&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-manager/&quot;&gt;Cloudera Manager&lt;/a&gt; is up to 5.14&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-navigator/&quot;&gt;Cloudera Navigator&lt;/a&gt; is up to 2.13&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-director/&quot;&gt;Cloudera Director&lt;/a&gt; is up to 2.7&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-data-science-workbench/&quot;&gt;Cloudera Data Science Workbench&lt;/a&gt; has hit 1.3&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-nifi/minifi/&quot;&gt;Apache NiFi MiNiFi&lt;/a&gt; has seen 0.4 releases of it’s C++ and Java versions&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-atlas/&quot;&gt;Apache Atlas&lt;/a&gt; has a 1.0 alpha release out&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hortonworks have an update on &lt;a href=&quot;/technologies/apache-hadoop/yarn/&quot;&gt;YARN&lt;/a&gt; support for long running applications coming in Hadoop 3.1 - &lt;a href=&quot;https://hortonworks.com/blog/first-class-support-long-running-services-apache-hadoop-yarn/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Something interesting from Gartner - some thoughts on the Data Warehouse, the Data Lake, and what roles each of them play in a larger logical data warehouse - &lt;a href=&quot;https://blogs.gartner.com/henry-cook/2018/01/28/the-logical-data-warehouse-and-its-jobs-to-be-done/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;An introduction to Apache Pulsar, the latest Kafka competitor - &lt;a href=&quot;http://furkankamaci.com/apache-pulsar-distributed-pub-sub-messaging-system/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Neo4J - a comparison of &lt;a href=&quot;/tech-categories/rdf-databases/&quot;&gt;RDF Triple Stores&lt;/a&gt; and &lt;a href=&quot;/tech-categories/graph-databases/&quot;&gt;Labeled Property Graphs&lt;/a&gt; - &lt;a href=&quot;https://neo4j.com/blog/rdf-triple-store-vs-labeled-property-graph-difference/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Thoughts on Informatica’s entry to the cloud from Bloor - &lt;a href=&quot;https://www.bloorresearch.com/2018/01/informatica-pale-pink-maybe-orange/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Grab yourself a free copy of the &lt;a href=&quot;/technologies/apache-nifi/&quot;&gt;Apache NiFi&lt;/a&gt; For Beginners book from Hortonworks - &lt;a href=&quot;https://hortonworks.com/blog/introduction-apache-nifi/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2018/01/31/the-mid-week-news/</guid> </item> <item><title>The Mid Week News - 24/01/2018</title><link>http://ondataengineering.net/blog/2018/01/24/the-mid-week-news/</link><pubDate>Wed, 24 Jan 2018 07:30:00 +0000</pubDate> <description> &lt;p&gt;And it’s time for the news again… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-impala/&quot;&gt;Apache Impala&lt;/a&gt; has hit 2.11&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/greenplum/&quot;&gt;Greenplum&lt;/a&gt; continues it’s relentless development march to 5.4&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/alluxio/&quot;&gt;Alluxio&lt;/a&gt; is up to 1.7&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-hbase/&quot;&gt;Apache HBase&lt;/a&gt; has it’s first 2.0 beta out&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;There’s an update from LinkedIn on the latest with &lt;a href=&quot;/technologies/apache-gobblin/&quot;&gt;Apache Gobblin&lt;/a&gt; including new features added since 0.11 - &lt;a href=&quot;https://engineering.linkedin.com/blog/2018/01/gobblin-enters-apache-incubation&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 2 of Cloudera’s &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt; deployment blog posts is up, focusing on “where Cloudera services and roles should be placed across the various nodes in your clusters” - &lt;a href=&quot;http://blog.cloudera.com/blog/2018/01/deploy-cloudera-edh-clusters-like-a-boss-revamped-part-2/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Data Artisans - information on checkpointing in &lt;a href=&quot;/technologies/apache-flink/&quot;&gt;Apache Flink&lt;/a&gt; - &lt;a href=&quot;https://data-artisans.com/blog/managing-large-state-apache-flink-incremental-checkpointing-overview&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From ZDNet - an interview with Marko A. Rodriguez - the creator of the Titan graph database (now JanusGraph), DSE Graph and TinkerPop - &lt;a href=&quot;http://www.zdnet.com/article/from-graph-to-the-world-pioneering-a-database-virtual-machine/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Bloor have issued a Market Report Paper on SQL Engines on Hadoop - covering Hadoop &lt;a href=&quot;/tech-categories/query-engines/&quot;&gt;query engines&lt;/a&gt; capabilities, transactional (operational) capabilities and some &lt;a href=&quot;/tech-categories/analytical-databases/&quot;&gt;analytical database&lt;/a&gt; - &lt;a href=&quot;https://www.bloorresearch.com/research/sql-engines-hadoop/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2018/01/24/the-mid-week-news/</guid> </item> <item><title>An Update</title><link>http://ondataengineering.net/blog/2018/01/19/an-update/</link><pubDate>Fri, 19 Jan 2018 07:30:00 +0000</pubDate> <description> &lt;p&gt;So if you’ve been following this site, it’s probably become clear that the pace of updates has slowed quite considerably - unfortunately at this moment I don’t have the same time available on a regular basis that I did a few months ago.&lt;/p&gt; &lt;p&gt;So for the next few months until things quieten down I’m going to change my strategy a little. There will still be (semi-regular) updates, however these will appear as and when they’re ready, and they’ll be no posts on a Monday with any sort of commitment for what will happen that week. We will keep up with the weekly news updates however, to make sure we’re staying on top of the technologies we’ve looked at to date.&lt;/p&gt; &lt;p&gt;And we’ll try and get a few more contributors involved - if you’re interested in contributing of helping in any way then please do get in touch.&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2018/01/19/an-update/</guid> </item> <item><title>The Mid Week News - 17/01/2018</title><link>http://ondataengineering.net/blog/2018/01/17/the-mid-week-news/</link><pubDate>Wed, 17 Jan 2018 07:30:00 +0000</pubDate> <description> &lt;p&gt;After last weeks bumper edition, we’re back to normal this week… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-nifi/&quot;&gt;Apache NiFi&lt;/a&gt; has hit 1.5&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Teradata have spun off the support and development of Presto (originally developed at Facebook) into a new company called Starburst data - &lt;a href=&quot;https://www.bloorresearch.com/2017/12/teradata-spins-off-starburst/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;It looks like Lucene 7.1 (and therefore &lt;a href=&quot;/technologies/elasticsearch/&quot;&gt;ElasticSearch&lt;/a&gt; and presumably at some point &lt;a href=&quot;/technologies/apache-solr/&quot;&gt;Solr&lt;/a&gt;) can now support Attibute Based Access Control - &lt;a href=&quot;https://www.elastic.co/blog/attribute-based-access-control-with-xpack&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;An update from ZDNet on &lt;a href=&quot;/tech-vendors/mapr/&quot;&gt;MapR&lt;/a&gt; - &lt;a href=&quot;http://www.zdnet.com/article/mapr-midcourse-correction-puts-original-ceo-back-in-the-drivers-seat/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2018/01/17/the-mid-week-news/</guid> </item> <item><title>The Mid Week News - 10/01/2018</title><link>http://ondataengineering.net/blog/2018/01/10/the-mid-week-news/</link><pubDate>Wed, 10 Jan 2018 07:30:00 +0000</pubDate> <description> &lt;p&gt;It’s the first news back after the Christmas break, so brace yourself - it’s a massive bumper jam packed edition… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The big one this week is &lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Apache Hadoop&lt;/a&gt; 3.0 - there’s links to the release note on our &lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Hadoop&lt;/a&gt; page and some links below to some commentry&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/elasticsearch/&quot;&gt;Elasticsearch&lt;/a&gt; has hit 6.1, along with &lt;a href=&quot;/technologies/elastic-x-pack/&quot;&gt;X-Pack&lt;/a&gt; and &lt;a href=&quot;/technologies/elasticsearch-hadoop/&quot;&gt;Elasticsearch Hadoop&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-solr/&quot;&gt;Apache Solr&lt;/a&gt; 7.2 is out&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-hbase/&quot;&gt;Apache HBase&lt;/a&gt; 1.4 is out&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-drill/&quot;&gt;Apache Drill&lt;/a&gt; is up to 1.12 - Kafka support is interesting&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-knox/&quot;&gt;Apache Knox&lt;/a&gt; has hit 0.14&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-arrow/&quot;&gt;Apache Arrow&lt;/a&gt; has hit 0.8&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/pravega/&quot;&gt;Pravega&lt;/a&gt; - the Kafka challenger - has hit 0.2&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-nifi/minifi&quot;&gt;MiNiFi&lt;/a&gt; has seen 0.3 releases of it’s Java version&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudbreak/&quot;&gt;Cloudbreak&lt;/a&gt; has seen it’s second 2.x technology preview release - 2.2&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Both ZDNet and Datanami have posts on &lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Hadoop&lt;/a&gt; 3.0 and what the roadmap past this looks like - &lt;a href=&quot;http://www.zdnet.com/article/hadoop-3-confronts-the-realities-of-storage-growth/&quot;&gt;ZDNet&lt;/a&gt;; &lt;a href=&quot;https://www.datanami.com/2017/12/15/hadoop-3-0-ships-roadmap-reveal/&quot;&gt;Datanami&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Blog posts have appeared for &lt;a href=&quot;/technologies/apache-kudu/&quot;&gt;Kudu&lt;/a&gt; 1.6 and &lt;a href=&quot;/technologies/greenplum/&quot;&gt;Greenplum&lt;/a&gt; 5.3 that have been added to their technology pages. Greenplum is looking to move to a fully containerised deployment model - which is interesting.&lt;/li&gt; &lt;li&gt;Azure HDInsight has seen a big price reduction and a bunch of new announcements - &lt;a href=&quot;https://azure.microsoft.com/en-gb/blog/azure-hdinsight-announcements-significant-price-reduction-and-amazing-new-capabilities/&quot;&gt;link&lt;/a&gt;; &lt;a href=&quot;http://www.zdnet.com/article/microsofts-cloud-big-data-service-cuts-prices-up-to-52/&quot;&gt;ZDNet commentary&lt;/a&gt;&lt;/li&gt; &lt;li&gt;An excellent article from Ehud Kaldor and SwiftStack on the differences between NFS and &lt;a href=&quot;/tech-categories/object-stores/&quot;&gt;Object Storage&lt;/a&gt; - &lt;a href=&quot;https://www.swiftstack.com/blog/2018/01/04/nasing-object-storage/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Hortonworks have published a set of pre-canned streaming analytics projects using &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;HDP&lt;/a&gt; and &lt;a href=&quot;/technologies/hortonworks-data-flow/&quot;&gt;HDF&lt;/a&gt;, including Ad Serving, Clickstream Analysis and Predictive Maintenance - &lt;a href=&quot;https://hortonworks.com/blog/applying-big-data-streaming-analytics-in-real-world/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A couple of old Databricks announcements we didn’t cover at the time for some reason &lt;ul&gt; &lt;li&gt;Databricks Unified Analytics Platform - Databricks runtime + interactive collaborative notebooks and dashboards + production job / notebook scheduling + enterprise security - &lt;a href=&quot;https://databricks.com/product/unified-analytics-platform&quot;&gt;homepage&lt;/a&gt;; &lt;a href=&quot;https://databricks.com/blog/2017/10/05/build-complex-data-pipelines-with-unified-analytics-platform.html&quot;&gt;blog&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Databricks Delta - a service over cloud blog stores like S3 that adds ACID transactions and support for automatic data indexing - &lt;a href=&quot;https://databricks.com/product/databricks-delta&quot;&gt;homepage&lt;/a&gt;; &lt;a href=&quot;https://databricks.com/blog/2017/10/25/databricks-delta-a-unified-management-system-for-real-time-big-data.html&quot;&gt;blog&lt;/a&gt;&lt;/li&gt; &lt;li&gt;And some thoughts from ZDNet - &lt;a href=&quot;http://www.zdnet.com/article/the-future-of-the-future-spark-big-data-insights-streaming-and-deep-learning-in-the-cloud/&quot;&gt;Spark in the cloud&lt;/a&gt;; &lt;a href=&quot;http://www.zdnet.com/article/databricks-is-no-longer-playing-david-and-goliath/&quot;&gt;Databricks strategy&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Merv Adrian’s latest Hadoop tracker is up detailing the component versions used by the major &lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Hadoop&lt;/a&gt; vendors - &lt;a href=&quot;https://blogs.gartner.com/merv-adrian/2018/01/03/january-2018-hadoop-tracker/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;If you’ve got some time for reading, AtScale have a list of their top 10 posts and articles from 2017 - &lt;a href=&quot;http://blog.atscale.com/what-youve-missed-in-2017&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From ZDNet, their thoughts on big data in 2018 and the move to the cloud - &lt;a href=&quot;http://www.zdnet.com/article/big-data-2018-cloud-storage-becomes-the-de-facto-data-lake/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;The excellent db-engines site have announced their database of the year - &lt;a href=&quot;https://db-engines.com/en/blog_post/76&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;DZone have published a Refcard for &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Kafka&lt;/a&gt; covering a whole pile of useful getting started information - &lt;a href=&quot;https://dzone.com/refcardz/apache-kafka&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A good write up of the features in &lt;a href=&quot;/technologies/elasticsearch/&quot;&gt;Elasticsearch&lt;/a&gt; 6.0 from Logz.io - &lt;a href=&quot;https://logz.io/blog/elastic-stack-6-new/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Are you running &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Kafka&lt;/a&gt; - we have a couple of posts this week from NewRelic and Confluent on monitoring it - &lt;a href=&quot;https://blog.newrelic.com/2017/12/12/new-relic-kafkapocalypse/&quot;&gt;NewRelic&lt;/a&gt;; &lt;a href=&quot;https://www.confluent.io/blog/blog-post-on-monitoring-an-apache-kafka-deployment-to-end-most-blog-posts&quot;&gt;Confluent&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/microsoft-azure-blob-storage/&quot;&gt;Azure Blob Storage&lt;/a&gt; now supports an archive level tier - &lt;a href=&quot;https://azure.microsoft.com/en-au/blog/cloud-storage-now-more-affordable-announcing-general-availability-of-azure-archive-storage/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A deep drive into the &lt;a href=&quot;/technologies/apache-hadoop/yarn/&quot;&gt;YARN&lt;/a&gt; capacity scheduler from Hortonworks - &lt;a href=&quot;https://hortonworks.com/blog/yarn-capacity-scheduler/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Apache &lt;a href=&quot;/technologies/apache-flink/&quot;&gt;Flink&lt;/a&gt; - 2017 in review and plans for 2018 - &lt;a href=&quot;http://flink.apache.org/news/2017/12/21/2017-year-in-review.html&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;dataArtisans have responded to the Databricks Spark Streaming vs Flink benchmark - &lt;a href=&quot;https://data-artisans.com/blog/curious-case-broken-benchmark-revisiting-apache-flink-vs-databricks-runtime&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Apache Mnemonic and Trafodion have graduated from the Apache Incubator - &lt;a href=&quot;https://blogs.apache.org/foundation/entry/the-apache-software-foundation-announces25&quot;&gt;link&lt;/a&gt;; &lt;a href=&quot;http://incubator.apache.org/projects/trafodion&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;The &lt;a href=&quot;/technologies/apache-nifi/&quot;&gt;Apache Nifi&lt;/a&gt; project has released the first (0.1) version of the NiFi registry for the configuration management of flows - &lt;a href=&quot;https://nifi.apache.org/registry.html&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A write-up from ZDNet on &lt;a href=&quot;/technologies/streamsets-data-collector/&quot;&gt;Streamsets&lt;/a&gt; - &lt;a href=&quot;http://www.zdnet.com/article/streamsets-updates-etl-to-the-cloud-data-pipeline/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;It’s an old article, but still interesting - ZDNet looked at &lt;a href=&quot;/tech-categories/graph-databases/&quot;&gt;graph&lt;/a&gt; vs &lt;a href=&quot;/tech-categories/rdf-databases/&quot;&gt;rdf&lt;/a&gt; databases - &lt;a href=&quot;http://www.zdnet.com/article/graph-databases-and-rdf-its-a-family-affair/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;By comparison this is ancient (from 2015), but looks like a really good intro the the &lt;a href=&quot;/technologies/apache-hbase/&quot;&gt;HBase&lt;/a&gt; architecture from MapR - &lt;a href=&quot;https://mapr.com/blog/in-depth-look-hbase-architecture/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;At the risk of this becoming a ZDNet fest - their views on big data in 2017 and 2018 - &lt;a href=&quot;http://www.zdnet.com/article/big-data-crystal-balls-and-looking-glasses-reviewing-2017-predicting-2018/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;An update from the &lt;a href=&quot;/technologies/pravega/&quot;&gt;Pravega&lt;/a&gt; blog on their architecture and design principles - &lt;a href=&quot;http://blog.pravega.io/2017/12/14/i-have-a-stream/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;For the deeply technical - how to build a distributed log (&lt;a href=&quot;/tech-categories/streaming-data-stores/&quot;&gt;streaming data store&lt;/a&gt;) - &lt;a href=&quot;https://bravenewgeek.com/building-a-distributed-log-from-scratch-part-1-storage-mechanics/?0&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;And last but not least, from Sonra - dimensional modelling on Hadoop - &lt;a href=&quot;https://sonra.io/2017/05/15/dimensional-modeling-and-kimball-data-marts-in-the-age-of-big-data-and-hadoop/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2018/01/10/the-mid-week-news/</guid> </item> <item><title>The Plan For This Week - 08/01/2018</title><link>http://ondataengineering.net/blog/2018/01/08/the-plan-for-this-week/</link><pubDate>Mon, 08 Jan 2018 07:30:00 +0000</pubDate> <description> &lt;p&gt;Welcome back - I hope you’ve had some time off and a relaxing break, I certainly have.&lt;/p&gt; &lt;p&gt;It’s time to get back into it, however before we push on I’m going to take this week just to catch up with a bunch of minor updates to existing technology and technology category pages based on stuff that’s happened over the last few months that I’ve not had time to fold into the site.&lt;/p&gt; &lt;p&gt;Expend a bumper news update on Wednesday, and a post on Friday with some details of the changes.&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2018/01/08/the-plan-for-this-week/</guid> </item> <item><title>The Week That Was</title><link>http://ondataengineering.net/blog/2017/12/15/the-week-that-was/</link><pubDate>Fri, 15 Dec 2017 07:30:00 +0000</pubDate> <description> &lt;p&gt;So I’ve been looking at self service data preparation tools this week, and it’s fair to say that once again the topic at hand has turned out to be far much more that I expected… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Last week we looked at data ingest - getting data to your analytical platform from which point you could then conform, standardise, integrate and otherwise prepare it for analytics. This is difficult - the complexity of this preparation, the variety and volume of input data, the potentially widely varying levels of data quality, and the range of analytics you might want to do make this extreemly challenging - just look at every failed or massively overrun BI or analytics project.&lt;/p&gt; &lt;p&gt;This is going to be a really important area for us to look at one this site - perhaps the most important one. And these week I’ve stumbled into it by accident before I was ready.&lt;/p&gt; &lt;p&gt;Self service data preparation tools - I’ve seen some of these before I thought - they’re basically tools that allow you to do basic ingestion and transformation of ad-hoc data sources, often targeting power users or analysts rather than data engineers.&lt;/p&gt; &lt;p&gt;And that may have been true a few years ago, but it’s clear this is an area that’s seen massive change over the last few years, to the point where there are now a huge range of tools covering a range of capabilities including data cataloging (crawling your data sources and constructing models of how it all fits together, often supported by machine learning), data profiling, test data management, data preparation (targeting both analysts and power users with user friendly and powerful graphical user interfaces and data engineers via extensions to existing and established data integration and transformation tools) as well as all the follow up stuff including workflow management, data quality management, metadata management and data governance.&lt;/p&gt; &lt;p&gt;And these tools all cover different capabilities - although there are some stand alone tools there’s a huge range that cover multiple capabilities, from traditional data integration tools that have added new functionality, data lake management tools, analytics tools that including data ingest/preparation functionality, semantic web technologies, data warehouse automation tools, and all in one end to end analytical tools.&lt;/p&gt; &lt;p&gt;So it’s going to take me a while to get to the bottom of these, and I feel like I have a lot of reading to do.&lt;/p&gt; &lt;p&gt;For now, we’re going to take a three week break for Christmas, back on the 8th of January. When we come back we might take a quick look at streaming analytics, maybe a bit of a review of commercial analyst reporting, and then we’ll dive headlong into this.&lt;/p&gt; &lt;p&gt;Have a good holiday everyone, and we’ll see you soon…&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/12/15/the-week-that-was/</guid> </item> <item><title>The Mid Week News - 13/12/2017</title><link>http://ondataengineering.net/blog/2017/12/13/the-mid-week-news/</link><pubDate>Wed, 13 Dec 2017 07:30:00 +0000</pubDate> <description> &lt;p&gt;It’s time for your weekly dose of the news, but don’t worry, it’s a fairly light one this week… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-calcite/&quot;&gt;Apache Calcite&lt;/a&gt; has hit 1.5&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-flink/&quot;&gt;Apache Flink&lt;/a&gt; has hit 1.4&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-kudu/&quot;&gt;Apache Kudu&lt;/a&gt; has hit 1.6&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/greenplum/&quot;&gt;Greenplum&lt;/a&gt; has hit 5.3&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;From Cloudera, a deep dive into &lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Hadoop&lt;/a&gt; security and delegation tokens - &lt;a href=&quot;http://blog.cloudera.com/blog/2017/12/hadoop-delegation-tokens-explained/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From The Register, thoughts on &lt;a href=&quot;/tech-categories/object-stores/&quot;&gt;object storage&lt;/a&gt; and the increasing challenge of scale out file systems - &lt;a href=&quot;https://www.theregister.co.uk/2017/12/11/the_failure_of_object_storage/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Thoughts on choosing an &lt;a href=&quot;/tech-categories/hadoop-distributions/&quot;&gt;Hadoop Distribution&lt;/a&gt; from Kognitio - &lt;a href=&quot;https://kognitio.com/blog/which-hadoop-distribution-is-right-for-you/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From the SmartCat blog (via DZone), problems with &lt;a href=&quot;/technologies/apache-kafka/kafka-streams/&quot;&gt;Kafka Stream&lt;/a&gt; - &lt;a href=&quot;https://www.smartcat.io/blog/2017/problem-with-kafka-streams&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From InfoQ - thoughts on Amazon Nepture, their new cloud graph database - &lt;a href=&quot;https://www.infoq.com/news/2017/12/amazon-neptune?utm_campaign=infoq_content&amp;amp;utm_source=infoq&amp;amp;utm_medium=feed&amp;amp;utm_term=Database&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/12/13/the-mid-week-news/</guid> </item> <item><title>The Plan For This Week - 11/12/2017</title><link>http://ondataengineering.net/blog/2017/12/11/the-plan-for-this-week/</link><pubDate>Mon, 11 Dec 2017 07:30:00 +0000</pubDate> <description> &lt;p&gt;So we looked at data ingestion tools last week, however there was once type that we didn’t include that I want to look at this week, and that’s data wrangling tools - tools designed for the one off manual ingestion of data, focusing on quickly and easily cleaning and standardising data from whatever form it’s in into a standard form for onwards processing.&lt;/p&gt; &lt;p&gt;See you at the end of the week…&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/12/11/the-plan-for-this-week/</guid> </item> <item><title>Thoughts on Data Ingestion Technologies</title><link>http://ondataengineering.net/blog/2017/12/08/thoughts-on-data-ingestion-technologies/</link><pubDate>Fri, 08 Dec 2017 07:45:00 +0000</pubDate> <description> &lt;p&gt;So we’ve been looking at &lt;a href=&quot;/tech-categories/data-ingestion/&quot;&gt;data ingestion&lt;/a&gt; technologies - let’s talk about it… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Before you can exploit data, you need to get it into a position where you can exploit it. In some limited cases that might be where it currently sites, but in most cases that means moving it to somewhere you can run more expansive and expensive analytics.&lt;/p&gt; &lt;p&gt;The challenge here is the potential range of different sources you might want to acquire data from - event logs, relational databases, NoSQL databases, mainframes, application APIs, cloud platforms, file based feeds etc. And although any good data integration or transformation tool will include adapters to read data from a wide range of sources, reliably getting data in is always a challenge, and you’ll quickly end up either building a framework for ingesting data or with huge amounts of copied and pasted code.&lt;/p&gt; &lt;p&gt;So what are these challenges? Let’s brain dump:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supporting both batch and continuous ingestion of data&lt;/li&gt; &lt;li&gt;If you’re pulling data, working out what data needs acquiring (that hasn’t been acquired before), and if it’s being pushed to you, making sure that you’re not missing anything&lt;/li&gt; &lt;li&gt;Support for acquiring full snapshots, new data and CDC feeds (containing new, changed and deleted records)&lt;/li&gt; &lt;li&gt;Capturing data continuously for systems that aren’t designed for it (mainframes, databases, applications)&lt;/li&gt; &lt;li&gt;Distributed collection - do you need to run agents on remote machines to capture data, and maybe pre-do some processing nearer the edge before forwarding it on&lt;/li&gt; &lt;li&gt;For file based feeds, file based checks, including headers and footers, completeness and format&lt;/li&gt; &lt;li&gt;Resilience and reliability - if you’re receiving a million records a minute what happens when you’re down&lt;/li&gt; &lt;li&gt;Tracking what data you’ve received and when you received it&lt;/li&gt; &lt;li&gt;Checking that there are things changing in your source that’s going to impact your analytics - has it introduced new fields, new values for fields you’re not expecting, changed data types; do the fields still contain what they contained when you originally wrote your analytics?&lt;/li&gt; &lt;li&gt;Tracking metrics of the data you’re receiving - do you have stats (and graphs) or how much data you’re receiving, what the profile of the data looks like so you can spot trends and deviations?&lt;/li&gt; &lt;li&gt;Acquiring new data without spending weeks writing, debugging and testing lots of code&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;TLDR - it’s easy to acquire data, it’s really difficult to do this in a reliable and robust way that doesn’t include huge amounts of development and maintenance.&lt;/p&gt; &lt;p&gt;And this is where specialist data ingestion tools come in. They don’t do anything a general purpose data transformation / ingestion tool can’t do - they just come with a huge amount of the functionality you need for ingesting data out of the box that you’d otherwise have to build yourself, and focusing on making this as simple, quick and robust as possible.&lt;/p&gt; &lt;p&gt;If I need to spell this out any more clearly - if you’re not using a data ingestion tool to capture your data, then it’ll be well worth your time looking at one.&lt;/p&gt; &lt;p&gt;And there’s a really interesting range to choose from now. If you’re looking for a generally purpose tool, there’s a range of open source and commercial offers available, from &lt;a href=&quot;/technologies/apache-nifi/&quot;&gt;Apache Nifi&lt;/a&gt; which we’ve looked at both, to &lt;a href=&quot;/technologies/streamsets-data-collector/&quot;&gt;StreamSets Data Collector&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-gobblin/&quot;&gt;Apache Gobblin (Incubating)&lt;/a&gt; that we looked at this week.&lt;/p&gt; &lt;p&gt;And then there’s a range of capabilities that have different focuses, from continuous event ingestion (or log shipping), to database unloads and continuous ingestion from databases.&lt;/p&gt; &lt;p&gt;And please remember - this is about the aquisition of data and not transformation or aggregation for analytics. You’re trying to get it to your analytics platform in a state where it’s ready for onward processing, but without doing so much work that the chance of a failure means you’ve got lots of data lying on the floor. We’ll look at tools for doing the analytics soon…&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/12/08/thoughts-on-data-ingestion-technologies/</guid> </item> <item><title>Data Ingestion</title><link>http://ondataengineering.net/tech-categories/data-ingestion/</link><pubDate>Fri, 08 Dec 2017 07:30:00 +0000</pubDate> <description> &lt;p&gt;Our list of and information on commercial, open source and cloud based data ingestion tools, including NiFi, StreamSets, Gobblin, Logstash, Flume, FluentD, Sqoop, GoldenGate and alternatives to these.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Category Definition&lt;/h2&gt; &lt;!-- Tech Vendor metadata --&gt; &lt;p&gt;Specialist tools designed to acquire and ingest data into an analytical platform ready for analysis or for further transformation to support analysis. And although more general purpose data integration/transformation tools can fulfil this function, specialist data ingestion tools provide capabilities designed to make this faster and more reliable. Key features include support for remote agents to acquire and forward data, GUIs for configuring ingestion pipelines, support for data quality checks to monitor and/or reject incoming data, and basic file and record level transformations on top of the standard functionality to acquire data from a wide range of sources out of the box. &lt;!--more--&gt;&lt;/p&gt; &lt;h2 id=&quot;general-purpose-ingestion-tools&quot;&gt;General Purpose Ingestion Tools&lt;/h2&gt; &lt;p&gt;These tools support both batch and streaming ingestion from a wide range of data sources:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-nifi/&quot;&gt;Apache Nifi&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Open source, with commercial support available from Hortonworks through &lt;a href=&quot;/technologies/hortonworks-data-flow/&quot;&gt;Hortonworks Data Flow&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/streamsets-data-collector/&quot;&gt;StreamSets Data Collector&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Open source, with commercial support available from StreamSets&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-gobblin/&quot;&gt;Apache Gobblin (Incubating)&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Open Source Java framework for managing big data ingestion, including replication, organisation and lifecycle management&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Skool&lt;/td&gt; &lt;td&gt;Open source tool from BT for bring database and file data into Hadoop through generation of Sqoop, Hive, Pig and Oozie code from configuration; open sourced in September 2016 but has seen limited development since - &lt;a href=&quot;https://github.com/BT-OpenSource/Skool&quot;&gt;https://github.com/BT-OpenSource/Skool&lt;/a&gt;; &lt;a href=&quot;https://blog.cloudera.com/blog/2016/09/skool-an-open-source-data-integration-tool-for-apache-hadoop-from-british-telecom/&quot;&gt;https://blog.cloudera.com/blog/2016/09/skool-an-open-source-data-integration-tool-for-apache-hadoop-from-british-telecom/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;streaming-ingestion-tools&quot;&gt;Streaming Ingestion Tools&lt;/h2&gt; &lt;p&gt;Tools specialising in the ingestion of log files or events, with support for distributed collection and forwarding of data, sometimes called log shipping tools. There’s a write up of some of the tools available from Sematext: &lt;a href=&quot;https://sematext.com/blog/logstash-alternatives/&quot;&gt;https://sematext.com/blog/logstash-alternatives/&lt;/a&gt;&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Logstash&lt;/td&gt; &lt;td&gt;Heavily integrated with ElasticSearch but also supports a number of other targets; open source with commercial support from Elastic as part of their ELK stack - &lt;a href=&quot;https://www.elastic.co/products/logstash&quot;&gt;https://www.elastic.co/products/logstash&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Beats&lt;/td&gt; &lt;td&gt;Lightweight technology written in Go to forward events to Logstash; open source with commercial support from Elastic as part of their ELK stack - &lt;a href=&quot;https://www.elastic.co/products/beats&quot;&gt;https://www.elastic.co/products/beats&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-flume/&quot;&gt;Apache Flume&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Runs on Hadoop and supports the continuous ingestion of data using a set of independent agents connected together into pipelines&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Fluentd&lt;/td&gt; &lt;td&gt;Ruby based tool, part of the Cloud Native Computing Foundation; open source, with commercial support available from TreasureData - &lt;a href=&quot;http://www.fluentd.org/&quot;&gt;http://www.fluentd.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Logagent-js&lt;/td&gt; &lt;td&gt;JavaScript based tool; open source, with commercial support available from Sematext - &lt;a href=&quot;https://github.com/sematext/logagent-js&quot;&gt;https://github.com/sematext/logagent-js&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;rsyslog&lt;/td&gt; &lt;td&gt;Focused on log processing, with lineage back to UNIX syslogd; written in C; open source, with commercial support available from Adiscon - &lt;a href=&quot;http://www.rsyslog.com/&quot;&gt;http://www.rsyslog.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Syslog-ng&lt;/td&gt; &lt;td&gt;Focused on log processing, with lineage back to UNIX syslogd; written in C; open source, with commercial support available from BalaBit - &lt;a href=&quot;https://syslog-ng.org/&quot;&gt;https://syslog-ng.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gollum&lt;/td&gt; &lt;td&gt;Open source project from Trivago; written in Go, quiet, but with new releases still being produced - &lt;a href=&quot;https://github.com/trivago/gollum/&quot;&gt;https://github.com/trivago/gollum/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LogZoom&lt;/td&gt; &lt;td&gt;Open source tool from PacketZoom for processing data from processing data from Beats, written in Go, however inactive since November 2016 - &lt;a href=&quot;https://github.com/packetzoom/logzoom&quot;&gt;https://github.com/packetzoom/logzoom&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Heka&lt;/td&gt; &lt;td&gt;Open source tool from Mozilla, however inactive since August 2016 - &lt;a href=&quot;https://github.com/mozilla-services/heka&quot;&gt;https://github.com/mozilla-services/heka&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Suro&lt;/td&gt; &lt;td&gt;Open source tool from Netflix, however inactive since December 2015 - &lt;a href=&quot;https://github.com/Netflix/suro&quot;&gt;https://github.com/Netflix/suro&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Scribe&lt;/td&gt; &lt;td&gt;Open source tool from Facebook, however inactive since May 2014 - &lt;a href=&quot;https://github.com/facebookarchive/scribe&quot;&gt;https://github.com/facebookarchive/scribe&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;database-unload-tools&quot;&gt;Database Unload Tools&lt;/h2&gt; &lt;p&gt;The following are specialist tools for unloading data form databases. Most data transformation tools and processing tools will also be able to unload data from databases, and are therefore an alternative to using a specialist tool:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-sqoop/&quot;&gt;Apache Sqoop&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Specialist technology for moving bulk data between Hadoop and structured (relational) databases.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;database-change-capture-tools&quot;&gt;Database Change Capture Tools&lt;/h2&gt; &lt;p&gt;The following technologies support the continuous capture and ingestion of record change events from databases, and are sometimes known as change data capture tools:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Oracle GoldenGate for Big Data 12c&lt;/td&gt; &lt;td&gt;Commercial product for the continuous replication of data from a wide range of relational databases into a wide range of “Big Data” targets - &lt;a href=&quot;https://www.oracle.com/middleware/data-integration/goldengate/big-data/index.html&quot;&gt;https://www.oracle.com/middleware/data-integration/goldengate/big-data/index.html&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;IBM Infosphere Data Replication&lt;/td&gt; &lt;td&gt;Continuous replication from relational databases, including IBM systems on mainframes to a range of systems including kafka and Hadoop - &lt;a href=&quot;https://www.ibm.com/us-en/marketplace/infosphere-data-replication&quot;&gt;https://www.ibm.com/us-en/marketplace/infosphere-data-replication&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;SyncSort DMX Change Data Capture&lt;/td&gt; &lt;td&gt;Tool for continually capturing data from mainframe databases - &lt;a href=&quot;https://www.syncsort.com/en/Products/BigData/DMX-Change-Data-Capture&quot;&gt;https://www.syncsort.com/en/Products/BigData/DMX-Change-Data-Capture&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Quest Shareplex&lt;/td&gt; &lt;td&gt;Commercial product for the continuous replication of data from Oracle or SQL Server to a range of targets including Kafka, Hadoop and flat files; previously known as Dell Shareplex, SharePlex for Oracle and Quest Data Connector for Oracle and Hadoop - &lt;a href=&quot;https://www.quest.com/products/shareplex/&quot;&gt;https://www.quest.com/products/shareplex/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Attunity Replicate&lt;/td&gt; &lt;td&gt;Commercial technology for the continuous replication of data between a wide variety of sources including Kafka, relational and analytical databases, mainframes, Hadoop and the cloud; with a free limited Express edition - &lt;a href=&quot;https://www.attunity.com/products/replicate/&quot;&gt;https://www.attunity.com/products/replicate/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Continuent Tungsten Replicator&lt;/td&gt; &lt;td&gt;Continuous replication of Oracle, MySQL and Amazon RDS databases to Hadoop, Vertica, RedShift and others, with an open source version available - &lt;a href=&quot;https://www.continuent.com/solutions/#bigdata&quot;&gt;https://www.continuent.com/solutions/#bigdata&lt;/a&gt;; &lt;a href=&quot;https://github.com/continuent/tungsten-replicator&quot;&gt;https://github.com/continuent/tungsten-replicator&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Dbvisit Replicate&lt;/td&gt; &lt;td&gt;Continuous replication of data from Oracle to a number of targets including Hadoop and Kafka - &lt;a href=&quot;http://www.dbvisit.com/products/dbvisit_replicate_real_time_oracle_database_replication/&quot;&gt;http://www.dbvisit.com/products/dbvisit_replicate_real_time_oracle_database_replication/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;streaming-data-store-ingestion&quot;&gt;Streaming Data Store Ingestion&lt;/h2&gt; &lt;p&gt;A number of &lt;a href=&quot;/tech-categories/streaming-data-stores/&quot;&gt;streaming data stores&lt;/a&gt; have integrated tools for the aquisition of data:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-kafka/kafka-connect/&quot;&gt;Kafka Connect&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Framework for building scalable and reliable integrations between Kafka and other technologies, including the ingestion of data, that’s part of the core Apache Kafka technology&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/mapr-streams-clients-and-tools/&quot;&gt;MapR Streams Tools&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Bundles Kafka Connect for &lt;a href=&quot;/technologies/mapr-file-system/mapr-streams&quot;&gt;MapR-ES&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon Kinesis Streams&lt;/td&gt; &lt;td&gt;Includes an Amazon Kinesis Agent for capture and ingestion of data - &lt;a href=&quot;https://aws.amazon.com/kinesis/streams/&quot;&gt;https://aws.amazon.com/kinesis/streams/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;cloud-based-ingestion-tools&quot;&gt;Cloud Based Ingestion Tools&lt;/h2&gt; &lt;p&gt;The following are cloud based ingestion as a service tools, primarily for ingesting data into cloud based analytical platforms:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Amazon Kinesis Firehose&lt;/td&gt; &lt;td&gt;Streaming data movement, with support for basic transformation including routing, splitting and batching - &lt;a href=&quot;https://aws.amazon.com/kinesis/firehose/&quot;&gt;https://aws.amazon.com/kinesis/firehose/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;other-tools&quot;&gt;Other Tools&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Apache Chukwa&lt;/td&gt; &lt;td&gt;Specialist technology for the ingestion of continuous data flows into an Hadoop cluster, and the subsequent management and analysis of the data; donated by Yahoo in 2010 but now largely abandoned - &lt;a href=&quot;https://chukwa.apache.org/&quot;&gt;https://chukwa.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Apache ManifoldCF&lt;/td&gt; &lt;td&gt;Framework for replicating data from content repositories to analytical search technologies - &lt;a href=&quot;http://manifoldcf.apache.org/&quot;&gt;http://manifoldcf.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/tech-categories/data-ingestion/</guid> </item> <item><title>Apache Gobblin (Incubating)</title><link>http://ondataengineering.net/technologies/apache-gobblin/</link><pubDate>Wed, 06 Dec 2017 07:30:00 +0000</pubDate> <description> &lt;p&gt;Java based framework for ingesting data into Hadoop. Ingestion jobs are defined through job configuration files, and are made up of a number of stages - a Source identifies work to be done and generates Work Units which are then processed by Tasks, with Tasks consisting of an Extractor (reads the records to be processed), one or more Converters (a 1:N transformation of records), a Quality Checker (covers both record and file checks), a Fork Operator (allows data to be written to multiple targets) and a Writer (writes out completed records), with the output of a completed task being committed by a Publisher. Gobblin ships with a number of standard components, including support for a range of sources and targets, as well as supporting custom implementations of any stage. Jobs can be run using a number of frameworks, including MapReduce (with all tasks running as mapper only jobs), YARN, and as Java threads within a single JVM, with some modes also supporting an internal scheduler and job management engine. Supports job locks (to ensure multiple instances of the same job don't run at the same time), job history metadata (via a job execution history store that supports a REST API that can be used to monitor jobs), exactly-once processing support (via Publisher commits), failure handling (retrying both within and across jobs), capture and forwarding of execution and data quality metrics, post processing of data (e.g. to remove duplicates or generate aggregations), partitioned writers, job configuration file templates, Hive table registration, high availability, data retention management (automatically deleting old data according to a number of retention rules), and data purging (Gobblin Compliance). Developed at LinkedIn from late 2013, first announced in November 2014 and open sourced shortly afterwards, before being donated to the Apache Foundation in February 2017, and with stated deployments at a number of large organisations.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Gobblin&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Open Source - Active&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;December 2017 - v0.11&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.5&lt;/td&gt; &lt;td&gt;2015-09-28&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://engineering.linkedin.com/big-data/bridging-batch-and-streaming-data-ingestion-gobblin&quot;&gt;Annoucement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.6&lt;/td&gt; &lt;td&gt;2015-12-21&lt;/td&gt; &lt;td&gt;https://github.com/apache/incubator-gobblin/releases/tag/gobblin_0.6.0&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.7&lt;/td&gt; &lt;td&gt;2016-05-18&lt;/td&gt; &lt;td&gt;https://github.com/apache/incubator-gobblin/releases/tag/gobblin_0.7.0&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://engineering.linkedin.com/blog/2016/06/announcing-gobblin-0-7-0--going-beyond-ingestion&quot;&gt;Announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Dataset lifecycle features&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.8&lt;/td&gt; &lt;td&gt;2016-09-03&lt;/td&gt; &lt;td&gt;https://github.com/apache/incubator-gobblin/releases/tag/gobblin_0.8.0&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.9&lt;/td&gt; &lt;td&gt;2016-12-19&lt;/td&gt; &lt;td&gt;https://github.com/apache/incubator-gobblin/releases/tag/gobblin_0.9.0&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.10&lt;/td&gt; &lt;td&gt;2017-05-05&lt;/td&gt; &lt;td&gt;https://github.com/apache/incubator-gobblin/releases/tag/gobblin_0.10.0&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt;First Apache release&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.11&lt;/td&gt; &lt;td&gt;2017-07-20&lt;/td&gt; &lt;td&gt;https://github.com/apache/incubator-gobblin/releases/tag/gobblin_0.11.0&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://gobblin.apache.org/&quot;&gt;http://gobblin.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://gobblin.readthedocs.io/en/latest/l&quot;&gt;http://gobblin.readthedocs.io/en/latest/l&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://engineering.linkedin.com/data-ingestion/gobblin-big-data-ease&quot;&gt;https://engineering.linkedin.com/data-ingestion/gobblin-big-data-ease&lt;/a&gt; - announcement&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.slideshare.net/ShirshankaDas/apache-gobblin-bridging-batch-and-streaming-data-integration-big-data-meetup-2017&quot;&gt;https://www.slideshare.net/ShirshankaDas/apache-gobblin-bridging-batch-and-streaming-data-integration-big-data-meetup-2017&lt;/a&gt; - presentation from May 2017&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/apache/incubator-gobblin/releases&quot;&gt;https://github.com/apache/incubator-gobblin/releases&lt;/a&gt; - release announcements / history&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/technologies/apache-gobblin/</guid> </item> <item><title>The Mid Week News - 06/12/2017</title><link>http://ondataengineering.net/blog/2017/12/06/the-mid-week-news/</link><pubDate>Wed, 06 Dec 2017 07:30:00 +0000</pubDate> <description> &lt;p&gt;Right - time for your weekly updates on new software releases and interesting new information and posts, with a big dump from AWS re:Invent this week… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-beam/&quot;&gt;Apache Beam&lt;/a&gt; has hit 2.2&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/druid/&quot;&gt;Druid&lt;/a&gt; has hit 0.11&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;After the Azure product dump a few weeks ago, it’s Amazon’s turn via AWS re:Invent: &lt;ul&gt; &lt;li&gt;Amazon Neptune - a graph/RDF database as a service with support for TinkerPop Gremlin and RDF SPARQL - &lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2017/11/amazon-neptune-fast-reliable-graph-database-built-for-the-cloud/&quot;&gt;announcement&lt;/a&gt; and &lt;a href=&quot;https://aws.amazon.com/blogs/aws/amazon-neptune-a-fully-managed-graph-database-service/&quot;&gt;blog&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Amazon SageMaker - service for building, training and deploying machine learning at scale - &lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2017/11/introducing-amazon-sagemaker/&quot;&gt;announcement&lt;/a&gt; and &lt;a href=&quot;https://aws.amazon.com/blogs/aws/sagemaker/&quot;&gt;blog&lt;/a&gt;&lt;/li&gt; &lt;li&gt;AWS Fargate - provisioning of containers on AWS without managing servers or clusters - &lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2017/11/introducing-aws-fargate-a-technology-to-run-containers-without-managing-infrastructure/&quot;&gt;announcement&lt;/a&gt; and &lt;a href=&quot;https://aws.amazon.com/blogs/aws/aws-fargate/&quot;&gt;blog&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Elastic Kubernetes Service (EKS) - Kubernetes as a service - &lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2017/11/introducing-amazon-elastic-container-service-for-kubernetes/&quot;&gt;announcement&lt;/a&gt; and &lt;a href=&quot;https://aws.amazon.com/blogs/aws/amazon-elastic-container-service-for-kubernetes/&quot;&gt;blog&lt;/a&gt;&lt;/li&gt; &lt;li&gt;S3 Select and Glacier Select - retrieve subsets of stored objects by running select queries server side - &lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2017/11/amazon-s3-select-is-now-available-in-limited-preview/&quot;&gt;S3 announcement&lt;/a&gt;, &lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2017/11/amazon-glacier-select-makes-big-data-analytics-of-archive-data-possible/&quot;&gt;Glacier announcement&lt;/a&gt; and &lt;a href=&quot;https://aws.amazon.com/blogs/aws/s3-glacier-select/&quot;&gt;blog&lt;/a&gt;&lt;/li&gt; &lt;li&gt;See also summaries from &lt;a href=&quot;https://www.theregister.co.uk/2017/11/29/amazon_aws_kubernetes/&quot;&gt;The Register&lt;/a&gt;, from &lt;a href=&quot;https://www.infoq.com/news/2017/12/aws-reinvent-day-one&quot;&gt;InfoQ&lt;/a&gt;, and the &lt;a href=&quot;https://aws.amazon.com/blogs/aws/category/events/reinvent/&quot;&gt;motherlist of blog posts&lt;/a&gt; relating to re:Invent from Amazon&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;From Cloudera, infrastructure considerations for deploying &lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt; - &lt;a href=&quot;http://blog.cloudera.com/blog/2017/11/deploy-cloudera-edh-clusters-like-a-boss-revamped-part-1/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;MapR have posted their thoughts on &lt;a href=&quot;/technologies/apache-drill/&quot;&gt;Apache Drill&lt;/a&gt; as part of the &lt;a href=&quot;/technologies/mapr-converged-data-platform/&quot;&gt;MapR Converged Data Platform&lt;/a&gt;, and their view of it as “a unified SQL access layer across files, tables and streams”, along (of course) with some new benchmarks - &lt;a href=&quot;https://community.mapr.com/community/products/blog/2017/11/29/apache-drill-version-111-on-mapr-release-overview&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;An interesting post of MariaDB AX, the data warehouse solution from MariaDB that’s built on MariaDB ColumnStore, on bulk and streaming ingestion of data - &lt;a href=&quot;https://mariadb.com/resources/blog/real-time-data-streaming-mariadb-ax&quot;&gt;link&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;AtScale now runs over Amazon RedShift - &lt;a href=&quot;http://blog.atscale.com/atscale_aws_redshift&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Confluent have a new blog post on Confluent Platform 4.0 (&lt;a href=&quot;/technologies/confluent-open-source/&quot;&gt;Confluent Open Source&lt;/a&gt; and &lt;a href=&quot;/technologies/confluent-enterprise/&quot;&gt;Confluent Enterprise&lt;/a&gt;) - &lt;a href=&quot;https://www.confluent.io/blog/introducing-confluent-platform-4-0/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From ZDNet, an interview on &lt;a href=&quot;/technologies/apache-flink/&quot;&gt;Apache Flink&lt;/a&gt; and thoughts on the wider ecosystem - &lt;a href=&quot;http://www.zdnet.com/article/real-time-applications-are-going-places/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Google, another post on the separation of storage and compute with BigQuery - &lt;a href=&quot;https://cloud.google.com/blog/big-data/2017/11/separation-of-storage-and-compute-in-bigquery&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.crail.io/&quot;&gt;Crail&lt;/a&gt; has been accepted to the Apache Incubator - we last saw this in October when it was submitted, so that’s a pretty quick turn around. As a recap, this looks like a high performance distributed and tiered (in memory, flash and disk) storage layer for temporary data that provides memory, storage and network access that bypasses the JVM and OS, and with integration to Spark (as a custom Spark Suffler that improves sort performance by a factor of five) and Hadoop (via an HDFS adaptor).&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/12/06/the-mid-week-news/</guid> </item> <item><title>StreamSets Data Collector</title><link>http://ondataengineering.net/technologies/streamsets-data-collector/</link><pubDate>Tue, 05 Dec 2017 00:00:00 +0000</pubDate> <description> &lt;p&gt;General purpose technology for the movement of data between systems, including the ingestion of batch and streaming data into an analytical platform. Pipelines are configured in a graphical user interface, and consist of a single origin, one or more processor stages and then one or more destinations, with support for a wide range of source/destination technologies and processor transformations. Supports a wide range of data formats, executors (tasks that can be triggered based on events from pipelines, e.g. to send e-mails or run a shell script), handling of erroroneous records, support for CDC CRUD records, previewing of data within the editor UI, real-time reporting and alerting on a range of execution and data quality metrics, and the ability to dynamically handle changes to schemas and the semantic meaning of data. Can run in standalone mode (as a single process, with the option to run single or multi-threaded) or as a Spark Straming or MapReduce job on a cluster. Java based, Open Source under the Apache 2.0 licence, hosted on GitHub, with development led by StreamSets who also provide commercial support and a Dataflow Performance Manager commercial add on for for managing dataflow operations. Started in October 2014, with a v1.0 release in September 2015.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;StreamSets&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;December 2017 - v3.0&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://streamsets.com/product/&quot;&gt;https://streamsets.com/product/&lt;/a&gt; - product homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://streamsets.com/docs/&quot;&gt;https://streamsets.com/docs/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/streamsets/datacollector&quot;&gt;https://github.com/streamsets/datacollector&lt;/a&gt; - source code&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://streamsets.com/blog&quot;&gt;https://streamsets.com/blog&lt;/a&gt; - blog&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/technologies/streamsets-data-collector/</guid> </item> <item><title>The Plan For This Week - 04/12/2017</title><link>http://ondataengineering.net/blog/2017/12/04/the-plan-for-this-week/</link><pubDate>Mon, 04 Dec 2017 07:30:00 +0000</pubDate> <description> &lt;p&gt;Right, time to push on with more technology categories and actually add some meaningful content to this site, and we’re going to focus on data movement, ingestion and transformation tools for a little while.&lt;/p&gt; &lt;p&gt;First up this week, technologies for the continuous ingestion of event data.&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/12/04/the-plan-for-this-week/</guid> </item> <item><title>Technology Categories</title><link>http://ondataengineering.net/blog/2017/12/01/technology-categories/</link><pubDate>Fri, 01 Dec 2017 07:30:00 +0000</pubDate> <description> &lt;p&gt;So this has taken far too long, and some proper updates to this site are long overdue, but let’s summarise what I’ve been looking at for the last three weeks. And yes, I said they’d be new content last week, but in the end the stuff I had (which was old from when I was planning the site) just wasn’t up to scratch, so it’s been binned rather than published. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;So the big update has been some (minor) structural changes. Firstly, the &lt;a href=&quot;/technologies/&quot;&gt;technology&lt;/a&gt;, &lt;a href=&quot;/tech-categories/&quot;&gt;technology category&lt;/a&gt; and &lt;a href=&quot;/tech-vendors/&quot;&gt;technology vendor&lt;/a&gt; index pages are now part of the open source content repository for this site, meaning that they’re editable just like any of the technology, vendor or category pages. And secondly, the technology category pages now have a meta description under the header, with the summary of the technology category now moved into the body of the content.&lt;/p&gt; &lt;p&gt;And with the new index pages has come a completely redesigned &lt;a href=&quot;/tech-categories/&quot;&gt;technology category index page&lt;/a&gt;, with all the technology categories now organised into sections (categories!), with a bit more description and structure. And as part of doing this I’ve taken the opportunity to review the technology categories we have and make some tweaks and adjustments - I’m still not entirely happy, but I’ve spent far too much time thinking about it, and it’s time to move on.&lt;/p&gt; &lt;p&gt;But I have made some updates to the technology categories as follows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I’ve moved/copied a bunch of commercial and open source Hadoop based technologies from the &lt;a href=&quot;/tech-categories/query-engines/&quot;&gt;query engines&lt;/a&gt; page to the &lt;a href=&quot;/tech-categories/analytical-databases/&quot;&gt;analytical databases&lt;/a&gt; page. I’ve debated for far to long what the different between these categories are, but I think I’ve got to somewhere I’m comfortable with, and hopefully the technology category descriptions now reflect this.&lt;/li&gt; &lt;li&gt;I’ve added RecordService and the Hive Metastore to the Data Storage Services section of the &lt;a href=&quot;/tech-categories/data-storage-formats/&quot;&gt;data storage formats&lt;/a&gt; page. I’m thinking about splitting this section out into it’s own category at some point, as the storage of structured data that can then be exploited by multiple tools is an area that requires more consideration.&lt;/li&gt; &lt;li&gt;I’ve moved the analytical graph databases from the &lt;a href=&quot;/tech-categories/graph-analytics/&quot;&gt;graph analytics&lt;/a&gt; page into the &lt;a href=&quot;/tech-categories/analytical-databases/&quot;&gt;analytical databases&lt;/a&gt; page, leaving the graph analytics page to focus on graph analytics run over external data. I’ve also broken out the &lt;a href=&quot;/tech-categories/graph-databases/&quot;&gt;graph databases&lt;/a&gt; that also support analytics into their own section.&lt;/li&gt; &lt;li&gt;The &lt;a href=&quot;/tech-categories/in-memory-databases/&quot;&gt;in memory databases&lt;/a&gt; page is no longer included on the index page, as all the information is now replicated to other pages with the exception of some operational relational database stuff. Once this has a new home, we’ll drop this category.&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/12/01/technology-categories/</guid> </item> <item><title>The Mid Week News - 29/11/2017</title><link>http://ondataengineering.net/blog/2017/11/29/the-mid-week-news/</link><pubDate>Wed, 29 Nov 2017 07:30:00 +0000</pubDate> <description> &lt;p&gt;To tide you over whilst the content I promised never arrives, let’s look at the news, and it feels like a interesting crop this week… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The Confluent platform - &lt;a href=&quot;/technologies/confluent-open-source/&quot;&gt;Confluent Open Source&lt;/a&gt; and &lt;a href=&quot;/technologies/confluent-enterprise/&quot;&gt;Confluent Enterprise&lt;/a&gt; - has hit 4.0 based on the Apache Kafka 1.0 release&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cloudera have announced Altus Analytical DB - their cloud based SQL analytics service. We’ll take a deeper look at this in a few week. &lt;a href=&quot;http://vision.cloudera.com/introducing-cloudera-altus-analytic-db-beta-for-cloud-based-data-warehousing/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zenko Orbit has been announced - a web based portal for managing object storage data across multiple clouds, for example when used behind &lt;a href=&quot;/technologies/zenko/cloudserver/&quot;&gt;Zenko CloudServer&lt;/a&gt; - &lt;a href=&quot;https://www.zenko.io/blog/introducing-zenko-orbit/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Confluent Cloud is now generally available on AWS - &lt;a href=&quot;https://www.confluent.io/blog/confluent-cloud-enterprise-ready-hosted-apache-kafka/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;If you’ve upgraded to Elastic Cloud 1.1 and Kibana’s not available - Elastic have a workaround for you! &lt;a href=&quot;https://www.elastic.co/blog/elastic-cloud-enterprise-1-1-0-upgrade-issues&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;This is pretty neat - from “the morning paper”, a in-memory key value store implemented directly on NIC FPGAs that can do 1.22 billion KV operations per second when running on 10 NICs - &lt;a href=&quot;https://blog.acolyer.org/2017/11/23/kv-direct-high-performance-in-memory-key-value-store-with-programmable-nic/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Confluent, an update on transactions in &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt; - &lt;a href=&quot;https://www.confluent.io/blog/transactions-apache-kafka/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;An update on the upcoming features in &lt;a href=&quot;/technologies/apache-flink/&quot;&gt;Apache Flink&lt;/a&gt; 1.4 and 1.5 - &lt;a href=&quot;http://flink.apache.org/news/2017/11/22/release-1.4-and-1.5-timeline.html&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-impala/&quot;&gt;Apache Impala&lt;/a&gt; has graduated from the Apache Incubator - &lt;a href=&quot;https://blogs.apache.org/foundation/entry/the-apache-software-foundation-announces24&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/11/29/the-mid-week-news/</guid> </item> <item><title>The Plan For This Week - 27/11/2017</title><link>http://ondataengineering.net/blog/2017/11/27/the-plan-for-this-week/</link><pubDate>Mon, 27 Nov 2017 07:30:00 +0000</pubDate> <description> &lt;p&gt;So having (pretty much) finished my extended and rather more painful tidy up of what we’ve done so far, I’ve got a bunch of content to put live this week, before we move on to a brand new technology category next week. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;For the record, the &lt;a href=&quot;/technologies/&quot;&gt;technology&lt;/a&gt;, &lt;a href=&quot;/tech-vendors/&quot;&gt;technology vendor&lt;/a&gt; and &lt;a href=&quot;/tech-categories/&quot;&gt;technology category&lt;/a&gt; index pages are now part of the content site, meaning that these are now editable as per any other content page. They’ll be a bunch of changes to these going live this week.&lt;/p&gt; &lt;p&gt;I’ve also slightly changed the structure of technology category pages (and will do a similar thing for technology and technology vendor pages in due course). The description of these pages is now a meta description of the page rather than a summary of the technology category, with this moved down to be the initial text in the content.&lt;/p&gt; &lt;p&gt;Oh, add the draft banner has now moved to the left hand sidebar.&lt;/p&gt; &lt;p&gt;The content this week is coming from me digging through my old notes I made when planning the site, which includes a few technology summaries I did whilst planning what I wanted to do. We’ll stick these live this week, along with a bunch of updates to the technology category pages, and a new re-designed technology category index page.&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/11/27/the-plan-for-this-week/</guid> </item> <item><title>The Mid Week News - 22/11/2017</title><link>http://ondataengineering.net/blog/2017/11/22/the-mid-week-news/</link><pubDate>Wed, 22 Nov 2017 07:30:00 +0000</pubDate> <description> &lt;p&gt;It’s Wednesday, which means it’s time for the news, and there’s big MapR and Microsoft Azure announcements this week… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Hadoop&lt;/a&gt; version 2.9 is out&lt;/li&gt; &lt;li&gt;The &lt;a href=&quot;/technologies/apache-nifi/minifi/&quot;&gt;MiNiFi&lt;/a&gt; C++ version has seen a 0.3 release&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/greenplum/&quot;&gt;Greenplum&lt;/a&gt; continues it’s regular release schedule with a 5.2 release&lt;/li&gt; &lt;li&gt;Hortonworks &lt;a href=&quot;/technologies/cloudbreak/&quot;&gt;Cloudbreak&lt;/a&gt; has a 2.1 technical preview out with all new documentation&lt;/li&gt; &lt;li&gt;The &lt;a href=&quot;/technologies/elasticsearch/&quot;&gt;Elasticsearch&lt;/a&gt; 6.0 details have been updated with a link to their blog post on the removal of mapping types&lt;/li&gt; &lt;li&gt;The &lt;a href=&quot;/technologies/cloudera-data-science-workbench/&quot;&gt;Cloudera Data Science Workbench&lt;/a&gt; 1.2 details have been updated with a link to the latest Cloudera blog post&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The &lt;a href=&quot;/technologies/mapr-converged-data-platform/&quot;&gt;MapR Converged Data Platform&lt;/a&gt; v6.0 is out - we’ll wait for the dust to settle a bit before updating the site - &lt;a href=&quot;https://mapr.com/products/whats-new/6-0/&quot;&gt;what’s new&lt;/a&gt;; &lt;a href=&quot;http://www.zdnet.com/article/mapr-6-0-converges-control-of-data-at-rest-and-in-motion-on-the-same-pane-of-glass/&quot;&gt;ZDNet write-up&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Microsoft’s big Connect event has thrown out a bunch of announcements: &lt;ul&gt; &lt;li&gt;Azure Databricks is a new service jointly developed with Databricks that brings Spark as a service as a first class citizen into Azure - we’ll look more at this in the coming weeks I think - &lt;a href=&quot;https://azure.microsoft.com/en-gb/blog/a-technical-overview-of-azure-databricks/&quot;&gt;link&lt;/a&gt;; &lt;a href=&quot;https://databricks.com/blog/2017/11/15/introducing-azure-databricks.html&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Azure Cosmos DB (their NoSQL database) now supports an Apache Cassandra compatible API to join the existing SQL, Gemlin (Neo4j), MongoDB and Azure Table Store APIs - making it a true multi model database supporting wide column storage, graph, relational, document and key value store use cases - &lt;a href=&quot;https://azure.microsoft.com/en-gb/blog/azure-cosmosdb-microsoft-connect-2017/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Azure Time Series Insights (TSI) - their time series databases has hit general availability - &lt;a href=&quot;https://azure.microsoft.com/en-gb/blog/microsoft-announces-the-general-availability-of-azure-time-series-insights/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Azure Database for MariaDB has been announced in preview - &lt;a href=&quot;https://azure.microsoft.com/en-gb/blog/mariadb-postgresql-and-mysql-more-choices-on-microsoft-azure/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;ZDNet have a good write up - &lt;a href=&quot;http://www.zdnet.com/article/microsoft-gets-data-fabulous-at-nyc-event/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;There’s also a summary from Microsoft themselves - &lt;a href=&quot;https://azure.microsoft.com/en-gb/blog/connect-2017-gettopannouncementslist-cloud-data-ai/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;From DZone - an introduction to Pulsar - &lt;a href=&quot;https://dzone.com/articles/a-developers-introduction-to-the-pulsar-streaming&quot;&gt;link&lt;/a&gt;, the Kafka alternative from Yahoo&lt;/li&gt; &lt;li&gt;A view from Kognitio on why it’s not open source (although it can be used over Hadoop for free), and what their differentiators are - &lt;a href=&quot;https://kognitio.com/blog/why-isnt-kognitio-open-source/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A good write up from the Knoldus blog (which is always good value for money) on the architecture of &lt;a href=&quot;/technologies/apache-storm/&quot;&gt;Apache Storm&lt;/a&gt; - &lt;a href=&quot;https://blog.knoldus.com/2017/11/14/apache-storm-architecture/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Influx have announced IFQL - a new query language for &lt;a href=&quot;/technologies/influxdb/&quot;&gt;InfluxDB&lt;/a&gt; - &lt;a href=&quot;https://www.influxdata.com/blog/announcing-ifql-a-new-query-language-and-engine-for-influxdb/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;An introduction from Cloudera to &lt;a href=&quot;/technologies/cloudera-data-science-workbench/&quot;&gt;Cloudera Data Science Workbench&lt;/a&gt; and &lt;a href=&quot;/technologies/cloudera-altus/&quot;&gt;Cloudera Altus&lt;/a&gt; - &lt;a href=&quot;http://blog.cloudera.com/blog/2017/11/cloud-scale-modeling-with-cloudera-altus/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/11/22/the-mid-week-news/</guid> </item> <item><title>The Plan For This Week - 20/11/2017</title><link>http://ondataengineering.net/blog/2017/11/20/the-plan-for-this-week/</link><pubDate>Mon, 20 Nov 2017 07:30:00 +0000</pubDate> <description> &lt;p&gt;So I said last week was going to be the last catch up week, but (as is becoming a depressingly predictable pattern recently) I’m going to overrun.&lt;/p&gt; &lt;p&gt;However, I’ve been taking the time to do some well needed structural updates to the site - hopefully sorting my SEO problem, and allowing me to publish new custom technology, vendor and category index pages this week.&lt;/p&gt; &lt;p&gt;So, bear with me please. Some final updates this week and then next we’ll start on our final set of technology categories before we move onto Chapter 3.&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/11/20/the-plan-for-this-week/</guid> </item> <item><title>The Mid Week News - 15/11/2017</title><link>http://ondataengineering.net/blog/2017/11/15/the-mid-week-news/</link><pubDate>Wed, 15 Nov 2017 07:30:00 +0000</pubDate> <description> &lt;p&gt;News time again, let’s catch up on what’s new… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/elasticsearch/&quot;&gt;Elasticsearch&lt;/a&gt; has hit the big 6.0, along with &lt;a href=&quot;/technologies/elastic-x-pack/&quot;&gt;X-Pack&lt;/a&gt; and &lt;a href=&quot;/technologies/elasticsearch-hadoop/&quot;&gt;Elasticsearch Hadoop&lt;/a&gt;, with &lt;a href=&quot;/technologies/elastic-cloud/&quot;&gt;Elastic Cloud&lt;/a&gt; up to 1.1&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-apex/&quot;&gt;Apache Apex&lt;/a&gt; has seen a 3.8 release of it’s Malhar library&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-phoenix/&quot;&gt;Apache Phoenix&lt;/a&gt; has seen a 4.13 release&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/influxdb/&quot;&gt;InfluxDB&lt;/a&gt; is up to 1.4&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/zenko/cloudserver/&quot;&gt;Zenko CloudServer&lt;/a&gt; is up to 7.1&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ZDNet have an interview with the CEO of Elastic - &lt;a href=&quot;http://www.zdnet.com/article/elasticsearch-6-0-not-that-new-but-quite-improved/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;IBM Data Science Experience (DSX) is now certified on &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;HDP&lt;/a&gt; - &lt;a href=&quot;https://hortonworks.com/blog/certification-ibm-data-science-experience-dsx-hdp-win-win-customers/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;An interview from ODBMS Industry Watch with Head of Data Technical Field for Pivotal and the founder and CEO Datometry. There’s some significant product plugs in there, but it’s worth a read - &lt;a href=&quot;http://www.odbms.org/blog/2017/11/on-the-future-of-data-warehousing-interview-with-jacque-istok-and-mike-waas/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From DataArtisans - an updated on CEP with &lt;a href=&quot;/technologies/apache-flink/&quot;&gt;Apache Flink&lt;/a&gt; - &lt;a href=&quot;https://data-artisans.com/blog/complex-event-processing-flink-cep-update&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;More from Confluent for using &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt; and KSQL in micro services architectures - &lt;a href=&quot;https://www.confluent.io/blog/building-a-microservices-ecosystem-with-kafka-streams-and-ksql/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/11/15/the-mid-week-news/</guid> </item> <item><title>Thoughts on MapR</title><link>http://ondataengineering.net/blog/2017/11/10/thoughts-on-mapr/</link><pubDate>Fri, 10 Nov 2017 07:30:00 +0000</pubDate> <description> &lt;p&gt;I’ve somehow spent (pretty much) all of these week taking another look at MapR - the time taken being a combination of me getting sucked down some rabbit holes, and the contradictory and confusing nature of MapR’s public material. But I think we’ve got somewhere, so let’s renew what we already thought about MapR and what might have changed recently… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Firstly, let’s review what I said in &lt;a href=&quot;/blog/2017/04/28/the-week-that-was/&quot;&gt;The Week That Was - 28/04/2017&lt;/a&gt;:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;I have to admit to being surprised by MapR’s offerings. I’d always assumed they were a knock-off Hadoop distribution that was trying to find leverage by embedding a bunch of commercial components, however what’s become clear is that what they’re selling is not an Hadoop distribution but an enterprise data platform (based on MapR-FS) that just happens to have Hadoop compatibility. In short, &lt;a href=&quot;/technologies/mapr-file-system&quot;&gt;MapR-FS&lt;/a&gt; is a highly resilient, scalable and performant, with support for full random read/write access, multi-tenancy, block level replication, snapshots, quotas, extensive and flexible access control, which supports a fully POSIX compliant filesystem with HDFS, NFS and FUSE APIs, a document and wide column datastore with OJAI and HBase APIs, a streaming data stores with Kafka compatible APIs, master-slave and master-master replication of database and streaming data stores, plus YARN support, meaning you can run any Hadoop compatible tool over the top. That’s a lot of capability in a single platform, which feels like it’s going to drive a strong TCO story.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;And in &lt;a href=&quot;/blog/2017/05/05/the-week-that-was/&quot;&gt;The Week That Was - 05/05/2017&lt;/a&gt;:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;What I’ve really liked about MapR is their strategy around their common data platform to underpin a bunch of different data storage capabilities. I talked a little bit about their data platform &lt;a href=&quot;/blog/2017/04/28/the-week-that-was/&quot;&gt;last time&lt;/a&gt;, but this week as part of looking at &lt;a href=&quot;/technologies/mapr-file-system/mapr-db/&quot;&gt;MapR-DB&lt;/a&gt; and &lt;a href=&quot;/technologies/mapr-file-system/mapr-streams/&quot;&gt;MapR-Streams&lt;/a&gt; I’ve been thinking about how this compares and contrasts to Hadoop. Firstly, they’re both aiming to provide a common data platform that provides the ability to have a single cluster than can provide flexibility and value for money by allowing you to exploit the same infrastructure for multiple use cases. MapR appears to have fully embraced this, ensuring they support the ability to scale, partition and manage the platform in ways that Hadoop can’t yet, and by providing capabilities that Hadoop (and more specifically HDFS) doesn’t that actually make it work as a general purpose data platform - full random read and write access for starters. I’m also taken by MapR’s ability to provide access to the common data platform at different layers - rather than just build capabilities on top of their file system API, they’ve integrated (for example) MapR-DB at a much lower level, providing a range of benefits over HBase running over HDFS. It’s clear that Hadoop still has a long way to go to fulfil it’s potential, and without addressing some of it’s limitations we’re going to continue to see new technologies opting to implement their own storage systems from scratch (Kudu being a great example), leading to Hadoop clusters running multiple independent storage stacks on the same data nodes, which feels like it’s defeating the point.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Looking back, that feels like it all still holds true. So what’s changed recently?&lt;/p&gt; &lt;p&gt;Well, the answer is fundamentally nothing (and yet I’ve spent almost an entire week looking at this!). In the time since we last looked at MapR there hasn’t been even a minor release (5.2 came out in August 2016) so there’s no major new functionality or product changes - my guess is they’re gearing up for a big 6.0 release given they’ve started to talk about some elements of this.&lt;/p&gt; &lt;p&gt;However, it looks like they’re having a push around widening the use cases for their &lt;a href=&quot;/technologies/mapr-converged-data-platform/&quot;&gt;Converged Data Platform&lt;/a&gt; and the number of ways they bundle and market this. When we last looked this was already also available as MapR Edge (a version designed to run on low power devices at the edge of the network) and MapR Converged Data Platform for Docker (a version designed to provide persistent storage for docker containers), however they’ve now introduce a new version - MapR-XD - a version built around MapR-FS bundled with their monitoring and management software (and one would assume the new Orbit Cloud Suite - details below), that’s being positioned as a cloud scale data store / fabric which will provide a single layer over on-premises and cloud storage, with support for automatic tiering, mirroring and replication.&lt;/p&gt; &lt;p&gt;(NOTE: MapR’s material is inconsistent in whether MapR-XD is a re-branding of MapR-FS or a new product that includes MapR-FS, however I’m going with &lt;a href=&quot;https://community.mapr.com/message/59688-what-is-the-difference-between-mapr-fs-and-mapr-xd&quot;&gt;this&lt;/a&gt; and the fact that MapR-XD has it’s own installation page separate from MapR under &lt;a href=&quot;https://maprdocs.mapr.com/home/install.html&quot;&gt;here&lt;/a&gt; and say that it’s a new product.)&lt;/p&gt; &lt;p&gt;This has then been supplemented via a couple of new add-ons.&lt;/p&gt; &lt;p&gt;Firstly the MapR Orbit Cloud Suite, which adds full cloud support to the MapR Converged Data Platform (and appears to provide a bunch of the functionality of MapR-XD), including support for deployment of cloud infrastructure along with MapR, integration with cloud object stores, plus mirroring and replication, with support for multi-tenancy, object tiering and with OpenStack integration announced.&lt;/p&gt; &lt;p&gt;The second is the MapR Data Science Refinery, a docker based analytics notebook powered by Apache Zeppelin that fully integrates with the MapR Converged Data Platform. MapR have been pushing the use of the Converged Data Platform as a data science and machine learning platform for a while now, and this feels like it supports that.&lt;/p&gt; &lt;p&gt;Oh, and they’ve renamed MapR Streams to MapR-ES (Event Streams), as &lt;a href=&quot;https://community.mapr.com/thread/21827-what-is-mapr-es-event-data-streams&quot;&gt;apparently&lt;/a&gt; people were often wrongly assuming it was a streaming engine like Storm or Flink.&lt;/p&gt; &lt;p&gt;So that’s that. If you haven’t already, I strongly suggest you have a read through the information we have on MapR on this site, starting with our &lt;a href=&quot;/tech-vendors/mapr/&quot;&gt;MapR&lt;/a&gt; vendor page and following the links through to the &lt;a href=&quot;/technologies/mapr-converged-data-platform/&quot;&gt;MapR Converged Data Platform&lt;/a&gt; and onwards to the products it packages and is managed by. It’ll only take you 10 minutes I promise!&lt;/p&gt; &lt;p&gt;We’ll be back next week with our final catch up week before we launch into some new technology categories. See you then.&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/11/10/thoughts-on-mapr/</guid> </item> <item><title>The Mid Week News - 08/11/2017</title><link>http://ondataengineering.net/blog/2017/11/08/the-mid-week-news/</link><pubDate>Wed, 08 Nov 2017 07:30:00 +0000</pubDate> <description> &lt;p&gt;It’s time to take my head out of MapR (updates and post hopefully coming tomorrow if I can push on though) and break for the news, although it’s a bit light this week… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The big one this week is that &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt; has hit 1.0&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-ignite/&quot;&gt;Apache Ignite&lt;/a&gt; has hit 2.3&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-kylin/&quot;&gt;Apache Kylin&lt;/a&gt; has hit 2.2&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Another benchmark to take with a pinch of salt - &lt;a href=&quot;/technologies/mapr-file-system/mapr-db/&quot;&gt;MapR-DB&lt;/a&gt; is faster than both Cassandra and HBase - &lt;a href=&quot;https://mapr.com/whitepapers/mike-leone-esg-lab-nosql-benchmark/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Hortonworks on extensibility in &lt;a href=&quot;/technologies/streaming-analytics-manager/&quot;&gt;Streaming Analytics Manager&lt;/a&gt; - &lt;a href=&quot;https://hortonworks.com/blog/streaming-analytics-manager-extensibility/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From The Register - it looks like the IBM Cloud is undergoing another name change - &lt;a href=&quot;https://www.theregister.co.uk/2017/11/02/ibm_renames_bluemix_ibm_cloud/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/11/08/the-mid-week-news/</guid> </item> <item><title>Thoughts on Hortonworks DataPlane and Cloudera SDX</title><link>http://ondataengineering.net/blog/2017/11/03/thoughts-on-hortonworks-dataplane-and-cloudera-sdx/</link><pubDate>Fri, 03 Nov 2017 07:30:00 +0000</pubDate> <description> &lt;p&gt;So I’ve spent most of this week reviewing and making some minor refreshes to our &lt;a href=&quot;/tech-vendors/hortonworks/&quot;&gt;Hortonworks&lt;/a&gt; and &lt;a href=&quot;/tech-vendors/cloudera/&quot;&gt;Cloudera&lt;/a&gt; information, and taking a further look at &lt;a href=&quot;/technologies/hortonworks-dataplane-service/&quot;&gt;Hortonworks DataPlane Service&lt;/a&gt; and Cloudera SDX. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Firstly, let’s review what I said in &lt;a href=&quot;/blog/2017/09/27/the-mid-week-news/&quot;&gt;The Mid Week News - 27/09/2017&lt;/a&gt;:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;The big news this week is the simultaneous big product announcements from Hortonworks and Cloudera that look like they might be similar capabilities, but I think are probably trying to solve subtly different problems - we’ll revisit these in a few weeks once there’s more information available and do some technology summaries.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.cloudera.com/products/sdx.html&quot;&gt;Cloudera SDX&lt;/a&gt; (Shared Data Experience, coming in CDH 5.13) appears to be trying to enable the “one” data platform experience that you get with an on premesis CDH cluster in the cloud, specifically a persistent shared storage layer with shared metadata, security and governance and a range of workloads on top. That looks different in the cloud - you probably don’t want a persistent Cloudera cluster that you’re paying for by the hour even if you’re not using it - so SDX gives you a shared storage layer using cloud object storage, a shared metadata and management layer, and then the ability to run compute workloads in isolated transient workload clusters managed through Cloudera Altus. The original sales pitch of a single shared Hadoop data platform re-imagined for the cloud. More details via a &lt;a href=&quot;http://vision.cloudera.com/introducing-cloudera-sdx-a-shared-data-experience-for-the-hybrid-cloud/&quot;&gt;Cloudera VISION blog post&lt;/a&gt; and a &lt;a href=&quot;http://blog.cloudera.com/blog/2017/09/cloudera-sdx-under-the-hood/&quot;&gt;Cloudera Engineering blog post&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://hortonworks.com/products/data-management/dataplane-service/&quot;&gt;Hortonworks Data Plane&lt;/a&gt; is again all about shared metadata, security and data management, but this time across a range of different data platforms - Hadoop, relational databases and your EDW, either on-premesis or in the cloud, and for data in motion or at rest. It’s open source, extensible for adding new services, with data lifecycle management being first up, allowing you to replicate, backup &amp;amp; restore and tier your data across your data platforms. It’s another cloud service (because obviously), and they talk about it as a Global Data Management Platform. More details via a &lt;a href=&quot;https://hortonworks.com/blog/category-emerges-introducing-hortonworks-dataplane-service/&quot;&gt;Hortonworks blog post&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;With Cloudera SDX, CDH 5.13 has come and gone, and there’s almost no new information about SDX. The 5.13 &lt;a href=&quot;http://community.cloudera.com/t5/Community-News-Release/ANNOUNCE-Cloudera-Enterprise-5-13-is-Now-Available/m-p/60879#M200&quot;&gt;announcement&lt;/a&gt; name checks SDX as the “SDX Cloud Reference Architecture”, which I think probably sums up what it is as much as anything, especially given there’s absolutely no reference to SDX in the Cloudera documentation, and there’s nothing on their site beyond the product page and two blog posts linked above. It feels like this is Cloudera pushing the traditional Hadoop one platform, lots of different workloads message, but now applying it to the cloud as well.&lt;/p&gt; &lt;p&gt;Hortonworks on the other hand seem to be heading in a slightly different direction with the new &lt;a href=&quot;/technologies/hortonworks-dataplane-service/&quot;&gt;Hortonworks DataPlane Service&lt;/a&gt;. The premise for this is that it becomes a single place to understand, manage and govern all the data your enterprise holds, wherever it may be - it’s a big ask, but it feels like there’s value there. Saying that it’s early days for this product is an understatement - it’s now had it’s first generally available release (see &lt;a href=&quot;https://hortonworks.com/blog/hdp-2-6-3-dataplane-service/&quot;&gt;this post&lt;/a&gt;) and there’s a big pile of documentation on the Hortonworks site, but the functionality at the moment is pretty limited, and there’s no visibility yet of plugin services coming from any of the Hortonworks partners. And this is an interesting change for Hortonworks, in that this is a commercial managed service offering and not open source software (although there’s no public sign up process yet and the documentation talks about how to install it), and it only works with Ambari managed clusters and you have to have a SmartSense ID. Which makes you wonder whether this a response to challenges in generating revenue from support and consultancy from fully open source software. It will also be interesting to see how this will impact Atlas and Ranger - you could easily see a world where a lot of the end user functionality in these products migrates into the DataPlane service. One to watch I think - it’ll be interesting to see where this goes.&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/11/03/thoughts-on-hortonworks-dataplane-and-cloudera-sdx/</guid> </item> <item><title>Hortonworks DataPlane Service</title><link>http://ondataengineering.net/technologies/hortonworks-dataplane-service/</link><pubDate>Thu, 02 Nov 2017 07:30:00 +0000</pubDate> <description> &lt;p&gt;An extensible platform for managing, governing and securing data, with capabilities delivered through plugable services. Includes three core capabiliies - the Data Services Catalog (allows plugable services to be registered and managed), Security Control (manages role based access control to information wihtin the platform and integration with LDAP for users and groups), and Data Source Integration (allows registration of data sources, with support currently limited to Ambari managed Hadoop clusters). Currently supports two services - Data Lifecycle Manager (DLM) (a production ready service for replicating data between clusters, with initial support for Hive tables and HDFS snapshottable directories, but with future plans to support point in time backup and restore and automatic tiering of data) and Data Steward Services (DSS) (a technical preview service for creating data asset collections and for viewing information on data assets, including poperties, tags, schemas, lineage, security, access audit events and statistics, with statistics provided via a background data profiler, and with supported data assets currently limited to Hive tables). Stated plan is for this to be a cloud service, however this is not currently generally available, and the documentation currently details installation steps for a local machine. Has dependancies on Atlas (for Hive metadata), Ranger (for access audit logs) and Spark (for data profile computation). First released in November 2017.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/hortonworks/&quot;&gt;Hortonworks&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;November 2017 - 1.0&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;td&gt;2017-11-01&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://hortonworks.com/blog/hdp-2-6-3-dataplane-service/&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/products/data-management/dataplane-service/&quot;&gt;https://hortonworks.com/products/data-management/dataplane-service/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/DPS1/DPS-1.0.0/index.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/DPS1/DPS-1.0.0/index.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/category-emerges-introducing-hortonworks-dataplane-service/&quot;&gt;https://hortonworks.com/blog/category-emerges-introducing-hortonworks-dataplane-service/&lt;/a&gt; - intro blog post&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;See HDP updates - HDP Search tracks HDP releases&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/technologies/hortonworks-dataplane-service/</guid> </item> <item><title>The Mid Week News - 01/11/2017</title><link>http://ondataengineering.net/blog/2017/11/01/the-mid-week-news/</link><pubDate>Wed, 01 Nov 2017 07:30:00 +0000</pubDate> <description> &lt;p&gt;It’s Wednesday, which means it’s time for the news… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-ambari/&quot;&gt;Ambari&lt;/a&gt; has hit 2.6&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Hadoop&lt;/a&gt; has hit 2.8.2, the first 2.8 GA release for production use&lt;/li&gt; &lt;li&gt;Cloudera have finally published their blog post on &lt;a href=&quot;/technologies/cloudera-data-science-workbench/&quot;&gt;Data Science Workbench&lt;/a&gt; version 1.2 - &lt;a href=&quot;http://blog.cloudera.com/blog/2017/10/new-in-cloudera-data-science-workbench-1-2-usage-monitoring-for-administrators/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hortonworks have announced the general availability of their first plugin for their new DataPlane Service - Data Lifecycle Manager (DLM) - &lt;a href=&quot;https://hortonworks.com/blog/disasters-can-instant-takes-village-build-hybrid-cloud-based-recovery-solution/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Influx have a blog post on the internals of &lt;a href=&quot;/technologies/influxdb/&quot;&gt;InfluxDB&lt;/a&gt; - &lt;a href=&quot;https://www.influxdata.com/blog/influxdb-internals-101-part-one/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Following up on the Linux Foundation open data licence last week, some expert views from The Register - &lt;a href=&quot;https://www.theregister.co.uk/2017/10/25/linux_foundation_data_licence_analysis/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From Amazon - how to build a Data Lake on &lt;a href=&quot;/tech-vendors/amazon-web-services/&quot;&gt;AWS&lt;/a&gt; with AWS Glue and S3 - &lt;a href=&quot;https://aws.amazon.com/blogs/big-data/build-a-data-lake-foundation-with-aws-glue-and-amazon-s3/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Thoughts from O’Reilly (via the Cloudera Vision Blog) on (Hadoop) data marts in the cloud - &lt;a href=&quot;https://www.oreilly.com/ideas/rethinking-data-marts-in-the-cloud&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Following up on our look at analytical databases, an interview from ODBMS Industry Watch with Colin Mahony from Vertica - &lt;a href=&quot;http://www.odbms.org/blog/2017/10/on-vertica-and-the-new-combined-micro-focus-company-interview-with-colin-mahony/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;There’s a new vulnerability in Apache Tika - CVE-2016-6809 - allows Java code execution for serialized objects embedded in MATLAB files - &lt;a href=&quot;http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-6809&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;And one in &lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Apache Hive&lt;/a&gt; - CVE-2017-12625 - issues with column masking over views - &lt;a href=&quot;http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2017-12625&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/11/01/the-mid-week-news/</guid> </item> <item><title>The Plan For This Week - 30/10/2017</title><link>http://ondataengineering.net/blog/2017/10/30/the-plan-for-this-week/</link><pubDate>Mon, 30 Oct 2017 07:30:00 +0000</pubDate> <description> &lt;p&gt;So, I think we’re probably done with looking at data storage and databases, however before we move on to data acquisition, processing and transformation tools, and the interesting set of supporting capabilities that live within a data ecosystem, there’s a bit of housekeeping to do.&lt;/p&gt; &lt;p&gt;So over this week (and maybe next), expect some random updates. I want to update and rework the technology categories home page to provide a bit more structure, and there are a few technologies I’ve been meaning to double back on now there should be some more documentation around beyond a press release (Hortonworks Data Plane, Cloudera SDX, MapR-XD/ES for starters). Plus I need to work out what technology categories might be coming up.&lt;/p&gt; &lt;p&gt;So hang in there - new content coming, but what and when might be a little random for the next few weeks…&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/10/30/the-plan-for-this-week/</guid> </item> <item><title>Thoughts on Query Engines and Analytical Databases</title><link>http://ondataengineering.net/blog/2017/10/27/thoughts-on-query-engines-and-analytical-databases/</link><pubDate>Fri, 27 Oct 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;Right, we’ve finally completed our look at &lt;a href=&quot;/tech-categories/analytical-databases/&quot;&gt;analytical databases&lt;/a&gt;, with a short stop to look at &lt;a href=&quot;/tech-categories/query-engines&quot;&gt;query engines&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;So let’s summarise and spew some thoughts… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Let’s start with &lt;a href=&quot;/tech-categories/analytical-databases/&quot;&gt;analytical databases&lt;/a&gt;, with a bit of noddy pseudo-history tale telling. Once upon a time the relational database was invented, but these initially focused on transactional use creates - creating, finding, updating and deleting records - what’s now sometimes referred to as OLTP (online transaction processing). However SQL was a great and so people started using it to try and generate reports on the data in their relational databases - queries that focused on aggregating and joining significant portions of their data. And that was generally fine, until data volumes increased to the point where databases designed for transactional workloads struggled to handle the loads generated from these reports or analytics - fetching lots of data off disk and aggregating it was expensive.&lt;/p&gt; &lt;p&gt;And so the analytical OLAP (online analytical processing) databases were born - designed to primarily support the large full table scan aggregation queries, while still retaining the core relational database support for transactions. These were marketed as data warehouse or analytical databases, and with them came a bunch of new technologies - parallelism and the invention of the MPP (massively parallel processing) database, as full table scans and aggregations lend themselves well to partitioning and parallelisation (even if joins do not); columnar compression, which enables faster and more efficient table scans of columns from database tables; pre-aggregation of data, through materialized views and pre-generated cubes; and a range of new functionality designed to complement SQL as an analytical tool, such as support for machine learning, geographical analytics, map reduce and custom analytical functions. Today, these are often sold as appliances, bundling clusters of compute and storage servers with huge bandwidth interconnects between them, however many have now also started embracing the cloud, being available as a cloud service but also to a lesser extents as cloud native software. Open source projects in this space however are scarce.&lt;/p&gt; &lt;p&gt;Instead, the the charge of open source software into the space ended up being spearheaded by Hadoop. Whilst analytical databases have long separated storage and compute (with some interesting abilities to push some parts of the query down to the storage layer), it was Hadoop that’s formalised this by separating them into completely separate and interchangeable components. This can be seen from the very first versions of Hadoop, in that it was made up of two products - HDFS and MapReduce - storage and compute, and over time there’s been evolution on both sides. In storage, HDFS has remained a constant, but there’s been huge innovation in the storage formats of data (see our &lt;a href=&quot;/tech-categories/data-storage-formats/&quot;&gt;Data Storage Formats&lt;/a&gt; page). On the compute side, there’s been a veritable explosion of &lt;a href=&quot;/tech-categories/query-engines&quot;&gt;query engines&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;These started out as batch query engines - every query starting up a new job (initial MapReduce) to go and read all the data and execute the query. However there’s been significant push into low latency high concurrency query engines, led by the big Hadoop vendors - Cloudera with &lt;a href=&quot;/technologies/apache-impala/&quot;&gt;Impala&lt;/a&gt; and Hortonworks with &lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Hive&lt;/a&gt; - both trying to make Hadoop a realistic competitor in the analytical database market. Compared to analytical databases Hadoop query engines are new technologies - their query optimisers and SQL compliance generally lag the mature analytical databases, however they’re seeing significant investment, and they’re not at the point where they’ll probably support most of your use cases.&lt;/p&gt; &lt;p&gt;And this split of storage and compute has brought some interesting benefits. This first is support for the whole “schema-on-read” shtick - the idea that you can query your raw data without having to do any preparation first. It’s slow, and painful, but for an initial exploratory analysis it’s a valuable tool. And of course if there’s value in the data, you’re still feel to do some work that makes it quicker, easier and more efficient to query. The second is that these tools have naturally evolved some level of query federation - if I want to exploit raw un prepared data I have to accept that this data may be in multiple places in a range of formats, and if I have a query engine that’s separated from the underlying storage, why not make that storage pluggable, and support multiple storage platforms. And so many of these tools support querying over a range of data sources, from HDFS, to S3, to HBase, to relational and NoSQL database, and (interestingly) some emerging support for Kafka . They don’t have the level of sophistication around semantic layers and caching and materialisation of data the &lt;a href=&quot;/tech-categories/data-virtualization/&quot;&gt;Data Virtualization&lt;/a&gt; technologies do, but it can still be a hugely valuable capability.&lt;/p&gt; &lt;p&gt;And now we’re starting to see commercial vendors get involved, both large established vendors (Teradata, IBM, Oracle) who are updating their products to run over HDFS and external data stores, but also new vendors in this space who’ve seen an opportunity to sell commercial products in this space that offer a level of functionality and maturity that maybe some of the open source products can’t match. If you’re running open source Hadoop, some of these may well be worth a look.&lt;/p&gt; &lt;p&gt;Finally - a couple of footnotes…&lt;/p&gt; &lt;p&gt;Although many of these tools support SQL, there are many that have their own query languages - Pig has Pig Latin, MRQL has it’s own language - and of course you could probably count most of the graph and machine learning projects as query languages, which suggests I’ve probably not named this category particularly well. SQL is always going to be the dominant language in this space however, and anything with it’s own query language is unlikely to make an impact.&lt;/p&gt; &lt;p&gt;It’s also worth commenting on the role of the Hive metadata in the Hadoop query engine ecosystem. If you’re separating compute and storage, you need some way of telling the compute what data’s available for query and what format it’s in. Within Hive, this was the Hive Metastore, and this is now gradually being adopted as the standard in this space, giving some interesting interoperability options, in that if you define a table in the Hive Metastore, you can query that with either Hive or Impala (or any other technology that uses the Metastore). And now there’s a proposal to break the Metastore out of the Hive project into it’s own top level Apache project, to reflect the wider role it has in the Hadoop ecosystem.&lt;/p&gt; &lt;p&gt;Right - that will do, and hopefully brings the protracted birth of our analytical databases content to and end. Have a good weekend everyone, and we’ll see you on Monday for a week or two of random catch up and clean up.&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/10/27/thoughts-on-query-engines-and-analytical-databases/</guid> </item> <item><title>Analytical Databases</title><link>http://ondataengineering.net/tech-categories/analytical-databases/</link><pubDate>Thu, 26 Oct 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;Our list of and information on commercial, open source and cloud based analytical databases, including Teradata, Exadata, Redhift and alternatives to these.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Category Definition&lt;/h2&gt; &lt;!-- Tech Vendor metadata --&gt; &lt;p&gt;Full stack databases (supporting both storage and query of data) that focus on analytical or OLAP use cases that generally involve large scanning or aggregation operations. Typically support distributed parallel execution of queries (and are therefore commonly referred to as MPP databases) with columnar compression, and often support a range of analytics beyond SQL queries, for example cube based MDX queries, machine learning, graph or geographical analytics. Some technologies also support a level of query federation using external tables (for example over data in Hadoop). Some technologies run over Hadoop (exploiting HDFS and YARN), but will either use their own proprietary data format or will be positioned as a self contained analytical database. &lt;!--more--&gt;&lt;/p&gt; &lt;h2 id=&quot;further-information&quot;&gt;Further Information&lt;/h2&gt; &lt;p&gt;Gartner’s &lt;a href=&quot;https://www.gartner.com/doc/3614317&quot;&gt;Magic Quadrant for Data Management Solutions for Analytics - Feb 2017&lt;/a&gt; covers most of the technologies in this list (along with a range of others, including Hadoop vendors), and is available from many of the vendors, including &lt;a href=&quot;https://www.cloudera.com/content/dam/www/marketing/resources/analyst-reports/gartners-magic-quadrant-for-data-warehouse-and-data-management-solutions-for-analytics.png.landing.html&quot;&gt;Cloudera&lt;/a&gt;, &lt;a href=&quot;https://mapr.com/blog/gartner-2016-magic-quadrant-data-warehouse-and-database-management-solutions-analytics/&quot;&gt;MapR&lt;/a&gt; and &lt;a href=&quot;http://blog.memsql.com/gartner-magic-quadrant-analytics/&quot;&gt;MemSQL&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Forrester’s &lt;a href=&quot;https://www.forrester.com/report/The+Forrester+Wave+Big+Data+Warehouse+Q2+2017/-/E-RES136478&quot;&gt;Big Data Warehouse Wave Q2 2017&lt;/a&gt; covers a similar set of technologies, and is available from &lt;a href=&quot;https://hortonworks.com/info/big-data-solution-will-help-make-big-difference/&quot;&gt;Hortonworks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;See our &lt;a href=&quot;/tech-categories/query-engines/&quot;&gt;Query Engines&lt;/a&gt; page for details of technologies that support similar capabilities but over external data (e.g. data in HDFS or source databases).&lt;/p&gt; &lt;h2 id=&quot;commercial-technologies&quot;&gt;Commercial Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;IBM Db2 Warehouse (formerly dashDB for Analytics)&lt;/td&gt; &lt;td&gt;IBM Db2 and BLU (in memory) for Docker container supported infrastructure (also available as an appliance and cloud service - see below) - &lt;a href=&quot;https://www.ibm.com/aw-en/marketplace/db2-warehouse&quot;&gt;https://www.ibm.com/aw-en/marketplace/db2-warehouse&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Teradata Database&lt;/td&gt; &lt;td&gt;MPP database with support for a range of data warehouse and analytics functions; deployable on private or public cloud (also available as an appliance and a cloud service - see below) - &lt;a href=&quot;http://www.teradata.co.uk/products-and-services/teradata-database&quot;&gt;http://www.teradata.co.uk/products-and-services/teradata-database&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Teradata Aster Analytics&lt;/td&gt; &lt;td&gt;Analytics platform that supports graph, text and IoT analysis plus machine learning (also available as an appliance and a cloud service - see below) - &lt;a href=&quot;http://www.teradata.co.uk/products-and-services/analytics-from-aster-overview&quot;&gt;http://www.teradata.co.uk/products-and-services/analytics-from-aster-overview&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Vertica&lt;/td&gt; &lt;td&gt;MPP columnar database with support for a range of analytical functions including machine learning; deployable on commodity infrastructure or public/private cloud - &lt;a href=&quot;https://www.vertica.com/overview/&quot;&gt;https://www.vertica.com/overview/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Actian Vector&lt;/td&gt; &lt;td&gt;MPP columnar database with support for vectorized execution, small incremental inserts and min-max indices, with a free community edition for databases under 1 Tb - &lt;a href=&quot;https://www.actian.com/analytic-database/vector-smp-analytic-database/&quot;&gt;https://www.actian.com/analytic-database/vector-smp-analytic-database/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;InfoBrightDB&lt;/td&gt; &lt;td&gt;Columnar database, now sold by Ignite Technologies - &lt;a href=&quot;http://www.ignitetech.com/solutions/information-technology/infobrightdb&quot;&gt;http://www.ignitetech.com/solutions/information-technology/infobrightdb&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;open-source-technologies&quot;&gt;Open Source Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/greenplum/&quot;&gt;Greenplum&lt;/a&gt;&lt;/td&gt; &lt;td&gt;MPP database based on PostgreSQL, with support for multiple storage models and analytical capabilities (including graph); open sourced in October 2015&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;MariaDB ColumnStore&lt;/td&gt; &lt;td&gt;Columnar storage for MariaDB (the open source fork of MySQL) based on a fork of InfiniDB - &lt;a href=&quot;https://mariadb.com/products/technology/columnstore&quot;&gt;https://mariadb.com/products/technology/columnstore&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;MariaDB AX&lt;/td&gt; &lt;td&gt;Data warehousing solution based on MariaDB ColumnStore, with commercial support available from MariaDB - &lt;a href=&quot;https://mariadb.com/products/solutions/olap-database-ax&quot;&gt;https://mariadb.com/products/solutions/olap-database-ax&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;MonetDB&lt;/td&gt; &lt;td&gt;Open source columnar database - &lt;a href=&quot;https://www.monetdb.org/&quot;&gt;https://www.monetdb.org/&lt;/a&gt; ; &lt;a href=&quot;https://www.monetdbsolutions.com/&quot;&gt;https://www.monetdbsolutions.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;InifiDB&lt;/td&gt; &lt;td&gt;Open source columnar database, inactive since March 2015 - &lt;a href=&quot;https://github.com/infinidb/infinidb&quot;&gt;https://github.com/infinidb/infinidb&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Pinot&lt;/td&gt; &lt;td&gt;Open source realtime distributed OLAP datastore from LinkedIn - &lt;a href=&quot;https://github.com/linkedin/pinot&quot;&gt;https://github.com/linkedin/pinot&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;hadoop-based-open-source-technologies&quot;&gt;Hadoop Based Open Source Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Apache Hive&lt;/a&gt;&lt;/td&gt; &lt;td&gt;An analytical database when used with LLAP, ORCFile and Tez; runs over Hadoop&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-impala/&quot;&gt;Apache Impala&lt;/a&gt;&lt;/td&gt; &lt;td&gt;An analytical database when used with Kudu or Parquet over HDFS; runs over Hadoop&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hawq/&quot;&gt;Apache HAWQ (Incubating)&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A port of the Greenplum MPP database (which itself is based on PostgreSQL) to run over Hadoop&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-tajo/&quot;&gt;Apache Tajo&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Distributed analytical database engine that runs over Hadoop&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Presto&lt;/td&gt; &lt;td&gt;An analytical database when used over Hive/Hadoop , originally created and open sourced by Facebook - &lt;a href=&quot;https://prestodb.io/&quot;&gt;https://prestodb.io/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/druid&quot;&gt;Druid&lt;/a&gt;&lt;/td&gt; &lt;td&gt;OLAP database supporting real time aggregations of streaming data using HDFS/S3 as backing storage&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;hadoop-based-commercial-technologies&quot;&gt;Hadoop Based Commercial Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Teradata Aster Analytics on Hadoop&lt;/td&gt; &lt;td&gt;Teradata Aster running natively on Hadoop as a YARN application - &lt;a href=&quot;http://www.teradata.co.uk/products-and-services/aster-analytics-on-hadoop/&quot;&gt;http://www.teradata.co.uk/products-and-services/aster-analytics-on-hadoop/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Vertica on Hadoop&lt;/td&gt; &lt;td&gt;Vertica running on Hadoop - &lt;a href=&quot;https://www.vertica.com/product/vertica-for-sql-on-hadoop/&quot;&gt;https://www.vertica.com/product/vertica-for-sql-on-hadoop/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Actian Vector H&lt;/td&gt; &lt;td&gt;Version of Action Vector that runs as a native YARN app but requires data to be loaded into its proprietary data format - &lt;a href=&quot;https://www.actian.com/analytic-database/vectorh-sql-hadoop/&quot;&gt;https://www.actian.com/analytic-database/vectorh-sql-hadoop/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;appliances&quot;&gt;Appliances&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Oracle Exadata&lt;/td&gt; &lt;td&gt;An appliance consisting of an Oracle RAC cluster combined with a set of storage nodes via high bandwidth interconnect, with support for hybrid columnar compression - &lt;a href=&quot;https://www.oracle.com/engineered-systems/exadata/index.html&quot;&gt;https://www.oracle.com/engineered-systems/exadata/index.html&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Microsoft Analytics Platform System&lt;/td&gt; &lt;td&gt;An appliance built around SQL Server Parallel Data Warehouse and PolyBase - &lt;a href=&quot;https://www.microsoft.com/en-us/sql-server/analytics-platform-system&quot;&gt;https://www.microsoft.com/en-us/sql-server/analytics-platform-system&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;IBM Integrated Analytics System&lt;/td&gt; &lt;td&gt;Appliance built around Db2 Warehouse and BLU (in memory) acceleration with support for Spark - &lt;a href=&quot;https://www.ibm.com/us-en/marketplace/integrated-analytics-system&quot;&gt;https://www.ibm.com/us-en/marketplace/integrated-analytics-system&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;IBM PureData System for Analytics (formally Netezza)&lt;/td&gt; &lt;td&gt;Appliance utilising FPGA chips to run elements of queries in hardware, with support for a range of languages including R - &lt;a href=&quot;https://www.ibm.com/us-en/marketplace/puredata-system-for-analytics#product-header-top&quot;&gt;https://www.ibm.com/us-en/marketplace/puredata-system-for-analytics#product-header-top&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Teradata Integrated Data Warehouses&lt;/td&gt; &lt;td&gt;A family of Teradata database appliances - &lt;a href=&quot;http://www.teradata.co.uk/products-and-services/data-warehouse-overview&quot;&gt;http://www.teradata.co.uk/products-and-services/data-warehouse-overview&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Teradata Aster Analytics&lt;/td&gt; &lt;td&gt;Aster appliance - &lt;a href=&quot;http://www.teradata.co.uk/products-and-services/aster-big-analytics-appliance/&quot;&gt;http://www.teradata.co.uk/products-and-services/aster-big-analytics-appliance/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Pivotal EMC Data Computing Appliance (DCA)&lt;/td&gt; &lt;td&gt;Greenplum appliance - &lt;a href=&quot;https://pivotal.io/emc-dca&quot;&gt;https://pivotal.io/emc-dca&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;cloud-services&quot;&gt;Cloud Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Amazon Redshift&lt;/td&gt; &lt;td&gt;A MPP analytical database, with support for columnar storage and the ability to query data in Amazon S3 as external tables (Redshift Spectrum) - &lt;a href=&quot;https://aws.amazon.com/redshift/&quot;&gt;https://aws.amazon.com/redshift/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google Big Query&lt;/td&gt; &lt;td&gt;Analytical SQL database service, with cost based on storage and query execution - &lt;a href=&quot;https://cloud.google.com/bigquery/&quot;&gt;https://cloud.google.com/bigquery/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure SQL Data Warehouse&lt;/td&gt; &lt;td&gt;Scalable analytical database, with support for Azure Data Lake Store external tables - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/sql-data-warehouse/&quot;&gt;https://azure.microsoft.com/en-us/services/sql-data-warehouse/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Oracle Exadata Cloud&lt;/td&gt; &lt;td&gt;Oracle Exadata as a managed cloud service (including as an Oracle managed on premises offering) - &lt;a href=&quot;https://cloud.oracle.com/database&quot;&gt;https://cloud.oracle.com/database&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;IBM Db2 Warehouse (formerly dashDB for Analytics)&lt;/td&gt; &lt;td&gt;IBM Db2 and BLU (in memory) acceleration as a cloud service - &lt;a href=&quot;https://www.ibm.com/us-en/marketplace/db2-warehouse-on-cloud&quot;&gt;https://www.ibm.com/us-en/marketplace/db2-warehouse-on-cloud&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Teradata IntilliCloud&lt;/td&gt; &lt;td&gt;Teradata Database, Hadoop and Aster as a service - &lt;a href=&quot;http://www.teradata.co.uk/products-and-services/intellicloud&quot;&gt;http://www.teradata.co.uk/products-and-services/intellicloud&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Snowflake&lt;/td&gt; &lt;td&gt;Data Warehouse for the cloud, with separated compute and storage, columnar storage, vectorized execution, adaptive optimisation (no indexes, keys or tuning required) and support for semi-structured (JSON, Avro and XML) data - &lt;a href=&quot;https://www.snowflake.net/&quot;&gt;https://www.snowflake.net/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;analytical-in-memory-technologies&quot;&gt;Analytical In Memory Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;MemSQL&lt;/td&gt; &lt;td&gt;Distributed in memory relational database, with wire compatibility with MySQL and support for row and columnar storage, and a free community edition - &lt;a href=&quot;http://www.memsql.com/&quot;&gt;http://www.memsql.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;SAP HANA&lt;/td&gt; &lt;td&gt;In memory relational DBMS primarily focused on accelerating SAP applications - &lt;a href=&quot;https://www.sap.com/products/hana.html&quot;&gt;https://www.sap.com/products/hana.html&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;EXASOL&lt;/td&gt; &lt;td&gt;In memory MPP database with columnar compression and SQL support - &lt;a href=&quot;http://www.exasol.com/&quot;&gt;http://www.exasol.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;MapD&lt;/td&gt; &lt;td&gt;In memory, column store, SQL relational database that runs on GPUs - &lt;a href=&quot;https://www.mapd.com/&quot;&gt;https://www.mapd.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Kinetica&lt;/td&gt; &lt;td&gt;Distributed in memory relational database that runs on GPUs - &lt;a href=&quot;https://www.kinetica.com&quot;&gt;https://www.kinetica.com&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;analytical-graph-databases&quot;&gt;Analytical Graph Databases&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;DataStax Enterprise&lt;/td&gt; &lt;td&gt;Commercial product built on Apache Cassandra with the addition of graph and search capabilities - &lt;a href=&quot;https://www.datastax.com/&quot;&gt;https://www.datastax.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;TigerGraph&lt;/td&gt; &lt;td&gt;Commercial hybrid OLTP/OLAP graph database that claims order of magnitude performance and scalability improvements over it’s competitors; previously known as GraphSQL - &lt;a href=&quot;http://www.tigergraph.com&quot;&gt;http://www.tigergraph.com&lt;/a&gt;, &lt;a href=&quot;http://www.zdnet.com/article/tigergraph-a-graph-database-born-to-roar/&quot;&gt;http://www.zdnet.com/article/tigergraph-a-graph-database-born-to-roar/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GraphBase&lt;/td&gt; &lt;td&gt;Commercial graph database designed for use in AI applications - &lt;a href=&quot;https://graphbase.ai/&quot;&gt;https://graphbase.ai/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;JanusGraph&lt;/td&gt; &lt;td&gt;Open source distributed graph database that runs over a number of storage backends (including Cassandra, HBase and BigTable), with TinkerPop support including support for graph analytics; previously known as Titan - &lt;a href=&quot;http://janusgraph.org/&quot;&gt;http://janusgraph.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;TinkerGraph&lt;/td&gt; &lt;td&gt;In memory graph databases that’s part of TinkerPop as a reference implementation - &lt;a href=&quot;http://tinkerpop.apache.org&quot;&gt;http://tinkerpop.apache.org&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GRAKN.AI&lt;/td&gt; &lt;td&gt;Open Source graph database designed for AI use cases that also supports graph analytics - &lt;a href=&quot;https://grakn.ai&quot;&gt;https://grakn.ai&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Complexible Stardog&lt;/td&gt; &lt;td&gt;RDF database that also support property graphs and data virtualisation, with a community edition available - &lt;a href=&quot;http://www.stardog.com/&quot;&gt;http://www.stardog.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/tech-categories/analytical-databases/</guid> </item> <item><title>The Mid Week News - 25/10/2017</title><link>http://ondataengineering.net/blog/2017/10/25/the-mid-week-news/</link><pubDate>Wed, 25 Oct 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;It’s news time again… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/greenplum/&quot;&gt;Greenplum&lt;/a&gt; has hit 5.1&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-data-science-workbench/&quot;&gt;Cloudera Data Science Workbench&lt;/a&gt; has hit 1.2&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Some interesting slides from Todd Lipcon on the Kudu blog on Hybrid Transactional/Analytic Processing (HTAP) and how &lt;a href=&quot;/technologies/apache-kudu/&quot;&gt;Kudu&lt;/a&gt; compares to Google Spanner - &lt;a href=&quot;https://kudu.apache.org/2017/10/23/nosql-kudu-spanner-slides.html&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A couple of updates from Neo4j from its GraphConnect conference in New York, including the contribution of a Cypher interface for &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt;, and new analytical functions in Neo4J - &lt;a href=&quot;http://www.zdnet.com/article/sparkier-faster-more-graph-databases-and-neo4j-moving-on/&quot;&gt;ZDNet&lt;/a&gt;; &lt;a href=&quot;https://www.theregister.co.uk/2017/10/24/neo4j_native_graph_platform/&quot;&gt;The Register&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Comparison of &lt;a href=&quot;/technologies/apache-cassandra/&quot;&gt;Apache Cassandra&lt;/a&gt; and ScyllaDB (the Cassandra port to C++) from ZDNet - &lt;a href=&quot;http://www.zdnet.com/article/a-rock-and-a-hard-place-between-scylladb-and-cassandra/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From The Register - The Linux Foundation have a new open-data licence - &lt;a href=&quot;https://www.theregister.co.uk/2017/10/23/linux_foundation_community_data_license_agreement/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From the Influx blog - getting started with &lt;a href=&quot;/technologies/influxdb/&quot;&gt;InfluxDB&lt;/a&gt; in 5 minutes - &lt;a href=&quot;https://www.influxdata.com/blog/zero-awesome-in-5-minutes/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;The John Snow Labs have released an NLP library for &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Apache Spark&lt;/a&gt; - &lt;a href=&quot;https://databricks.com/blog/2017/10/19/introducing-natural-language-processing-library-apache-spark.html&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Gartner have released their latest Magic Quadrant for Distributed File Systems and Object Storage. No significant changes, thoughts from The Register are &lt;a href=&quot;https://www.theregister.co.uk/2017/10/19/gartner_2017_object_storage_magic_quadrant/&quot;&gt;here&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/10/25/the-mid-week-news/</guid> </item> <item><title>Query Engines</title><link>http://ondataengineering.net/tech-categories/query-engines/</link><pubDate>Tue, 24 Oct 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;Our list of and information on commercial, open source and cloud based query engines, including Hive, Impala, Drill, Pig, Kognitio, Jethro, Amazon Athena, Azure Data Lake Analytics and alternatives to these.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Category Definition&lt;/h2&gt; &lt;!-- Tech Vendor metadata --&gt; &lt;p&gt;Engines that allow analytical queries expressed in a high level language (often SQL) to be run over one or more underlying data stores or databases, often including HDFS (and often using table definitions from the Hive Metastore), but with support for other Hadoop, relational and NoSQL databases commonly supported. Will support exploitation of raw data (focusing on schema on read and the ability to query across sources) and/or exploitation of data prepared for analytics (focusing on competing with analytical databases). Many technologies started as batch query engines (with high query startup costs and limited support for concurrent queries), but most can now be considered interactive with support for multiple concurrent low latency queries. Given the propensity for querying over Hadoop data using SQL, many of these technologies are often referred to as SQL-on-Hadoop technologies. &lt;!--more--&gt;&lt;/p&gt; &lt;h2 id=&quot;further-information&quot;&gt;Further Information&lt;/h2&gt; &lt;p&gt;Bloor have a &lt;a href=&quot;https://www.bloorresearch.com/research/sql-engines-hadoop/&quot;&gt;SQL Engines on Hadoop Market Report Paper&lt;/a&gt; that covers some of these technologies.&lt;/p&gt; &lt;p&gt;See our &lt;a href=&quot;/tech-categories/data-storage-formats/&quot;&gt;Data Storage Formats&lt;/a&gt; page for information on file formats to use with these query engines.&lt;/p&gt; &lt;p&gt;See our &lt;a href=&quot;/tech-categories/analytical-databases/&quot;&gt;Analytical Databases&lt;/a&gt; page for information on technologies that provide analytical capabilities as a self contained storage and query engine stack.&lt;/p&gt; &lt;h2 id=&quot;open-source-technologies&quot;&gt;Open Source Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Apache Hive&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Supports the execution of SQL queries over data in HDFS using MapReduce, Spark or Tez based on tables defined in the Hive Metastore&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-impala/&quot;&gt;Apache Impala&lt;/a&gt;&lt;/td&gt; &lt;td&gt;An MPP query engine that supports the execution of SQL queries over in HDFS, HBase, Kudu and S3 based on tables defined in the Hive Metastore&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Presto&lt;/td&gt; &lt;td&gt;Distributed SQL query engine over data in HDFS, NoSQL and relational databases and Kafka, originally created and open sourced by Facebook - &lt;a href=&quot;https://prestodb.io/&quot;&gt;https://prestodb.io/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-drill/&quot;&gt;Apache Drill&lt;/a&gt;&lt;/td&gt; &lt;td&gt;An MPP query engine that supports queries over one or more underlying databases or datasets without first defining a schema and with the ability to join data from multiple data stores together.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Apache Lens&lt;/td&gt; &lt;td&gt;Provides a cube based federated view over a range of data stores including HDFS, HBase, relational databases, S3 and Redshift - &lt;a href=&quot;http://lens.apache.org/&quot;&gt;http://lens.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-spark/spark-sql/&quot;&gt;Apache Spark SQL&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Hive compatible SQL query engine that use Spark to execute queries over any Spark supported data source&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-pig/&quot;&gt;Apache Pig&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Technology for running analytical and data processing jobs written in Pig Latin against data in Hadoop using MapReduce, Tez and Spark&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Apache MRQL (Incubating)&lt;/td&gt; &lt;td&gt;Supports the execution of MRQL queries over data in Hadoop using MapReduce, Hama, Spark or Flink - &lt;a href=&quot;http://mrql.apache.org/&quot;&gt;http://mrql.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-kylin/&quot;&gt;Kylin&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Supports the querying of Hive tables as OLAP cubes&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;commercial-technologies&quot;&gt;Commercial Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Kognitio&lt;/td&gt; &lt;td&gt;In memory database engine that can run as a YARN application on Hadoop over data in HDFS (as a free offering) or as a standalone cluster over data in HDFS, the cloud and other databases (as a commercial offering with a free trial) - &lt;a href=&quot;https://kognitio.com/&quot;&gt;https://kognitio.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Jethro&lt;/td&gt; &lt;td&gt;SQL query engine over HDFS and S3 that supports indexing, auto generation of cubes and results caching - &lt;a href=&quot;https://jethro.io/&quot;&gt;https://jethro.io/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;AtScale&lt;/td&gt; &lt;td&gt;Cube based semantic layer with query optimisation, virtual cube caching and row level security over Hadoop, RedShift and SQL data sources - &lt;a href=&quot;https://atscale.com/&quot;&gt;https://atscale.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;IBM Big SQL&lt;/td&gt; &lt;td&gt;SQL engine that runs on Hadoop over Hive tables, but that can also federate into RDMS and NoSQL databases and object stores - &lt;a href=&quot;https://www.ibm.com/us-en/marketplace/big-sql&quot;&gt;https://www.ibm.com/us-en/marketplace/big-sql&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Oracle Big Data SQL&lt;/td&gt; &lt;td&gt;Allows federated queries from an Oracle databases over Hadoop and NoSQL databases, with push down of logic and support for Oracle security - &lt;a href=&quot;https://www.oracle.com/database/big-data-sql/index.html&quot;&gt;https://www.oracle.com/database/big-data-sql/index.html&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Kyvos Insights&lt;/td&gt; &lt;td&gt;OLAP cubes on Hadoop - &lt;a href=&quot;http://www.kyvosinsights.com/olap-cubes-on-hadoop/&quot;&gt;http://www.kyvosinsights.com/olap-cubes-on-hadoop/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;cloud-technologies&quot;&gt;Cloud Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Amazon Athena&lt;/td&gt; &lt;td&gt;SQL query service over data in Amazon S3 - &lt;a href=&quot;https://aws.amazon.com/athena/&quot;&gt;https://aws.amazon.com/athena/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Data Lake Analytics&lt;/td&gt; &lt;td&gt;Massively parallel analytics job service, with support for U-SQL, R, Python, and .NET - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/data-lake-analytics/&quot;&gt;https://azure.microsoft.com/en-us/services/data-lake-analytics/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/tech-categories/query-engines/</guid> </item> <item><title>The Week That Was</title><link>http://ondataengineering.net/blog/2017/10/20/the-week-that-was/</link><pubDate>Fri, 20 Oct 2017 07:45:00 +0100</pubDate> <description> &lt;p&gt;Another week, another apology.&lt;/p&gt; &lt;p&gt;For various reasons the new analytical databases technology category pages aren’t ready yet. They’re almost done however, and will be going up all next week.&lt;/p&gt; &lt;p&gt;Thank you for your patience.&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/10/20/the-week-that-was/</guid> </item> <item><title>The Mid Week News - 18/10/2017</title><link>http://ondataengineering.net/blog/2017/10/18/the-mid-week-news/</link><pubDate>Wed, 18 Oct 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;News news news! A bunch of new technology releases and interested blog posts for your purusal this week… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;CDH&lt;/a&gt; has seen a 5.13 release, with Kudu now fully bundled and Spark 1.x deprecated&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-manager/&quot;&gt;Cloudera Manager&lt;/a&gt; is up to 5.13&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-navigator/&quot;&gt;Cloudera Navigator&lt;/a&gt; is up to 2.12&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-director/&quot;&gt;Cloudera Director&lt;/a&gt; is up to 2.6&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-phoenix/&quot;&gt;Apache Phoenix&lt;/a&gt; is up to 4.12&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-solr&quot;&gt;Apache Solr&lt;/a&gt; is up to 7.1&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Microsoft have released/updated &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/architecture/aws-professional/services&quot;&gt;their Azure to AWS services comparison&lt;/a&gt;&lt;/li&gt; &lt;li&gt;LinkedIn have posted about &lt;a href=&quot;https://engineering.linkedin.com/blog/2017/10/streaming-data-pipelines-with-brooklin&quot;&gt;Brooklin&lt;/a&gt; - their internal product (planned to be open sourced in 2018) for moving streaming data around and performing change data capture on source databases&lt;/li&gt; &lt;li&gt;Uber have posted about &lt;a href=&quot;https://eng.uber.com/athenax/&quot;&gt;AthenaX&lt;/a&gt; their technology for running SQL analytics over streaming data using Flink&lt;/li&gt; &lt;li&gt;An interesting post from DB Engines on &lt;a href=&quot;https://db-engines.com/en/blog_post/72&quot;&gt;multi-model databases&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Hortonworks have posted on &lt;a href=&quot;https://hortonworks.com/blog/automated-validation-apache-hadoop-ecosystem/&quot;&gt;how they test their Hadoop distribution&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A post from Google on BigQuery, and how it’s separation of data and processing &lt;a href=&quot;https://cloud.google.com/blog/big-data/2017/10/separation-of-compute-and-state-in-google-bigquery-and-cloud-dataflow-and-why-it-matters.html&quot;&gt;gives near linear scalability&lt;/a&gt;, comparing it’s performance to Impala, Spark, Hive and Presto&lt;/li&gt; &lt;li&gt;Cloudera have been looking at &lt;a href=&quot;/technologies/microsoft-azure-data-lake-store/&quot;&gt;Azure Data Lake Store&lt;/a&gt;, concluding that [performance “compares favourably” to using network-attached Azure disk storage - &lt;a href=&quot;http://blog.cloudera.com/blog/2017/10/a-look-at-adls-performance-throughput-and-scalability/&quot;&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;And another benchmark - this time DataBricks claiming that Spark &lt;a href=&quot;/technologies/apache-spark/structured-streaming/&quot;&gt;Structured Streaming&lt;/a&gt; is 5 times faster than Flink and Kafka Streams - &lt;a href=&quot;https://databricks.com/blog/2017/10/11/benchmarking-structured-streaming-on-databricks-runtime-against-state-of-the-art-streaming-systems.html&quot;&gt;link&lt;/a&gt; - UPDATE 2018-01-05: See dataArtisans response &lt;a href=&quot;https://data-artisans.com/blog/curious-case-broken-benchmark-revisiting-apache-flink-vs-databricks-runtime&quot;&gt;here&lt;/a&gt;&lt;/li&gt; &lt;li&gt;And finally, there’s a new security vulnerability in Solr - &lt;a href=&quot;http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2017-12629&quot;&gt;CVE-2017-12629&lt;/a&gt; - a remote code execution issue&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/10/18/the-mid-week-news/</guid> </item> <item><title>The Week That Was</title><link>http://ondataengineering.net/blog/2017/10/13/the-week-that-was/</link><pubDate>Fri, 13 Oct 2017 07:45:00 +0100</pubDate> <description> &lt;p&gt;As suspected, no updates this week - I’m still trying to work out how to make sense of analytical database technologies and present these in a useful way.&lt;/p&gt; &lt;p&gt;So hold tight - a bunch (few) technology categories and some thoughts will be coming next week.&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/10/13/the-week-that-was/</guid> </item> <item><title>The Mid Week News - 11/10/2017</title><link>http://ondataengineering.net/blog/2017/10/11/the-mid-week-news/</link><pubDate>Wed, 11 Oct 2017 07:45:00 +0100</pubDate> <description> &lt;p&gt;It’s news time again - come and see what new tech releases and interesting reading we have for you this week! &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/mesosphere-dcos/&quot;&gt;Mesosphere DC/OS&lt;/a&gt; is up to 1.10&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/mesosphere-marathon&quot;&gt;Mesosphere Marathon&lt;/a&gt; is up to 1.5&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/rex-ray/&quot;&gt;REX-Ray&lt;/a&gt; is up to 0.10&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/hue&quot;&gt;Hue&lt;/a&gt; is up to 4.1&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-flume&quot;&gt;Flume&lt;/a&gt; is up to 1.8&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;From ZDNet, &lt;a href=&quot;http://www.zdnet.com/article/towards-a-unifying-data-theory-and-practice-combining-operations-analytics-and-streaming/&quot;&gt;thoughts&lt;/a&gt; on SnappyData, and the convergence of OLTP, OLAP and streaming analytics&lt;/li&gt; &lt;li&gt;From The Register, &lt;a href=&quot;/tech-vendors/microsoft-azure/&quot;&gt;Azure&lt;/a&gt; now supports a dedicated tool for provisioning Spark based on Azure Batch&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/tech-vendors/amazon-web-services/&quot;&gt;Amazon&lt;/a&gt; EMR now supports &lt;a href=&quot;/technologies/apache-livy&quot;&gt;Livy&lt;/a&gt;, as well as new versions of Hue, Presto, Flink and Pig.&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://content.pivotal.io/blog/apache-madlib-comes-of-age&quot;&gt;Thoughts&lt;/a&gt; on MADLib from Pivotal following it’s graduation from the Apache incubator. We’ll be looking more at capabilities like this over this week and next.&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-nifi/&quot;&gt;Apache Nifi&lt;/a&gt; has security vulnerablity - &lt;a href=&quot;http://www.cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2017-12623&quot;&gt;CVE-2017-12623&lt;/a&gt; - authorized user could upload a template which contained malicious code and accessed sensitive files via an XML External Entity (XXE) attack&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.crail.io/&quot;&gt;Crail&lt;/a&gt; has been &lt;a href=&quot;https://wiki.apache.org/incubator/CrailProposal&quot;&gt;submitted&lt;/a&gt; to the Apache Incubator - looks like a high performance distributed and tiered (in memory, flash and disk) storage layer for temporary data that provides memory, storage and network access that bypasses the JVM and OS, and with integration to Spark (as a custom Spark Suffler that improves sort performance by a factor of five) and Hadoop (via an HDFS adaptor).&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/10/11/the-mid-week-news/</guid> </item> <item><title>The Plan For This Week - 10/10/2017</title><link>http://ondataengineering.net/blog/2017/10/10/the-plan-for-this-week/</link><pubDate>Tue, 10 Oct 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;First up, yes, we’re a day late on this post - apologies. Don’t get confused - it is Tuesday today and not Monday.&lt;/p&gt; &lt;p&gt;So I think we have one last technology category to see off before we move on from data storage and database technologies, and that’s analytical databases - those that specialise in large scanning analytical queries and that combine support for SQL with a range of other analytical capabilities.&lt;/p&gt; &lt;p&gt;This feels like another monster category, so don’t expect much content this week, and I have a feeling it’s going to be next week before we can wrap everything up.&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/10/10/the-plan-for-this-week/</guid> </item> <item><title>Thoughts on Hadoop Data Formats</title><link>http://ondataengineering.net/blog/2017/10/06/thoughts-on-hadoop-data-formats/</link><pubDate>Fri, 06 Oct 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;This week we looked at two file formats for Hadoop; &lt;a href=&quot;/technologies/apache-orc&quot;&gt;ORC&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-carbondata&quot;&gt;CarbonData&lt;/a&gt; and a new in-memory data structure specification; &lt;a href=&quot;/technologies/apache-arrow&quot;&gt;Arrow&lt;/a&gt;. Earlier on this year we looked at data serialisation frameworks - &lt;a href=&quot;/technologies/apache-avro&quot;&gt;Avro&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-parquet&quot;&gt;Parquet&lt;/a&gt;. File formats, data serialisation frameworks, specifications… Ahh, what a minefield! Today I’d like to try and make sense of it all by looking at the evolution of these various data formats to see how we got here. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Back in the old days, it wasn’t uncommon to process gzipped plain text delimited formatted data using MapReduce. However, if the data was supplied as lots of &lt;a href=&quot;https://blog.cloudera.com/blog/2009/02/the-small-files-problem&quot;&gt;small files&lt;/a&gt;, eventually, pressure would be put on HDFS because of a hard limit on the number of files it can physically track. At the same time if too few large files were supplied, MapReduce wouldn’t be able to efficiently carve up the data for parallel processing. SequenceFile was the first file format to address both of these issues by providing a container in which many small files could be put into a larger single file with synchronisation markers to permit efficient splitting of files to distribute the workload. The file format also allowed for different types of compressions to be used inside of the file to provide a finer level of compression control whilst still maintaining its splitability characteristics. And for a while, SequenceFile was fine if you were just using it to do large-scale distributed batch processing or building a self-contained system using Java, such a &lt;a href=&quot;/technologies/apache-hbase&quot;&gt;storage manager&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;As a file format, &lt;a href=&quot;/technologies/apache-avro&quot;&gt;Avro&lt;/a&gt; builds on top of the &lt;a href=&quot;http://avro.apache.org/docs/current/spec.html#Object+Container+Files&quot;&gt;container file format&lt;/a&gt; and synchronisation marker ideas but adds the ability to model relational and complex data. It achieves this through the concept of a schema that allows you to specify the structure, type and the meaning of your data, and when distributed together with your data, you’ll have self describing data that can be used as both a wire format for communication and a serialisation format for persistent data. But Avro’s creator - Doug Cutting didn’t just stop there, Avro had a much grander aspiration. Its design accommodated schema evolution which allowed your data to be long lived and reusable by other applications outside of the Hadoop and Java ecosystem.&lt;/p&gt; &lt;p&gt;So far, when it comes to serialising data for persistence, our file formats have been writing consecutive elements of a row next to each other on disk. Around 2010/2011, there were several research projects experimenting with column-oriented data layout designs on HDFS. The idea behind a columnar data structure was to lay out your data so that column values were adjacent to one another. For read queries that only process a small subset of columns but over a large number of rows at a time (the so called typical analytical workload), it was possible to read only the required column values off disk, thereby minimising disk I/O. Contrary to the more traditional row-oriented data layout for the same type of query, you were forced to read all the other columns off disk, keep the columns required by the query and discard the unused ones. Column data being uniform in type, also had the additional benefit of being highly compressible, allowing the same data to be stored on disk in a smaller form and further reducing I/O. This approach to optimise data placement was popularised by databases such as &lt;a href=&quot;https://www.monetdb.org&quot;&gt;MonetDB&lt;/a&gt; and &lt;a href=&quot;https://www.vertica.com&quot;&gt;HP Vertica&lt;/a&gt;. The tradeoff would be slower writes, but this proved a good optimisation for analytical workloads. The record columnar file format (RCFile) came out of one of these research projects and became widely adopted in the Hadoop ecosystem.&lt;/p&gt; &lt;p&gt;By 2013, there was gathering interest towards using Hadoop for interactive, data warehouse-style SQL queries, and combining Hive with RCFile for data storage was a popular choice. &lt;a href=&quot;/tech-vendors/hortonworks&quot;&gt;Hortonworks&lt;/a&gt; launched the &lt;a href=&quot;https://hortonworks.com/blog/100x-faster-hive&quot;&gt;Stinger initiative&lt;/a&gt; with the goal to dramatically speed up Hive and make it more enterprise-ready. Building on top of the columnar file format of RCFile, the team introduced Avro’s schema concept to allow the format to model complex nested structures which wasn’t previously supported by RCFile. Armed with this metadata, it was also possible to intelligently select an appropriate compression schema based on a column’s data type. The format also allowed additional metadata such as min and max indexes to be collected which could then be later used to intelligently skip irrelevant parts of the data without the need for large, complex, or manually maintained indexes. Other improvements introduced over Avro and RCFile was the ability to identify the boundaries on which files could be split with having to scan for synchronisation markers. This new file format was known as the &lt;a href=&quot;/technologies/apache-orc&quot;&gt;Optimized Record Columnar (ORC)&lt;/a&gt; File. The Avro team had also been experimenting with a columnar file format design called Trevni which was picked up by &lt;a href=&quot;/tech-vendors/cloudera&quot;&gt;Cloudera&lt;/a&gt; and Twitter to develop &lt;a href=&quot;/technologies/apache-parquet&quot;&gt;Parquet&lt;/a&gt;. On the other hand, ORC was spun out of Hive into a separate project but was initially kept closely integrated. Parquet was positioned as a more general-purpose columnar file format for use with any Hadoop framework, although essentially both projects shared the same fundamental ideas. Today, they both stand in their own right and are integrated with a number of different frameworks.&lt;/p&gt; &lt;p&gt;Up until now we’ve being looking at data formats that are primarily optimised for a single type of query analysis. This is not ideal if you want your data processing platform to support a wide spectrum of different types of query analysis. &lt;a href=&quot;/technologies/apache-carbondata&quot;&gt;CarbonData&lt;/a&gt; from Huawei aims to tackle the ‘one format to rule them all’ idea heads on. Building on the previous formats, CarbonData introduces multi-dimensional key indexes inspired from the likes of Mondrian (an early open source OLAP server) and Apache &lt;a href=&quot;/technologies/apache-kylin&quot;&gt;Kylin&lt;/a&gt; to support multi-dimensional OLAP style queries, inverted indexes for count distinct like operations, and the ability to group columns together to support detailed queries which fetch many columns out of a wide table.&lt;/p&gt; &lt;p&gt;Finally, we come to &lt;a href=&quot;/technologies/apache-arrow&quot;&gt;Apache Arrow&lt;/a&gt;. Unlike the previous data formats we’ve discussed, Arrow isn’t about serialising data to disk, but is an in-memory data format that focuses on CPU throughput for efficient processing and for data exchange between process/systems without serialisation and deserialisation. Similar to ORC and Parquet, data is structured in a columnar structure, so when it comes to analytical workloads, only the required data can be supplied to the CPU. This data placement strategy takes full advantage of the on-chip cache storage (which is 100x faster to access than main memory), pipelining, and SIMD (Single Instruction Multiple Data) instructions which work on multiple data values simultaneously in a single CPU clock cycle. As an in-memory data format, this concept wasn’t new and had already been implemented in both &lt;a href=&quot;/technologies/apache-drill&quot;&gt;Drill&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-hive/hive-server&quot;&gt;Hive&lt;/a&gt;. What is special about Arrow is its goal to define a standard interchange format to allow sharing of data between processes without the overhead of moving or transforming the data. This is important when you want to put together a data processing platform that isn’t limited only to Hadoop.&lt;/p&gt; &lt;p&gt;In summary, we’ve seen that the original Hadoop data formats were designed to solve very specific use cases. As the drive to develop Hadoop into a more general purpose analytics platform and expand its use outside of the Java ecosystem, the data formats have rapidly evolved, with each new format building on top of the ideas of its predecessor. While the Hadoop ecosystem continues to evolve to cater for new use cases, I expect to see continued innovation in this space.&lt;/p&gt; &lt;p&gt;So I hope this little journey has helped you navigate your way through the complex and rapidly evolving collection of Hadoop data formats - on Monday I’ll hand you back over to Peter.&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/10/06/thoughts-on-hadoop-data-formats/</guid> </item> <item><title>Data Storage Formats</title><link>http://ondataengineering.net/tech-categories/data-storage-formats/</link><pubDate>Thu, 05 Oct 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;Our list of and information on data storage formats, including Avro, Parquet, ORCCFile, Carbondata and alternatives to these.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Category Definition&lt;/h2&gt; &lt;!-- Tech Vendor metadata --&gt; &lt;p&gt;Libraries that support the storage of data on disk for data storage, real-time or batch analytics. Popularised by the use of distributed file systems in analytical platforms, common features include support for schema evolution (the ability to make changes to the schema but still read all historical data), support for both row and columnar data layouts (supporting efficient batch processing and analytical workloads respectively), complex record formats including nested objects and arrays, indexing to support random data access, and support for efficient inserts, updates and deletes as well as ACID transactions. &lt;!--more--&gt;&lt;/p&gt; &lt;h2 id=&quot;further-information&quot;&gt;Further Information&lt;/h2&gt; &lt;p&gt;A good introduction from Silicon Valley Data Science to text, SequenceFile, Avro, Parquet and ORC, along with some benchmarks - &lt;a href=&quot;http://www.svds.com/dataformats/&quot;&gt;http://www.svds.com/dataformats/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Comparison of JSON, Avro, ORC and Parquet, including benchmarks, from August 2016 - &lt;a href=&quot;https://www.slideshare.net/HadoopSummit/file-format-benchmark-avro-json-orc-parquet-65740483&quot;&gt;https://www.slideshare.net/HadoopSummit/file-format-benchmark-avro-json-orc-parquet-65740483&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Comparison of Parquet, Avro, HBase and Kudu, from January 2017 - &lt;a href=&quot;https://db-blog.web.cern.ch/blog/zbigniew-baranowski/2017-01-performance-comparison-different-file-formats-and-storage-engines&quot;&gt;https://db-blog.web.cern.ch/blog/zbigniew-baranowski/2017-01-performance-comparison-different-file-formats-and-storage-engines&lt;/a&gt;&lt;/p&gt; &lt;h2 id=&quot;text-based-formats&quot;&gt;Text Based Formats&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Plain Text&lt;/td&gt; &lt;td&gt;Often formatted as either delimited (e.g. comma separated values (CSV) or pipe separated values (PSV)) or fixed width fields&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;XML&lt;/td&gt; &lt;td&gt;Supports the use of external schema definitions, but format can be verbose and serialisation/deserialisation performance often poor&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;JSON (Javascript Object Notation)&lt;/td&gt; &lt;td&gt;More efficient and terse than XML, but still suffers poor serialisation/deserialise performance&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;row-based-formats&quot;&gt;Row Based Formats&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;SequenceFile&lt;/td&gt; &lt;td&gt;Part of the Hadoop project, a container of binary key/value pairs used to store multiple smaller files inside a single large file - &lt;a href=&quot;https://wiki.apache.org/hadoop/SequenceFile&quot;&gt;https://wiki.apache.org/hadoop/SequenceFile&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-avro/&quot;&gt;Avro&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Binary serialisation format supporting schema evolution and complex record types&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;column-based-formats&quot;&gt;Column Based Formats&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;RCFile&lt;/td&gt; &lt;td&gt;Early implementation of a columnar record format as part of the Apache Hive project - &lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/RCFile&quot;&gt;https://cwiki.apache.org/confluence/display/Hive/RCFile&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-orc/&quot;&gt;ORCFile&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Evolution of RCFile, spun out into it’s own Apache project&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-parquet/&quot;&gt;Parquet&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Columnar format spun out from the Avro Trevni format&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-carbondata/&quot;&gt;CarbonData&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Columnar format created by Huawei to address a number of perceived shortcomings in existing formats&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;map-based-formats&quot;&gt;Map Based Formats&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;MapFile, SetFile, ArrayFile, BloomMapFile&lt;/td&gt; &lt;td&gt;Part of the Hadoop project and built on SequenceFile, with MapFile being the original file format used by HBase - &lt;a href=&quot;http://blog.cloudera.com/blog/2011/01/hadoop-io-sequence-map-set-array-bloommap-files/&quot;&gt;http://blog.cloudera.com/blog/2011/01/hadoop-io-sequence-map-set-array-bloommap-files/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;TFile&lt;/td&gt; &lt;td&gt;Container of key-value pairs, part of the Hadoop project - &lt;a href=&quot;http://hadoop.apache.org/docs/r2.6.1/api/org/apache/hadoop/io/file/tfile/TFile.html&quot;&gt;http://hadoop.apache.org/docs/r2.6.1/api/org/apache/hadoop/io/file/tfile/TFile.html&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;HFile&lt;/td&gt; &lt;td&gt;HBase storage format based on TFile and the SSTable format from the Google BigTable paper - &lt;a href=&quot;http://hbase.apache.org/book.html#_hfile_format_2&quot;&gt;http://hbase.apache.org/book.html#_hfile_format_2&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;And of course, any &lt;a href=&quot;/tech-categories/object-stores/&quot;&gt;object store&lt;/a&gt;, NoSQL key value store or &lt;a href=&quot;/tech-categories/nosql-wide-column-stores/&quot;&gt;NoSQL wide column store&lt;/a&gt; can be used to store data by key.&lt;/p&gt; &lt;h2 id=&quot;data-storage-services&quot;&gt;Data Storage Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-kudu/&quot;&gt;Kudu&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Storage engine that supports efficient storage of data, insert/update/deletes and strong query performance&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/recordservice&quot;&gt;RecordService&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Abstraction layer for accessing structured data in Hadoop&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hive/hive-metastore/&quot;&gt;Hive Metastore&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Standard technology for defining structure of data in Hadoop&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;in-memory-formats&quot;&gt;In Memory Formats&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-arrow/&quot;&gt;Arrow&lt;/a&gt;&lt;/td&gt; &lt;td&gt;In memory columnar data format supporting high performance data exchange and fast analytical access&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/tech-categories/data-storage-formats/</guid> </item> <item><title>The Mid Week News - 04/10/2017</title><link>http://ondataengineering.net/blog/2017/10/04/the-mid-week-news/</link><pubDate>Wed, 04 Oct 2017 07:45:00 +0100</pubDate> <description> &lt;p&gt;Time for the news again, with all our updates on new technology releases and interesting things to read… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-calcite/&quot;&gt;Apache Calcite&lt;/a&gt; is up to 1.14&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-nifi/&quot;&gt;Apache NiFi&lt;/a&gt; is up to 1.4&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/elastic-cloud/&quot;&gt;Elastic Cloud&lt;/a&gt; is now generally available on GCP (after being in beta since April)&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudera-altus/&quot;&gt;Cloudera Altus&lt;/a&gt; now supports Azure&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/elasticsearch/&quot;&gt;Elasticsearch&lt;/a&gt; 6.0 release candidate 1 (RC1) (not for production) is out&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;From MapR, and these are always good for a read, a &lt;a href=&quot;https://mapr.com/blog/database-comparison-an-in-depth-look-at-mapr-db/&quot;&gt;post&lt;/a&gt; on why &lt;a href=&quot;/technologies/mapr-file-system/mapr-db/&quot;&gt;MapR-DB&lt;/a&gt; is better than Cassandra, HBase and others&lt;/li&gt; &lt;li&gt;And another face-off - this time a &lt;a href=&quot;https://brewing.codes/2017/09/25/flink-vs-spark/&quot;&gt;post&lt;/a&gt; on &lt;a href=&quot;/technologies/apache-flink/&quot;&gt;Flink&lt;/a&gt; vs &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Spark&lt;/a&gt;&lt;/li&gt; &lt;li&gt;It’s &lt;a href=&quot;https://www.confluent.io/blog/build-deploy-scalable-machine-learning-production-apache-kafka/&quot;&gt;another interesting Confluence post&lt;/a&gt; (YMMV), this time on machine learning with &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Kafka&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.zdnet.com/article/strata-nyc-2017-to-hadoop-go-jump-in-a-data-lake/&quot;&gt;Thoughts from ZDNet&lt;/a&gt; on the recent Strata NY event&lt;/li&gt; &lt;li&gt;HDInsights now support &lt;a href=&quot;https://azure.microsoft.com/en-us/blog/general-availability-of-hdinsight-interactive-query-blazing-fast-data-warehouse-style-queries-on-hyper-scale-data-2/&quot;&gt;Interactive Query&lt;/a&gt;, aka Hive on LLAP as a service&lt;/li&gt; &lt;li&gt;There are a bunch of security vulnerability announcements this week &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2017-9792&quot;&gt;CVE-2017-9792&lt;/a&gt; - malicious user with “ALTER” permissions on an Impala table can access any other Kudu table data&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2017-9797&quot;&gt;CVE-2017-9797&lt;/a&gt; - unauthenticated client can enter multi-user authentication mode in Apache Geode&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2017-9794&quot;&gt;CVE-2017-9794&lt;/a&gt; - user with read privileges can use the gfsh command line utility to execute queries with Apache Geode&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/10/04/the-mid-week-news/</guid> </item> <item><title>Apache ORC</title><link>http://ondataengineering.net/technologies/apache-orc/</link><pubDate>Wed, 04 Oct 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;Self-describing, type-aware, columnar file format to enable efficient querying and storage of data on Hadoop. Provides built-in storage indexes, column statistics and bloom filters to allow execution engines to implement predicate and projection push-down, partition pruning and cost based optimisation for low latency reads. Uses multi-version concurrency control to support ACID transactions and allow Hive to implement bulk insert, update, delete and streaming ingest (micro batch) use cases. Implements type-aware encoding for efficient compression (run-length for integer and dictionary for string). Schema definition is stored along side the data and supports all primitive data types and complex nested data structures. Uses protocol buffers to store meta data. Comes with a Java library for reading and writing the file format and includes a MapReduce compatible API, a C++ library for reading the file format (donated by Vertica) and a set of Java and C++ tools for inspecting and benchmarking ORC files. Created by Hortonworks in January 2013 as part of the initiative to massively speed up Hive and improve the storage efficiency of data stored in Hadoop, split off from Apache Hive to become a separate top level Apache project in April 2015 with a 1.0 release in January 2016.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;ORC&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;October 2017 - v1.4.0&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://orc.apache.org/&quot;&gt;https://orc.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://git-wip-us.apache.org/repos/asf/orc.git/&quot;&gt;https://git-wip-us.apache.org/repos/asf/orc.git/&lt;/a&gt; - source code&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://orc.apache.org/docs/&quot;&gt;https://orc.apache.org/docs/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/100x-faster-hive/&quot;&gt;https://hortonworks.com/blog/100x-faster-hive/&lt;/a&gt; - initial announcement&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/apache-orc-launches-as-a-top-level-project/&quot;&gt;https://hortonworks.com/blog/apache-orc-launches-as-a-top-level-project/&lt;/a&gt; - top level project announcement, including summary of technology that support it&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://orc.apache.org/news/&quot;&gt;https://orc.apache.org/news/&lt;/a&gt; - news page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://orc.apache.org/docs/releases.html&quot;&gt;https://orc.apache.org/docs/releases.html&lt;/a&gt; - details of releases&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/technologies/apache-orc/</guid> </item> <item><title>Apache CarbonData</title><link>http://ondataengineering.net/technologies/apache-carbondata/</link><pubDate>Tue, 03 Oct 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;Unified storage solution for Hadoop based on an indexed columnar data format, focusing on providing efficient processing and querying capabilities for disparate data access patterns. Data is loaded in batch, encoded, indexed using multiple strategies, compressed and written to HDFS using a columnar file format. Provides a number of highly configurable indexes (multi-dimensional key, min/max index, and inverted index), global dictionary encoding and column grouping to support interactive style OLAP queries, high throughput scan queries, low latency point queries and individual record queries. Also supports batch updates and deletes using delta bitmap files and compaction. Written in Java using Apache Thrift, supports all common primitive data types and complex nested data types including array and structures. Consists of several modules, the format specification and core implementation (columnar storage, indexing, compression, encoding), Hadoop input/output format interface, deep integration with Spark, interfacing to Spark SQL and the DataFrame API and connectors for Hive and Presto. Started back in 2013 at Huawei's India R&amp;D center, donated to the Apache Foundation in 2015, graduated in April 2017, with a stable (1.1.0) release in May 2017, and under active development.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;CarbonData&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;February 2018 - 1.3&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.3&lt;/td&gt; &lt;td&gt;2017-02-03&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/CARBONDATA/Apache+CarbonData+1.3.0+Release&quot;&gt;release notes&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://carbondata.apache.org&quot;&gt;http://carbondata.apache.org&lt;/a&gt; - CarbonData homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://carbondata.apache.org/mainpage.html&quot;&gt;http://carbondata.apache.org/mainpage.html&lt;/a&gt; - CarbonData documentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/carbondata&quot;&gt;https://blogs.apache.org/carbondata&lt;/a&gt; - CarbonData blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/CARBONDATA/Releases&quot;&gt;https://cwiki.apache.org/confluence/display/CARBONDATA/Releases&lt;/a&gt; - details of releases&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/technologies/apache-carbondata/</guid> </item> <item><title>Apache Arrow</title><link>http://ondataengineering.net/technologies/apache-arrow/</link><pubDate>Mon, 02 Oct 2017 07:45:00 +0100</pubDate> <description> &lt;p&gt;In-memory data structure specification for building columnar based data systems. Provides a standard interchange format to allow sharing of data between processes on a node without the overhead of moving or transforming the data, permits O(1) random access and has the ability to represent both flat relational structures and complex hierarchical nested data. Data is organised using a columnar structure memory-layout making it cache efficient for analytical workloads (which typically group all data relevant to a column operation together) and allows execution engines to take advantage of modern CPU SIMD (Single Instruction Multiple Data) instructions which work on multiple data values simultaneously in a single CPU clock cycle. Comes with reference implementations in Java and C++ and a Python interface to the C++ libraries (Ruby and JavaScript language bindings are in progress). Seeded from the Apache Drill project and promoted directly to a top level Apache project in February 2016 followed by an initial 0.1 release in October 2016. Used in a range of other projects including Drill, Spark, Impala, Kudu, Pandas and others. Has not yet reached a v1.0 milestone, but is still under active development with a range of contributors from a number of other Apache and non-Apache data projects.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Arrow&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/apache/&quot;&gt;The Apache Software Foundation&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;January 2018 - v0.8&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;0.8&lt;/td&gt; &lt;td&gt;2017-12-18&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://arrow.apache.org/blog/2017/12/18/0.8.0-release/&quot;&gt;blog post&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h1 id=&quot;links&quot;&gt;Links&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://arrow.apache.org/&quot;&gt;https://arrow.apache.org/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href=&quot;http://git.apache.org/arrow.git/&quot;&gt;http://git.apache.org/arrow.git/&lt;/a&gt; - source code&lt;/p&gt; &lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://info.dataengconf.com/the-future-of-column-oriented-data-processing-with-arrow-and-parquet&quot;&gt;http://info.dataengconf.com/the-future-of-column-oriented-data-processing-with-arrow-and-parquet&lt;/a&gt; - introduction to Arrow&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces87&quot;&gt;https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces87&lt;/a&gt; - top level project announcement&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://arrow.apache.org/blog/&quot;&gt;https://arrow.apache.org/blog/&lt;/a&gt; - blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://arrow.apache.org/release/&quot;&gt;https://arrow.apache.org/release/&lt;/a&gt; - release and change summary&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/technologies/apache-arrow/</guid> </item> <item><title>The Plan For This Week - 02/10/2017</title><link>http://ondataengineering.net/blog/2017/10/02/the-plan-for-this-week/</link><pubDate>Mon, 02 Oct 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;It’s a guest publication week this week - our old friend Jeff Moszuti has a bunch of technology summaries on record format libraries (specifically Apache Arrow, CarbonData and ORCFile), and some thoughts to share with us at the end of the week.&lt;/p&gt; &lt;p&gt;We’ll start today with Apache Arrow…&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/10/02/the-plan-for-this-week/</guid> </item> <item><title>Thoughts on Graph Technologies</title><link>http://ondataengineering.net/blog/2017/09/29/thoughts-on-graph-technologies/</link><pubDate>Fri, 29 Sep 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;So, I said four technology categories and three technology summaries this week. One day you’ll learn.&lt;/p&gt; &lt;p&gt;But let’s talk about what we did manage to achieve this week, specifically technology category pages on &lt;a href=&quot;/tech-categories/rdf-databases/&quot;&gt;RDF Databases&lt;/a&gt;, &lt;a href=&quot;/tech-categories/graph-databases/&quot;&gt;Graph Databases&lt;/a&gt; and &lt;a href=&quot;/tech-categories/graph-analytics/&quot;&gt;Graph Analytics&lt;/a&gt;… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;So I’m not going to go over what a graph is, but suffice it to say that if you have data that’s modelled (or can be modelled) as a graph, then you might want to consider using some graph technologies.&lt;/p&gt; &lt;p&gt;However, given it’s still just data, the primary use cases are exactly the same as any other type of data. We want to manage and master the data in some sort of operational system (giving us fine grained ACID transactions at the entity/relationship level - the so-called OLTP use case), and we want to analytics over the data (large scans over lots of data to generates insight - the so-called OLAP use case).&lt;/p&gt; &lt;p&gt;And as per other operational databases, we need to be able to query the data as well as create/update it. For operational graph databases this focuses on graph traversals - essentially finding a set of nodes and relationships that match a pattern (all books by an author named John Smith) by finding an initial subset of nodes (the name John Smith), and then following relationships to match the pattern (named and then wrote), with maybe some aggregations at the end.&lt;/p&gt; &lt;p&gt;For the operational management of graph data there are two primary technology categories:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/tech-categories/rdf-databases/&quot;&gt;RDF Databases&lt;/a&gt; implement the W3C RDF data model standard that describes data as subject–predicate–object expressions (or triples), with support for ontologies that define the list of valid subject/object and predicate types. The benefit of RDF databases is their maturity (they’ve been around forever in technology terms, with a wide range of commercial and open source technologies), but also the standardisation driven by the W3C. There’s a W3C standard query language (SPARQL) that all RDF databases support, there are standard ontologies (OWL and RDFS), and there are a vast range of RDF creation, extraction, processing and visualisation tools that will work with your data. There’s a vast amount more here around RDF and the semantic web (that allows exploitation of text content as RDF data) that I’d love to come back to one day.&lt;/p&gt; &lt;p&gt;Then you have your &lt;a href=&quot;/tech-categories/graph-databases/&quot;&gt;Graph Databases&lt;/a&gt;. Again, these are operational databases, but they have a slightly more expressive data model that RDF databases by supporting both labels (or types, although RDF nodes are also typed) and properties (name/value pairs) against nodes and link, resulting in the term “labelled property graph”. That’s not to say you can’t do properties in RDF graphs (they’re just more relationships and nodes), however there are a number of RDF databases that explicitly support properties, as well as a number of graph databases that also support RDF/SPARQL. There’s no one standard query language for graph databases, however there are two popular options. The first is Cypher, the language used by Neo4j, which now has an open source specification (&lt;a href=&quot;http://www.opencypher.org/&quot;&gt;http://www.opencypher.org/&lt;/a&gt;) and has been adopted by a number of graph databases. The second is TinkerPop Gremlin (part of the Apache TinkerPop project), however rather than a language specification this is an entire abstraction layer that can be bolted on top of a graph database, with all the associated performance implications. Neo4j is the big cheese in the graph databases space, but it’s an active thriving technology area with a wide range of commercial and open source technologies to choose from.&lt;/p&gt; &lt;p&gt;However, before you piling into graph databases, have a look at &lt;a href=&quot;https://blog.acolyer.org/2017/07/07/do-we-need-specialized-graph-databases-benchmarking-real-time-social-networking-applications/&quot;&gt;The Morning Paper review of the “Do we need specialized graph databases? Benchmarking real-time social networking applications” paper from the University of Waterloo, Ontario from May 2017&lt;/a&gt;. Considering the technologies you already have before introducing a new one is never bad advice, and the paper looks at a number of graph use cases where relational databases actually perform better than dedicated graph databases. It also looks at the performance impact of using TinkerPop Gremlin over a native API, and the results aren’t good.&lt;/p&gt; &lt;p&gt;And so on to graph analytics. As per other types of data, there are options here for analytical databases (that focus on large scanning aggregation rather than transactional workloads) as well as analytical processing engines (batch engines that run over external data, for example MapReduce/Spark over HDFS for structured data).&lt;/p&gt; &lt;p&gt;Pragel feels like the originator here - Google’s technology that executed its PageRank algorithm. This implemented (and probably popularised) the BSP execution model (which can very crudely be describes as an equivalent of MapReduce for graph data). It’s another iterative model that can be distributed across a cluster, with each iteration generating “messages” between nodes that are used as the input for the next iteration.&lt;/p&gt; &lt;p&gt;Unlike batch analytics over structured data however, the use case for batch analytics over graph data is less clear. The specialist tools that have been created that implement the BSP modek (Giraph, Hama, GraphX) have never really taken off, and all are seeing limited active development. There are a number of analytical databases (Greenplum and Aster for starters) that support graph queries using a BSP execution model, but again this hasn’t seen widespread adoption in this space. And although TinkerPop now has a graph compute model, this is only supported by a limited number of databases.&lt;/p&gt; &lt;p&gt;It feels like time will tell in this space - there doesn’t appear to be clear use cases driving new technical capabilities at the moment, but maybe machine learning will change that.&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/09/29/thoughts-on-graph-technologies/</guid> </item> <item><title>Graph Analytics</title><link>http://ondataengineering.net/tech-categories/graph-analytics/</link><pubDate>Thu, 28 Sep 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;Our list of and information on commercial and open source graph analytics engines and databases, including Giraph, Hama, GraphX, Flink, JanusGraph, Stardog, TinkerPop and alternatives to these.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Category Definition&lt;/h2&gt; &lt;!-- Tech Vendor metadata --&gt; &lt;p&gt;Technologies that support the execution of analytics over graph data in an external underlying store (often HDFS), generally over the entire graph database to generate aggregated results, identify data of interest, or to enrich the graph. Processing is often based on a BSP (Bulk Synchronous Processing) model made famous by Pregel, the model created by Google to run their PageRank algorithm. &lt;!--more--&gt;&lt;/p&gt; &lt;h2 id=&quot;pregel&quot;&gt;Pregel&lt;/h2&gt; &lt;p&gt;The Morning Paper blog from Adrian Colyer has a good &lt;a href=&quot;https://blog.acolyer.org/2015/05/26/pregel-a-system-for-large-scale-graph-processing/&quot;&gt;introduction to Pragel&lt;/a&gt;, and the original paper is also &lt;a href=&quot;http://www.dcs.bbk.ac.uk/~dell/teaching/cc/paper/sigmod10/p135-malewicz.pdf&quot;&gt;available online&lt;/a&gt;&lt;/p&gt; &lt;h2 id=&quot;analytics-engines&quot;&gt;Analytics Engines&lt;/h2&gt; &lt;p&gt;The following technologies all implement a graph analytics engine over external data, generally using a BSP execution model&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-giraph/&quot;&gt;Giraph&lt;/a&gt;&lt;/td&gt; &lt;td&gt;An iterative, highly scalable graph processing system based on Pregel and built over MapReduce&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hama/&quot;&gt;Hama&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A general purpose BSP (Bulk Synchronous Parallel) processing engine inspired by Pregel and DistBelief that runs over Mesos or YARN.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-spark/graphx/&quot;&gt;Spark/GraphX&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Spark library for processing graphs and running graph algorithms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-flink/&quot;&gt;Flink&lt;/a&gt; Gelly&lt;/td&gt; &lt;td&gt;Graph processing API and library on top of Apache Flink&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gaffer&lt;/td&gt; &lt;td&gt;Open source project for running analytics over very large graphs in HDFS, Accumulo or HBase - &lt;a href=&quot;https://github.com/gchq/Gaffer&quot;&gt;https://github.com/gchq/Gaffer&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;apache-tinkerpop&quot;&gt;Apache TinkerPop&lt;/h2&gt; &lt;p&gt;Apache TinkerPop provides support for running analytics over graphs from Gremlin using an external query engine (or GraphComputer) - see &lt;a href=&quot;http://tinkerpop.apache.org/docs/3.3.0/reference/#graphcomputer&quot;&gt;http://tinkerpop.apache.org/docs/3.3.0/reference/#graphcomputer&lt;/a&gt; for further information. TinkerPop includes GraphComputer adapters for Spark and Giraph out of the box, with the analytics generally running on an external cluster reading the data on job startup from the source graph database via TinkerPop.&lt;/p&gt; &lt;p&gt;Not all graph databases that support TinkerPop support the execution of graph analytics - those that do are listed as OLAP databases at &lt;a href=&quot;http://tinkerpop.apache.org/#graph-systems&quot;&gt;http://tinkerpop.apache.org/#graph-systems&lt;/a&gt;.&lt;/p&gt; &lt;h2 id=&quot;analytical-databases&quot;&gt;Analytical Databases&lt;/h2&gt; &lt;p&gt;See also our &lt;a href=&quot;/tech-categories/analytical-databases/&quot;&gt;Analytical Databases&lt;/a&gt; page for databases that support graph analytics, including specialist graph analytical databases&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/tech-categories/graph-analytics/</guid> </item> <item><title>The Mid Week News - 27/09/2017</title><link>http://ondataengineering.net/blog/2017/09/27/the-mid-week-news/</link><pubDate>Wed, 27 Sep 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;It’s news time again, and there are big announcements from Cloudera and Hortonworks this week… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-solr/&quot;&gt;Apache Solr&lt;/a&gt; has hit 7.0&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/alluxio/&quot;&gt;Alluxio&lt;/a&gt; is up to 1.6&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-ignite/&quot;&gt;Apache Ingite&lt;/a&gt; is up to 2.2&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-parquet/&quot;&gt;Apache Parquet&lt;/a&gt; C++ is up to 1.3&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Cloudera/Hortonworks technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The big news this week is the simultaneous big product announcements from Hortonworks and Cloudera that look like they might be similar capabilities, but I think are probably trying to solve subtly different problems - we’ll revisit these in a few weeks once there’s more information available and do some technology summaries.&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/products/sdx.html&quot;&gt;Cloudera SDX&lt;/a&gt; (Shared Data Experience, coming in CDH 5.13) appears to be trying to enable the “one” data platform experience that you get with an on premesis CDH cluster in the cloud, specifically a persistent shared storage layer with shared metadata, security and governance and a range of workloads on top. That looks different in the cloud - you probably don’t want persistent Cloudera cluster that you’re paying for by the hour even if you’re not using it - so SDX gives you a shared storage layer using cloud object storage, a shared metadata and management layer, and then the ability to run compute workloads in isolated transient workload clusters managed through Cloudera Altus. The original sales pitch of a single shared Hadoop data platform re-imagined for the cloud. More details via a &lt;a href=&quot;http://vision.cloudera.com/introducing-cloudera-sdx-a-shared-data-experience-for-the-hybrid-cloud/&quot;&gt;Cloudera VISION blog post&lt;/a&gt; and a &lt;a href=&quot;http://blog.cloudera.com/blog/2017/09/cloudera-sdx-under-the-hood/&quot;&gt;Cloudera Engineering blog post&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/products/data-management/dataplane-service/&quot;&gt;Hortonworks Data Plane&lt;/a&gt; is again all about shared metadata, security and data management, but this time across a range of different data platforms - Hadoop, relational databases and your EDW, either on-premesis or in the cloud, and for data in motion or at rest. It’s open source, extensible for adding new services, with data lifecycle management being first up, allowing you to replicate, backup &amp;amp; restore and tier your data across your data platforms. It’s another cloud service (because obviously), and they talk about it as a Global Data Management Platform. More details via a &lt;a href=&quot;https://hortonworks.com/blog/category-emerges-introducing-hortonworks-dataplane-service/&quot;&gt;Hortonworks blog post&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/mapr-file-system/mapr-db/&quot;&gt;MapR DB&lt;/a&gt; 6.0 has &lt;a href=&quot;https://community.mapr.com/community/products/blog/2017/09/25/mapr-db-60-the-modern-database-for-global-data-intensive-applications&quot;&gt;been announced&lt;/a&gt; and will be available Q4 2017. There’s been a bunch of changes in the MapR stack over the last couple of months that I’ve not been keeping up to date with (the introduction of MapR XD for starters), so we’ll loop back round in a couple of weeks to refresh our MapR information.&lt;/li&gt; &lt;li&gt;Hortonworks are &lt;a href=&quot;https://hortonworks.com/blog/3x-faster-interactive-query-hive-llap/&quot;&gt;crowing&lt;/a&gt; about the increase in &lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Hive&lt;/a&gt; performance in HDP 2.6 and its support for the full suite of 99 TPC-DS queries&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://kudu.apache.org/2017/09/18/kudu-consistency-pt1.html&quot;&gt;Part 1&lt;/a&gt; on the &lt;a href=&quot;/technologies/apache-kudu/&quot;&gt;Apache Kudu&lt;/a&gt; consitency model&lt;/li&gt; &lt;li&gt;Looks like Hortonworks’ &lt;a href=&quot;https://hortonworks.com/blog/yinception-yarn-based-container-cloud-certify-hadoop-hadoop/&quot;&gt;are proud&lt;/a&gt; of the fact they run docker containers on &lt;a href=&quot;/technologies/apache-hadoop/yarn/&quot;&gt;YARN&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.influxdata.com/blog/introduction-to-influxdatas-influxdb-and-tick-stack/&quot;&gt;An introduction&lt;/a&gt; from InfluxData on &lt;a href=&quot;/technologies/influxdb/&quot;&gt;InfluxDB&lt;/a&gt; and the TICK stack&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/09/27/the-mid-week-news/</guid> </item> <item><title>Graph Databases</title><link>http://ondataengineering.net/tech-categories/graph-databases/</link><pubDate>Tue, 26 Sep 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;Our list of and information on commercial, open source and cloud based graph databases, including Neo4j, OrientDB, JanusGraph and alternatives to these.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Category Definition&lt;/h2&gt; &lt;!-- Tech Vendor metadata --&gt; &lt;p&gt;Operational (OLTP) databases designed to support labelled property graph data models, where nodes and relationships have labels (types) and lists of property name-value pairs. Focus is on transactional ACID inserts/updates and traversal queries using index free adjacency (allows direct navigation between nodes without the use of indexes) over subsets of data rather than batch analytics over all data. Populate query interfaces/languages include Cypher (originated with Neo4J but now open source) and TinkerPop Gremlin (part of the TinkerPop framework that provides query capabilities and graph analytics over graph data). &lt;!--more--&gt;&lt;/p&gt; &lt;h2 id=&quot;further-information&quot;&gt;Further Information&lt;/h2&gt; &lt;p&gt;The Morning Paper review of the “Do we need specialized graph databases? Benchmarking real-time social networking applications” paper from the University of Waterloo, Ontario from May 2017 is worth a read before you go any further - this covers a number of use cases where relational databases perform better than dedicated graph databases, and also looks at the performance impact of using TinkerPop over a native API - &lt;a href=&quot;https://blog.acolyer.org/2017/07/07/do-we-need-specialized-graph-databases-benchmarking-real-time-social-networking-applications/&quot;&gt;https://blog.acolyer.org/2017/07/07/do-we-need-specialized-graph-databases-benchmarking-real-time-social-networking-applications/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;DB Engines has a list of graph databases at &lt;a href=&quot;https://db-engines.com/en/ranking/graph+dbms&quot;&gt;https://db-engines.com/en/ranking/graph+dbms&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The Forrester Graph Databases Market Overview (&lt;a href=&quot;https://www.forrester.com/report/Market+Overview+Graph+Databases/-/E-RES121473&quot;&gt;https://www.forrester.com/report/Market+Overview+Graph+Databases/-/E-RES121473&lt;/a&gt;) covers a number of graph databases, including DataStax Enterprise, FlockDB, Neo4j and OrientDB, and has a good introduction to RDF and Graph databases.&lt;/p&gt; &lt;p&gt;The Gartner Magic Quadrant for Operational Database Management Systems (&lt;a href=&quot;https://www.gartner.com/doc/3147919&quot;&gt;https://www.gartner.com/doc/3147919&lt;/a&gt;) also includes a number of graph databases, including Datastax Enterprise, Neo4j, NuoDB, and OrientDB. This can be downloaded from Microsoft (&lt;a href=&quot;https://info.microsoft.com/CO-SQL-CNTNT-FY16-09Sep-14-MQOperational-Register.html&quot;&gt;https://info.microsoft.com/CO-SQL-CNTNT-FY16-09Sep-14-MQOperational-Register.html&lt;/a&gt;) and Nuo (&lt;a href=&quot;http://go.nuodb.com/gartner-magic-quadrant.html&quot;&gt;http://go.nuodb.com/gartner-magic-quadrant.html&lt;/a&gt;) for free.&lt;/p&gt; &lt;p&gt;Bloor have a primer on graph technologies (&lt;a href=&quot;http://www.bloorresearch.com/research/spotlight/all-about-graphs-a-primer/&quot;&gt;http://www.bloorresearch.com/research/spotlight/all-about-graphs-a-primer/&lt;/a&gt;), a 2016 Graph and RDF Databases Market Report (&lt;a href=&quot;http://www.bloorresearch.com/research/market-report/graph-and-rdf-databases-2016/&quot;&gt;http://www.bloorresearch.com/research/market-report/graph-and-rdf-databases-2016/&lt;/a&gt;) and a 2016 Graph and RDF Databases Market Update (&lt;a href=&quot;http://www.bloorresearch.com/research/market-update/graph-and-rdf-databases-market-update-2016/&quot;&gt;http://www.bloorresearch.com/research/market-update/graph-and-rdf-databases-market-update-2016/&lt;/a&gt;), all of which are free to download for personal non-commercial use, and which cover a number of graph databases including ArangoDB, BlazeGraph, DataStax Enterprise, OrientDB and Neo4j.&lt;/p&gt; &lt;p&gt;And Neo4j have a number of eBooks available for free download from &lt;a href=&quot;https://neo4j.com/&quot;&gt;https://neo4j.com/&lt;/a&gt; and &lt;a href=&quot;https://neo4j.com/books/&quot;&gt;https://neo4j.com/books/&lt;/a&gt;, including a copy of O’Reilly’s Graph Databases.&lt;/p&gt; &lt;h2 id=&quot;query-languages--interfaces&quot;&gt;Query Languages / Interfaces&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Cypher&lt;/td&gt; &lt;td&gt;Graph query language, originally from Neo4j, now open source and used by a wide range of graph databases - &lt;a href=&quot;http://www.opencypher.org/&quot;&gt;http://www.opencypher.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;TinkerPop Gremlin&lt;/td&gt; &lt;td&gt;Query interface that uses low level adapters to execute graph traversals against any compatible graph databases - &lt;a href=&quot;https://tinkerpop.apache.org/gremlin.html&quot;&gt;https://tinkerpop.apache.org/gremlin.html&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;See the note above on the “Do we need specialized graph databases? Benchmarking real-time social networking applications” paper and the performance impact of using Apache TinkerPop&lt;/p&gt; &lt;h2 id=&quot;commercial-oltp-graph-databases&quot;&gt;Commercial OLTP Graph Databases&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Neo4j&lt;/td&gt; &lt;td&gt;ACID-compliant transactional database with native graph storage and processing; open source with commercial edition; utilises Cypher - &lt;a href=&quot;https://neo4j.com/&quot;&gt;https://neo4j.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;OrientDB&lt;/td&gt; &lt;td&gt;Multi-model (key-value, graph and document) NoSQL database with TinkerPop support and both community and enterprise editions - &lt;a href=&quot;http://orientdb.com/graph-database/&quot;&gt;http://orientdb.com/graph-database/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;ArrangoDB&lt;/td&gt; &lt;td&gt;Multi-model (key-value, graph and document) NoSQL database with ACID transactions, TinkerPop compatibility and it’s own AQL query language with support for cluster deployments (including running over Mesos) - &lt;a href=&quot;http://www.arangodb.com&quot;&gt;http://www.arangodb.com&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Sparksee&lt;/td&gt; &lt;td&gt;Graph database formally known as DEX, with support for a range of languages and use on mobile devices and TinkerPop support- &lt;a href=&quot;http://www.sparsity-technologies.com/&quot;&gt;http://www.sparsity-technologies.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;AgensGraph&lt;/td&gt; &lt;td&gt;Commercial multi-model (relational and graph) databases - &lt;a href=&quot;http://www.agensgraph.com/&quot;&gt;http://www.agensgraph.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Redis Graph&lt;/td&gt; &lt;td&gt;Graph database over Redis that supports a subset of neo4j’s Cypher query language - &lt;a href=&quot;http://redismodules.com/modules/redis-graph/&quot;&gt;http://redismodules.com/modules/redis-graph/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;open-source-oltp-graph-databases&quot;&gt;Open Source OLTP Graph Databases&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Dgraph&lt;/td&gt; &lt;td&gt;Open source graph database written in Go - &lt;a href=&quot;https://dgraph.io/&quot;&gt;https://dgraph.io/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;HyperGraphDB&lt;/td&gt; &lt;td&gt;Open Source Java graph database - &lt;a href=&quot;http://hypergraphdb.org&quot;&gt;http://hypergraphdb.org&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;InfoGrid&lt;/td&gt; &lt;td&gt;Open Source Java graph database with support for building REST APIs over the top - &lt;a href=&quot;http://infogrid.org/trac/&quot;&gt;http://infogrid.org/trac/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;VelocityDB&lt;/td&gt; &lt;td&gt;Open Source C# .NET embeddable/distributed graph database - &lt;a href=&quot;https://velocitydb.com/&quot;&gt;https://velocitydb.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Apache S2Graph (Incubating)&lt;/td&gt; &lt;td&gt;OLTP graph database built on Apache HBase - &lt;a href=&quot;https://s2graph.incubator.apache.org/&quot;&gt;https://s2graph.incubator.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;HGraphDB&lt;/td&gt; &lt;td&gt;Open Source implementation of TinkerPop API for Apache HBase - &lt;a href=&quot;https://github.com/rayokota/hgraphdb&quot;&gt;https://github.com/rayokota/hgraphdb&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Sqlg&lt;/td&gt; &lt;td&gt;Open Source implementation of TinkerPop API over relational databases - &lt;a href=&quot;http://www.sqlg.org/&quot;&gt;http://www.sqlg.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Unipop&lt;/td&gt; &lt;td&gt;Open source TinkerPop API over a range of backends including Elasticsearch and JDBC &lt;a href=&quot;https://github.com/unipop-graph/unipop&quot;&gt;https://github.com/unipop-graph/unipop&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;commercial-oltpanalytical-graph-databases&quot;&gt;Commercial OLTP/Analytical Graph Databases&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;DataStax Enterprise&lt;/td&gt; &lt;td&gt;Commercial product built on Apache Cassandra with the addition of graph and search capabilities - &lt;a href=&quot;https://www.datastax.com/&quot;&gt;https://www.datastax.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;TigerGraph&lt;/td&gt; &lt;td&gt;Commercial hybrid OLTP/OLAP graph database that claims order of magnitude performance and scalability improvements over it’s competitors; previously known as GraphSQL - &lt;a href=&quot;http://www.tigergraph.com&quot;&gt;http://www.tigergraph.com&lt;/a&gt;, &lt;a href=&quot;http://www.zdnet.com/article/tigergraph-a-graph-database-born-to-roar/&quot;&gt;http://www.zdnet.com/article/tigergraph-a-graph-database-born-to-roar/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GraphBase&lt;/td&gt; &lt;td&gt;Commercial graph database supporting operational and analytical use cases designed for use in AI applications - &lt;a href=&quot;https://graphbase.ai/&quot;&gt;https://graphbase.ai/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GRAKN.AI&lt;/td&gt; &lt;td&gt;Open Source graph database designed for AI use cases that also supports graph analytics - &lt;a href=&quot;https://grakn.ai&quot;&gt;https://grakn.ai&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Complexible Stardog&lt;/td&gt; &lt;td&gt;RDF database that also support property graphs and data virtualisation, with a community edition available - &lt;a href=&quot;http://www.stardog.com/&quot;&gt;http://www.stardog.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;open-source-oltpanalytical-graph-databases&quot;&gt;Open Source OLTP/Analytical Graph Databases&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;JanusGraph&lt;/td&gt; &lt;td&gt;Open source distributed graph database that runs over a number of storage backends (including Cassandra, HBase and BigTable), with TinkerPop support including support for graph analytics; previously known as Titan - &lt;a href=&quot;http://janusgraph.org/&quot;&gt;http://janusgraph.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;TinkerGraph&lt;/td&gt; &lt;td&gt;In memory graph databases that’s part of TinkerPop as a reference implementation, supporting both TinkerPop OLTP and OLAP use cases - &lt;a href=&quot;http://tinkerpop.apache.org&quot;&gt;http://tinkerpop.apache.org&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;combined-graphrdf-databases&quot;&gt;Combined Graph/RDF Databases&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;OpenLink Virtuoso Universal Server&lt;/td&gt; &lt;td&gt;Supports persistence of documents, relational, RDF and graph data - &lt;a href=&quot;https://virtuoso.openlinksw.com/&quot;&gt;https://virtuoso.openlinksw.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Franz AllegroGraph&lt;/td&gt; &lt;td&gt;Commercial ACID compliant that supports both RDF and property graphs, with a free edition available - &lt;a href=&quot;https://allegrograph.com/allegrograph/&quot;&gt;https://allegrograph.com/allegrograph/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;BlazeGraph&lt;/td&gt; &lt;td&gt;Open Source RDF graph database with property graph features, queryable via SPARQL and Tinkerpop - &lt;a href=&quot;https://www.blazegraph.com/&quot;&gt;https://www.blazegraph.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;graph-databases-as-a-service&quot;&gt;Graph Databases as a Service&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Microsoft Azure Cosmos DB&lt;/td&gt; &lt;td&gt;Massively scalable, low latency multi-model (key-value, graph, wide column and document) NoSQL database, including support for the Gremlin API - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/cosmos-db/&quot;&gt;https://azure.microsoft.com/en-us/services/cosmos-db/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;IBM Graph&lt;/td&gt; &lt;td&gt;Graph database as a service built using JanusGraph - &lt;a href=&quot;https://www.ibm.com/us-en/marketplace/graph&quot;&gt;https://www.ibm.com/us-en/marketplace/graph&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon Neptune (Preview)&lt;/td&gt; &lt;td&gt;Graph and RDF database service - &lt;a href=&quot;https://aws.amazon.com/neptune/&quot;&gt;https://aws.amazon.com/neptune/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;historical--deprecated-graph-databases&quot;&gt;Historical / Deprecated Graph Databases&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Objectivity InfiniteGraph&lt;/td&gt; &lt;td&gt;End of life, with functionality being migrated into Objectivity/DB and ThingSpan - &lt;a href=&quot;http://www.objectivity.com/products/infinitegraph/&quot;&gt;http://www.objectivity.com/products/infinitegraph/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;FlockDB&lt;/td&gt; &lt;td&gt;Open Source distributed graph database from Twitter, however no longer maintained - &lt;a href=&quot;https://github.com/twitter-archive/flockdb&quot;&gt;https://github.com/twitter-archive/flockdb&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GlobalsDB&lt;/td&gt; &lt;td&gt;Open Source, now dead - &lt;a href=&quot;https://github.com/Globals/GlobalsDB&quot;&gt;https://github.com/Globals/GlobalsDB&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/tech-categories/graph-databases/</guid> </item> <item><title>RDF Databases</title><link>http://ondataengineering.net/tech-categories/rdf-databases/</link><pubDate>Mon, 25 Sep 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;Our list of and information on commercial, open source and cloud based RDF databases and associated technologies, including MarkLogic, AllegroGraph, Stardog, BlazeGraph and alternatives to these.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Category Definition&lt;/h2&gt; &lt;!-- Tech Vendor metadata --&gt; &lt;p&gt;Databases designed to support the storage of RDF (or triple) data. RDF (Resource Description Framework) is a W3C data model standard that describes data as subject–predicate–object expressions (or triples). This allows the creation of graphs of knowledge, however unlike more general purpose graph databases, there is no support for properties or labels - everything is represented using triples. Data is queried using the SPARQL query language (another W3C standard). Internally, data can be considered to be stored as a single table containing three columns (the subject, predicate and object), with indexing to support the traversal and enumeration of predicates (relationships) for a given subject. A number of triple ontologies (or schemas) are also available that define standard subject/object and predicate types allowing for data interchange, including OWL and RDFS. The W3C standards were introduced to support semantic web and Linked Open Data use cases that focus on semantics and inference. &lt;!--more--&gt;&lt;/p&gt; &lt;h2 id=&quot;further-information&quot;&gt;Further Information&lt;/h2&gt; &lt;p&gt;The W3C standards for RDF and SPARQL are available online at &lt;a href=&quot;https://www.w3.org/RDF/&quot;&gt;https://www.w3.org/RDF/&lt;/a&gt; and &lt;a href=&quot;http://www.w3.org/TR/rdf-sparql-query/&quot;&gt;http://www.w3.org/TR/rdf-sparql-query/&lt;/a&gt; respectively. The relevant Wikipedia pages are also good places to start at &lt;a href=&quot;https://en.wikipedia.org/wiki/Resource_Description_Framework&quot;&gt;https://en.wikipedia.org/wiki/Resource_Description_Framework&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/SPARQL&quot;&gt;https://en.wikipedia.org/wiki/SPARQL&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;DB Engines has a list of RDF databases at &lt;a href=&quot;https://db-engines.com/en/ranking/rdf+store&quot;&gt;https://db-engines.com/en/ranking/rdf+store&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The Forrester Graph Databases Market Overview (&lt;a href=&quot;https://www.forrester.com/report/Market+Overview+Graph+Databases/-/E-RES121473&quot;&gt;https://www.forrester.com/report/Market+Overview+Graph+Databases/-/E-RES121473&lt;/a&gt;) covers a number of RDF databases, including StarDog, AllegroGraph and Oracle, and has a good introduction to RDF and Graph databases.&lt;/p&gt; &lt;p&gt;And Bloor have a primer on graph technologies (&lt;a href=&quot;http://www.bloorresearch.com/research/spotlight/all-about-graphs-a-primer/&quot;&gt;http://www.bloorresearch.com/research/spotlight/all-about-graphs-a-primer/&lt;/a&gt;), a 2016 Graph and RDF Databases Market Report (&lt;a href=&quot;http://www.bloorresearch.com/research/market-report/graph-and-rdf-databases-2016/&quot;&gt;http://www.bloorresearch.com/research/market-report/graph-and-rdf-databases-2016/&lt;/a&gt;) and a 2016 Graph and RDF Databases Market Update (&lt;a href=&quot;http://www.bloorresearch.com/research/market-update/graph-and-rdf-databases-market-update-2016/&quot;&gt;http://www.bloorresearch.com/research/market-update/graph-and-rdf-databases-market-update-2016/&lt;/a&gt;), all of which are free to download for personal non-commercial use, and which cover a number of RDF databases including AllegroGraph, BlazeGraph, Cray, GraphDB, MarkLogic, Stardog and Virtuoso.&lt;/p&gt; &lt;h2 id=&quot;rdf-frameworks&quot;&gt;RDF Frameworks&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Eclipse RDF4J&lt;/td&gt; &lt;td&gt;An Eclipse open source project for working with RDF data, including provision of a standard SPARQL interface that can be integrated with backend databases. Previously known as Sesame - &lt;a href=&quot;http://rdf4j.org/&quot;&gt;http://rdf4j.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Jena&lt;/td&gt; &lt;td&gt;Framework for developing Semantic Web and Linked Data applications in Java - &lt;a href=&quot;http://jena.apache.org/&quot;&gt;http://jena.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Commons RDF&lt;/td&gt; &lt;td&gt;Commons library for working with RDF data - &lt;a href=&quot;http://commons.apache.org/proper/commons-rdf/&quot;&gt;http://commons.apache.org/proper/commons-rdf/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Redland&lt;/td&gt; &lt;td&gt;Open source C libraries for working with RDF data - &lt;a href=&quot;http://librdf.org/&quot;&gt;http://librdf.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;CubicWeb&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.cubicweb.org/&quot;&gt;https://www.cubicweb.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;commercial-rdf-databases&quot;&gt;Commercial RDF Databases&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;MarkLogic&lt;/td&gt; &lt;td&gt;Commercial ACID compliant XML/JSON document store with support for creation of triple indexes over documents queryable via SPARQL - &lt;a href=&quot;http://www.marklogic.com/&quot;&gt;http://www.marklogic.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;OpenLink Virtuoso Universal Server&lt;/td&gt; &lt;td&gt;Supports persistence of documents, relational, RDF and graph data - &lt;a href=&quot;https://virtuoso.openlinksw.com/&quot;&gt;https://virtuoso.openlinksw.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Franz AllegroGraph&lt;/td&gt; &lt;td&gt;Commercial ACID compliant that supports both RDF and property graphs, with a free edition available - &lt;a href=&quot;https://allegrograph.com/allegrograph/&quot;&gt;https://allegrograph.com/allegrograph/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Ontotext GraphDB&lt;/td&gt; &lt;td&gt;Commercial RDF database, previously known as OWLIM, and with a free edition available - &lt;a href=&quot;https://ontotext.com/products/graphdb/&quot;&gt;https://ontotext.com/products/graphdb/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Complexible Stardog&lt;/td&gt; &lt;td&gt;RDF database that also support property graphs and data virtualisation, with a community edition available - &lt;a href=&quot;http://www.stardog.com/&quot;&gt;http://www.stardog.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Dydra&lt;/td&gt; &lt;td&gt;Cloud based - &lt;a href=&quot;https://dydra.com/&quot;&gt;https://dydra.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;SparkleDB&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.syssurge.com/Products/SparkleDB/Home.aspx&quot;&gt;https://www.syssurge.com/Products/SparkleDB/Home.aspx&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Cray Graph Engine&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.cray.com/products/analytics/cray-graph-engine&quot;&gt;https://www.cray.com/products/analytics/cray-graph-engine&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Oracle Spatial and Graph option for Oracle Database&lt;/td&gt; &lt;td&gt;Adds RDF (and graph) capabilities to the Oracle database - &lt;a href=&quot;http://www.oracle.com/technetwork/database/options/spatialandgraph/overview/index.html&quot;&gt;http://www.oracle.com/technetwork/database/options/spatialandgraph/overview/index.html&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RDF Graph for Oracle NoSQL Database&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://www.oracle.com/technetwork/database/database-technologies/nosqldb/overview/index.html&quot;&gt;http://www.oracle.com/technetwork/database/database-technologies/nosqldb/overview/index.html&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;open-source-rdf-databases&quot;&gt;Open Source RDF Databases&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;BlazeGraph&lt;/td&gt; &lt;td&gt;Open Source RDF graph database with property graph features, queryable via SPARQL and Tinkerpop - &lt;a href=&quot;https://www.blazegraph.com/&quot;&gt;https://www.blazegraph.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;4store&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://github.com/4store/4store&quot;&gt;https://github.com/4store/4store&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RedStore&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.aelius.com/njh/redstore/&quot;&gt;https://www.aelius.com/njh/redstore/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Mulgara&lt;/td&gt; &lt;td&gt;Open source java RDF database - &lt;a href=&quot;http://mulgara.org/&quot;&gt;http://mulgara.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;BrightstarDB&lt;/td&gt; &lt;td&gt;Open Source RDF database for the .NET platform - &lt;a href=&quot;http://brightstardb.com/&quot;&gt;http://brightstardb.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Strabon&lt;/td&gt; &lt;td&gt;Spatiotemporal RDF store - &lt;a href=&quot;http://www.strabon.di.uoa.gr/&quot;&gt;http://www.strabon.di.uoa.gr/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Rya (Incubating)&lt;/td&gt; &lt;td&gt;RDF triple store built on Apache Accumulo - &lt;a href=&quot;http://rya.apache.org/&quot;&gt;http://rya.apache.org/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/tech-categories/rdf-databases/</guid> </item> <item><title>The Week That Was</title><link>http://ondataengineering.net/blog/2017/09/22/the-week-that-was/</link><pubDate>Fri, 22 Sep 2017 07:45:00 +0100</pubDate> <description> &lt;p&gt;When I decided this week was graph technologies week, I think I probably knew that it was too big a topic to do in one week, and so it’s proved. So, so new content this week, but to sweeten the blow, next week there should be four technology categories and at least three technology summaries coming down the pipe.&lt;/p&gt; &lt;p&gt;So apologies there’s not much this week, but next week should be a bumper week!&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/09/22/the-week-that-was/</guid> </item> </channel> </rss>
