<?xml version="1.0" encoding="UTF-8"?> <rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"> <channel><title>OnDataEngineering</title><description>A collaborative site for independent, critical and technical thinking on the use cases, architectural patterns and technologies relating to the transformation and preparation of data for exploitation.</description><link>http://ondataengineering.net/</link><atom:link href="http://ondataengineering.net/feed.xml" rel="self" type="application/rss+xml" /> <item><title>Elasticsearch</title><link>http://ondataengineering.net/technologies/elasticsearch/</link><pubDate>Tue, 04 Jul 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;A distributed search server built on Apache Lucene that supports a number of advanced analytics over search results. Data is stored in indexes, with each index able to support multiple schemas (types), with the data itself sharded to support distributed parallel queries, with multiple replicas of each shard providing resilience and redundancy. Supports both pre-defined and schemaless types, all standard Lucene functionality (including faceting, grouping, clustering, hit highlighting, geo support, near real time indexing), the ability to update and delete documents (by id or query), upsert operations, batch operations, re-indexing (from one index into a second index), generated or calculated fields, document versioning and optimistic concurrency control, nested searches based on sub-documents or explicit parent-child document links, templated searches, a range of aggregations (include support for metrics, bucketing results, matrix calculations and custom aggregations using pipelines), custom analysers for indexing data, custom transformation pipelines prior to indexing (via an ingest node) and registered queries that are executed against newly indexed data (percolation). Comes with a REST API, with clients available for a range of languages including Java, C#, Python, JavaScript, PHP, Perl and Ruby. First released in February 2010, with a 1.0 release in February 2014. Development is led by Elastic, who were formed in 2012 by the creator of Elasticsearch and a lead Lucene contributor, and who provide commercial support and a number of commercial add-ons.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;Elastic&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;July 2017 - v5.4&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/mapr-monitoring/&quot;&gt;MapR Monitoring&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.elastic.co/products/elasticsearch&quot;&gt;https://www.elastic.co/products/elasticsearch&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.elastic.co/guide/index.html&quot;&gt;https://www.elastic.co/guide/index.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.elastic.co/blog&quot;&gt;https://www.elastic.co/blog&lt;/a&gt; - Elastic blog&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/technologies/elasticsearch/</guid> </item> <item><title>The Plan For This Week - 03/07/2017</title><link>http://ondataengineering.net/blog/2017/07/03/the-plan-for-this-week/</link><pubDate>Mon, 03 Jul 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;So this week I’m planning to look at search technologies.&lt;/p&gt; &lt;p&gt;One of the things I want to dig into and explore is the role of search in analytics - search to support users finding stuff on your website, or to find stuff in your enterprise document management system is one thing, but what role does it play if you want to analyse and understand data.&lt;/p&gt; &lt;p&gt;Which means we’ll probably limit our technology list to those technologies that support analytics rather than just search, but let’s see how we go.&lt;/p&gt; &lt;p&gt;See you on Friday…&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/07/03/the-plan-for-this-week/</guid> </item> <item><title>Streaming Data Stores</title><link>http://ondataengineering.net/blog/2017/06/30/streaming-data-stores/</link><pubDate>Fri, 30 Jun 2017 08:00:00 +0100</pubDate> <description> &lt;p&gt;So this week we’ve been looking at &lt;a href=&quot;/technologies/streaming-data-stores&quot;&gt;streaming data stores&lt;/a&gt;, technologies for the buffering and long term storage of continuous data streams for consumption by multiple downstream consumers.&lt;/p&gt; &lt;p&gt;And as part of that look, I’ve updated our &lt;a href=&quot;/technologies/apache-kafka&quot;&gt;Apache Kafka&lt;/a&gt; pages, and we’ve taken a look at some new technologies - &lt;a href=&quot;/technologies/pravega&quot;&gt;Pravega&lt;/a&gt;, the new kid on the block, and &lt;a href=&quot;/technologies/confluent-open-source&quot;&gt;Confluent Open Source&lt;/a&gt; and &lt;a href=&quot;/technologies/confluent-enterprise&quot;&gt;Confluent Enterprise&lt;/a&gt;, Confluents offerings built around Kafka.&lt;/p&gt; &lt;p&gt;So let’s spout some thoughts on streaming data stores and the technologies we’ve looked at this week. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;I going to make a very bold statement - in a few years time, I have a sneaking suspicion that streaming data stores will be seen as the biggest shakeup of the technology space for getting data into analytical systems for thirty years. Part of this is driven by the rise of streaming analytics, where these technologies are pretty much a de-facto standard for connecting streaming data flows together, but I think they’re going to become the standard for connecting any data processing chains together (most of which I think will become more real-time continuous flows anyway). What I’m really interested about with these technologies is the way they address some of the issues in building batch pipelines - without significant engineering effort these are often extreemly tightly coupled together, causing significant issues if jobs fail or you want to change your pipeline by adding in new flows or re-generating data stores. Persistent data buffers or queues help solve a lot of these problems de-coupling jobs, and although this concept exists in a number of commercial data integration tools, it’s never been a mainstream concept until now.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/technologies/apache-kafka&quot;&gt;Apache Kafka&lt;/a&gt; is obviously the technology that’s driven most of this change, and is now seeing significant traction, with commercial support available from Confluent and inclusion in most Hadoop distributions. However the market for these technologies is extreemly immature, and Kafka itself has a number of potential limitations depending on your use case - the primary one being that it’s probably no as elastic as it could be.&lt;/p&gt; &lt;p&gt;Which is why it’s nice to see new technologies appearing in this space, showing there’s a healthy demand for these technologies, and providing competition and diversity. &lt;a href=&quot;/technologies/mapr-streams&quot;&gt;MapR Streams&lt;/a&gt; is one, providing a Kafka compatible API over the MapR file system, and their are a range of cloud base services listed on the &lt;a href=&quot;/technologies/streaming-data-stores&quot;&gt;streaming data stores&lt;/a&gt; technology page, however the one we looked at this week was &lt;a href=&quot;/technologies/pravega&quot;&gt;Pravega&lt;/a&gt; - an open source product coming out of Dell EMC. They’re aiming to address what they see as the limitations of Kafka (summarised &lt;a href=&quot;https://www.slideshare.net/FlinkForward/flink-forward-sf-2017-srikanth-satya-tom-kaitchuck-pravega-storage-reimagined-for-streaming-world/50&quot;&gt;here&lt;/a&gt;), and I think they’re going to be an interesting technology to watch, but it’s still extreemly early days for them - they don’t have a production release yet, there is a significant capability gap to Kafka, and they’re going to need some significant commercial backing and vendor support if they’re going to be successful. What’s clear is that they’re off to a great start, and have already built a significant development community.&lt;/p&gt; &lt;p&gt;Confluent are the biggest backers of Kafka, and we looked at some of their offerings this week as well. They have what they call their Confluent Platform, which in two flavours - &lt;a href=&quot;/technologies/confluent-open-source&quot;&gt;Confluent Open Source&lt;/a&gt; and &lt;a href=&quot;/technologies/confluent-enterprise&quot;&gt;Confluent Enterprise&lt;/a&gt;. If you’re planning on using the open source version of Kafka, Confluent Open Source may well be worth a look, even if it’s just to adopt some of their open source components. Confluent Enterprise then gives you full support and extra management features, and what’s interesting here is that although the Hadoop vendors have started bundling Kafka, they don’t yet have an equivalent range of capabilities to Confluent.&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/06/30/streaming-data-stores/</guid> </item> <item><title>Confluent Enterprise</title><link>http://ondataengineering.net/technologies/confluent-enterprise/</link><pubDate>Thu, 29 Jun 2017 08:15:00 +0100</pubDate> <description> &lt;p&gt;A package of software built around Apache Kafka and the Confluent Open Source product, with the addition of a number of commercial closed source products including a JMS client, Control Centre (for managing Kafka clusters), Multi DC Replication (active-active replication between Kafka clusters) and Auto Data Balancing. The JMS client is an implementation of the standard JMS provider interface over a Kafka topic. Control Centre is a web based UI that supports system health monitoring (broker and topic metrics and statuses based on information from the Confluent Metrics Reporter, a plugin for Kafka clusters that reports metrics to a Kafka topic), real time stream monitoring (statistics on the production and consumption of messages including the level of consumption and latency based on statistics from Confluent Monitoring Interceptors, a plugin for Kafka producers and consumers that reports statistics to a Kafka topic), the GUI based creation of Kafka connect pipelines, viewing of cluster and topic information, and e-mail alerting based on custom triggers on on topic, consumer group or broker metrics. Multi DC Replication is an optional licenced connector for Kafka connect that enables replication between two remote Kafka clusters, including active-active synchronisation. Auto Data Balancing is a tool for re-balancing topic partitions across cluster nodes, recommending moves based on information form the Confluent Metrics Reporter and rack awareness to ensure load is distributed evenly across the cluster, and easily allowing for the additional or removal of nodes. Also includes the Confluent Support Metrics features which collects broker and cluster metadata and metrics and forwards these onto Confluent for proactive support. Confluent Enterprise is the commercial version of their Confluent Platform, with an open source version also available as Confluent Open Source. Includes full commercial support for all open and closed source products. First GA release was version 1.0 of the Confluent Platform in February 2015.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;Confluent&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;June 2017 - v3.2&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Packages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt;, &lt;a href=&quot;/technologies/confluent-open-source/&quot;&gt;Confluent Open Source&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/product/confluent-enterprise/&quot;&gt;https://www.confluent.io/product/confluent-enterprise/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://docs.confluent.io/current/&quot;&gt;http://docs.confluent.io/current/&lt;/a&gt; - Confluent Platform documentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/blog/&quot;&gt;https://www.confluent.io/blog/&lt;/a&gt; - Confluent blog&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/technologies/confluent-enterprise/</guid> </item> <item><title>Confluent Open Source</title><link>http://ondataengineering.net/technologies/confluent-open-source/</link><pubDate>Thu, 29 Jun 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;A package of open source projects built around Apache Kafka with the addition of the Confluent Schema Registry, Kafka REST Proxy, a number of connectors for Kafka Connect and a number of Kafka clients (language SDKs). The Schema Registry allows Kafka message schemas to be defined and versioned centrally, with schemas stored in a Kafka topic, a REST interface for managing schemas, support for schema evolution (with support for backwards, forwards and full compatibility between versions), plugins for Kafka clients to serialise / deserialise messages using the schemas, and support for running as a distributed service. The REST Proxy provides a REST interface onto a Kafka cluster, with support for viewing cluster metadata (covering brokers, topics, partitions and configuration) and both submitting and consuming messages, with support for JSON, JSON-encoded Avro and base64 messages, and integration to the Schema Registry for Avro messages. Bundled connectors for Kafka Connect include HDFS, JDBC, Elasticsearch and S3. Bundled client libraries (all open source) include those for C/C++, Go, .NET and Python. Also includes a Version Collector that reports version information to Confluent. Used to include Camus, a tool for unloading Kafka topics to HDFS, but this has now been deprecated in favour of Kafka Connect. Development of the open source projects is led by Confluent, who then bundle and distribute them for free as the Confluent Open Source version of their Confluent Platform, with the Confluent Enterprise version adding a number of closed source features and commercial support for all open and closed source products. Available as a zip, tar, deb or rpm package from Confluent, with all source code hosted on GitHub. First GA release was version 1.0 of the Confluent Platform in February 2015.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;Confluent&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;June 2017 - v3.2&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Packages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/confluent-enterprise/&quot;&gt;Confluent Enterprise&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/product/confluent-open-source/&quot;&gt;https://www.confluent.io/product/confluent-open-source/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://docs.confluent.io/current/&quot;&gt;http://docs.confluent.io/current/&lt;/a&gt; - Confluent Platform documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/confluentinc&quot;&gt;https://github.com/confluentinc&lt;/a&gt; - Confluent GitHub home&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/blog/&quot;&gt;https://www.confluent.io/blog/&lt;/a&gt; - Confluent blog&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/technologies/confluent-open-source/</guid> </item> <item><title>The Mid Week News - 28/06/2017</title><link>http://ondataengineering.net/blog/2017/06/28/the-mid-week-news/</link><pubDate>Wed, 28 Jun 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;News time, and it’s looking dangerously like this is becoming a weekly thing - it won’t last long. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-atlas&quot;&gt;Apache Atlas&lt;/a&gt; has graduated from the Apache incubator&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-calcite&quot;&gt;Apache Calcite&lt;/a&gt; has seen a 1.13 release&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/blog/building-real-time-streaming-etl-pipeline-20-minutes/&quot;&gt;More thoughts&lt;/a&gt; from Confluent on building ETL pipelines using &lt;a href=&quot;/technologies/apache-kafka&quot;&gt;Kafka&lt;/a&gt; and streaming technologies&lt;/li&gt; &lt;li&gt;An excellent &lt;a href=&quot;https://blog.bradfieldcs.com/you-are-not-google-84912cf44afb&quot;&gt;article&lt;/a&gt; from Ozan Onay at Bradfield School of Computer Science on why technologies built for the biggest companies in the world (Google, Amazon, LinkedIn) may not be right for you&lt;/li&gt; &lt;li&gt;From Cloudera - &lt;a href=&quot;http://blog.cloudera.com/blog/2017/06/offset-management-for-apache-kafka-with-apache-spark-streaming/&quot;&gt;how to manage your read position&lt;/a&gt; when reading from &lt;a href=&quot;/technologies/apache-kafka&quot;&gt;Kafka&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Two new updates from Cloudera on &lt;a href=&quot;/technologies/cloudera-altus&quot;&gt;Altus&lt;/a&gt; - one on &lt;a href=&quot;http://vision.cloudera.com/announcing-workload-analytics-for-cloudera-altus/&quot;&gt;workload analytics&lt;/a&gt; and the other on &lt;a href=&quot;http://blog.cloudera.com/blog/2017/06/announcing-support-for-spot-instances-in-cloudera-altus/&quot;&gt;support for AWS spot instances&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A post from Sematext on the &lt;a href=&quot;https://sematext.com/blog/2017/06/19/solr-vs-elasticsearch-differences/&quot;&gt;differences&lt;/a&gt; between &lt;a href=&quot;/technologies/apache-solr&quot;&gt;Solr&lt;/a&gt; and &lt;a href=&quot;/technologies/elastic-search&quot;&gt;Elastic Search&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A &lt;a href=&quot;https://cloud.google.com/blog/big-data/2017/06/how-qubit-deduplicates-streaming-data-at-scale-with-google-cloud-platform&quot;&gt;reference case&lt;/a&gt; from Google on how Qubit use Google Cloud Bigtable to identify (and drop) duplicate records&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/06/28/the-mid-week-news/</guid> </item> <item><title>Pravega</title><link>http://ondataengineering.net/technologies/pravega/</link><pubDate>Tue, 27 Jun 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;Technology for the buffering and long term storage of streaming data, designed for low latency and high throughput, with support for exactly once semantics, durable writes, strict ordering, dynamic scaling, transactions and long term storage backed by HDFS. Data is stored in named streams (continuous streams of bytes with serialisation and de-serialisation done in clients), with streams partitioned by a Routing Key into stream segments. Data is stored in two tiers, the first using Apache BookKeeper for recent data, the second using HDFS for long term storage, with automatic ageing of data and seamless reads across tiers. Operates on a publish/subscribe model, with subscribers able to select any point in history to read from. Supports automatic scaling of streams (dynamically increasing or decreasing the number of stream segments based on the operations per second on the stream), exactly once semantics (ensuring records are read once and once only even after failure), durable writes (data is persisted before write operations are acknowledged), transactions (multiple events can be committed in a single operation), ordered streams (events will always be read in the same order they're written), ReaderGroups (allows multiple subscribers to co-ordinate reads from a single stream) and a state synchroniser API (allowing multiple clients to synchronise arbitrary state through Pravega). Supports a Java SDK and out of the box integration with Flink, along with support for deployment using docker swarm, dc/os and AWS (all currently in development). Open sourced under an Apache 2.0 licence, started in July 2016 within Dell EMC, and does not yet have a first formal release, but is under active development by a wider range of contributors. Stated plans for future functionality include automatic deletion of data based on a retention period, support for other tier 2 storage technologies, access control, runtime metrics and Spark support.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Open Source - Active&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;June 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://pravega.io/&quot;&gt;http://pravega.io/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://pravega.io/docs/&quot;&gt;http://pravega.io/docs/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://pravega.io/docs/pravega-concepts/&quot;&gt;http://pravega.io/docs/pravega-concepts/&lt;/a&gt; - key concepts&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/pravega/pravega&quot;&gt;https://github.com/pravega/pravega&lt;/a&gt; - GitHub repo&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://blog.pravega.io/&quot;&gt;http://blog.pravega.io/&lt;/a&gt; - blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/pravega/pravega/releases&quot;&gt;https://github.com/pravega/pravega/releases&lt;/a&gt; - release history&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/technologies/pravega/</guid> </item> <item><title>Streaming Data Stores</title><link>http://ondataengineering.net/tech-categories/streaming-data-stores/</link><pubDate>Mon, 26 Jun 2017 08:00:00 +0100</pubDate> <description> &lt;p&gt;Technologies for the persistent storage of continuous streams of data, with data access based on a publish/subscribe model. Should support multiple independant publishers and subscribers, the ability to add new subscribers and replay the history of a stream, horizontal scalability and load balancing, durable writes, ordered streams (data is always read in the order it was written), high throughput and low latency characteristics, handling of updates and deletes to source records, and the ability to secure the data.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;!-- Tech Vendor metadata --&gt; &lt;h2 id=&quot;open-source-technologies&quot;&gt;Open Source Technologies&lt;/h2&gt; &lt;p&gt;The following are open source Streaming Data Store technologies:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Technology for buffering and storing real-time streams of data between publishers to subscribers, with a focus on high throughput at low latency.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/confluent-open-source/&quot;&gt;Confluent Open Source&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A package of open source projects built around &lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt; with the addition of the Confluent Schema Registry, Kafka REST Proxy, a number of connectors for Kafka Connect and a number of Kafka clients (language SDKs).&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/pravega/&quot;&gt;Pravega&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Technology for the buffering and long term storage of streaming data, designed for low latency and high throughput, with support for exactly once semantics, durable writes, strict ordering, dynamic scaling, transactions and long term storage backed by HDFS.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;commercial-technologies&quot;&gt;Commercial Technologies&lt;/h2&gt; &lt;p&gt;The following are commercial Streaming Data Store technologies:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/confluent-enterprise/&quot;&gt;Confluent Enterprise&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A commercial version of the Confluent Open Source product, with the addition of a number of commercial closed source products including a JMS client, Control Centre (for managing Kafka clusters), Multi DC Replication (active-active replication between Kafka clusters) and Auto Data Balancing.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/mapr-streams&quot;&gt;MapR Streams&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Extension to the MapR FileSystem to provide streaming data storage capabilities and a Kafka compatible API&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;technologies-available-as-a-service&quot;&gt;Technologies Available as a Service&lt;/h2&gt; &lt;p&gt;The following are Streaming Data Store technologies available as a managed service in the cloud:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Confluent Cloud&lt;/td&gt; &lt;td&gt;Confluent Enterprise as a service - &lt;a href=&quot;https://www.confluent.io/confluent-cloud/&quot;&gt;https://www.confluent.io/confluent-cloud/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon Kinesis Streams&lt;/td&gt; &lt;td&gt;Streaming data storage and publish service - &lt;a href=&quot;https://aws.amazon.com/kinesis/streams/&quot;&gt;https://aws.amazon.com/kinesis/streams/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Event Hubs&lt;/td&gt; &lt;td&gt;Elastic service for the buffering and publishing of streaming event data - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/event-hubs/&quot;&gt;https://azure.microsoft.com/en-us/services/event-hubs/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google Cloud Pub/Sub&lt;/td&gt; &lt;td&gt;Real time message and streaming data service with “at least once” delivery - &lt;a href=&quot;https://cloud.google.com/pubsub/&quot;&gt;https://cloud.google.com/pubsub/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;hadoop-distributions&quot;&gt;Hadoop Distributions&lt;/h2&gt; &lt;p&gt;&lt;a href=&quot;/technologies/apache-kafka/&quot;&gt;Apache Kafka&lt;/a&gt; is also bundled with a number of Hadoop distributions:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;Cloudera CDH&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Cloudera’s distribution of Hadoop, available in free and commercial versions and in the cloud&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-flow/&quot;&gt;Hortonworks Data Flow&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A distribution of a set of Apache open source technologies (primarily NiFi, Kafka and Storm) for processing streaming data, available for free with commercial support also available&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt; &lt;td&gt;An Apache open source distribution of Hadoop&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/tech-categories/streaming-data-stores/</guid> </item> <item><title>The Plan For This Week - 26/06/2017</title><link>http://ondataengineering.net/blog/2017/06/26/the-plan-for-this-week/</link><pubDate>Mon, 26 Jun 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;So the plan for this week is to look at streaming data stores - Kafka, Pravega and maybe some of the cloud based services.&lt;/p&gt; &lt;p&gt;Message Ends.&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/06/26/the-plan-for-this-week/</guid> </item> <item><title>Hadoop Distributions</title><link>http://ondataengineering.net/tech-categories/hadoop-distributions/</link><pubDate>Fri, 23 Jun 2017 17:30:00 +0100</pubDate> <description> &lt;p&gt;Products or services built around Hadoop (or an Hadoop compatible core) combined with a number of Hadoop compatible products. Hadoop compatibility covers the use of YARN (for resource management of multiple jobs running on the same infrastructure) and HDFS (for local storage of data with support for co-locating processing with the data).&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;!-- Tech Vendor metadata --&gt; &lt;h2 id=&quot;further-information&quot;&gt;Further Information&lt;/h2&gt; &lt;p&gt;See also our Hadoop (HDFS and YARN) &lt;a href=&quot;/tech-categories/hadoop-distributions/ecosystem/&quot;&gt;ecosystem diagrams&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also have a summary of the &lt;a href=&quot;/tech-vendors/odpi/&quot;&gt;ODPi&lt;/a&gt; organisation that’s trying to drive compatibility between Hadoop distributions.&lt;/p&gt; &lt;h2 id=&quot;commercial-distributions&quot;&gt;Commercial Distributions&lt;/h2&gt; &lt;p&gt;The following are distributions from commercial vendors for installation on pre-provisioned infrastructure, with many also including tooling for programmatically provisioning infrastructure when installing in cloud environments.&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;Cloudera CDH&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A distribution of Hadoop based on the addition of a number of closed source products, including Cloudera Manager (for installing and managing clusters), Cloudera Director (for installing in cloud environments) and Cloudera Navigator (for managing metadata and the encryption of data). Available in free and commercial editions.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A distribution of Hadoop based on a commitment to the Apache open source ecosystem, utilising only open source products with minimal extra patching. Uses Ambari for installing and managing clusters, and Cloudbreak for installing in cloud environments. Free to use with commercial support available.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/mapr-converged-data-platform/&quot;&gt;MapR Converged Data Platform&lt;/a&gt;&lt;/td&gt; &lt;td&gt;A data platform built around MapR-FS (along with MapR-DB and MapR-Streams) that provides Hadoop compatibility (via YARN and the MapR-FS HDFS compatible API) and is bundled with a package of Hadoop projects via the MapR Ecosystem Pack. Available in free and commercial editions.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Syncfusion Big Data Platform&lt;/td&gt; &lt;td&gt;Distribution for Windows - &lt;a href=&quot;https://www.syncfusion.com/products/big-data&quot;&gt;https://www.syncfusion.com/products/big-data&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;See also our &lt;a href=&quot;/tech-categories/hadoop-distributions/distribution-comparison/&quot;&gt;comparison&lt;/a&gt; of the major commercial Hadoop distributions.&lt;/p&gt; &lt;h2 id=&quot;hadoop-cloud-offerings&quot;&gt;Hadoop Cloud Offerings&lt;/h2&gt; &lt;p&gt;The following are cloud based Hadoop service offerings, supporting the programmatic provisioning and management of Hadoop clusters. Many also provide higher level APIs that allow for submission and management of individual Hadoop jobs, with some services allowing clusters to be automatically provisioned to execute a job and then terminated afterwards.&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Amazon EMR&lt;/td&gt; &lt;td&gt;Hadoop as a service, with support for a wide range of Hadoop technologies and the ability to programmatically execute Hadoop jobs and dynamically provision clusters to execute these - &lt;a href=&quot;https://aws.amazon.com/emr/&quot;&gt;https://aws.amazon.com/emr/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Microsoft Azure HD Insight&lt;/td&gt; &lt;td&gt;Hadoop service based on &lt;a href=&quot;/technologies/hortonworks-data-platform&quot;&gt;HDP&lt;/a&gt; - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/hdinsight/&quot;&gt;https://azure.microsoft.com/en-us/services/hdinsight/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google Cloud Dataproc&lt;/td&gt; &lt;td&gt;Hadoop service, with support for MapReduce, Spark, Pig and Hive, and the ability to programatically submit and manage jobs - &lt;a href=&quot;https://cloud.google.com/dataproc/&quot;&gt;https://cloud.google.com/dataproc/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/cloudera-altus/&quot;&gt;Cloudera Altus&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Platform for accessing individual CDH capabilities as services, with the first capabilities supported being the execution of Spark, MapReduce or Hive (over MapReduce or Spark) jobs using managed CDH clusters on AWS cloud infrastructure over data in Amazon S3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;SAP Cloud Platform Big Data Services (previously Altiscale)&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://cloudplatform.sap.com/capabilities/data-storage/big-data.html&quot;&gt;https://cloudplatform.sap.com/capabilities/data-storage/big-data.html&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Rackspace&lt;/td&gt; &lt;td&gt;Based on Hortonworks HDP - &lt;a href=&quot;https://www.rackspace.com/big-data&quot;&gt;https://www.rackspace.com/big-data&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qubole Data Service&lt;/td&gt; &lt;td&gt;Runs on the major clouds - &lt;a href=&quot;https://www.qubole.com/&quot;&gt;https://www.qubole.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;hadoop-hardware-appliances&quot;&gt;Hadoop Hardware Appliances&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Teradata Appliance for Hadoop&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://www.teradata.com/products-and-services/appliance-for-hadoop&quot;&gt;http://www.teradata.com/products-and-services/appliance-for-hadoop&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Oracle Big Data Appliance&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.oracle.com/engineered-systems/big-data-appliance/index.html&quot;&gt;https://www.oracle.com/engineered-systems/big-data-appliance/index.html&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;non-commercial-options&quot;&gt;Non Commercial Options&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-bigtop/&quot;&gt;Apache Bigtop&lt;/a&gt;&lt;/td&gt; &lt;td&gt;An Apache open source distribution of Hadoop. Packages up a number of Apache Hadoop components, certifies their interoperability using an automated integration test suite, and packages them up as RPMs/DEBs packages for most flavours of Linux.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;OpenStack Sahara&lt;/td&gt; &lt;td&gt;Allows provisioning of Hadoop on OpenStack - &lt;a href=&quot;https://docs.openstack.org/developer/sahara/&quot;&gt;https://docs.openstack.org/developer/sahara/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Hops&lt;/td&gt; &lt;td&gt;A distribution based on Hops HDFS and Hops YARN which use a distributed MySQL database for metadata to increase performance and scalability, available as a cloud or on premises offering - &lt;a href=&quot;http://www.hops.io&quot;&gt;http://www.hops.io&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;historical--legacy-options&quot;&gt;Historical / Legacy Options&lt;/h2&gt; &lt;p&gt;The following are either no longer available, or are now simply re-badged versions of other distributions:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Intel Distribution for Apache Hadoop&lt;/td&gt; &lt;td&gt;Focused on optimisations for Intel processors, SSD disks and networking kit; ceased when Intel invested into Cloudera - see &lt;a href=&quot;https://newsroom.intel.com/news-releases/cloudera-intel-commit-to-accelerate-and-transform-how-enterprises-use-big-data-intel-makes-significant-equity-investment-in-cloudera/&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Pivotal HD&lt;/td&gt; &lt;td&gt;Pivotal has now partnered with Hortonworks - see &lt;a href=&quot;https://hortonworks.com/press-releases/hortonworks-pivotal-expand-relationship-deliver-enterprise-ready-modern-data-platforms-data-management-analytics/&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;IBM InfoSphere BigInsights&lt;/td&gt; &lt;td&gt;IBM has now partnered with Hortonworks - see &lt;a href=&quot;https://hortonworks.com/blog/data-met-science-anything-became-possible/&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/tech-categories/hadoop-distributions/</guid> </item> <item><title>Hadoop in the Cloud</title><link>http://ondataengineering.net/blog/2017/06/23/hadoop-in-the-cloud/</link><pubDate>Fri, 23 Jun 2017 08:00:00 +0100</pubDate> <description> &lt;p&gt;So this week has been a bit of a scattergun - one technology (&lt;a href=&quot;/technologies/cloudera-altus&quot;&gt;Cloudera Altus&lt;/a&gt;), three vendors (&lt;a href=&quot;/tech-vendors/amazon-web-services&quot;&gt;Amazon Web Services&lt;/a&gt;, &lt;a href=&quot;/tech-vendors/microsoft-azure&quot;&gt;Microsoft Azure&lt;/a&gt; and &lt;a href=&quot;/tech-vendors/google-cloud-platform&quot;&gt;Google Cloud Platform&lt;/a&gt; - and yes, I know they’re not technically vendors) and one (refreshed) technology category (&lt;a href=&quot;/tech-categories/hadoop-distributions/&quot;&gt;Hadoop Distributions&lt;/a&gt;) - with the overriding theme of thinking about Hadoop in the cloud. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Every Hadoop distribution that you can install on your own physical hardware can also be installed on virtual infrastructure in a cloud in exactly the same way, however this is a spectacular way to avoid the benefits a cloud brings, and unless you’re going to be using your cluster all the time, probably an expensive way to do it given you’ll pay for your infrastructure by the hour.&lt;/p&gt; &lt;p&gt;So what every Hadoop vendor has done (with technologies like &lt;a href=&quot;/technologies/cloudera-director&quot;&gt;Cloudera Director&lt;/a&gt; and &lt;a href=&quot;/technologies/cloudbreak&quot;&gt;Cloudbreak&lt;/a&gt;), and what all the cloud providers now offer, is a way to automatically provision the infrastructure you need as part of deploying your Hadoop cluster, allowing you to spin up entire Hadoop clusters in minutes, and enabling the use of transient clusters which only exist for the period of time required to run a specific job or workload (although you’ll obviously need to persist your output into a persistent store). And we’ve now moved to Hadoop as a service, and we don’t care about the infrastructure anymore.&lt;/p&gt; &lt;p&gt;What Cloudera have done with &lt;a href=&quot;/technologies/cloudera-altus&quot;&gt;Cloudera Altus&lt;/a&gt;, and what Amazon did with Elastic Map Reduce a long time ago, is to elevate the stack one level higher to the Hadoop job - you can submit an Hadoop job (Hive, Spark, MapReduce) to Altus, and it will provision some infrastructure, create a cluster, run the job and then tear everything down. And we’ve now moved to Hadoop jobs as a service, and we don’t care about the clusters anymore.&lt;/p&gt; &lt;p&gt;If you’re working in the cloud this feels like the natural evolution of the cloud story, and to be blunt, even if you’re working on premises, having an on premises cloud that allows you to do this seems like an natural evolution (and in which case OpenStack Sahara might well be worth a look). And this feels like an area of differentiation between the various Hadoop cloud offerings at the moment - a lot of them allow you to programmatically run jobs via their API, but few allow the execution of jobs to automatically provision and tear down clusters, although you can obviously orchestrate this yourself.&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/06/23/hadoop-in-the-cloud/</guid> </item> <item><title>Google Cloud Platform</title><link>http://ondataengineering.net/tech-vendors/google-cloud-platform/</link><pubDate>Thu, 22 Jun 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;A cloud computing service operated by Google, with support for infrastructure, storage, databases and analytics services. First services were available in preview in April 2008.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Vendor Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Google&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;!-- Technology metadata --&gt; &lt;h2 id=&quot;infrastructure-services&quot;&gt;Infrastructure Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Google Compute Engine&lt;/td&gt; &lt;td&gt;Virtual servers - &lt;a href=&quot;https://cloud.google.com/compute/&quot;&gt;https://cloud.google.com/compute/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google Container Engine&lt;/td&gt; &lt;td&gt;Kubernetes cluster service - &lt;a href=&quot;https://cloud.google.com/container-engine/&quot;&gt;https://cloud.google.com/container-engine/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google Persistent Disk&lt;/td&gt; &lt;td&gt;Block storage for virtual machines - &lt;a href=&quot;https://cloud.google.com/persistent-disk/&quot;&gt;https://cloud.google.com/persistent-disk/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;storage-services&quot;&gt;Storage Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/google-cloud-storage/&quot;&gt;Google Cloud Storage&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Object store service with strong consistency, multiple storage tiers and deep integration to the Google Cloud ecosystem.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;compute-services&quot;&gt;Compute Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Google Cloud Functions&lt;/td&gt; &lt;td&gt;Serverless code execution service - &lt;a href=&quot;https://cloud.google.com/functions/&quot;&gt;https://cloud.google.com/functions/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;database-services&quot;&gt;Database Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Google Cloud Dataproc&lt;/td&gt; &lt;td&gt;Hadoop service, with support for MapReduce, Spark, Pig and Hive - &lt;a href=&quot;https://cloud.google.com/dataproc/&quot;&gt;https://cloud.google.com/dataproc/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google Cloud Spanner&lt;/td&gt; &lt;td&gt;Horizontally scalable relational database for transaction processing - &lt;a href=&quot;https://cloud.google.com/spanner/&quot;&gt;https://cloud.google.com/spanner/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google Cloud SQL&lt;/td&gt; &lt;td&gt;Managed database service, with support for MySQL and PostgreSQL - &lt;a href=&quot;https://cloud.google.com/sql/&quot;&gt;https://cloud.google.com/sql/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google Big Query&lt;/td&gt; &lt;td&gt;Analytical SQL database service - &lt;a href=&quot;https://cloud.google.com/bigquery/&quot;&gt;https://cloud.google.com/bigquery/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google Cloud Bigtable&lt;/td&gt; &lt;td&gt;NoSQL wide column store service - &lt;a href=&quot;https://cloud.google.com/bigtable/&quot;&gt;https://cloud.google.com/bigtable/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google Cloud Datastore&lt;/td&gt; &lt;td&gt;NoSQL document store service - &lt;a href=&quot;https://cloud.google.com/datastore/&quot;&gt;https://cloud.google.com/datastore/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;analytics-services&quot;&gt;Analytics Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Google Datalab&lt;/td&gt; &lt;td&gt;Web based data exploration and analysis tool based on Jupyter - &lt;a href=&quot;https://cloud.google.com/datalab/&quot;&gt;https://cloud.google.com/datalab/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google Data Studio&lt;/td&gt; &lt;td&gt;Drag and drop reporting and dashboarding tool - &lt;a href=&quot;https://cloud.google.com/data-studio/&quot;&gt;https://cloud.google.com/data-studio/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;streaming-data-services&quot;&gt;Streaming Data Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Google Cloud Pub/Sub&lt;/td&gt; &lt;td&gt;Real time message and streaming data service with “at least once” delivery - &lt;a href=&quot;https://cloud.google.com/pubsub/&quot;&gt;https://cloud.google.com/pubsub/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google Cloud Dataflow&lt;/td&gt; &lt;td&gt;Batch and streaming data flow service based on Apache Beam - &lt;a href=&quot;https://cloud.google.com/dataflow/&quot;&gt;https://cloud.google.com/dataflow/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;data-integration-services&quot;&gt;Data Integration Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Google Cloud Dataprep&lt;/td&gt; &lt;td&gt;Data preparation service (in private beta) for “visually exploring, cleaning, and preparing structured and unstructured data for analysis” - &lt;a href=&quot;https://cloud.google.com/dataprep/&quot;&gt;https://cloud.google.com/dataprep/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;machine-learning-services&quot;&gt;Machine Learning Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Google Cloud Machine Learning Engine&lt;/td&gt; &lt;td&gt;Machine learning service based on TensorFlow - &lt;a href=&quot;https://cloud.google.com/ml-engine/&quot;&gt;https://cloud.google.com/ml-engine/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google Machine Learning Services&lt;/td&gt; &lt;td&gt;Suite of machine learning services including video, image, speech and text analysis - &lt;a href=&quot;https://cloud.google.com/products/machine-learning/&quot;&gt;https://cloud.google.com/products/machine-learning/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/&quot;&gt;https://cloud.google.com/&lt;/a&gt; - Google Cloud Platform homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/products/&quot;&gt;https://cloud.google.com/products/&lt;/a&gt; - products homepage&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://cloudplatform.googleblog.com/&quot;&gt;https://cloudplatform.googleblog.com/&lt;/a&gt; - Google Cloud Platform blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/blog/big-data/&quot;&gt;https://cloud.google.com/blog/big-data/&lt;/a&gt; - Google Cloud Platform Big Data and Machine Learning blog&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/tech-vendors/google-cloud-platform/</guid> </item> <item><title>The Mid Week News - 21/06/2017</title><link>http://ondataengineering.net/blog/2017/06/21/the-mid-week-news/</link><pubDate>Wed, 21 Jun 2017 08:00:00 +0100</pubDate> <description> &lt;p&gt;Time for some news, and only a week since we last did it! &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-pig&quot;&gt;Apache Pig&lt;/a&gt; has seen a 0.17 release, with support for using Spark as an execution engine introduced to complement the existing support for Tez and MapReduce&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-kudu&quot;&gt;Apache Kudu&lt;/a&gt; has seen a 1.4 release&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudbreak&quot;&gt;Cloudbreak&lt;/a&gt; has seen a 1.16 release, adding support for Hortonworks Flex Support Subscription&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-impala&quot;&gt;Apache Impala&lt;/a&gt; has seen a 2.9 release&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/hortonworks-data-cloud-for-aws&quot;&gt;Hortonworks Data Cloud for AWS&lt;/a&gt; has a tech preview of it’s 2.0 release&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://www.zdnet.com/article/kafka-the-story-so-far/&quot;&gt;Interesting post&lt;/a&gt; on the history of &lt;a href=&quot;/technologies/apache-kafka&quot;&gt;Kafka&lt;/a&gt; from ZDNet&lt;/li&gt; &lt;li&gt;Yahoo have &lt;a href=&quot;https://yahooeng.tumblr.com/post/161855616651/open-sourcing-bullet-yahoos-forward-looking&quot;&gt;open sourced Bullet&lt;/a&gt;, a “forward looking query engine” for streaming data&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.datanami.com/2017/06/14/hortonworks-shifts-focus-streaming-analytics/&quot;&gt;A view&lt;/a&gt; from Datanami on the latest &lt;a href=&quot;/technologies/hortonworks-data-flow&quot;&gt;HDF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/part-4-sams-stream-builder-building-complex-stream-analytics-apps-without-code/&quot;&gt;Part 4&lt;/a&gt; of Hortonworks intro to &lt;a href=&quot;/technologies/hortonworks-data-flow&quot;&gt;HDF&lt;/a&gt; 3.0 looking at stream builder (the GUI for building streaming flows)&lt;/li&gt; &lt;li&gt;A little old, but a still useful &lt;a href=&quot;http://www.dbms2.com/2016/08/21/introduction-to-data-artisans-and-flink/&quot;&gt;view&lt;/a&gt; on Data Artisans and &lt;a href=&quot;/technologies/apache-flink&quot;&gt;Flink&lt;/a&gt; from Curt Monash&lt;/li&gt; &lt;li&gt;A more recent post from Curt, with &lt;a href=&quot;http://www.dbms2.com/2017/06/14/cloudera-altus/&quot;&gt;his views&lt;/a&gt; on &lt;a href=&quot;/technologies/cloudera-altus&quot;&gt;Cloudera Altus&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blog.knoldus.com/2017/06/13/spark-streaming-vs-kafka-stream/&quot;&gt;A view&lt;/a&gt; on &lt;a href=&quot;/technologies/apache-spark/spark-streaming&quot;&gt;Spark Streaming&lt;/a&gt; vs &lt;a href=&quot;/technologies/apache-kafka/kafka-streams&quot;&gt;Kafka Streams&lt;/a&gt;&lt;/li&gt; &lt;li&gt;More views on the IBM-Hortonworks &lt;a href=&quot;/technologies/hortonworks-data-platform&quot;&gt;HDP&lt;/a&gt; deal from &lt;a href=&quot;https://www.theregister.co.uk/2017/06/15/ibm_adopts_hortonworks_for_hadoop_distribution/&quot;&gt;The Register&lt;/a&gt; and &lt;a href=&quot;http://blogs.gartner.com/merv-adrian/2017/06/21/ibm-ends-hadoop-distribution-hortonworks-expands-hybrid-open-source/&quot;&gt;Gartner&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Hortonworks are &lt;a href=&quot;https://hortonworks.com/blog/hortonworks-when-youre-hot-youre-really-hot/&quot;&gt;blowing their trumpet&lt;/a&gt; (no pun intended) on their &lt;a href=&quot;/technologies/hortonworks-data-flow&quot;&gt;HDF&lt;/a&gt; 3.0 release and their IBM &lt;a href=&quot;/technologies/hortonworks-data-platform&quot;&gt;HDP&lt;/a&gt; deal&lt;/li&gt; &lt;li&gt;Cloudera have published &lt;a href=&quot;http://blog.cloudera.com/blog/2017/06/solr-memory-tuning-for-production-part-2/&quot;&gt;part 2&lt;/a&gt; of their Solr memory tuning guide&lt;/li&gt; &lt;li&gt;And finally, Databricks’ &lt;a href=&quot;https://databricks.com/blog/2017/05/31/top-5-reasons-for-choosing-s3-over-hdfs.html&quot;&gt;view&lt;/a&gt; on &lt;a href=&quot;/tech-categories/object-stores&quot;&gt;object storage&lt;/a&gt; (specifically S3) vs &lt;a href=&quot;/tech-categories/hadoop-compatible-filesystems&quot;&gt;HDFS&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/06/21/the-mid-week-news/</guid> </item> <item><title>Microsoft Azure</title><link>http://ondataengineering.net/tech-vendors/microsoft-azure/</link><pubDate>Wed, 21 Jun 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;A cloud computing service operated by Microsoft, with support for infrastructure, storage, databases and analytics services, available in 34 geographical regions. Announced in Otober 2008, with first services available in February 2010. Previously known as Windows Azure.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Vendor Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Azure&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;!-- Technology metadata --&gt; &lt;h2 id=&quot;infrastructure-services&quot;&gt;Infrastructure Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Azure Virtual Machines&lt;/td&gt; &lt;td&gt;Virtual servers with support for a range of operating systems and pre-build images, and for management at scale (Virtual Machine Scale Sets) - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/virtual-machines/&quot;&gt;https://azure.microsoft.com/en-us/services/virtual-machines/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Disk Storage&lt;/td&gt; &lt;td&gt;Persistent disk storage to support virtual machines - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/storage/unmanaged-disks/&quot;&gt;https://azure.microsoft.com/en-us/services/storage/unmanaged-disks/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;storage-services&quot;&gt;Storage Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Azure File Storage&lt;/td&gt; &lt;td&gt;Network storage, mountable on multiple machines over REST and SMB - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/storage/files/&quot;&gt;https://azure.microsoft.com/en-us/services/storage/files/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/microsoft-azure-blob-storage/&quot;&gt;Azure Blob Storage&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Highly scalable and resilient object storage&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/microsoft-azure-data-lake-store/&quot;&gt;Azure Data Lake Store&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Massively scalable HDFS compatible filesystem as a service, based on Microsoft’s Cosmos technology&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;compute-services&quot;&gt;Compute Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Azure Functions&lt;/td&gt; &lt;td&gt;Service for executing arbitrary code in response to a trigger or timer - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/functions/&quot;&gt;https://azure.microsoft.com/en-us/services/functions/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Batch&lt;/td&gt; &lt;td&gt;Service for executing batch jobs across a pool of compute servers - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/batch/&quot;&gt;https://azure.microsoft.com/en-us/services/batch/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;database-services&quot;&gt;Database Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Azure HDInsight&lt;/td&gt; &lt;td&gt;Hadoop service based on &lt;a href=&quot;/technologies/hortonworks-data-platform&quot;&gt;HDP&lt;/a&gt;, with support a range of Hadoop technologies including Spark, Hive, MapReduce, HBase, Storm, Kafka, and Microsoft R Server - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/hdinsight/&quot;&gt;https://azure.microsoft.com/en-us/services/hdinsight/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure SQL Database&lt;/td&gt; &lt;td&gt;Scalable relational database - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/sql-database/&quot;&gt;https://azure.microsoft.com/en-us/services/sql-database/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Database for MySQL&lt;/td&gt; &lt;td&gt;MySQL as a service - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/mysql/&quot;&gt;https://azure.microsoft.com/en-us/services/mysql/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Database for PostgreSQL&lt;/td&gt; &lt;td&gt;PostgreSQL as a service - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/postgresql/&quot;&gt;https://azure.microsoft.com/en-us/services/postgresql/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure SQL Data Warehouse&lt;/td&gt; &lt;td&gt;Scalable analytical database - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/sql-data-warehouse/&quot;&gt;https://azure.microsoft.com/en-us/services/sql-data-warehouse/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Redis Cache&lt;/td&gt; &lt;td&gt;Redis as a service - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/cache/&quot;&gt;https://azure.microsoft.com/en-us/services/cache/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Table Storage&lt;/td&gt; &lt;td&gt;A NoSQL wide column store service - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/storage/tables/&quot;&gt;https://azure.microsoft.com/en-us/services/storage/tables/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Cosmos DB&lt;/td&gt; &lt;td&gt;Massively scalable, low latency multi-model (key-value, graph and document) NoSQL database, previously known as Azure DocumentDB - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/cosmos-db/&quot;&gt;https://azure.microsoft.com/en-us/services/cosmos-db/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Search&lt;/td&gt; &lt;td&gt;Search service - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/search/&quot;&gt;https://azure.microsoft.com/en-us/services/search/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;analytics-services&quot;&gt;Analytics Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Azure Data Lake Analytics&lt;/td&gt; &lt;td&gt;Massively parallel analytics job service, with support for U-SQL, R, Python, and .NET - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/data-lake-analytics/&quot;&gt;https://azure.microsoft.com/en-us/services/data-lake-analytics/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Data Catalog&lt;/td&gt; &lt;td&gt;A metadata catalog service - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/data-catalog/&quot;&gt;https://azure.microsoft.com/en-us/services/data-catalog/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Time Series Insights&lt;/td&gt; &lt;td&gt;Storage, analytics and visualisation service for time series data, currently in preview - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/time-series-insights/&quot;&gt;https://azure.microsoft.com/en-us/services/time-series-insights/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;streaming-data-services&quot;&gt;Streaming Data Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Azure Event Hubs&lt;/td&gt; &lt;td&gt;Elastic service for the buffering and publishing of streaming event data - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/event-hubs/&quot;&gt;https://azure.microsoft.com/en-us/services/event-hubs/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Stream Analytics&lt;/td&gt; &lt;td&gt;Service for querying streams of data using a SQL like language - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/stream-analytics/&quot;&gt;https://azure.microsoft.com/en-us/services/stream-analytics/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;data-integration-services&quot;&gt;Data Integration Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Azure Data Factory&lt;/td&gt; &lt;td&gt;Data transformation workflow management - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/data-factory/&quot;&gt;https://azure.microsoft.com/en-us/services/data-factory/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Scheduler&lt;/td&gt; &lt;td&gt;Scheduling as a service - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/scheduler/&quot;&gt;https://azure.microsoft.com/en-us/services/scheduler/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;machine-learning-services&quot;&gt;Machine Learning Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Azure Machine Learning&lt;/td&gt; &lt;td&gt;Machine learning service - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/machine-learning/&quot;&gt;https://azure.microsoft.com/en-us/services/machine-learning/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Azure Cognitive Services&lt;/td&gt; &lt;td&gt;Suite of services including vision, speech and text analysis - &lt;a href=&quot;https://azure.microsoft.com/en-us/services/cognitive-services/&quot;&gt;https://azure.microsoft.com/en-us/services/cognitive-services/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://azure.microsoft.com/&quot;&gt;https://azure.microsoft.com/&lt;/a&gt; - Amazon Web Services homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://azure.microsoft.com/en-us/services/&quot;&gt;https://azure.microsoft.com/en-us/services/&lt;/a&gt; - products homepage&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://azure.microsoft.com/en-us/blog/&quot;&gt;https://azure.microsoft.com/en-us/blog/&lt;/a&gt; - Azure Blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://azure.microsoft.com/en-us/blog/topics/announcements/&quot;&gt;https://azure.microsoft.com/en-us/blog/topics/announcements/&lt;/a&gt; - Azure Blog (announcements)&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/tech-vendors/microsoft-azure/</guid> </item> <item><title>Amazon Web Services</title><link>http://ondataengineering.net/tech-vendors/amazon-web-services/</link><pubDate>Tue, 20 Jun 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;A subsidiary of Amazon.com that provides infrastructure and platform cloud services, including virtual machines and storage infrastructure services and plus database, analytics, real time data processing and data pipeline platform services, with services available in 16 geographical regions. Launched to support internal Amazon.com services in July 2002, with the first launch of a public service in November 2004, and now comfortably the largest cloud services provider.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Vendor Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Amazon&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;!-- Technology metadata --&gt; &lt;h2 id=&quot;infrastructure-services&quot;&gt;Infrastructure Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Amazon EC2&lt;/td&gt; &lt;td&gt;Virtual servers, with options to support native hosting of docker containers (Amazon EC2 Container Service) - &lt;a href=&quot;https://aws.amazon.com/ec2/&quot;&gt;https://aws.amazon.com/ec2/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon EBS&lt;/td&gt; &lt;td&gt;Block storage, used to provide local storage for virtual machine - &lt;a href=&quot;https://aws.amazon.com/ebs/&quot;&gt;https://aws.amazon.com/ebs/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;storage-services&quot;&gt;Storage Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Amazon Elastic File System&lt;/td&gt; &lt;td&gt;Network storage, mountable on multiple machines over NFS - &lt;a href=&quot;https://aws.amazon.com/efs/&quot;&gt;https://aws.amazon.com/efs/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/amazon-s3&quot;&gt;Amazon S3&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Highly scalable and resilient object storage, including local cost archival storage (Amazon Glacier)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;compute-services&quot;&gt;Compute Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;AWS Lambda&lt;/td&gt; &lt;td&gt;Service for executing arbitrary code in response to a triggers - &lt;a href=&quot;https://aws.amazon.com/lambda/&quot;&gt;https://aws.amazon.com/lambda/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;AWS Batch&lt;/td&gt; &lt;td&gt;Service for executing arbitrary batch jobs - &lt;a href=&quot;https://aws.amazon.com/batch/&quot;&gt;https://aws.amazon.com/batch/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;database-services&quot;&gt;Database Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Amazon EMR&lt;/td&gt; &lt;td&gt;Hadoop service, with support for a range of Hadoop technologies including HBase, Hive, Hue, Mahout, Oozie, Phoenix, Presto, Pig, Spark, Sqoop and Zeppelin - &lt;a href=&quot;https://aws.amazon.com/emr/&quot;&gt;https://aws.amazon.com/emr/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon Relational Database Service&lt;/td&gt; &lt;td&gt;Relational database service, with support for Amazon Aurora (a MySQL/PostgreSQL compatible database), PostgreSQL, MySQL, MariaDB, Oracle, and Microsoft SQL Server - &lt;a href=&quot;https://aws.amazon.com/rds/&quot;&gt;https://aws.amazon.com/rds/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon Redshift&lt;/td&gt; &lt;td&gt;A MPP analytical database, with support for columnar storage and the ability to query data in Amazon S3 as external tables (Redshift Spectrum) - &lt;a href=&quot;https://aws.amazon.com/redshift/&quot;&gt;https://aws.amazon.com/redshift/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon Elasticache&lt;/td&gt; &lt;td&gt;Redis or Memcached service - &lt;a href=&quot;https://aws.amazon.com/elasticache/&quot;&gt;https://aws.amazon.com/elasticache/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon DynamoDB&lt;/td&gt; &lt;td&gt;A NoSQL document store service - &lt;a href=&quot;https://aws.amazon.com/dynamodb/&quot;&gt;https://aws.amazon.com/dynamodb/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon Cloudsearch&lt;/td&gt; &lt;td&gt;Amazon search service - &lt;a href=&quot;https://aws.amazon.com/cloudsearch/&quot;&gt;https://aws.amazon.com/cloudsearch/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon ElasticSearch Service&lt;/td&gt; &lt;td&gt;Elasticsearch service - &lt;a href=&quot;https://aws.amazon.com/elasticsearch-service/&quot;&gt;https://aws.amazon.com/elasticsearch-service/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;analytics-services&quot;&gt;Analytics Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Amazon Athena&lt;/td&gt; &lt;td&gt;SQL query service over data in Amazon S3 - &lt;a href=&quot;https://aws.amazon.com/athena/&quot;&gt;https://aws.amazon.com/athena/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon Quicksight&lt;/td&gt; &lt;td&gt;Web based analytics and visualisation tool - &lt;a href=&quot;https://quicksight.aws/&quot;&gt;https://quicksight.aws/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;streaming-data-services&quot;&gt;Streaming Data Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Amazon Kinesis&lt;/td&gt; &lt;td&gt;Suite of services relating to the processing and analytics of streaming data - &lt;a href=&quot;https://aws.amazon.com/kinesis/&quot;&gt;https://aws.amazon.com/kinesis/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon Kinesis Streams&lt;/td&gt; &lt;td&gt;Streaming data storage and publish service - &lt;a href=&quot;https://aws.amazon.com/kinesis/streams/&quot;&gt;https://aws.amazon.com/kinesis/streams/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon Kinesis Firehose&lt;/td&gt; &lt;td&gt;Streaming data movement, with support for basic transformation including routing, splitting and batching - &lt;a href=&quot;https://aws.amazon.com/kinesis/firehose/&quot;&gt;https://aws.amazon.com/kinesis/firehose/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon Kinesis Analytics&lt;/td&gt; &lt;td&gt;SQL over data streams for transformation or analytics - &lt;a href=&quot;https://aws.amazon.com/kinesis/analytics/&quot;&gt;https://aws.amazon.com/kinesis/analytics/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;data-integration-services&quot;&gt;Data Integration Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;AWS Data Pipelines&lt;/td&gt; &lt;td&gt;Data transformation workflow management, with support for scheduling and dependancy management of jobs (e.g. EMR jobs or SQL queries) - &lt;a href=&quot;https://aws.amazon.com/datapipeline/&quot;&gt;https://aws.amazon.com/datapipeline/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;AWS Glue&lt;/td&gt; &lt;td&gt;Service for integrating data, with support for automating data discovery, conversion, mapping, and job scheduling, currently in preview only - &lt;a href=&quot;https://aws.amazon.com/glue/&quot;&gt;https://aws.amazon.com/glue/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;machine-learning-services&quot;&gt;Machine Learning Services&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Amazon Rekognition&lt;/td&gt; &lt;td&gt;Service for searching and analysing images - &lt;a href=&quot;https://aws.amazon.com/rekognition/&quot;&gt;https://aws.amazon.com/rekognition/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Amazon Machine Learning&lt;/td&gt; &lt;td&gt;Machine learning service - &lt;a href=&quot;https://aws.amazon.com/machine-learning/&quot;&gt;https://aws.amazon.com/machine-learning/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/&quot;&gt;https://aws.amazon.com/&lt;/a&gt; - Amazon Web Services homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/products/&quot;&gt;https://aws.amazon.com/products/&lt;/a&gt; - products homepage&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/blogs/big-data/&quot;&gt;https://aws.amazon.com/blogs/big-data/&lt;/a&gt; - AWS Big Data blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/blogs/database/&quot;&gt;https://aws.amazon.com/blogs/database/&lt;/a&gt; - AWS Databases blog&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/new/&quot;&gt;https://aws.amazon.com/new/&lt;/a&gt; - AWS Announcements&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/tech-vendors/amazon-web-services/</guid> </item> <item><title>Cloudera Altus</title><link>http://ondataengineering.net/technologies/cloudera-altus/</link><pubDate>Mon, 19 Jun 2017 08:00:00 +0100</pubDate> <description> &lt;p&gt;Platform for accessing individual CDH capabilities as services, with the first capabilities supported being the execution of Spark, MapReduce or Hive (over MapReduce or Spark) jobs using managed CDH clusters on AWS cloud infrastructure over data in Amazon S3. Jobs run on clusters within a defined AWS environment, which can be transient (created and terminated on demand) or persistent, with each cluster supporting one service type (Hive, Spark, MapReduce) with a fixed node count. Jobs can then be queued individually or in batch for execution against an existing cluster or against a dynamically created cluster, with jobs specified either by uploading a JAR to S3 (for Spark or MapReduce) or via a Hive script (either directly uploaded or uploaded to S3), and the ability to either halt or continue the queue on job failure. Supports access to clusters via SSH, read only access to Cloudera Manager, a SOCKS proxy to cluster web UIs (including the CM admin console, YARN history server and Spark history server), and access to server and workload logs (including the ability to write these to S3 for access after clusters have been terminated). All AWS nodes managed by Altus are tagged with the cluster name and node role (master, worker or Cloudera Manager) and bootstrap scripts can be specified for execution on nodes after cluster startup. Supports a web based UI and (Python) CLI, with full user authentication and role based access management, and integration with AWS security. Stated plan is to expand support to other cloud service providers (for example Azure and Google Cloud), and other CDH services (for example Data Science workloads). Launched in May 2017, with per node / per hour pricing.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Altus&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;June 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Uses&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/cloudera-cdh/&quot;&gt;Cloudera CDH&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/products/altus.html&quot;&gt;https://www.cloudera.com/products/altus.html&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://community.cloudera.com/t5/Community-News-Release/Announce-Cloudera-Altus-is-now-available/m-p/55007&quot;&gt;https://community.cloudera.com/t5/Community-News-Release/Announce-Cloudera-Altus-is-now-available/m-p/55007&lt;/a&gt; - announcement&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://vision.cloudera.com/simplifying-big-data-in-the-cloud/&quot;&gt;http://vision.cloudera.com/simplifying-big-data-in-the-cloud/&lt;/a&gt; - Cloudera Vision blog post&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2017/05/data-engineering-with-cloudera-altus/&quot;&gt;http://blog.cloudera.com/blog/2017/05/data-engineering-with-cloudera-altus/&lt;/a&gt; - Cloudera engineering blog post&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/documentation/altus.html&quot;&gt;https://www.cloudera.com/documentation/altus.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/technologies/cloudera-altus/</guid> </item> <item><title>The Plan For This Week - 19/06/2017</title><link>http://ondataengineering.net/blog/2017/06/19/the-plan-for-this-week/</link><pubDate>Mon, 19 Jun 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;As promised we’re going to start of this week by looking at Cloudera Altus, and then, despite the fact I’ve said we’ll be looking at different technology categories going forward, I want to send a few days summarising the capabilities available from AWS, Azure and Google Cloud (because I have a bunch of notes I want to write up). I then plan to close the week by actually doing what I said we’d be doing and adding a technology category, this time looking at Hadoop capabilities offered as a service (e.g. Altus), with a little rework on the &lt;a href=&quot;/tech-categories/hadoop-distributions/&quot;&gt;Hadoop Distributions&lt;/a&gt; page at the same time.&lt;/p&gt; &lt;p&gt;Have a great week, and if you’re in the UK, try to stay cool.&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/06/19/the-plan-for-this-week/</guid> </item> <item><title>The Week That Was - 16/06/2017</title><link>http://ondataengineering.net/blog/2017/06/16/the-week-that-was/</link><pubDate>Fri, 16 Jun 2017 08:00:00 +0100</pubDate> <description> &lt;p&gt;Let’s reminder ourselves of the plan for this week - &lt;a href=&quot;/technologies/microsoft-azure-data-lake-store&quot;&gt;Azure Data Lake Store&lt;/a&gt;, &lt;a href=&quot;/technologies/druid&quot;&gt;Druid&lt;/a&gt;, Cloudera Altus, &lt;a href=&quot;/technologies/apache-superset&quot;&gt;Apache Superset&lt;/a&gt; and Pravega. How did I do? Three out of five.&lt;/p&gt; &lt;p&gt;I’ll come back to Cloudera Altus first thing next week, and Pravega by looking at streaming data stores in the near future, but this week ended up being dominated by serendipity and Hortonworks’ &lt;a href=&quot;/technologies/hortonworks-data-flow&quot;&gt;HDF&lt;/a&gt; 3.0 release (and their two new technologies - &lt;a href=&quot;/technologies/schema-registry&quot;&gt;Schema Registry&lt;/a&gt; and &lt;a href=&quot;/technologies/streaming-analytics-manager&quot;&gt;Streaming Analytics Manager&lt;/a&gt;), and by a desire to have some content on some new and breaking stuff.&lt;/p&gt; &lt;p&gt;Oh, and this weeks news post ended up being a bit of a bumper post, with some stuff we need to dig into. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Let’s take the week in chronological order.&lt;/p&gt; &lt;p&gt;We started off by looking at &lt;a href=&quot;/technologies/microsoft-azure-data-lake-store&quot;&gt;Azure Data Lake Store&lt;/a&gt;, a wrap up from our lookup at &lt;a href=&quot;/tech-categories/hadoop-compatible-filesystems&quot;&gt;Hadoop Compatible Filesystems&lt;/a&gt;. If you’re working in the cloud, your options to date have been one of the big object stores (such as Amazon S3), but that’s going to give you limitations on the size of an individual file, and a performance hit based on the inability to read files in a massive parallel way. Azure Data Lake Store (appears to be) a pretty unique offering in the space, giving an HDFS compatible filesystem that addresses these limitations, but at huge scale in the cloud.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/technologies/druid&quot;&gt;Druid&lt;/a&gt; came next, by dint of it being included in tech preview in &lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;HDP&lt;/a&gt; 2.6. It seems a far more popular open source project than I’d considered, with some serious deployments, and delivers on a use case that traditionally would have required significant hardware and software investment to do at scale. What’s interesting is Hortonworks interest with it having been significant committers for a while, their plan to integrate it with Hive, and the fact they’re now bundling it with both HDP and HDF (although it’s in tech preview in both). That means they see a significant future for it, and to be honest I think that’s a pretty good bet.&lt;/p&gt; &lt;p&gt;Which leads us on to &lt;a href=&quot;/technologies/apache-superset&quot;&gt;Apache Superset&lt;/a&gt;, recently donated to the Apache Foundation, and the tool of choice for use with Druid. Again, it hits a use case that’s traditionally been the preserve of commercial products, and again Hortonworks are going in, having been committers for a while and now bundling it (indirectly) with HDF (see later). Along with Druid, it’s got to be well worth a look if you have sort of requirement for delivering OLAP / cube type capabilities to end users.&lt;/p&gt; &lt;p&gt;As part of the mid week news, we took a quick stop off to look at the bucket load of technologies that have been deprecated as part of HDP 2.6. &lt;a href=&quot;/technologies/apache-accumulo&quot;&gt;Apache Accumulo&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-kafka&quot;&gt;Apache Kafka&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-storm&quot;&gt;Apache Storm&lt;/a&gt; are going as they’re being moved into other Hortonworks products - Kafka and Storm into HDF (one presumes), but I’m less sure about Accumulo. Perhaps it’s destined to become an add-on like Hawq and Solr - time will tell I guess. Then there’s &lt;a href=&quot;/technologies/apache-flume&quot;&gt;Apache Flume&lt;/a&gt; (advice is to consider &lt;a href=&quot;/technologies/hortonworks-data-flow&quot;&gt;HDF&lt;/a&gt; instead), &lt;a href=&quot;/technologies/apache-mahout&quot;&gt;Apache Mahout&lt;/a&gt; (advice is to consider &lt;a href=&quot;/technologies/spark/mllib&quot;&gt;Spark MLLib&lt;/a&gt; instead), &lt;a href=&quot;/technologies/apache-slider&quot;&gt;Apache Slider&lt;/a&gt; (being folded into YARN) and &lt;a href=&quot;/technologies/hue&quot;&gt;Hue&lt;/a&gt; (advice is to consider &lt;a href=&quot;/technologies/apache-ambari/ambari-views&quot;&gt;Ambari Views&lt;/a&gt; instead). The more interesting one is &lt;a href=&quot;/technologies/apache-falcon&quot;&gt;Apache Falcon&lt;/a&gt;, which looks dead in the water with no commits for a number of months now, and with no clear replacement. The suggestion is that something’s coming, but it’s not clear what. If we get NiFi with intermediate files stored in HDFS and the ability to run arbitrary Spark / MapReduce jobs as processors then that would be lovely!&lt;/p&gt; &lt;p&gt;And so on to &lt;a href=&quot;/technologies/hortonworks-data-flow&quot;&gt;HDF&lt;/a&gt; 3.0. This looks like a big release for Hortonworks - they’re going all in with streaming data (IoT / analysis of data in movement / however else they’re selling it), and I can’t shake the feeling that they’re stealing a march on a bunch of competitors by selling an integrated set of technologies that fit this space, combined with the required security and governance bits. And yes, both MapR and Cloudera bundle Kafka and some sort of streaming tech (Storm for MapR, Spark Streaming for Cloudera), but it’s not a focus in the same way it is for Hortonworks. And then the announcements this week around the new technologies they’re adding to HDF just re-enforces the fact that they’re talking streaming analytics far more seriously that they’re competitors, and I think they’re going to reap the rewards.&lt;/p&gt; &lt;p&gt;So first up for HDF 3.0 was &lt;a href=&quot;/technologies/schema-registry&quot;&gt;Schema Registry&lt;/a&gt;. It fills a gap that Confluent had filled for Kafka with a commercial solution, but across all the technologies in the HDF stack. Having multiple jobs reading and writing the same data means that they all need to know the schema and understand / be updated as when the schema changes. You can solve this by bundling the schema in with the data (e.g. with &lt;a href=&quot;/technologies/apache-avro&quot;&gt;Avro&lt;/a&gt;), but the overhead of this when you’re dealing with individual records is huge. So you stick a schema version number of the record, and you go and get the actually schema from the registry. Interoperability between jobs and the ability to evolve schemas in streaming solutions - done. But it seems like Hortonworks have bigger plans for this product, with the idea that it could support other items such as business rules and machine learning models that need to be re-used in multiple places.&lt;/p&gt; &lt;p&gt;And then they’re their &lt;a href=&quot;/technologies/streaming-analytics-manager&quot;&gt;Streaming Analytics Manager&lt;/a&gt;) - a collection of bits that are designed to make building and operating streaming analytics solutions easier and simpler and to reduce the bar to entry. The operations stuff, and the bundling of &lt;a href=&quot;/technologies/druid&quot;&gt;Druid&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-superset&quot;&gt;Apache Superset&lt;/a&gt; to analyse the results of streaming analytics feels like a no brainer. The graphical GUI to building streaming apps over your streaming engine of choice (starting with Storm obviously) is going to be an interesting one to track to see what sort of uptake there is. I’m sure they’ll be views that say this is going to be a lowest common denominator solution, that it will lack flexibility and control, and although that’s true I’m not sure it matters. For 80% of streaming analytical use cases it will probably do the job, and allow you to deliver solutions in a fraction of the time it would take otherwise. For everything else - you can still drop down into the underlying tech just as you did before.&lt;/p&gt; &lt;p&gt;That’s pretty much it for this week (finally!), but one last thing I’d like to call out. Hortonworks’ other big announcement this week was their &lt;a href=&quot;https://hortonworks.com/blog/data-met-science-anything-became-possible/&quot;&gt;new partnership&lt;/a&gt;. IBM are going to drop their Hadoop distribution and resell HDP, and HDP will resell IBM’s BigQuery and Data Science Experience (DSX). This feels like the natural evolution of the market consolidation that’s been going on for a while, and is exactly the model that Pivotal took with their Hadoop distribution. Hortonworks get access to IBM’s Hadoop customer base, and access to a data science as a service solution, and IBM get to continue and enhance their Hadoop offering whilst reducing their costs. And it puts a new light on the Hortonworks’ announcements around support for PowerPC and IBM Spectrum Scale.&lt;/p&gt; &lt;p&gt;Right - I’m done with this week. See you back on Monday when I’ll summarise the plan for next week (that I then won’t keep to), but we’re definitely going to start the week by looking at Cloudera Altus.&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/06/16/the-week-that-was/</guid> </item> <item><title>Streaming Analytics Manager</title><link>http://ondataengineering.net/technologies/streaming-analytics-manager/</link><pubDate>Fri, 16 Jun 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;A suite of open source web based tools to develop and operate stream analytics solutions and analyse the results, with pluggable support for the underlying streaming engine. Consists of Stream Builder (a web based GUI for building streaming data flows), Stream Operations (a web based management and operations tools for streaming applications) and Stream Insight (a bundling of Druid and Apache Superset to serve and analyse the results of streaming applications). Stream Builder supports creation of streaming flows using a drag and drop GUI, with support for a range of sources (including Kafka and HDFS), processors (including joins, window/aggregate functions, normalisation/projection and PMML model execution), and sinks (including email, HDFS, HBase, Hive, JDBC, Druid, Cassandra, Kafka, OpenTSDB and Solr), as well as support for custom sources, processors, sinks and functions (including window functions), and the ability to automatically deploy and execute applications. Stream Operations supports the management of multiple execution environments, the deployment, execution and management of applications within an environment, the capture of stream metrics via pluggable metrics storage (with support for Ambari and OpenTSDB), and web based dashboards to monitor applications and visualise key metrics. Started by Hortonworks in May 2015, with an initial release as part of HDF 3.0 in June 2017.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;SAM, Streamline&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/hortonworks/&quot;&gt;Hortonworks&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;June 2017 - v0.5&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Packages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-superset/&quot;&gt;Apache Superset (incubating)&lt;/a&gt;, &lt;a href=&quot;/technologies/druid/&quot;&gt;Druid&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-flow/&quot;&gt;Hortonworks DataFlow&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/hortonworks/streamline&quot;&gt;https://github.com/hortonworks/streamline&lt;/a&gt; - GitHub repo&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/open-source/streaming-analytics-manager/&quot;&gt;https://hortonworks.com/open-source/streaming-analytics-manager/&lt;/a&gt; - Hortonworks homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDF3/HDF-3.0.0/bk_overview/content/ch_stream-analytics-overview.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDF3/HDF-3.0.0/bk_overview/content/ch_stream-analytics-overview.html&lt;/a&gt; - Hortonworks overview&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDF3/HDF-3.0.0/bk_schema-registry-user-guide/content/index.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDF3/HDF-3.0.0/bk_schema-registry-user-guide/content/index.html&lt;/a&gt; - Hortonworks user guide&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/part-2-hdf-blog-series-shared-schema-registry-important/&quot;&gt;https://hortonworks.com/blog/part-2-hdf-blog-series-shared-schema-registry-important/&lt;/a&gt; - intro blog post&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/technologies/streaming-analytics-manager/</guid> </item> <item><title>Schema Registry</title><link>http://ondataengineering.net/technologies/schema-registry/</link><pubDate>Thu, 15 Jun 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;A centralised registry for data schemas with support for NiFi, Kafka and Stream Analytics Manager, allowing schemas to be defined and versioned centrally and removing the need to attach schema to every piece of data. Supports versioning of schemas (with a definable compatibility policy that validates that schemas are forward compatible, backward compatible, both or none), the ability to store and serve JAR files for serialising and de-serialising data, a REST API, Java SDK and web based user interface for managing schemas. NiFi integration supports any RecordReader and RecordWriter processors (such as ConsumeKafkaRecord, PublishKafkaRecord, ConvertRecord, PutDatabaseRecord, QueryRecord and SplitRecord), with Kafka integration supports Kafka Producers and Consumers. Requires a MySQL backend for schema storage, and either local of HDFS storage for serialiser/de-serialiser JAR files. Stated plan is to support a wider range of schema types (currently only Avro schemas are support), a range of other registry requirements (e.g. templates, machine learning models or business rules), and for integration with Apache Atlas and Ranger. Started by Hortonworks in October 2016, with an initial release as part of HDF 3.0 in June 2017.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/hortonworks/&quot;&gt;Hortonworks&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;June 2017 - v0.3&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-flow/&quot;&gt;Hortonworks DataFlow&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/hortonworks/registry&quot;&gt;https://github.com/hortonworks/registry&lt;/a&gt; - GitHub repo&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://registry-project.readthedocs.io/en/latest/&quot;&gt;http://registry-project.readthedocs.io/en/latest/&lt;/a&gt; - Documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/open-source/schema-registry/&quot;&gt;https://hortonworks.com/open-source/schema-registry/&lt;/a&gt; - Hortonworks homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDF3/HDF-3.0.0/bk_overview/content/ch_sr-overview.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDF3/HDF-3.0.0/bk_overview/content/ch_sr-overview.html&lt;/a&gt; - Hortonworks overview&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDF3/HDF-3.0.0/bk_schema-registry-user-guide/content/index.html&quot;&gt;https://docs.hortonworks.com/HDPDocuments/HDF3/HDF-3.0.0/bk_schema-registry-user-guide/content/index.html&lt;/a&gt; - Hortonworks user guide&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/part-2-hdf-blog-series-shared-schema-registry-important/&quot;&gt;https://hortonworks.com/blog/part-2-hdf-blog-series-shared-schema-registry-important/&lt;/a&gt; - intro blog post&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/technologies/schema-registry/</guid> </item> <item><title>The Mid Week News - 14/06/2017</title><link>http://ondataengineering.net/blog/2017/06/14/the-mid-week-news/</link><pubDate>Wed, 14 Jun 2017 20:00:00 +0100</pubDate> <description> &lt;p&gt;Right, let’s try and do this a bit more regularly (although it’s a bit late today!), especially as it seems to have been a busy news week… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Technology updates (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/hortonworks-data-flow&quot;&gt;Hortonworks Data Flow&lt;/a&gt; has seen a 3.0 release, with the biggest changes being the introduction of two new products - Streaming Analytics Manager and Schema Registry - and a technical preview of SAM Stream Insights which bundles &lt;a href=&quot;/technologies/druid&quot;&gt;Druid&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-superset&quot;&gt;Apache Superset&lt;/a&gt;. We’ll talk more about this on Friday!&lt;/li&gt; &lt;li&gt;As part of it’s 2.6 release, &lt;a href=&quot;/technologies/hortonworks-data-platform&quot;&gt;Hortonworks Data Platform&lt;/a&gt; has deprecated a bunch of technologies that will be removed in HDP 3.0, including &lt;a href=&quot;/technologies/apache-falcon&quot;&gt;Falcon&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-flume&quot;&gt;Flume&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-mahout&quot;&gt;Mahout&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-slider&quot;&gt;Slider&lt;/a&gt; and &lt;a href=&quot;/technologies/hue&quot;&gt;Hue&lt;/a&gt;, and is moving &lt;a href=&quot;/technologies/apache-accumulo&quot;&gt;Accumulo&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-kafka&quot;&gt;Kafka&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-storm&quot;&gt;Storm&lt;/a&gt; out of HDP into other Hortonworks products. I’ll try and capture my thoughts on Friday.&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-falcon&quot;&gt;Apache Falcon&lt;/a&gt; now appears to be inactive, probably related to it’s deprecation from HDP&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-slider&quot;&gt;Apache Slider&lt;/a&gt; now also appears to be inactive, with a plan to fold support for long running services into YARN&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-nifi&quot;&gt;Apache NiFi&lt;/a&gt; continues it’s breakneck release schedule with a 1.3 release&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-solr/&quot;&gt;Apache Solr&lt;/a&gt; has seen a bump to 6.6&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/alluxio&quot;&gt;Alluxio&lt;/a&gt; has seen a 1.5 release, although details seem to be thin on the ground at the moment&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/hortonworks-data-cloud-for-aws/&quot;&gt;Hortonworks Data Cloud for AWS&lt;/a&gt; has skipped 1.15 and gone straight to 1.16&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/cloudbreak&quot;&gt;Cloudbreak&lt;/a&gt;, Hortonworks’ Hadoop in the cloud orchestration tool, has jumped to 1.14&lt;/li&gt; &lt;li&gt;ZepplinHub (the &lt;a href=&quot;/technologies/apache-zeppelin&quot;&gt;Apache Zeppelin&lt;/a&gt; managed service) has changed it’s name to Zepl&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-livy&quot;&gt;Livy&lt;/a&gt; has been &lt;a href=&quot;http://incubator.apache.org/projects/livy.html&quot;&gt;donated to the Apache Foundation&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hortonworks and IBM &lt;a href=&quot;https://hortonworks.com/blog/data-met-science-anything-became-possible/&quot;&gt;have announced&lt;/a&gt; a partnership agreement, whereby IBM will distribute &lt;a href=&quot;/technologies/hortonworks-data-platform&quot;&gt;HDP&lt;/a&gt; as its official Hadoop product, and Hortonworks will resell IBM’s Data Science Experience (DSX) and BigSQL. Hortonworks now also &lt;a href=&quot;https://hortonworks.com/blog/hdp-ibm-spectrum-scale-brings-enterprise-class-storage-place-analytics/&quot;&gt;certify HDP to run on IBM Spectrum Scale&lt;/a&gt;. Good summary from ZDNet &lt;a href=&quot;http://www.zdnet.com/article/ibm-and-hortonworks-go-steady-with-oem-deal/&quot;&gt;here&lt;/a&gt;. Come back on Friday for some of my random thoughts.&lt;/li&gt; &lt;li&gt;Hortonworks have announced a new &lt;a href=&quot;https://hortonworks.com/blog/what-is-a-flexible-support-subscription-about/&quot;&gt;flex support subscription&lt;/a&gt; for &lt;a href=&quot;/technologies/hortonworks-data-platform&quot;&gt;HDP&lt;/a&gt; that covers the usage of HDP on-premise, on IaaS, when deployed using Cloudbreak, or when used as HDCloud on AWS.&lt;/li&gt; &lt;li&gt;Cloudera have a summary of how to &lt;a href=&quot;http://blog.cloudera.com/blog/2017/06/apache-solr-memory-tuning-for-production/&quot;&gt;tune the memory usage&lt;/a&gt; of &lt;a href=&quot;/technologies/apache-solr&quot;&gt;Apache Solr&lt;/a&gt;&lt;/li&gt; &lt;li&gt;On the subject of &lt;a href=&quot;/technologies/apache-solr&quot;&gt;Solr&lt;/a&gt;, see &lt;a href=&quot;http://www.codewrecks.com/blog/index.php/2017/06/06/running-solrmeter-without-a-ui/&quot;&gt;this article&lt;/a&gt; for information on Solrmeter (a tool for testing Solr performance under heavy load)&lt;/li&gt; &lt;li&gt;An update from Yahoo &lt;a href=&quot;http://yahoohadoop.tumblr.com/post/161742444781/hbase-goes-fast-and-lean-with-the-accordion&quot;&gt;on Accordian&lt;/a&gt;, an update to &lt;a href=&quot;/technologies/apache-hbase&quot;&gt;Apache HBase&lt;/a&gt; to improve performance by doing more work in memory.&lt;/li&gt; &lt;li&gt;Databricks have &lt;a href=&quot;https://databricks.com/blog/2017/06/07/databricks-serverless-next-generation-resource-management-for-apache-spark.html&quot;&gt;announced Databricks Serverless&lt;/a&gt;, a fully managed Databricks (built on &lt;a href=&quot;/technologies/apache-spark&quot;&gt;Apache Spark&lt;/a&gt;) service that manages it’s own (virtual) infrastructure&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/06/14/the-mid-week-news/</guid> </item> <item><title>Apache Superset (incubating)</title><link>http://ondataengineering.net/technologies/apache-superset/</link><pubDate>Wed, 14 Jun 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;Web based tool for interactive exploration for OLAP style data, supporting interactive drag and drop querying, composable dashboards and a SQL workspace (SQL Lab). Originally built to query Druid, but now supports a wide range of SQL (and NoSQL) databases, with a lightweight semantic layer allowing control of how data sources are displayed in the UI and which fields can be filtered and aggregated. Users can create Slices (a visualisation of the results of an OLAP style query, with support for a range of visualisations including charts, heat maps, maps, pivot tables, and word clouds amongst others, the ability to configure the query using UI controls, and the ability to configure and customise the visualisation), with multiple slides then composable into a Dashboard (that also support interative filters that connect to multiple slices). Also supports a full SQL IDE (SQL Lab) that supports multiple tabs, a full query history, the ability to apply any data visualisation to results and to browse database metadata, and support for long-running queries using a backend query handler and results store. Other features include query results caching, a plug-in and extensibility framework, the ability to brand and skin the web application, and a robust security model for controlling access to slices, dashboards and data, with support for a range of authentication methods including OpenID, LDAP and OAuth. Originally developed by AirBnB in 2015 as Panoramix, before being renamed to Caravel and then to Superset. Donated to the Apache Foundation in June 2017 and still incubating, with development now led by AirBnB and Hortonworks.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Superset, Apache Superset&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Open Source - Active&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;June 2017 - v0.10&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/streaming-analytics-manager/&quot;&gt;Streaming Analytics Manager&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/airbnb/superset&quot;&gt;https://github.com/airbnb/superset&lt;/a&gt; - GitHub page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://airbnb.io/superset/&quot;&gt;http://airbnb.io/superset/&lt;/a&gt; - Documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://medium.com/airbnb-engineering/caravel-airbnb-s-data-exploration-platform-15a72aa610e5&quot;&gt;https://medium.com/airbnb-engineering/caravel-airbnb-s-data-exploration-platform-15a72aa610e5&lt;/a&gt; - introductory blog post&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/technologies/apache-superset/</guid> </item> <item><title>Druid</title><link>http://ondataengineering.net/technologies/druid/</link><pubDate>Tue, 13 Jun 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;An open source distributed database built to support sub-second OLAP / star schema style queries on both real-time and historical data, based on columnar storage and inverted indexes. All data must have a timestamp, one or more dimension fields, and then one or more measures, with data being aggregated by timestamp and dimension fields on ingest. Comes with a batch ingestor (with support for reading from HDFS, S3 and local files), a streaming ingestor (with support for local files and an HTTP endpoint), and a streaming data endpoint (Tranquility, with support for Kafka, Storm and Spark Streaming and an API for use with other systems), with real-time ingests not guaranteed under failure, but with supports hybrid architectures whereby real-time data ingests are replaced with batch refreshes when available. Architecture based on a number of different node types - historical nodes (which serve queries against a local cache of data that's been persisted in S3 or HDFS), real-time nodes (which support ingest and querying of streaming data, with data persisted and handed over to an historical node once aged), and broker nodes (which distribute queries to appropriate real-time and historical nodes and then collate the results). All data is segmented by date and time, with a metadata database (e.g. MySQL, PostgreSQL, or Derby) tracking segments and which nodes are serving them, and Apache ZooKeeper used for co-ordination and communication between nodes. Supports low latency lock free ingestion, a JSON REST endpoint for queries (with support for a range of query types including timeseries, TopN, groupBy and select), a range of SDKs, approximate and exact computations, multiple storage tiers (including data lifecycle rules on tiering and dropping data), metrics (for queries, ingestion, and coordination), rolling upgrades, HTTP authentication (including Kerberos, but no further security controls), and a number of experimental features including small dimension lookups (note that general joins are not supported), multi-value dimension fields and a SQL interface based on Apache Calcite. Started in 2011 by Metamarkets, open sourced under the GPL licence in October 2012, moving to an Apache licence in February 2015, with a wide range of companies listed on the Druid website as users, and natively supported by Apache Superset and Grafana (via a plugin). Commercial support available from Imply (which distribute their own product based on Druid including a SQL interface and a data exploration tool called Pivot), and currently in tech preview as part of the Hortonworks Data Platform, where it's being integrated with Apache Hive.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;Imply&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;June 2017 - v0.10&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Is packaged by&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/hortonworks-data-platform/&quot;&gt;Hortonworks Data Platform&lt;/a&gt;, &lt;a href=&quot;/technologies/streaming-analytics-manager/&quot;&gt;Streaming Analytics Manager&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://druid.io/&quot;&gt;http://druid.io/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/druid-io/druid&quot;&gt;https://github.com/druid-io/druid&lt;/a&gt; - GitHub repo&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/druid-io/tranquility&quot;&gt;https://github.com/druid-io/tranquility&lt;/a&gt; - Tranquility repo&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://druid.io/blog/2011/04/30/introducing-druid.html&quot;&gt;http://druid.io/blog/2011/04/30/introducing-druid.html&lt;/a&gt; - introductory blog post&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://imply.io/&quot;&gt;https://imply.io/&lt;/a&gt; - Imply homepage&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://druid.io/blog/&quot;&gt;http://druid.io/blog/&lt;/a&gt; - Druid blog&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/technologies/druid/</guid> </item> <item><title>The Plan For This Week - 12/06/2017</title><link>http://ondataengineering.net/blog/2017/06/12/the-plan-for-this-week/</link><pubDate>Mon, 12 Jun 2017 08:00:00 +0100</pubDate> <description> &lt;p&gt;It’s going to be a bit a a sweep up week this week - there are a number of (mostly new) technologies that have come up over the last few weeks I’d like to have a quick look at before we move onto our next technology catalogue.&lt;/p&gt; &lt;p&gt;So, in no particular order, the plan this week is to look at Azure Data Lake Store (scalable HDFS in the cloud - intriguing), Druid (now bundled with HDP in tech preview), Cloudera Altus (their new Data Engineering job execution service), Apache Superset (the new open source dashboard and reporting tool from AirBnB) and Pravega (the Kafka alternative from Dell EMC).&lt;/p&gt; &lt;p&gt;See you on Friday.&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/06/12/the-plan-for-this-week/</guid> </item> <item><title>Microsoft Azure Data Lake Store</title><link>http://ondataengineering.net/technologies/microsoft-azure-data-lake-store/</link><pubDate>Mon, 12 Jun 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;Massively scalable HDFS compatible filesystem as a service, based on Microsoft's Cosmos technology. Claims support for up to trillions of files and single files larger than one petabyte, with no limits on account sizes, file sizes or the amount of data that can be stored, and optimisation of parallel analytics workloads, with high throughput and IOPS performance. Supports user authentication via Azure Active Directory (AAD) (combined with OAuth and OpenID), role based access control for account management, POSIX ACLs for controlling access to data, encryption for both stored data and data in transit over the network,and built in auditing (of both data access and account management activities). Supports a standard WebHDFS API, an HDFS compatible interface (adl://) that's bundled with Apache Hadoop, a web UI (Data Explorer) and SDKs for a range of languages. Does not natively support geo-replication with filesystems limited to a region, but data can be manually replicated via a number of routes if required. First announced in April 2015, with a general availability release in November 2016.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Azure Data Lake Store&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;Microsoft&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;June 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://azure.microsoft.com/en-us/services/data-lake-store/&quot;&gt;https://azure.microsoft.com/en-us/services/data-lake-store/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-overview&quot;&gt;https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-overview&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blogs.msdn.microsoft.com/azuredatalake/2016/11/17/azure-data-lake-store-is-now-generally-available/&quot;&gt;https://blogs.msdn.microsoft.com/azuredatalake/2016/11/17/azure-data-lake-store-is-now-generally-available/&lt;/a&gt; - announcement post&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-comparison-with-blob-storage&quot;&gt;https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-comparison-with-blob-storage&lt;/a&gt; - comparison of Blob Storage and Data Lake Store&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/technologies/microsoft-azure-data-lake-store/</guid> </item> <item><title>Object Stores Wrap-Up</title><link>http://ondataengineering.net/blog/2017/06/09/object-stores-wrapup/</link><pubDate>Fri, 09 Jun 2017 08:00:00 +0100</pubDate> <description> &lt;p&gt;I’ve not quite settled into our pivot to looking at technologies by category, but I think we’re getting somewhere. Again, I feel like I’ve picked some really nasty areas to start with, but I’m hoping what we’ve ended up with is going to provide some value.&lt;/p&gt; &lt;p&gt;So let’s summarise where we’ve got to with our list of &lt;a href=&quot;/tech-categories/object-stores/&quot;&gt;Object Store&lt;/a&gt; technologies, and ponder those we took a deeper dive into over the last week. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Firstly, I’m defining an object store technology as being object store like from top to bottom (whatever that means), but in summary that means I’ve excluded more general purpose storage technologies that provide an object store like API, but at their heart are a different type of technology. That’s horribly woolly, but let’s see how it holds up over time.&lt;/p&gt; &lt;p&gt;If there is a decent list of object store technologies out there on the web I’ve not been able to find it, however I’m therefore hoping that what I’ve managed to pull together will actually provide some value. Firstly a big shout out to Philippe Nicolas for his &lt;a href=&quot;/http://www.theregister.co.uk/2016/07/15/the_history_boys_cas_and_object_storage_map/&quot;&gt;Object Storage Map&lt;/a&gt;, and Enrico Signoretti for his &lt;a href=&quot;http://www.juku.it&quot;&gt;juku.it&lt;/a&gt; blog and published research which were both extraordinary useful starting points. Oh, and to Gartner and IDC, for providing their view on the “Enterprise” market.&lt;/p&gt; &lt;p&gt;I’ve broken down the object store technologies into a bunch of categories. “Enterprise” services and products are those that the big analysts talk about, and there’s evaluations of most of these from analysts via the links I’ve included in the technology category page. However I’ve also tried to call out some of the technologies I’ve come access from smaller vendors that don’t or choose not to play in this space, but that still may have great technologies that may fit your use case. There’s also a surprising bunch of open source technologies - I’m sure I’m missing some, but I’ve tried to call these out as well.&lt;/p&gt; &lt;p&gt;And in terms of the specific technologies I’ve looked at, I’ve been surprised by the differences between the major cloud vendors. &lt;a href=&quot;/technologies/amazon-s3/&quot;&gt;Amazon S3&lt;/a&gt; is obviously the major player, but &lt;a href=&quot;/technologies/azure-blob-storage/&quot;&gt;Azure Blob Storage&lt;/a&gt; seems to be doing a lot to differentiate itself, providing strong consistency and support for multiple object types (such as block, page and append which support different read/write models and different use cases). &lt;a href=&quot;/technologies/google-cloud-storage/&quot;&gt;Google Cloud Storage&lt;/a&gt; seems to provide less differentiation, but obviously gives you heavy integration into the Google Cloud ecosystem.&lt;/p&gt; &lt;p&gt;The only other technology we had time to look at was &lt;a href=&quot;/technologies/scality-ring&quot;&gt;Scality Ring&lt;/a&gt;, the current favoured technology by Gartner, making it (I assume) the pre-eminent on-site technology option. There’s no a lot to say about it, because as usual with commercial products the website it suitably vague - at some point we should look to integrate real world feedback into this site to actually get an understanding of how useful some of these technologies are.&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/06/09/object-stores-wrapup/</guid> </item> <item><title>Object Stores</title><link>http://ondataengineering.net/tech-categories/object-stores/</link><pubDate>Fri, 09 Jun 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;Storage solutions whereby data is stored without any concept of folders or organisational structure, instead being referenced by a unique identifier, allowing for massively parallel and scalable solutions. Generally access via a REST API, with Amazon S3 the defacto standard, although many also support a range of file based interfaces as well, simulating a file system using the underlying object storage. Common features include support for multiple storage tiers, storage of custom metadata against data, replication of data for redundancy, and object versioning.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;!-- Tech Vendor metadata --&gt; &lt;h2 id=&quot;further-information&quot;&gt;Further Information&lt;/h2&gt; &lt;p&gt;Good explanation of object stores in comparison to file and block stores: &lt;a href=&quot;http://searchcloudstorage.techtarget.com/feature/How-an-object-store-differs-from-file-and-block-storage&quot;&gt;http://searchcloudstorage.techtarget.com/feature/How-an-object-store-differs-from-file-and-block-storage&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Excellent list of object stores including their history: &lt;a href=&quot;http://www.theregister.co.uk/2016/07/15/the_history_boys_cas_and_object_storage_map/&quot;&gt;http://www.theregister.co.uk/2016/07/15/the_history_boys_cas_and_object_storage_map/&lt;/a&gt;&lt;/p&gt; &lt;h2 id=&quot;enterprise-object-store-services&quot;&gt;“Enterprise” Object Store Services&lt;/h2&gt; &lt;p&gt;The three major cloud vendors all have object store services as follows:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/amazon-s3/&quot;&gt;Amazon S3&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Eventually consistent object store service&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/azure-blob-storage/&quot;&gt;Azure Blob Storage&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Strongly consistent object store service, with support for multiple object types (block, page and append)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/google-cloud-storage/&quot;&gt;Google Cloud Storage&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Strongly consistent object store service&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;All other “Enterprise” cloud vendors will also have a comparable offering, generally including Amazon S3 compatibility. Many are based on &lt;a href=&quot;/technologies/openstack-swift&quot;&gt;OpenStack Swift&lt;/a&gt;, including IBM Bluemix Object Storage and Rackspace Cloud Files.&lt;/p&gt; &lt;p&gt;The Gartner Magic Quadrant for Cloud Storage (July 2016) provides an evaluation of “Enterprise” cloud storage services (which includes object storage), and is available from Amazon &lt;a href=&quot;https://aws.amazon.com/resources/analyst-reports/#Gartner:_Magic_Quadrant_for_Public_Cloud_Storage_Services,_Worldwide&quot;&gt;here&lt;/a&gt; and from Microsoft &lt;a href=&quot;https://azure.microsoft.com/en-gb/resources/gartner-storage-magic-quadrant/&quot;&gt;here&lt;/a&gt; (requires registration).&lt;/p&gt; &lt;p&gt;Note that many of the Object Store products and technologies listed below are also available as services.&lt;/p&gt; &lt;h2 id=&quot;other-object-store-services&quot;&gt;Other Object Store Services&lt;/h2&gt; &lt;p&gt;BackBlaze B2 (&lt;a href=&quot;https://www.backblaze.com/b2/cloud-storage.html&quot;&gt;https://www.backblaze.com/b2/cloud-storage.html&lt;/a&gt;) is a smaller cloud vendor that claims to have the lowest cost object storage service.&lt;/p&gt; &lt;h2 id=&quot;open-source-object-store-technologies&quot;&gt;Open Source Object Store Technologies&lt;/h2&gt; &lt;p&gt;The following are all Open Source object store technologies:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/openstack-swift&quot;&gt;OpenStack Swift&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Supports multiple configurable storage tiers and backing storage; part of the OpenStack suite but can be installed stand-alone&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Ceph&lt;/td&gt; &lt;td&gt;Distributed object store that also supports block and file storage, with development led by RedHat - &lt;a href=&quot;http://ceph.com/&quot;&gt;http://ceph.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Redcurrent&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://www.redcurrant.io&quot;&gt;http://www.redcurrant.io&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Basho Riak S2 (formally CS)&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://basho.com/products/riak-s2/&quot;&gt;http://basho.com/products/riak-s2/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Joyent (Samsung) Triton Object Storage&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.joyent.com/triton/object-storage&quot;&gt;https://www.joyent.com/triton/object-storage&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Minio&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.minio.io/&quot;&gt;https://www.minio.io/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LinkedIn Ambry&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://github.com/linkedin/ambry&quot;&gt;https://github.com/linkedin/ambry&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Rook&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://rook.io/&quot;&gt;https://rook.io/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Open IO&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://openio.io/&quot;&gt;http://openio.io/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/scality-s3-server&quot;&gt;Scality S3&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Open source version of the Scality Ring S3 API targetting dev/test use caes, non-distributed, but with support for local, in memory or S3 backing storage&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;enterprise-object-store-products&quot;&gt;“Enterprise” Object Store Products&lt;/h2&gt; &lt;p&gt;Options for on premise object stores from enterprise vendors, many of which are also available as a service or as a hardware and software appliance, include:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;SwiftStack&lt;/td&gt; &lt;td&gt;Commercial on premise software solution based on &lt;a href=&quot;/technologies/openstack-swift&quot;&gt;OpenStack Swift&lt;/a&gt; with a number of added management features and synchronisation to the cloud, sold by the largest contributor to Swift - &lt;a href=&quot;https://www.swiftstack.com/&quot;&gt;https://www.swiftstack.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/scality-ring/&quot;&gt;Scality RING&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Native object store with POSIX filesystem support, and a range of object, file and OpenStack compatible APIs&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Caringo Swarm (formally CAStore)&lt;/td&gt; &lt;td&gt;Software and appliance solution - &lt;a href=&quot;https://www.caringo.com/&quot;&gt;https://www.caringo.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Cloudian HyperStore&lt;/td&gt; &lt;td&gt;Software and appliance solution - &lt;a href=&quot;https://cloudian.com/products/&quot;&gt;https://cloudian.com/products/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Dell EMC Elastic Cloud Storage (ECS)&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.emc.com/en-us/storage/ecs/&quot;&gt;https://www.emc.com/en-us/storage/ecs/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DDN WOS&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://www.ddn.com/products/object-storage-web-object-scaler-wos/&quot;&gt;http://www.ddn.com/products/object-storage-web-object-scaler-wos/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Exabloc&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.exablox.com/&quot;&gt;https://www.exablox.com/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RedHat Ceph&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.redhat.com/en/technologies/storage/ceph&quot;&gt;https://www.redhat.com/en/technologies/storage/ceph&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;HGST (Western Digial) ActiveScale&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.hgst.com/products/systems&quot;&gt;https://www.hgst.com/products/systems&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Hitachi (HDS) Content Platform (HCP)&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.hds.com/en-us/products-solutions/storage/content-platform.html&quot;&gt;https://www.hds.com/en-us/products-solutions/storage/content-platform.html&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;IBM Cloud Object Storage (previously Cleversafe)&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.ibm.com/cloud-computing/products/storage/object-storage/cloud/&quot;&gt;https://www.ibm.com/cloud-computing/products/storage/object-storage/cloud/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;NetApp StorageGRID Webscale&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://www.netapp.com/us/products/data-management-software/object-storage-grid-sds.aspx&quot;&gt;http://www.netapp.com/us/products/data-management-software/object-storage-grid-sds.aspx&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;The Gartner Magic Quadrant for Distributed File Systems and Object Storage (October 2016) includes information on a number of “Enterprise” on-premise object stores, and is available from Scality &lt;a href=&quot;http://storage.scality.com/report-gartner-magic-quadrant-storage.html&quot;&gt;here&lt;/a&gt;[requires registration] with a summary from The Register &lt;a href=&quot;https://www.theregister.co.uk/2016/10/21/gartners_not_scoffing_at_scofs_and_objects/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;There’s also a Gartner Critical Capabilities for Object Storage (March 2016) which includes an evaluation of 12 “Enterprise” object storage technologies (including Ceph and SwiftStack), and is summarised by The Register &lt;a href=&quot;https://www.theregister.co.uk/2016/04/18/gartner_object_storage_rankings/&quot;&gt;here&lt;/a&gt; with a link to the full report.&lt;/p&gt; &lt;p&gt;The IDC Object-Based Storage Marketscape (2016) provides information on a wide range of “Enterprise” object stores and is available from Scality &lt;a href=&quot;http://storage.scality.com/report-idc-marketscape-object-based-storage.html&quot;&gt;here&lt;/a&gt; with a summary from The Register &lt;a href=&quot;https://www.theregister.co.uk/2016/12/14/ibms_clever_and_safe_acquisition_gets_it_object_storage_leadership/&quot;&gt;here&lt;/a&gt;&lt;/p&gt; &lt;h2 id=&quot;other-object-store-products&quot;&gt;Other Object Store Products&lt;/h2&gt; &lt;p&gt;The following are other object store products:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Quantum Lattus Object Storage&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://www.quantum.com/products/bigdatamanagement/lattus/index.aspx&quot;&gt;http://www.quantum.com/products/bigdatamanagement/lattus/index.aspx&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;NooBaa&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.noobaa.com&quot;&gt;https://www.noobaa.com&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;parallel-distributed-filesystems&quot;&gt;Parallel Distributed Filesystems&lt;/h2&gt; &lt;p&gt;Many parallel distributed filesystems (such as Gluster) also support object store interfaces, simulating an object store using the underlying filesystem.&lt;/p&gt; &lt;h2 id=&quot;historical-technologies&quot;&gt;Historical Technologies&lt;/h2&gt; &lt;p&gt;The following technologies are no longer sold or maintained:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;CoreOS Torus&lt;/td&gt; &lt;td&gt;Development stopped Feb 2017 - &lt;a href=&quot;https://github.com/coreos/torus&quot;&gt;https://github.com/coreos/torus&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Skylable&lt;/td&gt; &lt;td&gt;Website no longer live - &lt;a href=&quot;http://www.skylable.com&quot;&gt;http://www.skylable.com&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/tech-categories/object-stores/</guid> </item> <item><title>Scality S3 Server</title><link>http://ondataengineering.net/technologies/scality-s3-server/</link><pubDate>Thu, 08 Jun 2017 07:45:00 +0100</pubDate> <description> &lt;p&gt;Open sourced object storage server based on the S3 compatible API from Scality RING, with the ability to proxy requests to other S3 services, use persistent local storage or in memory storage. Written in Node.js, and focused on dev/test use cases, as it is not a distributed system and does not support data replication or other features often found in object servers. First released in June 2016, hosted on GitHub under an Apache 2.0 licence, with commercial support available from Scality (as Scality S3 Enterprise Edition).&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;Scality&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;June 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://s3.scality.com/&quot;&gt;https://s3.scality.com/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/scality/S3&quot;&gt;https://github.com/scality/S3&lt;/a&gt; - GitHub repo&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://s3.scality.com/v1.0/page/s3-enterprise-edition&quot;&gt;https://s3.scality.com/v1.0/page/s3-enterprise-edition&lt;/a&gt; - Enterprise edition details&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.scality.com/about-us/press/scality-announces-s3-server/&quot;&gt;http://www.scality.com/about-us/press/scality-announces-s3-server/&lt;/a&gt; - original press release&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/technologies/scality-s3-server/</guid> </item> <item><title>Scality RING</title><link>http://ondataengineering.net/technologies/scality-ring/</link><pubDate>Thu, 08 Jun 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;A massively scalable commercial object store available as software for deployment on premises on commodity hardware. Based around a native object store core, but with POSIX filesystem support, and support for a range of APIs including file based (NFS, SMB and Linux FUSE), object based (S3 compatible and native Scality REST APIs), and OpenStack compatible (Swift, Cinder, Glance and Manila). Supports both variable level data replication and erasure coding, object encryption, file and object versioning, multi-site support (via synchronous and asynchronous replication, including support for replicating to Amazon S3), data location control, support for arbitrarily large objects, rolling upgrades and full authentication and access controls (including support for LDAP, Active Directory, AWS IAM and Kerberos). Comes with a CLI and web based GUI, and an add on solution (Scality Cloud Monitor) is available for monitoring and management. Sold by Scality, who were founded in 2010 and who focus on selling and supporting Scality RING, but who have also open sourced their S3 API as Scality S3 Server.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;Scality&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;June 2017 - 7&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://www.scality.com/products/ring/&quot;&gt;http://www.scality.com/products/ring/&lt;/a&gt; - Scality RING homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://www.scality.com/products/scality-cloud-monitor/&quot;&gt;http://www.scality.com/products/scality-cloud-monitor/&lt;/a&gt; - Scality Cloud Monitor homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://storage.scality.com/rs/963-KAI-434/images/Scality%20RING7%20Data%20Sheet.pdf&quot;&gt;http://storage.scality.com/rs/963-KAI-434/images/Scality%20RING7%20Data%20Sheet.pdf&lt;/a&gt; - data sheet&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/technologies/scality-ring/</guid> </item> <item><title>The Mid Week News - 07/06/2017</title><link>http://ondataengineering.net/blog/2017/06/07/the-mid-week-news/</link><pubDate>Wed, 07 Jun 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;We interupt the current broadcast for another (semi regular) catchup up on the news… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;New technology releases (details are on the relevant technology pages):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-apex/&quot;&gt;Apache Apex and DataTorrent RTS&lt;/a&gt; have both seen new releases&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-nifi/minifi&quot;&gt;MiNiFi&lt;/a&gt; has seen 0.2 releases of it’s Java and C++ versions&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-flink&quot;&gt;Apache Flink&lt;/a&gt; has seen a 1.3 release&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/hadoop&quot;&gt;Hadoop&lt;/a&gt; has seen it’s latest (alpha3) release of 3.0 - details &lt;a href=&quot;http://hadoop.apache.org/docs/r3.0.0-alpha3/index.html&quot;&gt;here&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other technology news:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cloudera have released Altus (&lt;a href=&quot;http://vision.cloudera.com/simplifying-big-data-in-the-cloud/&quot;&gt;blog post&lt;/a&gt;; &lt;a href=&quot;http://blog.cloudera.com/blog/2017/05/data-engineering-with-cloudera-altus/&quot;&gt;tech details&lt;/a&gt;; &lt;a href=&quot;http://community.cloudera.com/t5/Community-News-Release/Announce-Cloudera-Altus-is-now-available/m-p/55007#M175&quot;&gt;announcement&lt;/a&gt;, a service for running data engineering jobs (Spark, Hive and MapReduce) in the cloud on on-demand clusters. One for us to dig into further in the not too distant future I think.&lt;/li&gt; &lt;li&gt;Confluent have &lt;a href=&quot;https://www.infoq.com/news/2017/05/Confluent-Cloud-Kafka-AWS&quot;&gt;announced a cloud based offering&lt;/a&gt; of their &lt;a href=&quot;/technologies/apache-kafka&quot;&gt;Apache Kafka&lt;/a&gt; based solution, although it’s only in early access at the moment&lt;/li&gt; &lt;li&gt;Cockroach DB, a distributed SQL database that could be regarded as an open source version of Google Spanner &lt;a href=&quot;https://www.infoq.com/news/2017/06/Cockroach-DB-Production-Release&quot;&gt;has hit 1.0&lt;/a&gt;&lt;/li&gt; &lt;li&gt;AirBnB’s &lt;a href=&quot;https://github.com/airbnb/superset&quot;&gt;Superset&lt;/a&gt; has been donated to the Apache Foundation. This is well worth a look - it looks like an extreemly capable data exploration platform&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://pravega.io/&quot;&gt;Pravega&lt;/a&gt; is a new open source streaming storage system from Dell/EMC - see &lt;a href=&quot;https://siliconangle.com/blog/2017/04/17/dell-emc-takes-on-streaming-storage-with-open-source-solution-pravega-ffsf17/&quot;&gt;here&lt;/a&gt; for an introduction&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Technology updates:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A write up of the new capabilities in &lt;a href=&quot;/technologies/apache-nifi&quot;&gt;NiFi&lt;/a&gt; 1.2 for &lt;a href=&quot;https://blogs.apache.org/nifi/entry/record-oriented-data-with-nifi&quot;&gt;processing large volumes of records more efficiently&lt;/a&gt; and &lt;a href=&quot;https://blogs.apache.org/nifi/entry/real-time-sql-on-event&quot;&gt;running SQL on event streams&lt;/a&gt;&lt;/li&gt; &lt;li&gt;If you’re interested in &lt;a href=&quot;/technologies/apache-ignite&quot;&gt;Apache Ignite&lt;/a&gt;, there’s a two part getting starting set of blog posts from GridGain &lt;a href=&quot;https://www.gridgain.com/resources/blog/getting-started-apacher-ignitetm-part-i-0&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://www.gridgain.com/resources/blog/getting-started-apacher-ignitetm-part-2&quot;&gt;here&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-solr/&quot;&gt;Solr&lt;/a&gt; is getting a &lt;a href=&quot;https://sematext.com/blog/2017/05/10/solr-v2-api/&quot;&gt;new API&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2017/05/hdfs-maintenance-state/&quot;&gt;Options for doing system maintenance&lt;/a&gt; on &lt;a href=&quot;/technologies/apache-hadoop/hdfs/&quot;&gt;HDFS&lt;/a&gt; from Cloudera&lt;/li&gt; &lt;li&gt;Hortonworks are working on &lt;a href=&quot;/technologies/apache-spark/spark-sql&quot;&gt;Spark SQL&lt;/a&gt; &lt;a href=&quot;https://hortonworks.com/blog/row-column-level-control-apache-spark/&quot;&gt;integration with Apache Ranger&lt;/a&gt;, giving row/column level access control&lt;/li&gt; &lt;li&gt;Cloudera are trumpeting their work on &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Spark&lt;/a&gt; and some of the &lt;a href=&quot;http://vision.cloudera.com/new-capabilities-for-apache-spark-users/&quot;&gt;new features they’ve enabled&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2017/05/how-to-avoid-cloud-vendor-lock-in-to-minimize-cost-and-risk/&quot;&gt;Cloudera’s pitch&lt;/a&gt; for why you should use &lt;a href=&quot;/technologies/cloudera-director&quot;&gt;Cloudera Director&lt;/a&gt; to give you cloud independence&lt;/li&gt; &lt;li&gt;A &lt;a href=&quot;https://data-artisans.com/blog/apache-flink-1-3-0-evolution-stream-processing&quot;&gt;summary of the history&lt;/a&gt; of &lt;a href=&quot;/technologies/apache-flink&quot;&gt;Flink&lt;/a&gt; from dataArtisans&lt;/li&gt; &lt;li&gt;There’s a new &lt;a href=&quot;https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.1/bk_cloud-data-access/content/about.html&quot;&gt;cloud data access guide&lt;/a&gt; for &lt;a href=&quot;/technologies/hortonworks-data-platform&quot;&gt;Hortonwork’s Data Platform&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Interesting blog posts:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Automating testing of data pipelines and then doing continuous integration is definitely a topic I want to talk more about (but I say that of everything). In the meantime Databricks have an article on &lt;a href=&quot;https://databricks.com/blog/2017/06/02/integrating-apache-spark-cucumber-behavioral-driven-development.html&quot;&gt;using Cucumber with Spark&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Again from Databricks, and this feels topical for us - &lt;a href=&quot;https://databricks.com/blog/2017/05/31/top-5-reasons-for-choosing-s3-over-hdfs.html&quot;&gt;5 reasons for choosing S3 over HDFS&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Some thoughts on Open Source, licences and whether some commercial open source products are really open source from Bloor - &lt;a href=&quot;http://www.bloorresearch.com/analysis/the-open-source-dilemma/&quot;&gt;here&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A post from Cloudera on &lt;a href=&quot;http://blog.cloudera.com/blog/2017/05/bi-temporal-data-modeling-with-envelope/&quot;&gt;Envelope&lt;/a&gt;, a pre-developed Spark application for doing bi-temporal change management&lt;/li&gt; &lt;li&gt;Another one from The Morning Paper - &lt;a href=&quot;https://blog.acolyer.org/2017/05/30/mosaic-processing-a-trillion-edge-graph-on-a-single-machine/&quot;&gt;processing a trillion edge graph on a single machine&lt;/a&gt;&lt;/li&gt; &lt;li&gt;The latest update from Bloor on &lt;a href=&quot;http://www.bloorresearch.com/blog/im-blog/graph-update-4-performance-scalability-and-neo4j/&quot;&gt;graph technologies&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.infoq.com/news/2017/05/metrics-monitoring-robinhood&quot;&gt;A case study&lt;/a&gt; on the use of OpenTSDB, Grafana, Kafka and Riemann for metrics collection and monitoring at Robinhood Engineering&lt;/li&gt; &lt;li&gt;Confluent’s view on why &lt;a href=&quot;https://www.confluent.io/blog/the-future-of-etl-isnt-what-it-used-to-be/&quot;&gt;streaming is the new ETL&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/06/07/the-mid-week-news/</guid> </item> <item><title>OpenStack Swift</title><link>http://ondataengineering.net/technologies/openstack-swift/</link><pubDate>Tue, 06 Jun 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;An open source object store with eventual consistency, that's available from a number of vendors as both an on site solution and a cloud based service offering. Objects are organised into containers and indexed by string, with the option to list objects by prefix and to summarise results based on a delimiter allowing a filesystem to be approximated. Supports configurable storage policies (each using a different storage ring allowing for differing hardware and replication levels to be used), erasure coding as well as standard replication (with erase coding providing smaller storage overheads at the code of higher CPU and read and write data), and multi-region clusters (based on configuring affinity for local operations). Also supports container and object metadata, object versioning, container to container mirroring via background synchronisation, authorisation via tokens from OpenStack Keystone, access control via container ACLs, support for large objects via segmentation (multi-part uploads combined with a special manifest file), scheduled and bulk object deletion, time limited access URLs,and encryption of data at rest. Provides a REST API and client SDKs. Originally created by Rackspace in 2009, becoming one of the first OpenStack technologies, with contributors now including SwiftStack, RedHat, HP, Intel, IBM among others.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;OpenStack&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial Open Source&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;June 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.openstack.org/software/releases/ocata/components/swift&quot;&gt;https://www.openstack.org/software/releases/ocata/components/swift&lt;/a&gt; - project summary&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.openstack.org/developer/swift/&quot;&gt;https://docs.openstack.org/developer/swift/&lt;/a&gt; - documentation homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.openstack.org/developer/swift/overview_architecture.html&quot;&gt;https://docs.openstack.org/developer/swift/overview_architecture.html&lt;/a&gt; - architecture overview&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://developer.openstack.org/api-ref/object-storage/&quot;&gt;https://developer.openstack.org/api-ref/object-storage/&lt;/a&gt; - API reference&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/technologies/openstack-swift/</guid> </item> <item><title>Google Cloud Storage</title><link>http://ondataengineering.net/technologies/google-cloud-storage/</link><pubDate>Mon, 05 Jun 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;An object store service with strong consistency, multiple storage tiers and deep integration to the Google Cloud ecosystem. Objects are organised into buckets and indexed by string, with the option to list objects by prefix and to summarise results based on a delimiter allowing a filesystem to be approximated. Storage tiers supported include multi-regional (data is distributed across a geo region), regional, nearline and coldline (design for data accessed less than once per month/year respectively). Supports object lifecycle management allowing for automatic deletion or moving of objects between storage tiers. Supports versioning of objects, access control (via Google Cloud IAM, bucket and object ACLs and time-limited access via signed URLs), encryption of objects and support for SSL connections, auditing of object operations via Google Cloud Audit, gzip uncompression on read, custom domains, multi-part uploads via merging of objects after upload (Composite Objects), acccess and storage logs as downloadable CSV files and batching of request. Quotes a 99.999999999% guarentee that data won't be lost, and availability of 99.9% for regional and 99.95% for multi-regional storage tiers. Provides a web based management console (Google Cloud Platform Console), CLI (gsutil), JSON and XML REST API and SDKs for a wide range of languages.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/google-cloud-platform/&quot;&gt;Google Cloud Platform&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;June 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/storage/&quot;&gt;https://cloud.google.com/storage/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/storage/docs/&quot;&gt;https://cloud.google.com/storage/docs/&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/storage/docs/concepts&quot;&gt;https://cloud.google.com/storage/docs/concepts&lt;/a&gt; - key concepts&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/storage/release-notes&quot;&gt;https://cloud.google.com/storage/release-notes&lt;/a&gt; - release notes&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/technologies/google-cloud-storage/</guid> </item> <item><title>The Week That Was - 02/06/2017</title><link>http://ondataengineering.net/blog/2017/06/02/the-week-that-was/</link><pubDate>Fri, 02 Jun 2017 08:00:00 +0100</pubDate> <description> &lt;p&gt;Just a very quick update this week.&lt;/p&gt; &lt;p&gt;It’s been a much shorter week that usual due to me being elsewhere at the beginning of the week and only starting our look into object stores on Wednesday. Because of that we’ll continue into next week, and I’m therefore not going to summarise any of the technologies we’ve looked at this week today. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;I’m starting to settle on a new rhythm for these technology category pages. I think I’m looking for broad coverage rather than depth - so although I would love to work through all the major technologies in each category I think my priority has to be getting good coverage of the various categories I want to cover so that there’s a framework that hopefully other people can help me build out. However, I don’t want to provide technology category pages that provide no value and don’t contain any useful or valuable information, but I don’t want to stop or reduce the number of technology summaries I’m doing - and that’s the balance I’m trying to strike. I’m not quite there yet, but I’m going to try and do technology summaries Monday to Thursday and a technology category page and some thoughts on a Friday, with the technology category page focusing on providing valuable links and very brief lists of potential technologies that fit in the category that require further analysis - a valuable starting point for understanding the potential technology options.&lt;/p&gt; &lt;p&gt;I’ve no idea if it’ll work, or if it’s achievable, but let’s find out. Next week we’ll finish of our last technology summaries for object stores, catch-up on the news, and maybe have a look at Cloudera Altus or Azure Data Lake Store if we have time.&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/06/02/the-week-that-was/</guid> </item> <item><title>Microsoft Azure Blob Storage</title><link>http://ondataengineering.net/technologies/microsoft-azure-blob-storage/</link><pubDate>Fri, 02 Jun 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;An object store service with strong consistency, with support for multiple blob types (block, page and append), multiple storage tiers (hot and cold) and deep integration to the Azure ecosystem. Block blobs are comprised of one or more blocks with operations done at the block level with changes made visible via a final commit; page blobs are collections of 512-byte pages optimised for random read and write operations against one or more pages; and append blobs only support modification via the addition of new data to the end of the blob. Objects are organised into containers and indexed by string, with the option to list objects by prefix and to summarise results based on a delimiter allowing a filesystem to be approximated. Supports name-value pair metadata against containers and objects, both optimistic and pessimistic (lock based) concurrency, snapshots (providing read only access to objects as they were when the snapshot was taken), access control via access tokens (shared access signatures), public access to containers, configurable geo redundancy, encryption of objects (Azure Storage Service Encryption - SSE) and support for SSL connections, multi-part uploads, the use of custom domains, and logging and metrics (Azure Storage Analytics). Provides a REST API, web app (Azure Storage Explorer), a range of SDKs, a CLI and PowerShell integration.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Azure Blob Storage&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;Microsoft&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;June 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://azure.microsoft.com/en-us/services/storage/blobs/&quot;&gt;https://azure.microsoft.com/en-us/services/storage/blobs/&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/azure/storage/storage-introduction&quot;&gt;https://docs.microsoft.com/en-us/azure/storage/storage-introduction&lt;/a&gt; - documentation&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/rest/api/storageservices/&quot;&gt;https://docs.microsoft.com/en-us/rest/api/storageservices/&lt;/a&gt; - REST API documentation&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/technologies/microsoft-azure-blob-storage/</guid> </item> <item><title>Amazon S3</title><link>http://ondataengineering.net/technologies/amazon-s3/</link><pubDate>Thu, 01 Jun 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;An object store service with eventual consistency, focusing on massive durability and scalability, with support for multiple storage tiers (including Amazon Glacier) and deep integration to the AWS ecosystem. Objects are organised into buckets and indexed by string, with the option to list objects by prefix and to summarise results based on a delimiter allowing a filesystem to be approximated. Metadata against objects is managed via S3 Object Tags, key-value pairs applied to objects that can be added, modified or deleted at any time. Lifecycle management policies can be assigned to name prefixes or object tags to automatically delete objects or move them between storage tiers. Supports versioning of objects, access control (at the bucket or object level), replication of objects and metadata to a bucket in a different AWS region (cross-region replication), encryption of objects and support for SSL connections, full auditing of all object operations, analytics on object operations, multi-part uploads, multi-object deletions, a flat-file output of object names and metadata (S3 Inventory), downloads via the bittorrent protocol, static website hosting and time limited object download URLs. Quotes a 99.999999999% guarentee that data won't be lost, with data stored redundantly across multiple devices and facilities within the chosen region, and scalability past trillions of objects. Provides a web based management console, mobile management app, a REST API and SDKs for a wide range of languages. First launched in March 2006.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Simple Storage Service&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/amazon-web-services/&quot;&gt;Amazon Web Services&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;June 2017&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/s3&quot;&gt;https://aws.amazon.com/s3&lt;/a&gt; - home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/s3/details/&quot;&gt;https://aws.amazon.com/s3/details/&lt;/a&gt; - details&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/documentation/s3/&quot;&gt;https://aws.amazon.com/documentation/s3/&lt;/a&gt; - documentation&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/technologies/amazon-s3/</guid> </item> <item><title>Hadoop Compatible Filesystems</title><link>http://ondataengineering.net/blog/2017/05/26/hadoop-compatible-filesystems/</link><pubDate>Fri, 26 May 2017 07:45:00 +0100</pubDate> <description> &lt;p&gt;So how did we do with our look at &lt;a href=&quot;/tech-categories/hadoop-compatible-filesystems/&quot;&gt;Hadoop Compatible Filesystems&lt;/a&gt; this week? Spoiler - not as well as I’d hoped!&lt;/p&gt; &lt;p&gt;In hindsight, picking this as the first technology category to do was a daft idea. Not only have I been trying to work out what I want to achieve with these technology category pages, and the level and information I want to capture, but my journey into Hadoop compatible storage has involved a myriad of rabbit holes from object stores to enterprise scale out storage to in-memory to every technology and its dog having an Hadoop compatible API. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;So I’m going to do my best to summarise, but this journey will probably continue next week - there’s some further detail I want to dig into, I’m not entirely happy with the technology category page, I’ve got a big pile of information that probably means I’m probably doing object stores and scale out storage as the next technology categories to close the circle.&lt;/p&gt; &lt;p&gt;Firstly, there are two use cases for an Hadoop Compatible Filesystem. The first is as a drop in replacement for HDFS - you run your filesystem and compute on the same nodes and utilise data locality to do local filesystem reads wherever possible. The second is as a way of reading and writing data from a remote data store using the standard Hadoop Filesystem API - great if you want to access and write data back to external storage, but not a replacement for your local HDFS filesystem that you’ll still be using for temporary and intermediate data.&lt;/p&gt; &lt;p&gt;If you want to swap out HDFS in your Hadoop cluster for something else, there are some technologies you can look at that are HDFS but better. The obvious one being &lt;a href=&quot;/technologies/mapr-file-system/&quot;&gt;MapR-FS&lt;/a&gt;, but the &lt;a href=&quot;/technologies/quantcast-file-system/&quot;&gt;Quantcast File System&lt;/a&gt; and Hops-HDFS are both interesting examples of open source projects that have taken HDFS and tried to improve it.&lt;/p&gt; &lt;p&gt;If you want to read and write data from Hadoop to an external system via the Hadoop Filesystem API, then the world’s your oyster, as pretty much every file or object based storage technology now allows you to do this, and generally in a (reasonably) high performance parallel way. This is the area I want to dig into in more detail next week however - what are you options if you want to store large volumes of data for analytics outside of an Hadoop cluster.&lt;/p&gt; &lt;p&gt;And as it feels like I’ve only scratched the surface of this area, I’m going to reserve the right to come and revise this information in the future. I wasn’t planning to dig so far into file and object storage at this time, but here we are so let’s see where it leads us.&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/05/26/hadoop-compatible-filesystems/</guid> </item> <item><title>Hadoop Compatible Filesystems</title><link>http://ondataengineering.net/tech-categories/hadoop-compatible-filesystems/</link><pubDate>Fri, 26 May 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;A parallel distributed filesystem that implements the Hadoop FileSystem API and conforms to the Hadoop Compatible Filesystem specification, allowing it to be used in place of HDFS. The use of the FileSystem API (over native filesystem access) allows for parallel reads and location aware block placement, with the HCFS specification covering runtime behaviour. Note that Hadoop Compatible Filesystems (as per HDFS) are not fully POSIX compliant, there is no formal compatibility test suite (although a test suite that will highlight potential issues is available), and that some implementations (for example object stores) do not (and cannot) fully conform to the specification.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;!-- Tech Vendor metadata --&gt; &lt;h2 id=&quot;further-information&quot;&gt;Further Information&lt;/h2&gt; &lt;p&gt;The specification for Hadoop compatible filesystems is included in the Hadoop documentation &lt;a href=&quot;https://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-common/filesystem/introduction.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;h2 id=&quot;specialist-hadoop-compatible-filesystems&quot;&gt;Specialist Hadoop Compatible Filesystems&lt;/h2&gt; &lt;p&gt;The following technologies are all designed specifically to be Hadoop compatible and to be drop in replacements for HDFS within an Hadoop cluster, meaning that they can co-exist with YARN and other analytical compute workloads on the same nodes:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/hdfs/&quot;&gt;HDFS&lt;/a&gt;&lt;/td&gt; &lt;td&gt;The Hadoop Distributed Filesystem, bundled with Hadoop&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/mapr-file-system/&quot;&gt;MapR-FS&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Hadoop compatible, highly resilient and scalable distributed filesystem that also supports NoSQL table and streaming data native storage&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/quantcast-file-system/&quot;&gt;Quantcast File System&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Open source HDFS replacement, which focuses on improving performance and scalability over HDFS&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Hops-HDFS&lt;/td&gt; &lt;td&gt;Experimental solution based on HDFS but using a distributed MySQL cluster for metadata to increase performance and scalability - &lt;a href=&quot;http://www.hops.io/?q=content/hdfs&quot;&gt;http://www.hops.io/?q=content/hdfs&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;See our &lt;a href=&quot;/tech-categories/hadoop-distributions/&quot;&gt;Hadoop Distributions&lt;/a&gt; page for options on deploying Hadoop clusters utilising these technologies.&lt;/p&gt; &lt;h2 id=&quot;in-memory-hadoop-compatible-filesystems&quot;&gt;In Memory Hadoop Compatible Filesystems&lt;/h2&gt; &lt;p&gt;There are also a number of in memory data grids / storage layers that provide Hadoop compatibility and the option to run Hadoop jobs entirely in memory or across tiered storage:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-alluxio&quot;&gt;Apache Alluxio&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Distributed virtual storage layer over memory and tiered storage with support for a range of interfaces. Previously known as Tachyon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-ignite&quot;&gt;GridGain/Apache Ignite&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Distributed in-memory data fabric/grid, including support for an in-memory Hadoop compatible filesystem&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;cloud-hadoop-compatible-filesystems&quot;&gt;Cloud Hadoop Compatible Filesystems&lt;/h2&gt; &lt;p&gt;Azure has an Hadoop compatible filesystem as a service:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/technologies/microsoft-azure-data-lake-store&quot;&gt;Azure Data Lake Store&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Cloud based massively scalable HDFS compatible filesystem based on Microsoft Cosmos&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;other-technologies&quot;&gt;Other Technologies&lt;/h2&gt; &lt;p&gt;DataStax Enterprise has an HDFS compatible file system API:&lt;/p&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;DataStax Enterprise file system&lt;/td&gt; &lt;td&gt;Distributed Hadoop compatible filesystem that runs on DataStax Enterprise, replacing the now deprecated Cassandra File System (CFS) - &lt;a href=&quot;http://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/analytics/dsefsTOC.html&quot;&gt;http://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/analytics/dsefsTOC.html&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;parallel-distributed-filesystems&quot;&gt;Parallel Distributed Filesystems&lt;/h2&gt; &lt;p&gt;Parallel distributed filesystems provide similar capabilities to HDFS, including the ability to scale horizontally and to read a file in parallel from multiple nodes. Their general focus is on providing direct filesystem access plus NFS and object store APIs, and although most offer an Hadoop compatible API this is generally just to allow data to be exploited by Hadoop compatible workloads as a remote filesystem. Some may support installation on an Hadoop cluster as a drop in replacement for HDFS, however there are often compatibility issues and performance is often not as good as HDFS.&lt;/p&gt; &lt;h2 id=&quot;object-stores&quot;&gt;Object Stores&lt;/h2&gt; &lt;p&gt;Most object stores also provide Hadoop compatible APIs, and although this means that Hadoop can natively read and write from them using the Hadoop Filesystem API, they are not considered Hadoop Compatible Filesystems due to their lack of compliance to the compliance specification. More details can be found in the “Object Stores vs. Filesystems” section of the &lt;a href=&quot;https://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-common/filesystem/introduction.html&quot;&gt;specification page&lt;/a&gt;.&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/tech-categories/hadoop-compatible-filesystems/</guid> </item> <item><title>Chapter 2</title><link>http://ondataengineering.net/blog/2017/05/22/chapter-2/</link><pubDate>Mon, 22 May 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;Given that I’m going a little short-sighted staring at Hadoop, I think it’s time to widen our gaze a little and to start chapter 2 of this site. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;In the recently added &lt;a href=&quot;/tech-categories/hadoop-distributions/distribution-comparison/&quot;&gt;Hadoop distributions comparison&lt;/a&gt; page, I introduced some categories by which I grouped and compared the technologies in the Cloudera, Hortonworks and MapR technology stacks, however at no point did I try and accurately define was I mean by each of these categories.&lt;/p&gt; &lt;p&gt;So that’s what I’m planning to do now - work through these categories (and some that aren’t on there) to create a technology category page for each that summarises the range of technologies available in that space. I’ll continue doing technology summaries for popular technologies (or anything that catches my eye), with a focus on open source and cloud technologies (with a smattering of commercial ones as and where there’s enough public information to do so).&lt;/p&gt; &lt;p&gt;We’ll kick off tomorrow with our first category, once I’ve rolled the dice and picked one!&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/05/22/chapter-2/</guid> </item> <item><title>Hadoop Wrap-Up</title><link>http://ondataengineering.net/blog/2017/05/19/hadoop-wrap-up/</link><pubDate>Fri, 19 May 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;And so I think our current look into Hadoop is probably drawing to a close. That’s not because we’re run out of technologies to look at and thing to talk about, but because it’s time to move on. We’ll double back to this in the future - I’d like to dig into the major cloud technologies, and into the commercial world of Data Engineering in the future. I’m also hoping we’ll get contributions to help flesh out this information now we have a starting point.&lt;/p&gt; &lt;p&gt;But, let’s summarise where we’ve got to with Hadoop, and pass some comment on ODPi which we looked at yesterday. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Hadoop, it’s ecosystem, and the myriad of options for using it are a complex whirling maelstrom of sound and fury. Understanding it is like trying to catch a paper bag in a strong wind - no matter how close you might get, something is always changing, meaning you’re never going to get there. Fun hyperbole aside, my hope is that the content I’ve added to this site over the last few months becomes the starting point for a set of signposts to get you started - be that the &lt;a href=&quot;/tech-categories/hadoop-distributions/&quot;&gt;Hadoop Distributions&lt;/a&gt; page which provides a summary of the options for deploying or using Hadoop capabilities, the &lt;a href=&quot;/tech-categories/hadoop-distributions/ecosystem/&quot;&gt;Hadoop ecosystem&lt;/a&gt; page which tries to summarise the technologies that run over HDFS and YARN, the &lt;a href=&quot;/tech-categories/hadoop-distributions/distribution-comparison/&quot;&gt;Hadoop distributions comparison&lt;/a&gt; page which tries to summarise the different technologies bundled with different Hadoop distributions, or the myriad of technology summaries that might give you a starting point for understanding the different options and capabilities in the wider Hadoop ecosystem. A lot of this content isn’t finished or polished, so at this time this is very much a starting point, and some pages have big banners to clearly show they’re draft and not complete.&lt;/p&gt; &lt;p&gt;There’s been a lot written and said about ODPi, and for a long time I wasn’t sure I was ever going to understand what it was or what it was trying to do. Time however tends to make stuff clearer, and whereas originally ODP talked a lot about a common core for Hadoop, it seems to have settled down on defining some specifications for Hadoop compatibility to ensure that a single piece of software will run on any Hadoop distribution. That’s a noble goal, however I think there chances of achieving it are slim to none - they’re lacking platform vendor support (no Cloudera, MapR, Microsoft, Oracle), the specifications seem extreemly lightweight (meaning compliance to the spec is unlikely to mean compatibility with all certified platforms), there’s very little software vendor buy in (less than 10 software vendors at the time of writing have certified compliance) and it just doesn’t have the breadth (covering only HDFS, YARN, MapReduce and Hive at the moment). It feels like an extreemly expensive and challenging problem to solve, which which there appears to be little demand at the moment.&lt;/p&gt; &lt;p&gt;And just before we move on, I’m doing some minor tidying up as part of the last two weeks thinking, which I’ll now try to summarise:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Each distributions technology page now includes a link to the list of certified software that runs on it&lt;/li&gt; &lt;li&gt;I’ve update the Hadoop ecosystem page to remove Flume running over YARN (because it doesn’t)&lt;/li&gt; &lt;li&gt;Where technologies run over HDFS or YARN, I’ve make sure their technology summaries include this&lt;/li&gt; &lt;li&gt;I’ve moved some of the information on the technology vendor pages into the distribution pages, for example which cloud providers they run on&lt;/li&gt; &lt;li&gt;For technologies that are compatible with HDFS I’ve added a new technology relationship to show this&lt;/li&gt; &lt;li&gt;I’ve added links to the Hadoop ecosystem page to the YARN and HDFS pages&lt;/li&gt; &lt;li&gt;I’ve tweaked some of the descriptions on the Hadoop distributions page, for example to detail where there are free versions of a distribution&lt;/li&gt; &lt;li&gt;I’ve updated the Slider technology summary to reference Hoya and Koya&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So what’s next. I think it’s time to broaden out a little, and to start looking at some of the different types of technologies that might be of interest to us, and for each one look at the technology options available to us. So we’ll try that.&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/05/19/hadoop-wrap-up/</guid> </item> <item><title>ODPi</title><link>http://ondataengineering.net/tech-vendors/opdi/</link><pubDate>Thu, 18 May 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;ODPi is a non profit organisation and member of the Linux Foundation that distributes reference specifications for key Hadoop components and APIs to help drive compatibility between Hadoop distributions, sponsoring Apache Bigtop as a reference implementation. Compliance against the spec for platform vendors (to ensure any certified app will run on their platform) and software vendors (to ensure their app will run on any certified platform) is achieved through self-certification against a test suite that's bundled with Apache Bigtop. Current technologies covered by the specifications are HDFS, YARN, MapReduce, HCFS and Hive. Current certified distributions include Altiscale, ArenaData, Hortonworks, IBM and Infosys but notably does not include either Cloudera or MapR who have both publicly stated their objections to the organisation. Currently certified applications are limited to DataTorrent, Apache Hawq, SAS, Syncsort, WANDisco and a range of IBM technologies. Originally founded in February 2005 as the Open Data Platform with language that suggested it was looking to build a standard Hadoop core (the ODP core) based on HDFS, Ambari, YARN and MapReduce. Moved under the Linux Foundation in September 2015.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Vendor Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;Open Data Platform Initiative&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;!-- Technology metadata --&gt; &lt;h2 id=&quot;specifications&quot;&gt;Specifications&lt;/h2&gt; &lt;p&gt;ODPi manages and issues two specifications:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The &lt;a href=&quot;https://github.com/odpi/specs/blob/master/ODPi-Runtime.md&quot;&gt;Runtime Specification&lt;/a&gt;, which provides compatibility and interoperability for a range of key Hadoop components and APIs, including HDFS, YARN, MapReduce, HCFS (Hadoop Compatible Filesystems) and Hive (specifically SQL, JDBC and beeline). Requires the use of Hadoop 2.7, Java 7 or 8, a standard set of environment variables, no modification of the public API and any addition features functions to be committed to the ASF. Uses Apache Bigtop as a reference implementation, with Bigtop also providing the certification test suite.&lt;/li&gt; &lt;li&gt;The &lt;a href=&quot;https://github.com/odpi/specs/blob/master/ODPi-Operations.md&quot;&gt;Operations Specification&lt;/a&gt;, which provides the same for managing and monitoring Apache Hadoop clusters with Ambari as reference implementation, including a contrib management pack that allows any product based on Ambari to deploy the ODPi reference implementation.&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;td&gt;March 2016&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt;MVP for ODPi runtime spec, validation test suite and reference implementation covering HDFS, YARN and MapReduce&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.0&lt;/td&gt; &lt;td&gt;November 2016&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://www.theregister.co.uk/2016/11/14/odpi_20/&quot;&gt;viewpoint from TheRegister&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Added Operations Specification, and support for HCFS and Hive to the Runtime Specification&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2.1&lt;/td&gt; &lt;td&gt;April 2017&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;https://www.odpi.org/blog/2017/04/05/odpi-2-1-a-tick-for-the-future-tock&quot;&gt;summary&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Moved to using Apache Bigtop for validation testsuite and reference implementation - ODPi release now only includes specifications&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;criticisms&quot;&gt;Criticisms&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://vision.cloudera.com/the-open-data-platform-alliance/&quot;&gt;Cloudera’s statement&lt;/a&gt; was that there was no demand for a compliance standard from any of their partners, that the organisation was a pay-to-join vendor driven consortium that was contrary to the open source nature of Hadoop, and that open source trumps vendor driven standardisation with reference to Linux’s success and the subsequent Linux ecosystem over the attend to standardise *NIX distributions.&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/blog/our-view-open-data-platform/&quot;&gt;MapR’s statement&lt;/a&gt; largely echoed Cloudera’s with the primary argument being that there wasn’t a requirement, citing a Gartner survey in which less than 1% of companies thought vendor lock in was an issue, and that there was a significant lack of breadth in the compliance technologies&lt;/li&gt; &lt;li&gt;TheRegister has a good summary of Cloudera and MapR’s positions &lt;a href=&quot;http://www.theregister.co.uk/2015/04/24/mapr_odp_cloudera/&quot;&gt;here&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A &lt;a href=&quot;http://www.theregister.co.uk/2016/11/14/odpi_20/&quot;&gt;subsequent article from TheRegister&lt;/a&gt; to coincide with the release of ODPi 2.0 includes an interview with Charaka Goonatilake, the CTO of Panaseer, that covers their two main issues as a software vendor, specifically that it doesn’t cover enough technologies or go deep enough, meaning it doesn’t actually help them test their product once and have confidence it’s going to work across all Hadoop distributions.&lt;/li&gt; &lt;li&gt;ZDNet have also a &lt;a href=&quot;http://www.zdnet.com/article/odpi-runtime-spec-aims-to-defrag-hadoop/&quot;&gt;useful write-up&lt;/a&gt; of the challenges ODPi is experiencing&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.odpi.org/&quot;&gt;https://www.odpi.org/&lt;/a&gt; - ODPi homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/odpi&quot;&gt;https://github.com/odpi&lt;/a&gt; - ODPi repository&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/odpi/specs/wiki&quot;&gt;https://github.com/odpi/specs/wiki&lt;/a&gt; - ODPi specifications Wiki&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://pivotal.io/big-data/press-release/technology-leaders-unite-around-open-data-platform-to-increase-enterprise-adoption-of-hadoop-and-big-data&quot;&gt;Pivotal launch press release&lt;/a&gt;, including links to the press releases from Altiscale, SAS and Verizon&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://content.pivotal.io/blog/open-data-platform-initiative-putting-an-end-to-faux-pen-source-apache-hadoop-distributions&quot;&gt;Summary of the original ODP plans&lt;/a&gt; from Roman Shaposhnik&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.odpi.org/blog&quot;&gt;https://www.odpi.org/blog&lt;/a&gt; - ODPi blog&lt;/li&gt; &lt;li&gt;ODPi also have a subscription newsletter&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/tech-vendors/opdi/</guid> </item> <item><title>The Mid Week News - 17/05/2017</title><link>http://ondataengineering.net/blog/2017/05/17/the-mid-week-news/</link><pubDate>Wed, 17 May 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;Time for a short diversion from wrapping up our look at Hadoop to catch up on the news… &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;New technology releases:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Version 1.2 of &lt;a href=&quot;/technologies/apache-nifi/&quot;&gt;Apache NiFi&lt;/a&gt; is out. See the &lt;a href=&quot;https://cwiki.apache.org/confluence/display/NIFI/Release+Notes#ReleaseNotes-Version1.2.0&quot;&gt;summary of new changes&lt;/a&gt; for details.&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/technologies/apache-ignite&quot;&gt;Apache Ignite&lt;/a&gt; and it’s commercial edition GridGain Professional have seen a big 2.0 releases with a complete redesign of the off-heap memory architecture which should allow the extension of in-memory data structures to SSD disks. See the &lt;a href=&quot;https://blogs.apache.org/ignite/entry/apache-ignite-2-0-redesigned&quot;&gt;Ignite announcement&lt;/a&gt; and the &lt;a href=&quot;https://www.gridgain.com/resources/blog/gridgain-professional-edition-20-released-today&quot;&gt;GridGain Professional announcement&lt;/a&gt; for more details.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Interesting blog posts:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://www.ebaytechblog.com/2017/05/12/enhancing-the-user-experience-of-the-hadoop-ecosystem/&quot;&gt;eBay on their use of Hue, Zeppelin and Knox&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://dzone.com/articles/how-to-become-a-data-engineer&quot;&gt;Some thoughts on being a Data Engineer&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/blog/watermarks-tables-event-time-dataflow-model/&quot;&gt;A post from Confluent&lt;/a&gt; (who are therefore slightly biased) on why Kafka Tables provide a better solution that windowing and watermarks (with specific reference to the Beam API) for calculating rolling aggregates of revisions or updates&lt;/li&gt; &lt;li&gt;Hortonworks have been on a bit of a roll with multi-part blog posts: &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/part-1-hortonworks-building-successful-streaming-analytics-platforms/&quot;&gt;Building streaming analytics platforms&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/part-5-of-data-lake-3-0-yarn-and-containerization-supporting-docker-and-beyond/&quot;&gt;Thoughts on Data Lake 3.0&lt;/a&gt;, including docker support in YARN&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/apache-hive-druid-part-1-3/&quot;&gt;OLAP analytics with Druid and Hive&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://hortonworks.com/blog/enterprise-security-governance-part-1/&quot;&gt;Update on governance and security in HDP 2.6&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2017/05/how-to-backup-and-disaster-recovery-for-apache-solr-part-i/&quot;&gt;How to backup and recover Solr&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://data-artisans.com/blog/official-docker-images-apache-flink&quot;&gt;An official docker image for Flink&lt;/a&gt; is now available from Data Artisans&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/blog/how-get-started-spark-streaming-and-mapr-streams-using-kafka-api/&quot;&gt;Using Spark Streaming and MapR-Streams&lt;/a&gt; from the MapR blog&lt;/li&gt; &lt;li&gt;And a couple of interesting posts from Adrian Colyer’s The Morning Paper &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://blog.acolyer.org/2017/05/04/cherrypick-adaptively-unearthing-the-best-cloud-configurations-for-big-data-analytics/&quot;&gt;CherryPick - a system for determining the best cloud configurations for big data analytics&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://blog.acolyer.org/2017/05/15/trajectory-recovery-from-ash-user-privacy-is-not-preserved-in-aggregated-mobility-data/&quot;&gt;Reconstructing individual user data from aggregated mobility data&lt;/a&gt; - well worth a quick look&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/05/17/the-mid-week-news/</guid> </item> <item><title>Hadoop Technology Options</title><link>http://ondataengineering.net/blog/2017/05/16/hadoop-technology-options/</link><pubDate>Tue, 16 May 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;In the last post we looking at why you might want to use Hadoop - today I want to dig into the options for deploying or using Hadoop capabilities. Consider this a companion piece to the list of these options that’s now available on our &lt;a href=&quot;/tech-categories/hadoop-distributions/&quot;&gt;Hadoop Distributions&lt;/a&gt; page &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Broadly speaking, if you’re looking at deploying Hadoop the options you have fall into three categories.&lt;/p&gt; &lt;p&gt;Firstly, you can use an Hadoop managed service. This automates the provisioning and management of Hadoop, allowing you to easily create, scale and destroy clusters. You’ll obviously need to either have or get your data into the cloud, however this can be a great option for dipping your toe in the waters and exploring what Hadoop can do in a very cost effective way. However, if you want a persistent cluster, or if you’re doing anything at a significant scale, then this can get expensive very quickly, so it’s worth doing your sums first. Hadoop as a service does open up another use case however - if you already have you data in the cloud then you can use an Hadoop managed service to analyse that data in place using transient processing clusters - just spin up a cluster for a specific workload and then terminate it, meaning that you’re only paying whilst your workload is running. Note however that cloud object stores are significantly slower than HDFS running on local storage in a cloud based cluster.&lt;/p&gt; &lt;p&gt;Secondly, you can deploy and manage Hadoop yourself, either on your own infrastructure or on cloud infrastructure. The choice of your own vs cloud infrastructure is subject to all the usual considerations around TCO, manageability and having your data into the cloud, however both Cloudera and Hortonworks include tools (Cloudera Directory and Cloudbreak respectively) for managing cloud based deployments (including the provisioning of cloud infrastructure and the ability to scale up and down) that are also probably your prime options if you’re running an internal cloud such as OpenStack. Whatever infrastructure you use, you’re responsible for your Hadoop installation, which means you’ll need to make a decision about which Hadoop distribution to use, and whether you use a free open source version or purchase commercial support. You’ll also need to make sure you have access to all the specialist skills required to deploy, secure and manage your cluster - although the various management tools have improved significantly over the last few years, there are still significant decisions and configuration work to be done.&lt;/p&gt; &lt;p&gt;Lastly, you can use an Hadoop appliance - a prepackaged bundle of dedicated infrastructure and Hadoop software. There’s generally price premium for these, however they’re generally well architected for performance and can massively accelerate an onsite deployment, although they may or may not be easier to manage that a custom Hadoop deployment.&lt;/p&gt; &lt;p&gt;Selecting an Hadoop technology is never going to be a quick or easy process - the range of different options, the different ways it can be deployed, the range of different capabilities, the different cost models and level of support, the skills required and the management and maintenance costs make it a complex decision. However I’m hoping that the material on this site will give you a starting point for understanding the different options to help inform this decision making process. The &lt;a href=&quot;/tech-categories/hadoop-distributions/&quot;&gt;Hadoop Distributions&lt;/a&gt; page that lists the various options should be your first call, and I’ve started a comparison of the technologies bundled in the major distributions that we’ve looked at to date on an &lt;a href=&quot;/tech-categories/hadoop-distributions/distribution-comparison/&quot;&gt;Hadoop Distributions Comparison&lt;/a&gt; page. Neither of these are complete by a long stretch, however let me know of any obvious gaps, or even better send through a pull request with any new information.&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/05/16/hadoop-technology-options/</guid> </item> <item><title>Why Choose Hadoop?</title><link>http://ondataengineering.net/blog/2017/05/12/why-choose-hadoop/</link><pubDate>Fri, 12 May 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;So our Hadoop Distros week is going to be a bit longer than a week, but hey, we’re all flexible and adaptable right?&lt;/p&gt; &lt;p&gt;Today I’d like to spout some thoughts about why you might consider Hadoop, and I’d like to start but looking at it’s history. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;Hadoop was originally designed for the single specific use case of doing aggregations over enormous volumes of data, and the combination of HDFS and MapReduce delivered this capability in a way that (at least at the very large scale) was not possible before and hasn’t really been superseded since. Pig and Hive were introduced as nicer ways to write MapReduce code, but didn’t fundamentally change anything. Hadoop, being open sourced, was then picked up a number of companies (both vendors and users) and taken in a couple of (largely complementary) directions.&lt;/p&gt; &lt;p&gt;First, HDFS was positioned as a great place to put all your data so that you could run a range of analytics over the top - the mythical Data Lake (although we’ll definitely talk about the challenges of building a Data Lake with Hadoop at some point). This required new technologies to bring data in (Flume and Sqoop for example), new technologies to exploit this data (Spark and Mahout for example), and a way to make all these technologies play nicely together (YARN). However there’s only so far that data in a filesystem will get you in terms of analytics - if you’re looking to do anything outside of batch appends and scanning workloads you’re going to struggle.&lt;/p&gt; &lt;p&gt;And so other technologies were introduced that used HDFS as an underlying storage technology to enable new data storage capabilities - HBase as a NoSQL database and Solr for search indexing for example. And what that created was not a single place to put all your data, but a range of complementary technologies that support different use cases, but that can share underlying infrastructure. All of which is good.&lt;/p&gt; &lt;p&gt;But there were always going to be challenges trying to move Hadoop from where it started to a more general purpose capability - you’re always going to be pushing against it’s original architectural and design decisions. HDFS is not a general purpose cluster filesystem - it was designed for a specific use case (for example it can only do file appends rather than random updates and has a hard limit on the number of files it can hold based on the memory capacity of the Name Node for), which can cause limitations when trying to use it for more general purpose analytics or to underpin other technologies (Kudu has chosen not run over HDFS for example). And YARN was a relatively late addition to Hadoop, meaning many Hadoop technologies don’t support it (including Flume, Solr and HBase, although Hortonworks is trying to address this through Slider). Which means we’re now in a position whereby you potentially have multiple technologies competing rather than co-existing on our Hadoop cluster, which feels like it’s starting to dilute some of the potential value. If you’re interested in which technologies do or don’t run over HDFS / YARN, I’ve tried to summarise it in diagrammatic form &lt;a href=&quot;/tech-categories/hadoop-distributions/ecosystem/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;So what are your options around Hadoop? Firstly, as an ecosystem it contains a good set of technologies. HDFS plus Hive/Spark/etc. is a great platform for doing batch scanning analytical workloads. HBase and Solr are great technologies that stand up well to their competition, and Hive and Impala are starting to provide some serious competition for the established MPP database vendors. Deploying any one of these technologies to fulfil a role in your wider analytical ecosystem will do you well, but if you’re going to use a commercial service you’ll need to find a cost effective way of doing this - you don’t want to be buying the entire ecosystem if you’re not going to use it all. This is where Cloudera are going - starting to offer tailor packages that include sub-sets of the components focusing on specific use cases, and most of the Cloud or Hadoop as a Service offerings allow you to only pay for what you use.&lt;/p&gt; &lt;p&gt;Or you can deploy Hadoop as a common analytical platform - a single set of infrastructure and a single purchase to give you a single platform that can deliver you a range of capabilities and fulfil a range of roles in a cost effective way. Note that this generally only works if you’re deploying on site rather than using the Cloud however. This is where Hortonworks and MapR are focusing - Hortonworks is investing in technologies such as Slider to allow everything to integrate with YARN, and MapR have built their entire offering around their own shared multi-tenancy storage capability (MapR-FS) that is designed and built for exactly this use case (and fulfils it better than HDFS).&lt;/p&gt; &lt;p&gt;In summary, Hadoop like any technology has it’s strengths and weaknesses. It’s not the be all and end all, it’s certainly not going to solve all your problems and magically make you a data driven organisation, it’s not going to dramatically decrease your costs, and deploying it is going to be as much hard work than deploying any other technology. However I’m hoping that the material I’ve added to this site over the last few months will help you start to understand what Hadoop is and how it might mean one or more of your use cases, whether that’s helping you start an evaluation of how one of it’s technologies stacks up against it’s competition, or to understand the ecosystem and how a specific distribution or offering can meet your range of use cases.&lt;/p&gt; &lt;p&gt;So that’s some thoughts on why Hadoop - on Monday we’ll summarise the options for how you can get it…&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/05/12/why-choose-hadoop/</guid> </item> <item><title>Hadoop Ecosystem</title><link>http://ondataengineering.net/tech-categories/hadoop-distributions/ecosystem/</link><pubDate>Thu, 11 May 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;A view of the ecosystem around HDFS and YARN - the two major Hadoop components. Note that many technologies are not exclusive, and may run over other cluster filesystems or over other cluster management frameworks. Some technologies may be excluded for brevity and clarity.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;!-- Tech Vendor metadata --&gt; &lt;h2 id=&quot;hdfs-ecosystem&quot;&gt;HDFS Ecosystem&lt;/h2&gt; &lt;p&gt;&lt;img src=&quot;/images/hdfs-ecosystem.png&quot; alt=&quot;HDFS Ecosystem&quot; /&gt;&lt;/p&gt; &lt;h2 id=&quot;yarn-ecosystem&quot;&gt;YARN Ecosystem&lt;/h2&gt; &lt;p&gt;&lt;img src=&quot;/images/yarn-ecosystem.png&quot; alt=&quot;YARN Ecosystem&quot; /&gt;&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/tech-categories/hadoop-distributions/ecosystem/</guid> </item> <item><title>Hadoop Distributions Comparison</title><link>http://ondataengineering.net/tech-categories/hadoop-distributions/distribution-comparison/</link><pubDate>Wed, 10 May 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;A comparison of different Hadoop distributions, focusing on the different software packages available in each to enable an understanding of whether a distribution contains the appropriate software to meet a requirement or business case.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;!-- Tech Vendor metadata --&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Component&lt;/th&gt; &lt;th&gt;Cloudera&lt;/th&gt; &lt;th&gt;HW&lt;/th&gt; &lt;th&gt;MapR&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Cluster Management&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/yarn&quot;&gt;YARN&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/yarn&quot;&gt;YARN&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-slider&quot;&gt;Slider&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/yarn&quot;&gt;YARN&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-myriad&quot;&gt;Myriad&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href=&quot;/tech-categories/hadoop-compatible-filesystems&quot;&gt;Hadoop Compatible Filesystem&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/hdfs&quot;&gt;HDFS&lt;/a&gt;; &lt;a href=&quot;/technologies/recordservice&quot;&gt;RecordService&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/hdfs&quot;&gt;HDFS&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/mapr-file-system&quot;&gt;MapR-FS&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;NoSQL Datastore&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hbase&quot;&gt;HBase&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-accumulo&quot;&gt;Accumulo&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hbase&quot;&gt;HBase&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-accumulo&quot;&gt;Accumulo&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/mapr-file-system/mapr-db&quot;&gt;MapR-DB&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-hbase&quot;&gt;HBase&lt;/a&gt;;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;SQL Datastore&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-kudu&quot;&gt;Kudu&lt;/a&gt; + &lt;a href=&quot;/technologies/apache-impala&quot;&gt;Impala&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-phoenix&quot;&gt;Phoenix&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-hawq&quot;&gt;Hawq&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Streaming Datastore&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-kafka&quot;&gt;Kafka&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-kafka&quot;&gt;Kafka&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/mapr-file-system/mapr-streams/&quot;&gt;MapR-Streams&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Batch Analytics&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hive&quot;&gt;Hive&lt;/a&gt; (on Spark); &lt;a href=&quot;/technologies/apache-hadoop/map-reduce&quot;&gt;MapReduce&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-pig&quot;&gt;Pig&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-spark&quot;&gt;Spark&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hive&quot;&gt;Hive&lt;/a&gt; (on Tez); &lt;a href=&quot;/technologies/apache-hadoop/map-reduce&quot;&gt;MapReduce&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-pig&quot;&gt;Pig&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-spark&quot;&gt;Spark&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-hawq&quot;&gt;Hawq&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hive&quot;&gt;Hive&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-hadoop/map-reduce&quot;&gt;MapReduce&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-pig&quot;&gt;Pig&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-spark&quot;&gt;Spark&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Streaming Analytics&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-spark/spark-streaming&quot;&gt;Spark Streaming&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-spark/spark-streaming&quot;&gt;Spark Streaming&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-storm&quot;&gt;Storm&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-spark/spark-streaming&quot;&gt;Spark Streaming&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-storm&quot;&gt;Storm&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Graph Analytics&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-spark/graphx&quot;&gt;Spark GraphX&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-spark/graphx&quot;&gt;Spark GraphX&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-spark/graphx&quot;&gt;Spark GraphX&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Interactive SQL&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-impala&quot;&gt;Impala&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-hive&quot;&gt;Hive&lt;/a&gt; (LLAP)&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-drill&quot;&gt;Drill&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-impala&quot;&gt;Impala&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Machine Learning&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-mahout&quot;&gt;Mahout&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-spark/mllib&quot;&gt;Spark MLlib&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-mahout&quot;&gt;Mahout&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-spark/mllib&quot;&gt;Spark MLlib&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-mahout&quot;&gt;Mahout&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-spark/mllib&quot;&gt;Spark MLlib&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Search&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-solr&quot;&gt;Solr&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-solr&quot;&gt;Solr&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-solr&quot;&gt;Solr&lt;/a&gt; (available as an add on pack)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Data Ingestion&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-sqoop&quot;&gt;Sqoop&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-flume&quot;&gt;Flume&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-sqoop&quot;&gt;Sqoop&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-flume&quot;&gt;Flume&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-sqoop&quot;&gt;Sqoop&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-flume&quot;&gt;Flume&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Data Flow Management&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-falcon&quot;&gt;Falcon&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Workflow Management&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-oozie&quot;&gt;Oozie&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-oozie&quot;&gt;Oozie&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-oozie&quot;&gt;Oozie&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Security&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-sentry&quot;&gt;Sentry&lt;/a&gt;; &lt;a href=&quot;/technologies/recordservice&quot;&gt;RecordService&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-knox&quot;&gt;Knox&lt;/a&gt;; &lt;a href=&quot;/technologies/apache-ranger&quot;&gt;Ranger&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-sentry&quot;&gt;Sentry&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Management&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/cloudera-manager&quot;&gt;Cloudera Manager&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-ambari&quot;&gt;Ambari&lt;/a&gt;&lt;/td&gt; &lt;td&gt;MapR Installer; MapR Control System; &lt;a href=&quot;/technologies/mapr-monitoring&quot;&gt;MapR Monitoring&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Metadata&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/cloudera-navigator&quot;&gt;Cloudera Navigator&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-atlas&quot;&gt;Atlas&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Cloud Management&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/cloudera-director&quot;&gt;Cloudera Director&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/cloudbreak&quot;&gt;Cloudbreak&lt;/a&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;User Interfaces&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/hue&quot;&gt;Hue&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/apache-zeppelin&quot;&gt;Zeppelin&lt;/a&gt;; &lt;a href=&quot;/technologies/hue&quot;&gt;Hue&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;/technologies/hue&quot;&gt;Hue&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/tech-categories/hadoop-distributions/distribution-comparison/</guid> </item> <item><title>Hadoop Distros Week</title><link>http://ondataengineering.net/blog/2017/05/08/hadoop-distros-week/</link><pubDate>Mon, 08 May 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;We’ve now finished looking at the major players in the Hadoop distributions space, so I’d like to take this week to look at Hadoop and Hadoop distributions in a bit more detail, and flesh out our &lt;a href=&quot;tech-categories/hadoop_distributions/&quot;&gt;Hadoop Distributions&lt;/a&gt; page. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;There’s obviously a whole pile of technologies that sit in this space we’ve not looked at, so first steps will be to refresh our definition of an Hadoop distribution, and try and get a slightly more complete list in place, including the Cloud Hadoop-as-a-service offerings, the smaller more boutique vendors, the appliances and maybe a footnote on the distributions that are no longer with us.&lt;/p&gt; &lt;p&gt;I’d then like to look at the Hadoop ecosystem in a bit more detail, and try and map out which components sit where. I’d also like to try and do a comparison between the major offerings, focusing on the different technologies they include. And finally, assuming I’ve not run out of time, a quick look at ODPI (The Open Data Platform Initiative).&lt;/p&gt; &lt;p&gt;Right, time to crack on - see you shortly.&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/05/08/hadoop-distros-week/</guid> </item> <item><title>The Week That Was - 05/05/2017</title><link>http://ondataengineering.net/blog/2017/05/05/the-week-that-was/</link><pubDate>Fri, 05 May 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;It’s nearly the weekend, which means it’s time to summarise the week.&lt;/p&gt; &lt;p&gt;We started late this week (let’s hear it for public holidays), finishing off our look at MapR, before taking a quick look at Cloudera’s &lt;a href=&quot;/technologies/cloudera-data-science-workbench/&quot;&gt;Data Science Workbench&lt;/a&gt;. No technology summary today, but we’ll take a look at Apache Metron as part of this post. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;What I’ve really liked about MapR is their strategy around their common data platform to underpin a bunch of different data storage capabilities. I talked a little bit about their data platform &lt;a href=&quot;/blog/2017/04/28/the-week-that-was/&quot;&gt;last time&lt;/a&gt;, but this week as part of looking at &lt;a href=&quot;/technologies/mapr-file-system/mapr-db/&quot;&gt;MapR-DB&lt;/a&gt; and &lt;a href=&quot;/technologies/mapr-file-system/mapr-streams/&quot;&gt;MapR-Streams&lt;/a&gt; I’ve been thinking about how this compares and contrasts to Hadoop. Firstly, they’re both aiming to provide a common data platform that provides the ability to have a single cluster than can provide flexibility and value for money by allowing you to exploit the same infrastructure for multiple use cases. MapR appears to have fully embraced this, ensuring they support the ability to scale, partition and manage the platform in ways that Hadoop can’t yet, and by providing capabilities that Hadoop (and more specifically HDFS) doesn’t that actually make it work as a general purpose data platform - full random read and write access for starters. I’m also taken by MapR’s ability to provide access to the common data platform at different layers - rather than just build capabilities on top of their file system API, they’ve integrated (for example) MapR-DB at a much lower level, providing a range of benefits over HBase running over HDFS. It’s clear that Hadoop still has a long way to go to fulfil it’s potential, and without addressing some of it’s limitations we’re going to continue to see new technologies opting to implement their own storage systems from scratch (Kudu being a great example), leading to Hadoop clusters running multiple independent storage stacks on the same data nodes, which feels like it’s defeating the point.&lt;/p&gt; &lt;p&gt;I’ve also started wondering why there aren’t more common storage sub-system’s that multiple technologies leverage - not necessarily so that they could all co-exist on the same cluster (along this would be a side benefit), but just because storage systems are hard and complex, and it feels like there should be huge wins by having a strong and robust solution that can be leveraged for multiple capabilities. There are very few data platforms that don’t have some limitation or constraint, and that a world class storage system with a range of APIs implemented on top could be instantly competitive against a wide range of technologies. MapR are starting to demonstrate this - there’s certainly some evidence that MapR-Streams leverages their data platform and a Kafka compatible API to provide a solution that addresses a number of Kafka’s limitations.&lt;/p&gt; &lt;p&gt;Moving on, Cloudera’s &lt;a href=&quot;/technologies/cloudera-data-science-workbench/&quot;&gt;Data Science Workbench&lt;/a&gt; is now generally available. Their use of docker seems inspired - the flexibility this gives you to use different versions of different libraries in different notebooks, and to have this execution environment follow the notebook around feels like a huge win. It’s still early days for the product however - the number of interpreters seems light (not being able to run SQL or Solr searches directly in the notebook feels like a gap), and it remains to be seen how it will fair as a commercial product against the open source &lt;a href=&quot;/technologies/apache-zeppelin/&quot;&gt;Apache Zeppelin&lt;/a&gt; and Jupyer.&lt;/p&gt; &lt;p&gt;I said on Wednesday I’d do a technology summary of &lt;a href=&quot;http://metron.apache.org/&quot;&gt;Apache Metron&lt;/a&gt;, however I’ve decided as a packaged analytical application it’s probably slightly outside the conceptual remit of this site. It’s definitely worth digging into if you have time however, as it’s an interesting use case for what can be done with the Hadoop ecosystem, and an interesting capability in it’s own right. If you’re looking for reading material, there’s the &lt;a href=&quot;http://metron.apache.org/&quot;&gt;Apache Metron homepage&lt;/a&gt;, &lt;a href=&quot;https://hortonworks.com/apache/metron/&quot;&gt;Hortonwork’s overview of Metron&lt;/a&gt;, and their &lt;a href=&quot;http://docs.hortonworks.com/HDPDocuments/HCP1/HCP-1.1.0/index.html&quot;&gt;user documentation&lt;/a&gt;, but a good a place to start as any is the &lt;a href=&quot;https://cwiki.apache.org/confluence/display/METRON/Metron+Architecture&quot;&gt;architecture overview&lt;/a&gt; in the Apache Metron Wiki. In summary, the architectures probably fairly standard - Apache Kafka as an input point, fed by custom probes and Apache NiFi, with data then processed using Apache Storm supporting a level of data transformation and enrichment, real time alerting and built in scoring using machine learning models, with persistence of storage into HDFS and HBase, and then a range of dashboards and visualisation capabilities over the top. Apache Spark is in there somewhere as well.&lt;/p&gt; &lt;p&gt;That’s all (for this week) folks - see you on Monday.&lt;/p&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/05/05/the-week-that-was/</guid> </item> <item><title>Cloudera Data Science Workbench</title><link>http://ondataengineering.net/technologies/cloudera-data-science-workbench/</link><pubDate>Thu, 04 May 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;A web based notebook for interactive data analytics that uses docker to provide custom execution environments for each notebook. Supports Python, R and Scala interpreters, plus remote execution of Spark with out of the box support for Hadoop security. Notebook code is run within a docker container in a managed Kubernetes instance, allowing different libraries to be installed and used by different notebooks, and other dependancies to be installed via terminal access to the container or via custom Docker images. Also includes support for version control (via git), collaboration via shared projects, sharing of notebooks via HTTP URLs, publishing of notebooks as HTML and scheduled execution of notebooks via workflows (including dependancies on other jobs). Originally created by Sense.io, which was acquired by Cloudera in March 2016. Initial GA release was 1.0 in April 2017.&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/cloudera/&quot;&gt;Cloudera&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;May 2017 - v1.0&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;release-history&quot;&gt;Release History&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;version&lt;/td&gt; &lt;td&gt;release date&lt;/td&gt; &lt;td&gt;release links&lt;/td&gt; &lt;td&gt;release comment&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;td&gt;2017-04-26&lt;/td&gt; &lt;td&gt;&lt;a href=&quot;http://community.cloudera.com/t5/Community-News-Release/Announce-Cloudera-Data-Science-Workbench-is-now-available/m-p/54177#M173&quot;&gt;announcement&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Initial release&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/products/data-science-and-engineering/data-science-workbench.html&quot;&gt;https://www.cloudera.com/products/data-science-and-engineering/data-science-workbench.html&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://vision.cloudera.com/cloudera-data-science-workbench-self-service-data-science-for-the-enterprise/&quot;&gt;http://vision.cloudera.com/cloudera-data-science-workbench-self-service-data-science-for-the-enterprise/&lt;/a&gt; - introductory blog post&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.cloudera.com/documentation/data-science-workbench/latest.html&quot;&gt;https://www.cloudera.com/documentation/data-science-workbench/latest.html&lt;/a&gt; - documentation&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Blog updates via the Cloudera blog&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/technologies/cloudera-data-science-workbench/</guid> </item> <item><title>The Mid Week News - 03/05/2017</title><link>http://ondataengineering.net/blog/2017/05/03/the-mid-week-news/</link><pubDate>Wed, 03 May 2017 07:45:00 +0100</pubDate> <description> &lt;p&gt;So, I failed at the first hurdle in trying to do this weekly, however let’s carry on regardless.&lt;/p&gt; &lt;p&gt;This week - new products from Cloudera and Hortonworks, a bunch of Hortonworks and Cloudera releases that got missed last time, plus a collection of blog posts I’ve been collecting for a while. &lt;!--more--&gt;&lt;/p&gt; &lt;p&gt;In terms of the new products from Cloudera and Hortonworks, we’ve seen &lt;a href=&quot;https://www.cloudera.com/products/data-science-and-engineering/data-science-workbench.html&quot;&gt;Cloudera Data Science Workbench&lt;/a&gt; and &lt;a href=&quot;https://hortonworks.com/apache/metron/&quot;&gt;Apache Metron&lt;/a&gt; formally released recently. I’m aiming to do tech summaries for both this week and we’ll look at these a bit closer.&lt;/p&gt; &lt;p&gt;Some Hortonworks updates that we missed last time - &lt;a href=&quot;/technologies/hortonworks-data-cloud-for-aws&quot;&gt;Hortonworks Data Cloud for AWS&lt;/a&gt; has seen a new release to 1.14, &lt;a href=&quot;/technologies/hortonworks-smartsense/&quot;&gt;Hortonworks SmartSense&lt;/a&gt; got a bump to 1.4, and it looks like &lt;a href=&quot;/technologies/hortonworks-data-platform-for-windows/&quot;&gt;HDP for Windows&lt;/a&gt; got discontinued whilst I wasn’t looking - 2.4 was the final version!&lt;/p&gt; &lt;p&gt;And on the Cloudera front, &lt;a href=&quot;/technologies/cloudera-manager/&quot;&gt;Cloudera Manager&lt;/a&gt;, &lt;a href=&quot;/technologies/cloudera-navigator/&quot;&gt;Cloudera Navigator&lt;/a&gt; and &lt;a href=&quot;/technologies/cloudera-director&quot;&gt;Cloudera Director&lt;/a&gt; have all seen version bumps as part of the CDH 5.11 release&lt;/p&gt; &lt;p&gt;And finally, some assorted blog posts that have caught my attention recently:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cloudera have released &lt;a href=&quot;http://blog.cloudera.com/blog/2017/04/apache-impala-leads-traditional-analytic-database/&quot;&gt;their last blog post&lt;/a&gt; on how much faster Impala is than anything else. Expect one from Hortonworks shortly that shows that Hive LLAP is actually the fastest.&lt;/li&gt; &lt;li&gt;From the ever excellent “The Morning Paper”, &lt;a href=&quot;https://blog.acolyer.org/2017/03/06/hopfs-scaling-hierarchical-file-system-metadata-using-newsql-databases/amp/&quot;&gt;a summary of a research paper on HopFS&lt;/a&gt;, a version of HDFS where the in-memory metadata database in the Name Node is replaced with a distributed database, allowing it to scale to much larger numbers of files and dramatically increase throughput.&lt;/li&gt; &lt;li&gt;An interesting &lt;a href=&quot;http://www.odbms.org/blog/2017/03/on-the-new-developments-in-apache-spark-and-hadoop-interview-with-amr-awadallah/&quot;&gt;interview with the CTO of Cloudera&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Merv Adrian’s latest &lt;a href=&quot;http://blogs.gartner.com/merv-adrian/2017/03/16/hadoop-tracker-march-2017/&quot;&gt;Hadoop tracker&lt;/a&gt; is out. I’m not sure you can directly compare the component versions in Hadoop distributions given how much each vendor pulls patches forward, but it’s an interesting analysis never-the-less.&lt;/li&gt; &lt;li&gt;The Flink blog has been busy with a &lt;a href=&quot;http://data-flair.training/blogs/apache-flink-ecosystem-components-tutorial/&quot;&gt;summary of the Flink ecosystem&lt;/a&gt; and a &lt;a href=&quot;http://data-flair.training/blogs/spark-vs-flink-vs-hadoop-comparison/&quot;&gt;comparison of Flink to Spark and MapReduce&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.datanami.com/2017/03/13/hadoop-failed-us-tech-experts-say/&quot;&gt;Hadoop has failed us&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Some &lt;a href=&quot;http://www.zdnet.com/article/at-analyst-conference-cloudera-focuses-message-pleads-the-fifth-on-ipo-rumors/&quot;&gt;analysis on Cloudera’s strategy&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Tech deep dives into &lt;a href=&quot;https://blogs.apache.org/hbase/entry/accordion-hbase-breathes-with-in&quot;&gt;HBase In-Memory Compaction&lt;/a&gt; and &lt;a href=&quot;http://blog.cloudera.com/blog/2017/04/apache-kudu-read-write-paths/&quot;&gt;Kudu read write paths&lt;/a&gt;&lt;/li&gt; &lt;li&gt;And &lt;a href=&quot;https://www.theregister.co.uk/2017/04/07/google_cloud_platform_partners_with_elastic_in_new_analytics_stretch/&quot;&gt;Elasticsearch is coming to Google’s Cloud Platform&lt;/a&gt;&lt;/li&gt; &lt;li&gt;And last but not least - Matt Turck’s monster &lt;a href=&quot;http://mattturck.com/bigdata2017/&quot;&gt;2017 Big Data Landscape&lt;/a&gt; - essential reading&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/blog/2017/05/03/the-mid-week-news/</guid> </item> <item><title>MapR Converged Data Platform</title><link>http://ondataengineering.net/technologies/mapr-converged-data-platform/</link><pubDate>Wed, 03 May 2017 07:30:00 +0100</pubDate> <description> &lt;p&gt;A data platform built around MapR-FS (along with MapR-DB and MapR-Streams) that provides Hadoop compatibility (via YARN and the MapR-FS HDFS compatible API) and is bundled with a package of Hadoop projects via the MapR Ecosystem Pack. Comes with an installer (MapR Installer) and a web based user interface for management (MapR Control System). Available as a single node version (for development and testing), as MapR Edge (a small footprint edition that can run on low power and embedded hardware close to data sources to perform initial data filtering and processing before forwarding data on to a central cluster via MapR replication), and as MapR Converged Data Platform for Docker (a marketing name for using the Converged Data Platform as persistent storage for docker containers, which is supported by the availability of PACC, the Persistent Application Client Container, a docker image containing the client libraries required for connecting to a MapR Converged Data Platform). Distributed as a free community edition (which excludes some enterprise features such as snapshots, high availability, disaster recovery and replication), and as a number of commercial editions. First released as MapR v1.0 in 2010&lt;/p&gt; &lt;!-- Tech Category metadata --&gt; &lt;h2&gt;Technology Information&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Other Names&lt;/td&gt;&lt;td&gt;MapR Edge, MapR Converged Data Platform for Docker&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Vendors&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-vendors/mapr/&quot;&gt;MapR&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Categories&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/tech-categories/hadoop-distributions/&quot;&gt;Hadoop Distributions&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Type&lt;/td&gt;&lt;td&gt;Commercial&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Last Updated&lt;/td&gt;&lt;td&gt;May 2017 - 5.2&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Related Technologies&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td&gt;Packages&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/technologies/apache-hadoop/map-reduce/&quot;&gt;MapReduce&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hadoop/yarn/&quot;&gt;YARN&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-zookeeper/&quot;&gt;Apache ZooKeeper&lt;/a&gt;, &lt;a href=&quot;/technologies/mapr-ecosystem-pack/&quot;&gt;MapR Ecosystem Pack&lt;/a&gt;, &lt;a href=&quot;/technologies/mapr-file-system/&quot;&gt;MapR File System&lt;/a&gt;, &lt;a href=&quot;/technologies/mapr-file-system/mapr-db/&quot;&gt;MapR-DB&lt;/a&gt;, &lt;a href=&quot;/technologies/mapr-file-system/mapr-streams/&quot;&gt;MapR-Streams&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/products/mapr-converged-data-platform/&quot;&gt;https://mapr.com/products/mapr-converged-data-platform/&lt;/a&gt; - homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://maprdocs.mapr.com/home/MapROverview/c_overview_intro.html&quot;&gt;http://maprdocs.mapr.com/home/MapROverview/c_overview_intro.html&lt;/a&gt; - overview&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://maprdocs.mapr.com/home/ReleaseNotes/c_relnotes_intro.html&quot;&gt;http://maprdocs.mapr.com/home/ReleaseNotes/c_relnotes_intro.html&lt;/a&gt; - release notes&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://maprdocs.mapr.com/home/InteropMatrix/r_release_dates.html&quot;&gt;http://maprdocs.mapr.com/home/InteropMatrix/r_release_dates.html&lt;/a&gt; - release history&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://doc.mapr.com/&quot;&gt;http://doc.mapr.com/&lt;/a&gt; - documentation for previous releases (prior to 5.0)&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/products/edge&quot;&gt;https://mapr.com/products/edge&lt;/a&gt; - MapR Edge home page&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://community.mapr.com/community/products/blog/2017/03/14/introducing-mapr-edge&quot;&gt;https://community.mapr.com/community/products/blog/2017/03/14/introducing-mapr-edge&lt;/a&gt; - introduction to MapR Edge&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://mapr.com/products/persistent-application-client-container/&quot;&gt;https://mapr.com/products/persistent-application-client-container/&lt;/a&gt; - PACC homepage&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://community.mapr.com/community/products/blog/2017/02/07/persistence-in-the-age-of-microservices-introducing-mapr-converged-data-platform-for-docker&quot;&gt;https://community.mapr.com/community/products/blog/2017/02/07/persistence-in-the-age-of-microservices-introducing-mapr-converged-data-platform-for-docker&lt;/a&gt; - introduction to MapR Converged Data Platform for Docker&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&quot;news&quot;&gt;News&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Announcements via the MapR product announcements blog&lt;/li&gt; &lt;/ul&gt; </description> <guid isPermaLink="true">http://ondataengineering.net/technologies/mapr-converged-data-platform/</guid> </item> </channel> </rss>
