<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>OnDataEngineering</title>
		<description>A collaborative site for independent, critical and technical thinking on the use cases, architectural patterns and technologies relating to the transformation and preparation of data for exploitation.</description>
		<link>http://ondataengineering.net/</link>
		<atom:link href="http://ondataengineering.net/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Core Hadoop Technologies (pt3)</title>
				<link>http://ondataengineering.net/blog/2017/01/20/core-hadoop-technologies-pt3/</link>
				<pubDate>Fri, 20 Jan 2017 00:00:00 +0000</pubDate>
				<description>&lt;p&gt;Up today, our final look at the core technologies within the Hadoop ecosystem.
&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;First up are &lt;a href=&quot;/technologies/apache-avro&quot;&gt;Avro&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-parquet&quot;&gt;Parquet&lt;/a&gt;, both of which are key data formats used within the Hadoop ecosystem, but with different and contrasting focuses.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/technologies/apache-kafka&quot;&gt;Kafka&lt;/a&gt;’s a hot technology at the moment - deliverying high bandwidth low latency storage and processing of data streams, with reference cases handling millions of events per second.  If you’re looking at doing anything with streaming data it’s probably well worth a look.  Note that I’ve broken out &lt;a href=&quot;/technologies/apache-kafka/kafka-connect&quot;&gt;Kafka Connect&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-kafka/kafka-streams&quot;&gt;Kafka Streams&lt;/a&gt; as sub-projects.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/technologies/apache-pig&quot;&gt;Pig&lt;/a&gt; was one of the first technologies to provide a ore user friendly abstraction over MapReduce for developing Hadoop jobs.  It’s starting to show it’s age however, and although Hortonworks and Yahoo (who are heavy Pig users) seem to be investing heavily in Pig on Tez, and Cloudera seems to be supporting Pig on Spark (mirroring their Hive strategies), it’s difficult to see newcomers to Hadoop who don’t have an existing investment in Pig using it over Spark and other newer tools.&lt;/p&gt;

&lt;p&gt;And finally &lt;a href=&quot;/technologies/apache-oozie&quot;&gt;Oozie&lt;/a&gt; - a job scheduling an orchestration engine.  It’s been a staple of most Hadoop distributions for a while now, however it’s difficult to find many big references cases for it’s use, and it’s not the most user friendly tool.  Orchestration and management of data transformation pipelines feels like a huge technology gap at the moment - if anyone knows of any great technologies in this space please shout.&lt;/p&gt;

&lt;p&gt;As before - click on the links to see the technology information added to the site.&lt;/p&gt;

&lt;p&gt;That’s it for this week, and for the core Hadoop technologies - it’s been fun.  We’ll be back on Monday to start looking at Apache Bigtop, along with a change of pace…&lt;/p&gt;
</description>
				<guid isPermaLink="true">http://ondataengineering.net/blog/2017/01/20/core-hadoop-technologies-pt3/</guid>
			</item>
		
			<item>
				<title>Core Hadoop Technologies (pt2)</title>
				<link>http://ondataengineering.net/blog/2017/01/13/core-hadoop-technologies-pt2/</link>
				<pubDate>Fri, 13 Jan 2017 00:00:00 +0000</pubDate>
				<description>&lt;p&gt;And onwards with our look at the core Hadoop technologies.
&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;Today, I’ve added &lt;a href=&quot;/technologies/apache-solr/&quot;&gt;Solr&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-sqoop/&quot;&gt;Sqoop&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-spark/&quot;&gt;Spark&lt;/a&gt;, along with the Spark sub-projects &lt;a href=&quot;/technologies/apache-spark/spark-sql/&quot;&gt;Spark SQL&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-spark/spark-streaming/&quot;&gt;Spark Streaming&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-spark/mllib/&quot;&gt;MLlib&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-spark/graphx/&quot;&gt;GraphX&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Solr is one of the big two search technologies along with Elasticsearch, and although there’s debate around which is best (Elastic is probably slightly more developer friendly and supports slightly better analytics capabilities, whereas Solr has the more open development model being an Apache project), they’re both great technologies.&lt;/p&gt;

&lt;p&gt;Sqoop is interesting because of where it’s going - whereas the current version of Sqoop focuses on integration with structured databases, the pending version 2 (which admittedly has been in development for a long time now) evolves it slightly to support the batch ingest of any data into Hadoop.  It’s going to have stiff competition from Apache NiFi however if and when it’s finally released.&lt;/p&gt;

&lt;p&gt;And so to Spark - which claims to be the most active Open Source project in Big Data (as well as many other things).  What is clear however is it’s the one next gen data processing and transformation language that managed to catch significant momentum and adoption.  We can argue the toss on whether it’s the best technology, but it’s now bundled with all the Hadoop distributions, has a rapidly growing base of trained and experienced developers, and a rich ecosystem, which means it’s becoming the default answer to a whole bunch of use cases.&lt;/p&gt;
</description>
				<guid isPermaLink="true">http://ondataengineering.net/blog/2017/01/13/core-hadoop-technologies-pt2/</guid>
			</item>
		
			<item>
				<title>Core Hadoop Technologies (pt1)</title>
				<link>http://ondataengineering.net/blog/2017/01/06/core-hadoop-technologies/</link>
				<pubDate>Fri, 06 Jan 2017 00:00:00 +0000</pubDate>
				<description>&lt;p&gt;And we’re back - Happy New Year!&lt;/p&gt;

&lt;p&gt;Having started with the core Apache Hadoop project, we’re now going to look at the “core” technologies within the Hadoop space, based on those included in multiple distributions (many thanks to Merv Adrian from Gartner for his useful &lt;a href=&quot;http://blogs.gartner.com/merv-adrian/2016/07/30/hadoop-project-commercial-support-tracker-july-2016/&quot;&gt;tracker&lt;/a&gt;)
&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;There’s three Apache technologies added to the catalogue this week - &lt;a href=&quot;/technologies/apache-flume/&quot;&gt;Flume&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hbase/&quot;&gt;HBase&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-hive/&quot;&gt;Hive&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There’s no much to say have Flume or HBase right now, but we’ll take a more detailed look at both of these in the future.  Hive however, is more interesting.&lt;/p&gt;

&lt;p&gt;Firstly, it’s a hugely popular and important project that’s a corner stone of the Hadoop ecosystem, which in its short life has seen enormous change - a classic example of an Open Source technology that has mutated, evolved, consumed other projects and been pulled in multiple directions over time.  I plan to dig into the history of Hive in the not too distant future as I think it’s a great example of how an Open Source project can evolve.&lt;/p&gt;

&lt;p&gt;Secondly, it’s not one thing, but a collection of different components with very distinct roles all bundled together, which is why I’ve taken the decision to break it out into a number of sub-projects (&lt;a href=&quot;/technologies/apache-hive/hive-metastore/&quot;&gt;Hive Metastore&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hive/hive-server/&quot;&gt;Hive Server&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-hive/hcatalog/&quot;&gt;HCatalog&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;As before, click on the links above to see the information added to the site.&lt;/p&gt;

&lt;p&gt;That’s it for this week - next up is Solr, Sqoop and Spark.&lt;/p&gt;
</description>
				<guid isPermaLink="true">http://ondataengineering.net/blog/2017/01/06/core-hadoop-technologies/</guid>
			</item>
		
			<item>
				<title>Apache Hadoop</title>
				<link>http://ondataengineering.net/blog/2016/12/16/apache-hadoop/</link>
				<pubDate>Fri, 16 Dec 2016 00:00:00 +0000</pubDate>
				<description>&lt;p&gt;And so we begin our journey through the jungle of Data Engineering technologies by looking at the technology du jour - Apache Hadoop.
&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;There’s an entire ecosystem here that we’ll start to explore by looking at the major Hadoop vendors, but the first entries in our technology catalogue are the &lt;a href=&quot;/technologies/apache-hadoop/&quot;&gt;Apache Hadoop&lt;/a&gt; project itself, along with its sub-projects: &lt;a href=&quot;/technologies/apache-hadoop/yarn/&quot;&gt;YARN&lt;/a&gt;, &lt;a href=&quot;/technologies/apache-hadoop/hdfs/&quot;&gt;HDFS&lt;/a&gt; and &lt;a href=&quot;/technologies/apache-hadoop/map-reduce/&quot;&gt;MapReduce&lt;/a&gt;. Click on the links to view the technology information I’ve added to the site.&lt;/p&gt;

&lt;p&gt;This also brings our first technology vendor - the &lt;a href=&quot;/tech-vendors/apache/&quot;&gt;Apache Software Foundation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And with that it’s time for Christmas!  We’ll be back in three weeks with the first of the core technologies within the Hadoop space.&lt;/p&gt;
</description>
				<guid isPermaLink="true">http://ondataengineering.net/blog/2016/12/16/apache-hadoop/</guid>
			</item>
		
			<item>
				<title>The Technology Catalogue</title>
				<link>http://ondataengineering.net/blog/2016/12/14/the-technology-catalogue/</link>
				<pubDate>Wed, 14 Dec 2016 00:00:00 +0000</pubDate>
				<description>&lt;p&gt;The first step in this journey is going to be creation of a catalogue of the technologies that are going to be of interest to us as we explore the world of data engineering.
&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;The aim is to provide a valuable reference for anyone starting a technology evaluation to understand what technology options they might have for a given situation, or to understand if and how a technology might fit into an existing ecosystem.&lt;/p&gt;

&lt;p&gt;For each technology, the plan is therefore to provide a short summary describing the technology, along with its background and current status.  As mentioned in my previous post, for some technologies, I also want to do a deep dive to provide a longer summary with more detail that gives a solid introduction to the technology, and my hope is that we’ll get contributions to provide these for the vast majority of the technologies that I won’t get time to look at.&lt;/p&gt;

&lt;p&gt;As part of this we’ll need to look at providing a categorisation of technologies, although making this useful is going to be challenging given that multiple categories of technologies could be used to meet a given use case.  The technologies we’ll look at broadly fall into three groups however:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Data Transformation tools, be they one of the established commercial products or a new Open Source technology&lt;/li&gt;
  &lt;li&gt;Data Platforms, be they a traditional relational database, an Hadoop based data platform, a real time broker such as Kafka, or a NoSQL data platform&lt;/li&gt;
  &lt;li&gt;Technologies that address the other supporting capabilities around these, such as data catalogues or metadata management&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The plan is to start by looking at the key vendors in the Data Engineering space including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The pure play Hadoop distributions - Apache Big Top, Hortonworks, Cloudera and MapR&lt;/li&gt;
  &lt;li&gt;The rest of the Apache Data Engineering ecosystem&lt;/li&gt;
  &lt;li&gt;The major Cloud vendors - Amazon, Google and Azure&lt;/li&gt;
  &lt;li&gt;The big multi play vendors - IBM, Oracle, Teradata, Microsoft, Pivotal, SAP and SAS&lt;/li&gt;
  &lt;li&gt;The big specialist commercial data integration vendors - Ab Initio and Informatica&lt;/li&gt;
  &lt;li&gt;Other commercial data integration and data platform vendors - perhaps based at least partially on looking at the latest Gartner and Forrester reports&lt;/li&gt;
  &lt;li&gt;The major open source cloud scale companies, if only to see what they do in this space - Facebook, Netflix, LinkedIn, Google, Yahoo, eBay and Twitter&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This will only be a start on the technologies I’d expect to see in our catalogue however, so once this is done the plan is to then go through by technology category.&lt;/p&gt;

&lt;p&gt;This is obviously going to take some time given the vast range of technologies, so if you’re interested in contributing in whatever form, if you spot any issues or omissions, or if you have any comments you’d like to add, then please do share your thoughts.&lt;/p&gt;
</description>
				<guid isPermaLink="true">http://ondataengineering.net/blog/2016/12/14/the-technology-catalogue/</guid>
			</item>
		
			<item>
				<title>The Plan</title>
				<link>http://ondataengineering.net/blog/2016/12/12/the-plan/</link>
				<pubDate>Mon, 12 Dec 2016 00:00:00 +0000</pubDate>
				<description>&lt;p&gt;One more post before we get started.&lt;/p&gt;

&lt;p&gt;The following are my current thoughts for some of the topics I’d like to cover on this site, both as a reference for my future self to look back at my naive optimism, but also if anyone wants to start contributing to any of these now, or to start a discussion on any the later topics to start framing and exploring them.
&lt;!--more--&gt;&lt;/p&gt;

&lt;h2 id=&quot;theme-1---the-technology-catalogue&quot;&gt;Theme 1 - the technology catalogue&lt;/h2&gt;

&lt;p&gt;The plan here is to start building up a technology catalogue by looking at the key vendors in the Data Engineering space. This will only be a start on the technologies I’d expect to see in our catalogue however, so once this is done the plan is to then go through by technology category to complete the catalogue.&lt;/p&gt;

&lt;p&gt;I’d also like to look at providing a concise yet detailed introduction to some technologies that describes exactly what it is, how it works, and what the key features are.  So much material that can be found on the internet is marketing material that glosses over the information I’m interested in knowing to understand whether a technology might meet my use cases and integrate into my environment, and my hope is that I can use this site to address that.&lt;/p&gt;

&lt;h2 id=&quot;theme-2---data-engineering-use-cases&quot;&gt;Theme 2 - data engineering use cases&lt;/h2&gt;

&lt;p&gt;One thing I don’t want to do on this site is define another data ecosystem architecture - there are too many already, most of them are designed to sell specific technologies, and none of them will fit the range of different requirements and constraints that different organisations will have.&lt;/p&gt;

&lt;p&gt;However, what I do want to do is look at the range of different of different use cases that you might use data engineering technologies for, from a Data Lake (and we’ll look at what that overloaded term actually means) to a Data Warehouse (and why they’re still relevant), from the acquisition of data to the preparation of a Query Focused Dataset, and from the management of a data catalogue to the monitoring of data quality metrics.&lt;/p&gt;

&lt;p&gt;I’d then like to look at how different technologies and architectural patterns can support these use cases - how do you implement a Data Lake using Hadoop, what technologies support data governance and data catalogues, and how do the various streaming frameworks compare.&lt;/p&gt;

&lt;p&gt;As part of this I also want to look at the core principles behind Data Transformation, what state of the art in this space looks like, and how the established enterprise technologies compare to the new Open Source upstarts.&lt;/p&gt;

&lt;h2 id=&quot;theme-3---delivery&quot;&gt;Theme 3 - delivery&lt;/h2&gt;

&lt;p&gt;As if the above isn’t already massively ambitious enough, I’d also like to talk about the delivery of data solutions, including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;How we can use best practice delivery concepts (e.g. configuration management, continuous integration and testing, automated deployment, infrastructure and database management) and what these mean within a data solution&lt;/li&gt;
  &lt;li&gt;How we can bring some the new best practices from Lean and Agile into the data space, and what data transformation tools need to do in order to be able to support this&lt;/li&gt;
  &lt;li&gt;Why data projects can have a reputation for late delivery, cost overruns, poor quality data and a high cost of change, and what can be done about this&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I think that will more than do us.  Getting through that lot will take some time, but with help and contributions I think this site could be hugely valuable.&lt;/p&gt;
</description>
				<guid isPermaLink="true">http://ondataengineering.net/blog/2016/12/12/the-plan/</guid>
			</item>
		
			<item>
				<title>Big Data</title>
				<link>http://ondataengineering.net/blog/2016/12/09/big-data/</link>
				<pubDate>Fri, 09 Dec 2016 00:00:00 +0000</pubDate>
				<description>&lt;p&gt;Before we get stuck in, a short digression to talk about Big Data.
&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;There are many different definitions of Big Data, but for arguments sake let’s say this refers to the exploitation of data that would have previously been uneconomical due to the volume of data, the structure or format of the data (e.g. unstructured, semi-structured, or complex file formats such as video and audio) and the types of analytics required (e.g. path, graph or time series analysis)&lt;/p&gt;

&lt;p&gt;I’ve a couple of comments to make on this topic.&lt;/p&gt;

&lt;p&gt;Firstly, in my (humble) option Big Data is a marketing term (supported by new technologies - primarily Hadoop) that’s been exploited to sell these technologies and to give the industry something to talk about.  However, I think this has been a broadly positive thing, in that it’s brought data analytics to the mainstream, spurred uptake of new technologies and encouraged companies to invest in analytics and data processing that they may not have done previously. In any case it’s probably just about run its course now (as demonstrated as it’s fall down the far side of the hype curve), and has definitely resulted in the devaluation of other (perhaps more traditional) analytical capabilities which still have a role to play and in many cases deliver capabilities that Big Data technologies can’t yet match.&lt;/p&gt;

&lt;p&gt;Secondly, I think there’s been a lot of misinformation about Big Data and Big Data technologies. It’s not a replacement for existing BI/MI and analytical capabilities, and in fact needs to coexist and integrate with these in order to deliver on its promises. It’s not always cheaper or more performant than existing technologies, and won’t always reduce the timescales and costs for analytics or data exploration. And it’s not a new or innovative technology -  I know of companies that were analysing multi-petabyte data stores and doing real time analytics over ten years ago, parallel distributed file systems have been around for a lot longer than that, and there are many established technologies that have data processing capabilities that the new technologies are only just starting to catch up to.&lt;/p&gt;

&lt;p&gt;In terms of this site the plan is to look at the wider picture and take an holistic view of data transformation and exploitation.  I’ve therefore no plans to talk explicitly about Big Data, but in looking at the wider picture we will absolutely cover everything relating to it (the technologies and the new use cases these enable) alongside coverage of other new technologies, more established technologies and capabilities, and the interesting intersection between them all.&lt;/p&gt;
</description>
				<guid isPermaLink="true">http://ondataengineering.net/blog/2016/12/09/big-data/</guid>
			</item>
		
			<item>
				<title>Welcome</title>
				<link>http://ondataengineering.net/blog/2016/12/07/welcome/</link>
				<pubDate>Wed, 07 Dec 2016 00:00:00 +0000</pubDate>
				<description>&lt;p&gt;For me, one of the biggest challenges in exploiting data (be that through reporting, big data analytics, machine learning or any one of a dozen similar capabilities) is making sure you have the right data in the right place at the right time to allow you to do this efficiently.
&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;For example, &lt;a href=&quot;http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html&quot;&gt;this&lt;/a&gt; article from the New York Times and &lt;a href=&quot;http://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/&quot;&gt;this&lt;/a&gt; more recent one from Forbes talks about how the analysis of big data promises unique business insights, but for big-data scientists there is significant manual ‘janitor work’ (up to 80% of their time) required to prepare data, and although the research is sponsored by Data Wrangling tool vendors, the conclusions will resonate with many data scientists.  Combine this with the historical cost, delivery speed and agility issues typically associated with delivery data warehouse or reporting solutions, and for me it’s never been clearer that we need to get smarter at how we prepare and manage data.&lt;/p&gt;

&lt;p&gt;Part of the solution to this is better Data Engineering, ensuring the processes, tools, technologies, data platforms, regular data feeds and their data preparation jobs are in place to allow the data to be exploited in an efficient, reliable and repeatable way.  The aim of this site is therefore to try to offer independent, critical and technical thinking on the technologies, architectural patterns and delivery capabilities that can help address this.&lt;/p&gt;

&lt;p&gt;My hope is that this becomes a community owned and authored site of trusted reference material on these topics.  To that end, all the content on this site is licensed under the Creative Commons Attribution 4.0 International License and hosted in a public GitHub repository, and there are a set of Discourse forums for discussions. Details of how to contribute and get involved can be found on every page.&lt;/p&gt;
</description>
				<guid isPermaLink="true">http://ondataengineering.net/blog/2016/12/07/welcome/</guid>
			</item>
		
	</channel>
</rss>
