<!doctype html><html class="no-js" lang="en"><head><meta charset="utf-8" /><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="stylesheet" type="text/css" href="/assets/css/app.css" /> <script src="/assets/js/modernizr.min.js"></script> <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.5.18/webfont.js"></script> <script> WebFont.load({ google: { families: [ 'Lato:400,700,400italic:latin', 'Volkhov::latin' ] } }); </script> <noscript><link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic|Volkhov' rel='stylesheet' type='text/css' /> </noscript><title>Technologies - OnDataEngineering</title><meta property="og:title" content="Technologies" /><meta name="description" content="A catalogue of data transformation, data platform and other technologies used within the Data Engineering space" /><meta property="og:description" content="A catalogue of data transformation, data platform and other technologies used within the Data Engineering space" /><link rel="canonical" href="http://ondataengineering.net/technologies/" /><meta property="og:url" content="http://ondataengineering.net/technologies/" /><meta property="og:site_name" content="OnDataEngineering" /> <script type="application/ld+json"> { "@context": "http://schema.org", "@type": "WebPage", "headline": "Technologies", "description": "A catalogue of data transformation, data platform and other technologies used within the Data Engineering space", "logo": "http://ondataengineering.net/assets/img/logo.png", "url": "http://ondataengineering.net/technologies/" } </script><link type="text/plain" rel="author" href="http://ondataengineering.net/humans.txt" /><link rel="icon" sizes="32x32" href="http://ondataengineering.net/assets/img/favicon-32x32.png" /><link rel="icon" sizes="192x192" href="http://ondataengineering.net/assets/img/touch-icon-192x192.png" /><link rel="apple-touch-icon-precomposed" sizes="180x180" href="http://ondataengineering.net/assets/img/apple-touch-icon-180x180-precomposed.png" /><link rel="apple-touch-icon-precomposed" sizes="152x152" href="http://ondataengineering.net/assets/img/apple-touch-icon-152x152-precomposed.png" /><link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://ondataengineering.net/assets/img/apple-touch-icon-144x144-precomposed.png" /><link rel="apple-touch-icon-precomposed" sizes="120x120" href="http://ondataengineering.net/assets/img/apple-touch-icon-120x120-precomposed.png" /><link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://ondataengineering.net/assets/img/apple-touch-icon-114x114-precomposed.png" /><link rel="apple-touch-icon-precomposed" sizes="76x76" href="http://ondataengineering.net/assets/img/apple-touch-icon-76x76-precomposed.png" /><link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://ondataengineering.net/assets/img/apple-touch-icon-72x72-precomposed.png" /><link rel="apple-touch-icon-precomposed" href="http://ondataengineering.net/assets/img/apple-touch-icon-precomposed.png" /><meta name="msapplication-TileImage" content="http://ondataengineering.net/assets/img/msapplication_tileimage.png" /><meta name="msapplication-TileColor" content="#fabb00" /><body id="top-of-page" class=""><div id="navigation" class="contain-to-grid sticky"><nav class="top-bar" role="navigation" data-topbar><ul class="title-area"><li class="name"> <a href="/" title="OnDataEngineering/"> <span class="show-for-small-only">OnDataEngineering</span> <img class="show-for-medium-up" id="site-logo" src="http://ondataengineering.net/assets/img/logo.png" alt="OnDataEngineering"> </a><li class="toggle-topbar menu-icon"><a href="#"><span>Navigation</span></a></ul><section class="top-bar-section"><ul class="nav-menu-align"><li class="divider"><li class="divider"><li class="active"><a href="http://ondataengineering.net/technologies/">Technologies</a><li class="divider"><li><a href="http://ondataengineering.net/tech-categories/">Tech Categories</a><li class="divider"><li><a href="http://ondataengineering.net/tech-vendors/">Tech Vendors</a><li class="divider"><li><a href="http://ondataengineering.net/blog/">Blog</a><li class="divider"><li><a href="http://discourse.ondataengineering.net">Forums</a><li class="divider"><li class="has-dropdown"> <a href="http://ondataengineering.net/site/">Site</a><ul class="dropdown"><li><a href="http://ondataengineering.net/site/">Site Info</a><li><a href="http://ondataengineering.net/site/search/">Search</a><li><a href="http://ondataengineering.net/site/content-license/">Content License</a><li><a href="http://ondataengineering.net/site/subscribe/">Subscribe</a><li><a href="http://ondataengineering.net/site/contributing/">Contributing</a></ul><li class="divider"><li class="divider"><li class="divider"></ul></section></nav></div><div class="row t20"><div class="columns medium-12"><nav class="breadcrumbs" role="menubar" aria-label="breadcrumbs"> <a href="http://ondataengineering.net">Home</a> <a class="current">technologies</a></nav></div></div><div class="row t30"><div class="medium-9 columns medium-push-3"><article itemscope itemtype="http://schema.org/Article"><header><h1> <span itemprop="name">Technologies</span> <a class="button tiny radius" href="#" title="Have a new technology page to add to this site? Click here to create it and send us your proposed page" data-reveal-id="createModal">create &nbsp; <img style="height: 13px;" src="/assets/fonts/info-with-circle.svg"></a></h1></header><p itemprop="description"> A catalogue of data transformation, data platform and other technologies used within the Data Engineering space <span itemprop="articleSection"><table><tbody><tr><td> <a href="http://ondataengineering.net/technologies/alluxio/">Alluxio</a><td>A distributed virtual storage layer, supporting key-value and filesystem interfaces (including HDFS compatibility and a FUSE driver) with support for a range of computation and storage frameworks (including Spark, MapReduce, HBase and Hive) over multiple storage layers (including in-memory, local, network, cloud and cluster file systems) with the ability to create unified and tiered storage, for example to create an in memory filesystem backed by disk to accelerate analytics jobs. Supports a POSIX like access control model, and a CLI and web interface for browsing the storage layer. Java based, Open Source under the Apache 2.0 licence, hosted on GitHub, with development led by Alluxio (with significant external contributions), although they don't appear to yet provide commercial support (but do provide training). Started in December 2012, open sourced in April 2013, with a v1.0 release in February 2016. Formally known as Tachyon.<tr><td> <a href="http://ondataengineering.net/technologies/apache-avro/">Apache Avro</a><td>Data serialisation framework that supports both messaging and data storage. Primarily uses a compact binary format but also supports a JSON format. Supports a range of data structures (including records, enumerations, arrays and maps) with APIs for a wide range of both static and dynamically typed languages. Schema based, with schemas primarily specified in JSON, and support for both code generation from schema definitions as well as dynamic runtime usage. Schemas are serialised alongside data, with support for automatic schema resolution if the schema used to read the data differs from that used to write it. Started as an Hadoop sub-project by Cloudera in April 2009, with an initial v1.0 release in July 2009, before becoming a top level Apache project in May 2010. Has seen significant adoption in the Hadoop ecosystem.<tr><td> <a href="http://ondataengineering.net/technologies/apache-bigtop/">Apache Bigtop</a><td>An Apache open source distribution of Hadoop. Packages up a number of Apache Hadoop components, certifies their interoperability using an automated integration test suite, and packages them up as RPMs/DEBs packages for most flavours of Linux. Also includes virtual machine images and vagrant, docker and puppet recipes for deploying and working with Hadoop. Does not patch projects for distribution, but requires any fixes to be made upstream. An Apache Open Source project, started by Cloudera, donated to the Apache foundation in June 2011, graduating in September 2012, with a 1.0 release in August 2015 based on Hadoop 2.6. Since donating the project, Cloudera have backed away from it, with the project lead moving to Pivotal in December 2013. Now has a broad range of contributors, however usage by the major distributors is not clear.<tr><td> <a href="http://ondataengineering.net/technologies/apache-flume/">Apache Flume</a><td>Specialist technology for the continuous movement of data using a set of independent agents connected together into pipelines. Supports a wide range of sources, targets and buffers (channels), along with the ability to chain agents together and to modify and drop events in-flight. Designed to be highly reliable, and to support reconfiguration without the need for a restart. Heavily integrated with the Hadoop ecosystem. An Apache project, donated by Cloudera in June 2011, graduating in June 2012, with a v1.2 release (the first considered ready for production use) in July 2012. Java based, with commercial support available as part of most Hadoop distributions.<tr><td> <a href="http://ondataengineering.net/technologies/apache-giraph/">Apache Giraph</a><td>An iterative, highly scalable graph processing system built on top of MapReduce and based on Pregel, with a number of features added including a framework for creating re-usable code (called blocks). An Apache project, graduating in May 2012, having been originally donated by Yahoo in August 2011. Java based, no commercial support available, but is mature and has been adopted by a number of companies (including LinkedIn and most famously Facebook who scaled it to process a trillion edges), and has a number of active developers.<tr><td> <a href="http://ondataengineering.net/technologies/apache-hadoop/">Apache Hadoop</a><td>A distributed storage and compute platform consisting of a distributed filesystem (HDFS) and a cluster workload and resource management layer (YARN), along with MapReduce, a solution built on HDFS and YARN for massive scale parallel processing of data. Has an extensive ecosystem of compatible technologies. An Apache Open Source project, started in January 2006 as a Lucene sub-project, becoming a top level project in January 2008, with a 1.0 release in December 2011 (containing HDFS and MapReduce), and a 2.2 release (the first 2.x GA release) in October 2013 (adding YARN). Very active, with a deep and broad range of contributors, and backing from multiple commercial vendors.<tr><td> Apache Hadoop&nbsp;&gt;&nbsp; <a href="http://ondataengineering.net/technologies/apache-hadoop/hdfs/">HDFS</a><td>A highly resilient distributed cluster file system proven at extreme scale that supports user authentication, extended ACLs, snapshots, quotas, central caching, a REST API, an NFS gateway, rolling upgrades, transparent encryption and heterogeneous storage. Part of the original Hadoop code base, becoming an Apache Hadoop sub-project in July 2009.<tr><td> Apache Hadoop&nbsp;&gt;&nbsp; <a href="http://ondataengineering.net/technologies/apache-hadoop/map-reduce/">MapReduce</a><td>A data transformation and aggregation technology proven at extreme scale that works on key value pairs and consists of three transformation stages - map (a general transformation of the input key value pairs), shuffle (brings all pairs with the same key together) and reduce (an aggregation of all pairs with the same key). Part of the original Hadoop code base, becoming an Apache Hadoop sub-project in July 2009.<tr><td> Apache Hadoop&nbsp;&gt;&nbsp; <a href="http://ondataengineering.net/technologies/apache-hadoop/yarn/">YARN</a><td>Resource management and job scheduling &amp; monitoring for the Hadoop ecosystem. Includes support for capacity guarantees amongst other scheduling options. Added as an Apache Hadoop sub-project as part of Hadoop 2.x (with a GA release as part of 2.2 in October 2013) having been started in January 2008.<tr><td> <a href="http://ondataengineering.net/technologies/apache-hama/">Apache Hama</a><td>A general purpose BSP (Bulk Synchronous Parallel) processing engine inspired by Pregel and DistBelief that runs over Mesos or YARN. Supports BSP, graph computing and machine learning programming models, as well as Apache MRQL. An Apache project, donated in 2008, and graduated in 2012. Java based, with no commercial support available, limited case studies for it's use and limited active developers, with the last release being in June 2015.<tr><td> <a href="http://ondataengineering.net/technologies/apache-hbase/">Apache HBase</a><td>NoSQL wide-column datastore based on Google BigTable. Focuses on random real-time access to data, and supports horizontal scalability, consistent reads and writes, versioning and fine grained security controls. Runs on Hadoop and HDFS, and is heavily integrated with the Hadoop ecosystem. An Apache project, first released as part of Hadoop 0.15 in October 2007 before graduating as a top level project in May 2010. Java based, with commercial support available as part of most Hadoop distributions.<tr><td> <a href="http://ondataengineering.net/technologies/apache-hive/">Apache Hive</a><td>Technology that supports the exposure of data in Hadoop as structured tables and the execution of analytical SQL queries over these. Consists of a number of distinct components (that we treat as sub-projects) including Hive Metastore (stores the definitions of the structured tables), Hive Server (supports the execution of analytical SQL queries as MapReduce, Spark or Tez jobs) and HCatalog (allows MapReduce and Pig jobs to read and write Hive tables). First released by Facebook as an Hadoop contrib module in September 2008, becoming an Hadoop sub-project in November 2008, and a top level Apache project in September 2010, following a first official stable release (0.3) in April 2009. Java based, under active development from a number of large commercial sponsors, with commercial support available as part of most Hadoop distributions.<tr><td> Apache Hive&nbsp;&gt;&nbsp; <a href="http://ondataengineering.net/technologies/apache-hive/hcatalog/">HCatalog</a><td>Libraries for MapReduce and Pig to read and write data to and from Hive tables, albeit with some limitations. Also supports a CLI for querying and updating the Hive Metastore, however this doesn't support the full range of Hive DDL commands. Includes WebHCat, a REST API over the HCatalog CLI that also supports the execution of MapReduce, Pig, Hive and Sqoop jobs. Donated to the Apache foundation by Yahoo in March 2011, had WebHCat folded in in July 2012, graduating as a top level project in February 2013, but then almost immediately was folded into Hive in March 2013 as part of the Hive 0.11 release. Has seem limited development since this time.<tr><td> Apache Hive&nbsp;&gt;&nbsp; <a href="http://ondataengineering.net/technologies/apache-hive/hive-metastore/">Hive Metastore</a><td>A metadata service that allows structured tables to be defined over files in HDFS (and also HBase or Accumulo), providing an API that allows the metadata to be queried and updated by other tools including Impala, Spark SQL or RecordService. Supports partitioned and clustered tables, as well as complex field types such as arrays, maps and structs. Backed by a relational database (either MySQL, Postgres and Oracle). Part of the original Hive code base.<tr><td> Apache Hive&nbsp;&gt;&nbsp; <a href="http://ondataengineering.net/technologies/apache-hive/hive-server/">Hive Server</a><td>Supports the execution of SQL queries over data in HDFS based on tables defined in the Hive Metastore, as well as DDL to query and update the Hive Metastore. Focus is on analytical (OLAP) use cases, with some support for batch updates to data. Originally executed queries as MapReduce jobs, but significant investment from has seen support for executing queries as Spark and as Tez jobs, with work underway to support sub second query times using Tez. Recent changes have also seen it achieve significant SQL compliance, with support for SQL:2011 analytical functions on-going. Accepts queries over an API with JDBC and ODBC drivers available, and includes Beeline, a command line JDBC client. Technically referred to as Hive Server 2, and was introduced in Hive 0.11 as a replacement for the original Hive Server to address a number of concurrency and security issues.<tr><td> <a href="http://ondataengineering.net/technologies/apache-ignite/">Apache Ignite</a><td>A distributed in-memory data fabric/grid, supporting a number of use cases including a key value store (with SQL support), real time stream/event processing engine, arbitrary compute, long running service management, an in-memory HDFS compatible file system for acceleration of Hadoop jobs, and in-memory shared Spark RDDs. An Apache project, graduating in September 2015, having been originally donated by GridGain from their In-Memory Data Fabric product launched in 2007. Java based, with development lead by GridGain who also supply commercial support (as GridGain Professional with ongoing Q&amp;A and bug fixes before they're included in Ignite) along with GridGain Enterprise (which includes extra features such as a management GUI, enterprise security and rolling upgrades).<tr><td> <a href="http://ondataengineering.net/technologies/apache-kafka/">Apache Kafka</a><td>Technology for buffering and storing real-time streams of data between publishers to subscribers, with a focus on high throughput at low latency. Based on a distributed, horizontally scalable architecture, with messages organised into topics which are partitioned and replicated across nodes to provide resilience and written to disk to provide persistence. Topics may have multiple publishers and subscribers, with ability to do fault tolerant reads and to load balance across subscribers. Records consist of a key, value and timestamp, with the ability to compact topics to remove updates and deletes by key. Supports a full security model, and the ability to set quotas. Comes with a Java client, but clients for a wide range of languages are also available. Has two sub-projects (Kafka Connect and Kafka Streams) that are bundled with the main product. Originally developed at LinkedIn, being open sourced in January 2011, before being donated to the Apache Foundation in July 2011. Graduated in October 2012, and although it has not had a v1.0 release is considered production quality and stable. Development is primarily led by Confluent (which was founded by the team that built Kafka at LinkedIn), who distribute a Confluent Open Source product (which includes further clients and connectors) and a subscription based Confluent Enterprise product (which includes management, replication and data balancing features and commercial support under a subscription licence). Commercial support is also available from most Hadoop vendors.<tr><td> Apache Kafka&nbsp;&gt;&nbsp; <a href="http://ondataengineering.net/technologies/apache-kafka/kafka-connect/">Kafka Connect</a><td>Framework for building scalable and reliable integrations between Kafka and other technologies, either for importing or exporting data. Part of the core Apache Kafka open source technology, connectors are available for a wide range of systems, including Hadoop, relational, NoSQL and analytical databases, search technologies and message queues amongst others. Runs separately to Kafka, in either a stand-alone or distributed cluster mode, with a REST API for managing connectors.<tr><td> Apache Kafka&nbsp;&gt;&nbsp; <a href="http://ondataengineering.net/technologies/apache-kafka/kafka-streams/">Kafka Streams</a><td>Technology that allows stream processing to be added to a Kafka cluster, consuming and publishing events from and to Kafka topics (and potentially writing output to external systems). Based on an event-at-a-time model (i.e. not micro batch), with support for stateful processing, windowing, joining and re-processing data. Supports a low level DSL API, as well as a high level API that provides both stream and table abstractions (where tables present the latest record for each key). Introduced in Kafka 0.10.<tr><td> <a href="http://ondataengineering.net/technologies/apache-oozie/">Apache Oozie</a><td>Technology for managing workflows of jobs on Hadoop clusters. Primary concepts include workflows (a sequence of jobs modelled as a directed acyclic graph), coordinators (schedule the execution of workflows based on the time or the presence of data) and bundles (collections of coordinators), with all configuration specified in XML. Supports a range of technologies, including MapReduce, Pig, Hive, Sqoop, Spark, Java executables and shell scripts. Includes a server component, a metadata database for holding definitions and state (with support for a range of database technologies), a command line interface and a read only web interface for viewing the status of jobs. Also supports the parameterisation of workflows, the modelling of datasets (and the use of these to manage dependencies between workflows within coordinators), automatic retry and failure handling, and the ability to send job status notifications via HTTP or JMS. Open sourced by Yahoo in June 2010. Donated to the Apache Foundation in July 2011, graduating in August 2012. Commercial support available as part of most Hadoop distributions<tr><td> <a href="http://ondataengineering.net/technologies/apache-parquet/">Apache Parquet</a><td>Data serialisation framework that supports a columnar storage format to enable efficient querying of data. Built using Apache Thrift, and supports complex nested data structures, using techniques from the Google Dremel paper. Consists of three sub-projects, the specification and Thrift definitions (Parquet Format), the Java and Hadoop libraries (Parquet MR) and the C++ implementation (Parquet CPP). Created as a joint effort between Twitter and Cloudera based on work started as part of Avro Trevni, with an initial v1.0 release in July 2013. Donated to the Apache Foundation in May 2014, graduating in April 2015. Has seen significant adoption in the Hadoop ecosystem.<tr><td> <a href="http://ondataengineering.net/technologies/apache-pig/">Apache Pig</a><td>Technology for running analytical and data processing jobs against data in Hadoop. Jobs are written in Pig Latin (a custom procedural language that can be extended using user defined functions in a range of languages), which is then translated into Map Reduce or Tez (with Spark in development) for execution. Supports both a batch mode for running pre-defined scripts and an interactive mode, and connectors for reading and writing to HBase and Accumulo as well as HDFS. Originally developed at Yahoo in 2006 before being donated to the Apache Foundation in October 2007. Graduated as an Hadoop sub-project in October 2008, before becoming a top level project in September 2010. Although has not had a v1.0 release, has been production quality for many years. Commercial support available as part of most Hadoop distributions<tr><td> <a href="http://ondataengineering.net/technologies/apache-solr/">Apache Solr</a><td>A search server built on Apache Lucene with a REST-like API for loading and searching data. Supports a distributed deployment (SolrCloud) that can run over HDFS on an Hadoop cluster. Includes an administration web interface, an extensible plugin architecture, support for schemaless indexing, faceted, grouped and clustered results, hit highlighting, geo-spacial and graph searches, near real time indexing and searching, (experimental) streaming expressions for parallel compute (including support for MapReduce and SQL) and broad authentication and security capabilities. A sub-project of the Apache Lucene project, originally donated to the Apache foundation by CNET Networks in January 2006, graduating as a top level project in January 2007, before merging with the Lucene project in March 2010. Java based, with commercial support available as part of most Hadoop distributions (although this is bundled as Cloudera Search with CDH and HDP Search with HDP), as well as from Lucidworks.<tr><td> <a href="http://ondataengineering.net/technologies/apache-spark/">Apache Spark</a><td>A high performance general purpose distributed data processing engine based on directed acyclic graphs that primarily runs in memory, but can spill to disk if required, and which supports processing applications written in Java, Scala, Python and R. Includes a number of sub-projects that support more specialised analytics including Spark SQL, Spark Streaming, MLlib (machine learning) and GraphX (graph analytics). Requires a cluster manager (YARN, EC2 and Mesos are supported as well as standalone clusters) and can access data in a wide range of technologies (including HDFS, other Hadoop data sources, relational databases and NoSQL databases). An Apache project, originally started at UC Berkley in 2009, open sourced in 2010, and donated to the Apache foundation in June 2013, graduating in February 2014. v1.0 was released in May 2014, with a v2.0 release in July 2016. Java based, with development led by Databricks (who sell a Spark hosted service), and with commercial support available as part of most Hadoop distributions.<tr><td> Apache Spark&nbsp;&gt;&nbsp; <a href="http://ondataengineering.net/technologies/apache-spark/graphx/">GraphX</a><td>Spark library for processing graphs and running graph algorithms, based on graph model that supports directional edges with properties on both vertices and edges. Graphs are constructed from a pair of collections representing the edges and vertex, either directly from data on disk using builders, or prepared using other Spark functionality, with the ability to also view the graph as a set of triples. Supports a range of graph operations, as well as an optimised variant of the Pregel API, and a set of out of the box algorithms (including PageRank, connected components and triangle count). First introduced in Spark 0.9, with a production release as part of Spark 1.2, however has seen almost no new functionality since then.<tr><td> Apache Spark&nbsp;&gt;&nbsp; <a href="http://ondataengineering.net/technologies/apache-spark/mllib/">MLlib</a><td>Spark library for running Machine Learning algorithms. Supports a range of algorithms (including classifications, regressions, decision trees, recommendations, clustering and topic modelling), including iterative algorithms. First introduced in Spark 0.8 after being collaboratively developed with the UC Berkeley MLbase project.<tr><td> Apache Spark&nbsp;&gt;&nbsp; <a href="http://ondataengineering.net/technologies/apache-spark/spark-sql/">Spark SQL</a><td>Spark library for processing structured data, using either SQL statements or a DataFrame API. Supports querying and writing to local datasets (including JSON, Parquet, Avro, Orc and CSV) as well as external data sources (including Hive and JDBC), including the ability to query across data sources. Includes Catalyst, a cost based optimiser that turns high level operations into low level Spark DAGs for execution. Also includes a Hive compatible Thrift JDBC/ODBC server that's compatible with Beeline and the Hive JDBC and ODBC drivers, and a REPL CLI for interactive queries. First introduced in Spark 1.0, with a production release as part of Spark 1.3.<tr><td> Apache Spark&nbsp;&gt;&nbsp; <a href="http://ondataengineering.net/technologies/apache-spark/spark-streaming/">Spark Streaming</a><td>Spark library for continuous stream processing, that allows stream and batch processing (including Spark SQL and MLlib operations) to be combined. Uses a micro-batch execution model, leveraging core Spark to process each micro-batch, and provides fault tolerance through exactly-once processing semantics. Supports a number of data sources (including HDFS, sockets, Flume, Kafka, Kinesis and messaging buses), as well as functions to maintain state and to execute windowed operations. First introduced in Spark 0.7, with a production release as part of Spark 0.9.<tr><td> <a href="http://ondataengineering.net/technologies/apache-sqoop/">Apache Sqoop</a><td>Specialist technology for moving bulk data between Hadoop and structured (relational) databases. Command line based, with the ability to import and export data between a range of databases (including mainframe partitioned datasets) and HDFS, Hive, HBase and Accumulo. Supports parallel partitioned unloads, writing to Avro, Sequence File, Parquet and text files, incremental imports and saved jobs that can be shared via a simple metadata store. An Apache project, started in May 2009 as an Hadoop contrib module, migrating to a Cloudera GitHub project in April 2010 (with a v1.0 release shortly after), before being donated to the Apache foundation in June 2011, graduating in March 2012. The last major release (v1.4) was in November 2011, with only minor releases since then. However in January 2012 a significant re-write was announced as part of a proposed v2.0 release to address a number of usability, security and architectural issues. This will introduce a new Sqoop Server and Metadata Repository, supporting both a CLI and web UI, centralising job definitions, database connections and credentials, as well as enabling support for a wider range of connectors including NoSQL databases, Kafka and (S)FTP folders. Java based, with commercial support available as part of most Hadoop distributions.</table></span></article><div class="row b30"></div></div><div class="medium-3 columns medium-pull-9"><aside><div class="panel sidebar radius"><p>OnDataEngineering is a collaboratively authored site on the transformation and preparation of data for analytics, licensed under a Creative Commons Licence. Read more <a href="/site/">here</a><p>For details on how to contribute to this site see <a href="/site/contributing/">here</a></div><div class="panel sidebar radius b0"><p>Have a new technology page to add to this site? <a href="#" data-reveal-id="createModal">Create</a> it and send us your proposed page<p>Want to discuss any of these technologies? See the <a href="http://discourse.ondataengineering.net/c/technologies">Technologies</a> category of our Discourse forums<p><p></div></aside></div></div></div><div id="createModal" class="reveal-modal text-center" data-reveal aria-labelledby="modalTitle" aria-hidden="true" role="dialog"><p>Thank you for adding new content to the site.<br>To help you get started, please copy the following text to use as a template for the new page you're creating... <textarea rows="4" cols="20" name="createModalTextArea"></textarea> <a class="button small radius" href="javascript:void(0);" onclick="copyCreateModalText()">Copy To Clipboard</a> <a class="button small radius" href="https://github.com/OnDataEngineering/OnDataEngineeringContent/new/master/_technologies">Let's Go</a> <a class="close-reveal-modal" aria-label="Close">&#215;</a></div><div id="up-to-top" class="row"><div class="small-12 columns text-right"> <a class="iconfont" href="#top-of-page">&#xf108;</a></div></div><footer><div class="row"><div class="columns"><div id="footer" class="row panel radius" style="margin: 0px"><div class="medium-6 large-5 columns"><h5>Licencing</h5><p> <a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a> <br /> The contents of <span xmlns:dct="http://purl.org/dc/terms/" href="http://purl.org/dc/dcmitype/Text" property="dct:title" rel="dct:type">OnDataEngineering.net</span> is licensed under the <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>. For full details see <a xmlns:cc="http://creativecommons.org/ns#" href="http://ondataengineering.net/site/content-license/" rel="cc:morePermissions">here</a>.</div><div class="small-6 medium-3 large-3 large-offset-1 columns"><h5>Subscribe by</h5><ul class="no-bullet"><li > <a href="" title=""></a><li > <a href="https://twitter.com/OnDataEng" target="_blank" title="Follow us on twitter">Twitter</a><li > <a href="http://eepurl.com/cyQSqv" target="_blank" title="Subscribe to updates by e-mail">E-Mail</a><li > <a href="/atom.xml" title="Subscribe to Atom Feed">Atom</a><li > <a href="/feed.xml" title="Subscribe to RSS Feed">RSS</a></ul></div><div class="small-6 medium-3 large-3 columns"><h5>Credits</h5><ul class="no-bullet"><li > <a href="" title=""></a><li > <a href="http://jekyllrb.com/" target="_blank" title="Jekyll">Jekyll</a><li > <a href="http://phlow.github.io/feeling-responsive/" target="_blank" title="Feeling Responsive">Feeling Responsive</a><li > <a href="http://foundation.zurb.com/" target="_blank" title="Foundation">Foundation</a><li > <a href="http://entypo.com/" target="_blank" title="Icons by Daniel Bruce">Icons by Daniel Bruce</a><li > <a href="http://jch.penibelst.de/" target="_blank" title="Jekyll Compress HTML">Jekyll Compress HTML</a></ul></div></div></div></div></footer><script src="/assets/js/app.min.js"></script> <script> (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','https://www.google-analytics.com/analytics.js','ga'); ga('create', 'UA-88912556-1', 'auto'); ga('set', 'anonymizeIp', true); ga('send', 'pageview'); </script> <script> $(document).on('open.fndtn.reveal', '[data-reveal]', function () { request = $.ajax({ type: "GET", url: "/templates/_technologies", datatype: "text/plain", success: function(data) { target = $("textarea[name='createModalTextArea']"); target.html(data); } }); }); function copyCreateModalText() { target = $("textarea[name='createModalTextArea']"); target.select(); document.execCommand('copy'); } </script>
